---
layout: post
title: "Reading Parquet Metadata in Rust got 2x faster"
date: "2025-10-08 00:00:00"
author: "Andrew Lamb and Ed Seidl"
categories: [release]
---
<!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
-->


*Editors Note: While the [Apache Arrow] and [Apache Parquet] are separate projects,
the [arrow-rs] repository hosts the development of the  [parquet] Rust crate, a widely used
and high performance implementation.* 

## Summary

Version `57.0.0` of the  [parquet] Rust crate reads metadata twice as fast as
previous versions by using a custom parser instead of the code generated by a
Thrift compiler. Larger speedups are possible for more specialized usecases 
such as files with large column counts that do not require all statistics.  

<!-- AAL: TODO: update the benchmark and charts with results from 57.0.0 -->

<div style="display: flex; gap: 16px; justify-content: center; align-items: flex-start;">
  <img src="{{ site.baseurl }}/img/rust-parquet-metadata/results.png" width="100%" class="img-responsive" alt="" aria-hidden="true">
</div>

**Figure 1**: Performance of [Apache Parquet] metadata parsing using a
[custom thrift decoder] in [arrow-rs], scheduled for release in [57.0.0]. No
changes were made to the Parquet format itself. 
See the [benchmark page] for more details.

[parquet]: https://crates.io/crates/parquet
[Apache Arrow]: https://arrow.apache.org/
[Apache Parquet]: https://parquet.apache.org/
[custom thrift decoder]: https://github.com/apache/arrow-rs/issues/5854
[arrow-rs]: https://github.com/apache/arrow-rs
[57.0.0]: https://github.com/apache/arrow-rs/issues/7835

[benchmark page]: https://github.com/alamb/parquet_footer_parsing

<div style="display: flex; gap: 16px; justify-content: center; align-items: flex-start;">
  <img src="{{ site.baseurl }}/img/rust-parquet-metadata/scaling.png" width="100%" class="img-responsive" alt="Scaling behavior of custom thrift parser" aria-hidden="true">
</div>

**Figure 2**: Speedup of the [custom thrift decoder] for both string and floating point 
types, for `100`, `1000`, and `100,000` columns. The new parser is faster in all cases,
and the speedup is similar for all cases. See the [benchmark page] for more details.

## Introduction

[Apache Parquet] is a popular columnar storage format for big data processing. It
is designed to be efficient for both storage and query performance. Parquet
files consist of a header, a series of row groups, and a footer as shown in Figure 3. The footer
contains metadata about the file, including the schema, statistics, and other
information needed to read and process the data.

<!-- Image source: https://docs.google.com/presentation/d/1WjX4t7YVj2kY14SqCpenGqNl_swjdHvPg86UeBT3IcY -->
<div style="display: flex; gap: 16px; justify-content: center; align-items: flex-start;">
  <img src="{{ site.baseurl }}/img/rust-parquet-metadata/parquet.png" width="100%" class="img-responsive" alt="Physical File Structure of Parquet" aria-hidden="true">
</div>

**Figure 3:** Structure of a Parquet file, showing the header, row groups, and footer.

Footer parsing is often a critical step in reading Parquet files, as it provides
the necessary information to interpret the data stored in those Parsing the footer is
often on the critical query path for systems
that do not cache the required information from the footer (stateless), so the performance of
footer parsing can be a significant amount of query performance, especially
when reading from fast  local storage such as modern NVMe SSDs. Footer parsing
performance is especially important for files with many columns or row groups,
when queries only need to read a subset of columns, or when reading many small files.

Even though many low latency systems cache the parsed footer to avoid this cost,
explained in [Using External Indexes, Metadata Stores, Catalogs and Caches to Accelerate Queries on Apache Parquet],
the performance of footer parsing is still important when the cache is cold or 
the hit rate is low. 

<!-- Image source: https://docs.google.com/presentation/d/1WjX4t7YVj2kY14SqCpenGqNl_swjdHvPg86UeBT3IcY -->
<div style="display: flex; gap: 16px; justify-content: center; align-items: flex-start;">
  <img src="{{ site.baseurl }}/img/rust-parquet-metadata/flow.png" width="100%" class="img-responsive" alt="Typical parquet processing flow" aria-hidden="true">
</div>

**Figure 4**: Typical processing flow for processing of Parquet files for stateless and stateful systems.
The performance of footer parsing is important for both types of systems, but especially
for stateless systems that do not cache the parsed footer.

[Using External Indexes, Metadata Stores, Catalogs and Caches to Accelerate Queries on Apache Parquet]: https://datafusion.apache.org/blog/2025/08/15/external-parquet-indexes/

The speed of parsing metadata has grown in importance as parquet has been used
in more and more real time applications, where the latency of reading many
parquet files is important, such as in observability (TODO find citations),
interactive analytics, and most recently single point lookups for Results
Augmented Generation (RAG) applications to feed LLMs (TODO find citations),
which the latency of reading data from parquet files can be a significant part
of the overall latency.

An often criticized part of the Parquet format is that it uses [Apache Thrift]
for serialization of the metadata. Thrift is a flexible and efficient
serialization framework, but does not provide random access parsing. Other
formats such as [Flatbuffers] which do provide zero copy and random access
parsing have [been proposed as alternatives] given their theoretical performance
advantages. However, changing the Parquet format is a significant undertaking,
and requires buy-in from the community and ecosystem and can take years to be
adopted.

Despite the very real disadvantage of thrift, we have previously theorized in
[How Good is Parquet for Wide Tables (Machine Learning Workloads) Really?] that
there is still room for significant performance improvements in Parquet footer
parsing in Rust using the existing thrift format but improving the thrift
decoder implementation.

[How Good is Parquet for Wide Tables (Machine Learning Workloads) Really?]: https://www.influxdata.com/blog/how-good-parquet-wide-tables/
[Apache Thrift]: https://thrift.apache.org/
[Flatbuffers]: https://google.github.io/flatbuffers/



## Background: Apache Thrift
(TODO background here and enough to understand why it must be scanned

Thus, to parse the parquet metadata, the entire metadata must be scanned to locate
where the various fields are located, and then each field must be parsed individually.
However, CPUs are quite fast at scanning data, and a substantial amount of the 
time spend parsing generated code is parsing the values in individual fields and copying them into in-memory structures, especially
into indivdually allocated structures (such as a Rust String, or a C++ std::string (todo refeerences)))


## Traditional Parsing Approach using a thrift compiler
The Apache Arrow Rust implementation, as is done with other open source implementations
such as java and c++ (find code) parsed parquet metadata with a two step process, as shown in figure 2:
1. Use generated code from the the [thrift crate] to parse the thrift binary into inmemory structures
2. Convert the in-memory structures into arrow's own in-memory representation of parquet metadata (see [ParquetMetadat] todo citatiion)
but this crate is not optimized for speed, and so we have been looking for alternatives.

<!-- Image source: https://docs.google.com/presentation/d/1WjX4t7YVj2kY14SqCpenGqNl_swjdHvPg86UeBT3IcY -->
<div style="display: flex; gap: 16px; justify-content: center; align-items: flex-start;">
  <img src="{{ site.baseurl }}/img/rust-parquet-metadata/original-pipeline.png" width="100%" class="img-responsive" alt="Original Parquet Parsing Pipeline" aria-hidden="true">
</div>

Figure 2: Two step process to read parquet metadata: use code generated by the thrift crate to parse
the thrift binary into in-memory structures, then convert the in-memory structures into arrow's

## New Design: Custom Parser

As is typical of code generated from another tool, oportunities for optimization
are typically limited, both because the generated is not easy to modify.
(TODO find exmaples of trying to help the thrift crate be faster)

For example, the last release of the rust thrift compiler crate https://crates.io/crates/thrift/0.17.0
was three years ago, and the last commit to the repository was over a year ago
https://crates.io/crates/thrift
We hope we can also help make the rust thrift crate to be better. 


So we instead wrote a custom parser that parses the thrift binary directly into the needed
structures.


The speedup was achieved by using a custom parser that is optimized for the specific
subset of the Thrift format used by Parquet, and by using various performance optimizations.


<!-- Image source: https://docs.google.com/presentation/d/1WjX4t7YVj2kY14SqCpenGqNl_swjdHvPg86UeBT3IcY -->
<div style="display: flex; gap: 16px; justify-content: center; align-items: flex-start;">
  <img src="{{ site.baseurl }}/img/rust-parquet-metadata/new-pipeline.png" width="100%" class="img-responsive" alt="New Parquet Parsing Pipeline" aria-hidden="true">
</div>

Figure 3: New one step process to read parquet metadata: use a custom parser to parse the thrift binary directly
own in-memory representation of parquet metadata.

The custom parser is optimized for the specific subset of the Thrift format used
by Parquet, and by using various performance optimizations

### Example optimizations

The obvious optimization is to remove the intermediate step of creating
the in-memory structures generated by the thrift crate, and instead parse
the thrift binary directly into arrow's own in-memory representation of parquet metadata.

However, there are other optimizations that can now be applied such as: (TODO GET LIST)


### Maintainability
THe largest concern with this approach is that it is more difficult to maintain, as any changes to the parquet
format must be manually reflected in the custom parser, and the thrift definitions
do get updated (e.g. for the recent additions of [Geometry] and [Variant] tyoes)

However, one of the arrow-rs contributors, Jorn Horstmann (todo link) prototyped a macro
based approach, that generates the custom parser code using rust macros so that the
rust structures look very similar to the original thrift definitions. 

For exmaple, here is the thrift definition of a parquet file metadata:

(TOD)O


And the corresponding Rust structure that is created directly from the custom parser:

(TODO example
)

### Future Improvements
With a custom parser, we can now also consider other improvements such as
* Implementing special "skip" indexes to skip directly to the parts of the metadata
  that are needed for a particular query, such as the row group offsets
* Selectively decode only the statistics for columns that are needed for a particular query
