---
layout: post
title: "Reading Parquet Metadata in Rust got 2x faster"
date: "2025-10-30 00:00:00"
author: "Andrew Lamb", "Ed Seidel"
categories: [release]
---
<!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
-->

## Summary

(todo chaart here)
Figure 1: Speedup in reading parquet metadata in apache arrow rust implementation
See https://github.com/alamb/parquet_footer_parsing for more details

Parsing the metadata in [Apache Parquet] files is now twice as fast in the 57.0.0 release 
of apache arrow rust implementation. Despite the widely held believe that thrift parsing
can not be made faster, no changes to the format were required to achieve this speedup.
This is a significant improvement for workloads that
need to read many parquet files, as metadata must be read before any data can be processed
and thus is often a bottleneck.

## Introduction

Many people over the years have pointed out that reading parquet metadata is slow (todo find citations) 

The reason typically cited is because
the metadata is stored in the Apache Thrift format (todo link), whose variable length encoding
makes it space efficient, but relatively slow to parse.
This has lead to various proposals to replace parquet's metadata format with something
more efficient to parse, such as [flatbuffers] (TODO find proposal recently) or even
used to justify an entirely new file format such as F3, Nimble or The FastLanes File Format
(TFLF) (TODO find citations).

However, as Xiangpeng Hao explores in a previous blog post "how good is parquet
for wide tables really" (TODO link) there is clear evidence that the existing
implementations of thrift encoding can be made much faster. 

## Traditional Parsing Approach using thrift compiler
The Apache Arrow Rust implementation, as is done with other open source implementations
such as java and c++ (find code) parsed parquet metadata with a two step process, as shown in figure 2:
1. Use generated code from the the [thrift crate] to parse the thrift binary into inmemory structures
2. Convert the in-memory structures into arrow's own in-memory representation of parquet metadata (see [ParquetMetadat] todo citatiion)
but this crate is not optimized for speed, and so we have been looking for alternatives.

(digram here showing two step process)

Figure 2: Two step process to read parquet metadata: use code generated by the thrift crate to parse
the thrift binary into in-memory structures, then convert the in-memory structures into arrow's

## New Design: Custom Parser

As is typical of code generated from another tool, oportunities for optimization
are typically limited, both because the generated is not easy to modify.
(TODO find exmaples of trying to help the thrift crate be faster)

For example, the last release of the rust thrift compiler crate https://crates.io/crates/thrift/0.17.0
was three years ago, and the last commit to the repository was over a year ago
https://crates.io/crates/thrift
We hope we can also help make the rust thrift crate to be better. 


So we instead wrote a custom parser that parses the thrift binary directly into the needed
structures.


The speedup was achieved by using a custom parser that is optimized for the specific
subset of the Thrift format used by Parquet, and by using various performance optimizations.


(diagram here showing the one step process)

Figure 3: New one step process to read parquet metadata: use a custom parser to parse the thrift binary directly
own in-memory representation of parquet metadata.

The custom parser is optimized for the specific subset of the Thrift format used
by Parquet, and by using various performance optimizations

### Example optimizations

The obvious optimization is to remove the intermediate step of creating
the in-memory structures generated by the thrift crate, and instead parse
the thrift binary directly into arrow's own in-memory representation of parquet metadata.

However, there are other optimizations that can now be applied such as: (TODO GET LIST)


### Maintainability
THe largest concern with this approach is that it is more difficult to maintain, as any changes to the parquet
format must be manually reflected in the custom parser, and the thrift definitions
do get updated (e.g. for the recent additions of [Geometry] and [Variant] tyoes)

However, one of the arrow-rs contributors, Jorn Horstmann (todo link) prototyped a macro
based approach, that generates the custom parser code using rust macros so that the
rust structures look very similar to the original thrift definitions. 

For exmaple, here is the thrift definition of a parquet file metadata:

(TOD)O


And the corresponding rust structure that uses the custom parser:

(TODO example
)

### Future Improvements
With a custom parser, we can now also consider other improvements such as
* Implementing special "skip" indexes to skip directly to the parts of the metadata
  that are needed for a particular query, such as the row group offsets
* Selectively decode only the statistics for columns that are needed for a particular query
