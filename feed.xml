<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://arrow.apache.org/feed.xml" rel="self" type="application/atom+xml" /><link href="https://arrow.apache.org/" rel="alternate" type="text/html" /><updated>2025-12-12T22:05:06-05:00</updated><id>https://arrow.apache.org/feed.xml</id><title type="html">Apache Arrow</title><subtitle>Apache Arrow is the universal columnar format and multi-language toolbox for fast data interchange and in-memory analytics. It specifies a standardized language-independent column-oriented memory format for flat and nested data, organized for efficient analytic operations on modern hardware. It also provides computational libraries and zero-copy streaming messaging and interprocess communication. Languages currently supported include C, C++, C#, Go, Java, JavaScript, MATLAB, Python, R, Ruby, and Rust.</subtitle><entry><title type="html">Apache Arrow Go 18.5.0 Release</title><link href="https://arrow.apache.org/blog/2025/12/12/arrow-go-18.5.0/" rel="alternate" type="text/html" title="Apache Arrow Go 18.5.0 Release" /><published>2025-12-12T00:00:00-05:00</published><updated>2025-12-12T00:00:00-05:00</updated><id>https://arrow.apache.org/blog/2025/12/12/arrow-go-18.5.0</id><content type="html" xml:base="https://arrow.apache.org/blog/2025/12/12/arrow-go-18.5.0/"><![CDATA[<!--

-->
<p>The Apache Arrow team is pleased to announce the v18.5.0 release of Apache Arrow Go.
This minor release covers 38 commits from 17 distinct contributors.</p>
<h2>Contributors</h2>
<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code data-lang="console"><span class="gp">$</span><span class="w"> </span>git shortlog <span class="nt">-sn</span> v18.4.1..v18.5.0
<span class="go">    11	Matt Topol
     5	Alex
     5	pixelherodev
     2	Mandukhai Alimaa
     2	Rick  Morgans
     2	Sutou Kouhei
     1	Ahmed Mezghani
     1	Bryce Mecum
     1	Dhruvit Maniya
     1	Erez Rokah
     1	Jacob Romero
     1	James Guthrie
     1	Orson Peters
     1	Pierre Lacave
     1	Ruihao Chen
     1	Travis Patterson
     1	andyfan
</span></code></pre></div></div>
<h2>Highlights</h2>
<ul>
<li>Bumped substrait-go dependency to v7 <a href="https://github.com/apache/arrow-go/pull/526">#526</a></li>
</ul>
<h3>Arrow</h3>
<ul>
<li>Support customizing <code>ValueStr</code> output for Timestamp types <a href="https://github.com/apache/arrow-go/pull/510">#510</a></li>
<li>Series of fixes for the cdata integration including fixing some memory leaks <a href="https://github.com/apache/arrow-go/pull/513">#513</a> <a href="https://github.com/apache/arrow-go/pull/603">#603</a></li>
<li>Optimizations for comparisons and Take kernels <a href="https://github.com/apache/arrow-go/pull/574">#574</a> <a href="https://github.com/apache/arrow-go/pull/556">#556</a> <a href="https://github.com/apache/arrow-go/pull/563">#563</a> <a href="https://github.com/apache/arrow-go/pull/573">#573</a> <a href="https://github.com/apache/arrow-go/pull/557">#557</a></li>
<li>New temporal rounding methods added to compute <a href="https://github.com/apache/arrow-go/pull/572">#572</a></li>
<li>Fix concatenating out of order REE slices <a href="https://github.com/apache/arrow-go/pull/587">#587</a></li>
</ul>
<h3>Parquet</h3>
<ul>
<li>Fix nullable elements for FixedSizeList values <a href="https://github.com/apache/arrow-go/pull/585">#585</a></li>
<li>Add a build tag <code>pqarrow_read_only</code> for optimized builds that don't need to write <a href="https://github.com/apache/arrow-go/pull/569">#569</a></li>
<li>Better tracking of Total bytes and Total Compressed bytes <a href="https://github.com/apache/arrow-go/pull/548">#548</a></li>
<li>Dramatic performance gains for writing Parquet files (~70%-90% reduced memory, up to 10x faster in some cases) <a href="https://github.com/apache/arrow-go/pull/595">#595</a></li>
</ul>
<h2>Changelog</h2>
<h3>What's Changed</h3>
<ul>
<li>fix(parquet/pqarrow): Fix null_count column stats by @MasslessParticle in <a href="https://github.com/apache/arrow-go/pull/489">#489</a></li>
<li>chore: Use apache/arrow-dotnet for integration test by @kou in <a href="https://github.com/apache/arrow-go/pull/495">#495</a></li>
<li>feat(parquet): utilize memory allocator in <code>serializedPageReader</code> by @joechenrh in <a href="https://github.com/apache/arrow-go/pull/485">#485</a></li>
<li>chore: Automate GitHub Releases creation and site redirect by @kou in <a href="https://github.com/apache/arrow-go/pull/497">#497</a></li>
<li>use xnor for boolean equals function by @Dhruvit96 in <a href="https://github.com/apache/arrow-go/pull/505">#505</a></li>
<li>chore(ci): fix verify_rc finding latest go by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/512">#512</a></li>
<li>feat: Add support for specifying <code>Timestamp</code> <code>ValueStr</code> output layout by @erezrokah in <a href="https://github.com/apache/arrow-go/pull/510">#510</a></li>
<li>fix(arrow/cdata): Avoid calling unsafe.Slice on zero-length pointers by @orlp in <a href="https://github.com/apache/arrow-go/pull/513">#513</a></li>
<li>fix(arrow/compute): fix scalar comparison panic by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/518">#518</a></li>
<li>fix(arrow/array): fix panic in dictionary builders by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/517">#517</a></li>
<li>fix(parquet/pqarrow): unsupported dictionary types in pqarrow by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/520">#520</a></li>
<li>chore(parquet/metadata): use constant time compare for signature verify by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/528">#528</a></li>
<li>build(deps): update substrait to v7 by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/526">#526</a></li>
<li>fix(parquet): fix adaptive bloom filter duplicate hash counting, comparison logic, and GC safety by @Mandukhai-Alimaa in <a href="https://github.com/apache/arrow-go/pull/527">#527</a></li>
<li>refactor(arrow): last increment of the Record -&gt; RecordBatch migration by @Mandukhai-Alimaa in <a href="https://github.com/apache/arrow-go/pull/522">#522</a></li>
<li>fix: update iceberg substrait URN by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/541">#541</a></li>
<li>optimization: comparison: when DataType is static, skip reflection by @pixelherodev in <a href="https://github.com/apache/arrow-go/pull/542">#542</a></li>
<li>fix(parquet/pqarrow): decoding Parquet with Arrow dict in schema by @freakyzoidberg in <a href="https://github.com/apache/arrow-go/pull/551">#551</a></li>
<li>feat: support conversion of chunked arrays by @ahmed-mez in <a href="https://github.com/apache/arrow-go/pull/553">#553</a></li>
<li>format: regenerate internal/flatbuf from arrow repo and newer flatc by @pixelherodev in <a href="https://github.com/apache/arrow-go/pull/555">#555</a></li>
<li>Batch of small optimizations by @pixelherodev in <a href="https://github.com/apache/arrow-go/pull/556">#556</a></li>
<li>perf: optimize compute.Take for fewer memory allocations by @hamilton-earthscope in <a href="https://github.com/apache/arrow-go/pull/557">#557</a></li>
<li>optimization: compare: avoid initializing config when it's not needed by @pixelherodev in <a href="https://github.com/apache/arrow-go/pull/563">#563</a></li>
<li>optimization: schema: use slices.Sort instead of sort.Slice by @pixelherodev in <a href="https://github.com/apache/arrow-go/pull/564">#564</a></li>
<li>doc(parquet): document arrow parquet mappings by @amoeba in <a href="https://github.com/apache/arrow-go/pull/561">#561</a></li>
<li>fix: Metadata.Equal comparison with keys in different order by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/571">#571</a></li>
<li>perf(compute): optimize Take kernel for list types by @hamilton-earthscope in <a href="https://github.com/apache/arrow-go/pull/573">#573</a></li>
<li>build: add pqarrow_read_only build tags to avoid building write related code by @jacobromero in <a href="https://github.com/apache/arrow-go/pull/569">#569</a></li>
<li>[Go] [Parquet] pqarrow file-writer &amp; row-group-writer tracking total &amp; total compressed bytes by @DuanWeiFan in <a href="https://github.com/apache/arrow-go/pull/548">#548</a></li>
<li>[Go][Parquet] Fix FixedSizeList nullable elements read as NULL by @rmorgans in <a href="https://github.com/apache/arrow-go/pull/585">#585</a></li>
<li>[Go][Parquet] Refactor: extract visitListLike helper for list-like types by @rmorgans in <a href="https://github.com/apache/arrow-go/pull/586">#586</a></li>
<li>feat(compute): Take kernel for Map type by @hamilton-earthscope in <a href="https://github.com/apache/arrow-go/pull/574">#574</a></li>
<li>fix: correctly initialize SchemaField.ColIndex by @JamesGuthrie in <a href="https://github.com/apache/arrow-go/pull/591">#591</a></li>
<li>fix(arrow/array): fix concat for out of order REE slices by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/587">#587</a></li>
<li>new(arrow/compute): temporal rounding methods by @hamilton-earthscope in <a href="https://github.com/apache/arrow-go/pull/572">#572</a></li>
<li>chore(arrow): Bump package version to 18.5.0 by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/594">#594</a></li>
<li>perf(parquet): minor tweaks for iceberg write improvement by @hamilton-earthscope in <a href="https://github.com/apache/arrow-go/pull/595">#595</a></li>
<li>fix(arrow/cdata): fix leaks identified by leak-sanitizer by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/603">#603</a></li>
</ul>
<h3>New Contributors</h3>
<ul>
<li>@Dhruvit96 made their first contribution in <a href="https://github.com/apache/arrow-go/pull/505">#505</a></li>
<li>@erezrokah made their first contribution in <a href="https://github.com/apache/arrow-go/pull/510">#510</a></li>
<li>@orlp made their first contribution in <a href="https://github.com/apache/arrow-go/pull/513">#513</a></li>
<li>@pixelherodev made their first contribution in <a href="https://github.com/apache/arrow-go/pull/542">#542</a></li>
<li>@freakyzoidberg made their first contribution in <a href="https://github.com/apache/arrow-go/pull/551">#551</a></li>
<li>@ahmed-mez made their first contribution in <a href="https://github.com/apache/arrow-go/pull/553">#553</a></li>
<li>@hamilton-earthscope made their first contribution in <a href="https://github.com/apache/arrow-go/pull/557">#557</a></li>
<li>@jacobromero made their first contribution in <a href="https://github.com/apache/arrow-go/pull/569">#569</a></li>
<li>@DuanWeiFan made their first contribution in <a href="https://github.com/apache/arrow-go/pull/548">#548</a></li>
<li>@rmorgans made their first contribution in <a href="https://github.com/apache/arrow-go/pull/585">#585</a></li>
<li>@JamesGuthrie made their first contribution in <a href="https://github.com/apache/arrow-go/pull/591">#591</a></li>
</ul>
<p><strong>Full Changelog</strong>: <a href="https://github.com/apache/arrow-go/compare/v18.4.1...v18.5.0">https://github.com/apache/arrow-go/compare/v18.4.1...v18.5.0</a></p>]]></content><author><name>pmc</name></author><category term="release" /><summary type="html"><![CDATA[The Apache Arrow team is pleased to announce the v18.5.0 release of Apache Arrow Go. This minor release covers 38 commits from 17 distinct contributors. Contributors $ git shortlog -sn v18.4.1..v18.5.0 11 Matt Topol 5 Alex 5 pixelherodev 2 Mandukhai Alimaa 2 Rick Morgans 2 Sutou Kouhei 1 Ahmed Mezghani 1 Bryce Mecum 1 Dhruvit Maniya 1 Erez Rokah 1 Jacob Romero 1 James Guthrie 1 Orson Peters 1 Pierre Lacave 1 Ruihao Chen 1 Travis Patterson 1 andyfan Highlights Bumped substrait-go dependency to v7 #526 Arrow Support customizing ValueStr output for Timestamp types #510 Series of fixes for the cdata integration including fixing some memory leaks #513 #603 Optimizations for comparisons and Take kernels #574 #556 #563 #573 #557 New temporal rounding methods added to compute #572 Fix concatenating out of order REE slices #587 Parquet Fix nullable elements for FixedSizeList values #585 Add a build tag pqarrow_read_only for optimized builds that don't need to write #569 Better tracking of Total bytes and Total Compressed bytes #548 Dramatic performance gains for writing Parquet files (~70%-90% reduced memory, up to 10x faster in some cases) #595 Changelog What's Changed fix(parquet/pqarrow): Fix null_count column stats by @MasslessParticle in #489 chore: Use apache/arrow-dotnet for integration test by @kou in #495 feat(parquet): utilize memory allocator in serializedPageReader by @joechenrh in #485 chore: Automate GitHub Releases creation and site redirect by @kou in #497 use xnor for boolean equals function by @Dhruvit96 in #505 chore(ci): fix verify_rc finding latest go by @zeroshade in #512 feat: Add support for specifying Timestamp ValueStr output layout by @erezrokah in #510 fix(arrow/cdata): Avoid calling unsafe.Slice on zero-length pointers by @orlp in #513 fix(arrow/compute): fix scalar comparison panic by @zeroshade in #518 fix(arrow/array): fix panic in dictionary builders by @zeroshade in #517 fix(parquet/pqarrow): unsupported dictionary types in pqarrow by @zeroshade in #520 chore(parquet/metadata): use constant time compare for signature verify by @zeroshade in #528 build(deps): update substrait to v7 by @zeroshade in #526 fix(parquet): fix adaptive bloom filter duplicate hash counting, comparison logic, and GC safety by @Mandukhai-Alimaa in #527 refactor(arrow): last increment of the Record -&gt; RecordBatch migration by @Mandukhai-Alimaa in #522 fix: update iceberg substrait URN by @zeroshade in #541 optimization: comparison: when DataType is static, skip reflection by @pixelherodev in #542 fix(parquet/pqarrow): decoding Parquet with Arrow dict in schema by @freakyzoidberg in #551 feat: support conversion of chunked arrays by @ahmed-mez in #553 format: regenerate internal/flatbuf from arrow repo and newer flatc by @pixelherodev in #555 Batch of small optimizations by @pixelherodev in #556 perf: optimize compute.Take for fewer memory allocations by @hamilton-earthscope in #557 optimization: compare: avoid initializing config when it's not needed by @pixelherodev in #563 optimization: schema: use slices.Sort instead of sort.Slice by @pixelherodev in #564 doc(parquet): document arrow parquet mappings by @amoeba in #561 fix: Metadata.Equal comparison with keys in different order by @zeroshade in #571 perf(compute): optimize Take kernel for list types by @hamilton-earthscope in #573 build: add pqarrow_read_only build tags to avoid building write related code by @jacobromero in #569 [Go] [Parquet] pqarrow file-writer &amp; row-group-writer tracking total &amp; total compressed bytes by @DuanWeiFan in #548 [Go][Parquet] Fix FixedSizeList nullable elements read as NULL by @rmorgans in #585 [Go][Parquet] Refactor: extract visitListLike helper for list-like types by @rmorgans in #586 feat(compute): Take kernel for Map type by @hamilton-earthscope in #574 fix: correctly initialize SchemaField.ColIndex by @JamesGuthrie in #591 fix(arrow/array): fix concat for out of order REE slices by @zeroshade in #587 new(arrow/compute): temporal rounding methods by @hamilton-earthscope in #572 chore(arrow): Bump package version to 18.5.0 by @zeroshade in #594 perf(parquet): minor tweaks for iceberg write improvement by @hamilton-earthscope in #595 fix(arrow/cdata): fix leaks identified by leak-sanitizer by @zeroshade in #603 New Contributors @Dhruvit96 made their first contribution in #505 @erezrokah made their first contribution in #510 @orlp made their first contribution in #513 @pixelherodev made their first contribution in #542 @freakyzoidberg made their first contribution in #551 @ahmed-mez made their first contribution in #553 @hamilton-earthscope made their first contribution in #557 @jacobromero made their first contribution in #569 @DuanWeiFan made their first contribution in #548 @rmorgans made their first contribution in #585 @JamesGuthrie made their first contribution in #591 Full Changelog: https://github.com/apache/arrow-go/compare/v18.4.1...v18.5.0]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Arrow-rs Parquet 读取中的延迟物化（Late Materialization）实战深度解析</title><link href="https://arrow.apache.org/blog/2025/12/11/parquet-late-materialization-deep-dive-zh/" rel="alternate" type="text/html" title="Arrow-rs Parquet 读取中的延迟物化（Late Materialization）实战深度解析" /><published>2025-12-11T00:00:00-05:00</published><updated>2025-12-11T00:00:00-05:00</updated><id>https://arrow.apache.org/blog/2025/12/11/parquet-late-materialization-deep-dive-zh</id><content type="html" xml:base="https://arrow.apache.org/blog/2025/12/11/parquet-late-materialization-deep-dive-zh/"><![CDATA[<!--

-->
<p>本文深入探讨了在 <a href="https://github.com/apache/arrow-rs"><code>arrow-rs</code></a>（为 <a href="https://datafusion.apache.org/">Apache DataFusion</a> 等项目提供动力的读取器）的 <a href="https://parquet.apache.org/">Apache Parquet</a> 读取器中实现延迟物化（Late Materialization）的决策和陷阱。我们将看到一个看似简单的文件读取器如何通过复杂的逻辑来评估谓词——实际上它自身变成了一个<strong>微型查询引擎</strong>。</p>
<h2>1. 为什么要延迟物化？</h2>
<p>列式读取是 <strong>I/O 带宽</strong> 和 <strong>CPU 解码成本</strong> 之间的一场持久战。虽然跳过数据通常是好事，但跳过本身也有计算成本。<code>arrow-rs</code> 中 Parquet 读取器的目标是<strong>流水线式的延迟物化</strong>：首先评估谓词，然后访问投影列。对于过滤掉许多行的谓词，在评估之后再进行物化可以最大限度地减少读取和解码工作。</p>
<p>这种方法与 Abadi 等人的论文 <a href="https://www.cs.umd.edu/~abadi/papers/abadiicde2007.pdf">列式 DBMS 中的物化策略</a> 中的 <strong>LM-pipelined</strong> 策略非常相似：交错进行谓词评估和数据列访问，而不是一次性读取所有列并试图将它们<strong>重新拼接</strong>成行。</p>
<figure style="text-align: center;">
  <img src="/img/late-materialization/fig1.jpg" alt="LM-pipelined late materialization pipeline" width="100%" class="img-responsive">
</figure>
<p>为了使用延迟物化评估像 <code>SELECT B, C FROM table WHERE A &gt; 10 AND B &lt; 5</code> 这样的查询，读取器遵循以下步骤：</p>
<ol>
<li>读取列 <code>A</code> 并评估 <code>A &gt; 10</code> 以构建一个 <code>RowSelection</code>（一个稀疏掩码），代表最初幸存的行集。</li>
<li>使用该 <code>RowSelection</code> 读取列 <code>B</code> 中幸存的值，并评估 <code>B &lt; 5</code>，更新 <code>RowSelection</code> 使其更加稀疏。</li>
<li>使用细化后的 <code>RowSelection</code> 读取列 <code>C</code>（投影列），仅解码最终幸存的行。</li>
</ol>
<p>本文的其余部分将详细介绍代码如何实现这一路径。</p>
<hr />
<h2>2. Rust Parquet 读取器中的延迟物化</h2>
<h3>2.1 LM-pipelined（流水线延迟物化）</h3>
<p>“LM-pipelined”听起来像是教科书里的术语。在 <code>arrow-rs</code> 中，它简单地指一个按顺序运行的流水线：“读取谓词列 → 生成行选择 → 读取数据列”。这与<strong>并行</strong>策略形成对比，后者同时读取所有谓词列。虽然并行可以最大化多核 CPU 的利用率，但在列式存储中，流水线方法通常更优，因为每个过滤步骤都大幅减少了后续步骤需要读取和解析的数据量。</p>
<p>代码结构分为几个核心角色：</p>
<ul>
<li><strong><a href="https://github.com/apache/arrow-rs/blob/bab30ae3d61509aa8c73db33010844d440226af2/parquet/src/arrow/arrow_reader/read_plan.rs#L302">ReadPlan</a> / <a href="https://github.com/apache/arrow-rs/blob/bab30ae3d61509aa8c73db33010844d440226af2/parquet/src/arrow/arrow_reader/read_plan.rs#L34">ReadPlanBuilder</a></strong>：将“读取哪些列以及使用什么行子集”编码为一个计划。它不会预先读取所有谓词列。它读取一列，收紧选择，然后继续。</li>
<li><strong><a href="https://github.com/apache/arrow-rs/blob/bab30ae3d61509aa8c73db33010844d440226af2/parquet/src/arrow/arrow_reader/selection.rs#L139">RowSelection</a></strong>：有两种实现方式：用 <a href="https://en.wikipedia.org/wiki/Run-length_encoding">游程编码（Run-length encoding）</a> (RLE)（<a href="https://github.com/apache/arrow-rs/blob/bab30ae3d61509aa8c73db33010844d440226af2/parquet/src/arrow/arrow_reader/selection.rs#L66"><code>RowSelector</code></a>）来“跳过/选择 N 行”，或用 Arrow <a href="https://github.com/apache/arrow-rs/blob/a67cd19fff65b6c995be9a5eae56845157d95301/arrow-buffer/src/buffer/boolean.rs#L37"><code>BooleanBuffer</code></a> 作为位掩码来过滤行。它是沿着流水线传递稀疏性的核心机制。</li>
<li><strong><a href="https://github.com/apache/arrow-rs/blob/bab30ae3d61509aa8c73db33010844d440226af2/parquet/src/arrow/array_reader/mod.rs#L85">ArrayReader</a></strong>：负责解码。它接收一个<a href="https://github.com/apache/arrow-rs/blob/ce4edd53203eb4bca96c10ebf3d2118299dad006/parquet/src/arrow/arrow_reader/selection.rs#L139"><code>RowSelection</code></a>并决定读取哪些页以及解码哪些值。</li>
</ul>
<p><a href="https://github.com/apache/arrow-rs/blob/ce4edd53203eb4bca96c10ebf3d2118299dad006/parquet/src/arrow/arrow_reader/selection.rs#L139"><code>RowSelection</code></a> 可以在 RLE 和位掩码之间动态切换。当间隙很小且稀疏度很高时，位掩码更快；RLE 则对大范围的页级跳过更友好。关于这种权衡的细节将在 3.1 节中介绍。</p>
<p>再次考虑查询：<code>SELECT B, C FROM table WHERE A &gt; 10 AND B &lt; 5</code>：</p>
<ol>
<li><strong>初始</strong>：<code>selection = None</code>（相当于“全选”）。</li>
<li><strong>读取 A</strong>：<code>ArrayReader</code> 分批解码列 A；谓词构建一个布尔掩码；<a href="https://github.com/apache/arrow-rs/blob/bab30ae3d61509aa8c73db33010844d440226af2/parquet/src/arrow/arrow_reader/selection.rs#L149"><code>RowSelection::from_filters</code></a> 将其转换为稀疏选择。</li>
<li><strong>收紧</strong>：<a href="https://github.com/apache/arrow-rs/blob/bab30ae3d61509aa8c73db33010844d440226af2/parquet/src/arrow/arrow_reader/read_plan.rs#L143"><code>ReadPlanBuilder::with_predicate</code></a> 通过 <a href="https://github.com/apache/arrow-rs/blob/bab30ae3d61509aa8c73db33010844d440226af2/parquet/src/arrow/arrow_reader/selection.rs#L345"><code>RowSelection::and_then</code></a> 链接新的掩码。</li>
<li><strong>读取 B</strong>：使用当前的 <code>selection</code> 构建列 B 的读取器；读取器仅对选定的行执行 I/O 和解码，产生一个更稀疏的掩码。</li>
<li><strong>合并</strong>：<code>selection = selection.and_then(selection_b)</code>；投影列现在只解码极小的行集。</li>
</ol>
<p><strong>代码位置和草图</strong>：</p>
<div class="language-rust highlighter-rouge"><div class="highlight"><pre class="highlight"><code data-lang="rust"><span class="c1">// Close to the flow in read_plan.rs (simplified)</span>
<span class="k">let</span> <span class="k">mut</span> <span class="n">builder</span> <span class="o">=</span> <span class="nn">ReadPlanBuilder</span><span class="p">::</span><span class="nf">new</span><span class="p">(</span><span class="n">batch_size</span><span class="p">);</span>

<span class="c1">// 1) Inject external pruning (e.g., Page Index):</span>
<span class="n">builder</span> <span class="o">=</span> <span class="n">builder</span><span class="nf">.with_selection</span><span class="p">(</span><span class="n">page_index_selection</span><span class="p">);</span>

<span class="c1">// 2) Append predicates serially:</span>
<span class="k">for</span> <span class="n">predicate</span> <span class="k">in</span> <span class="n">predicates</span> <span class="p">{</span>
    <span class="n">builder</span> <span class="o">=</span> <span class="n">builder</span><span class="nf">.with_predicate</span><span class="p">(</span><span class="n">predicate</span><span class="p">);</span> <span class="c1">// internally uses RowSelection::and_then</span>
<span class="p">}</span>

<span class="c1">// 3) Build readers; all ArrayReaders share the final selection strategy</span>
<span class="k">let</span> <span class="n">plan</span> <span class="o">=</span> <span class="n">builder</span><span class="nf">.build</span><span class="p">();</span>
<span class="k">let</span> <span class="n">reader</span> <span class="o">=</span> <span class="nn">ParquetRecordBatchReader</span><span class="p">::</span><span class="nf">new</span><span class="p">(</span><span class="n">array_reader</span><span class="p">,</span> <span class="n">plan</span><span class="p">);</span>
</code></pre></div></div>
<p>我画了一个简单的流程图来说明这个流程，帮助你理解：</p>
<figure style="text-align: center;">
  <img src="/img/late-materialization/fig2.jpg" alt="Predicate-first pipeline flow" width="100%" class="img-responsive">
</figure>
<p>现在你已经了解了这个流水线是如何工作的，下一个问题是<strong>如何表示和组合这些稀疏选择</strong>（图中的 <strong>Row Mask</strong>），这就是 <code>RowSelection</code> 发挥作用的地方。</p>
<h3>2.2 组合行选择器 (<code>RowSelection::and_then</code>)</h3>
<p><a href="https://github.com/apache/arrow-rs/blob/ce4edd53203eb4bca96c10ebf3d2118299dad006/parquet/src/arrow/arrow_reader/selection.rs#L139"><code>RowSelection</code></a> 代表了最终将生成的行集。它目前使用 RLE (<code>RowSelector::select/skip(len)</code>) 来描述稀疏范围。<a href="https://github.com/apache/arrow-rs/blob/bab30ae3d61509aa8c73db33010844d440226af2/parquet/src/arrow/arrow_reader/selection.rs#L345"><code>RowSelection::and_then</code></a> 是“将一个选择应用于另一个”的核心操作：左侧参数是“已经通过的行”，右侧参数是“在通过的行中，哪些也通过了第二个过滤器”。输出是它们的布尔 AND。</p>
<p><strong>演练示例</strong>：</p>
<ul>
<li><strong>输入选择 A（已过滤）</strong>：<code>[Skip 100, Select 50, Skip 50]</code>（物理行 100-150 被选中）</li>
<li><strong>选择 B（在 A 内部过滤）</strong>：<code>[Select 10, Skip 40]</code>（在选中的 50 行中，只有前 10 行通过 B）</li>
<li><strong>结果</strong>：<code>[Skip 100, Select 10, Skip 90]</code>。</li>
</ul>
<p><strong>运行过程</strong>：
想象一下它就像拉拉链：我们同时遍历两个列表，如下所示：</p>
<ol>
<li><strong>前 100 行</strong>：A 是 Skip → 结果是 Skip 100。</li>
<li><strong>接下来的 50 行</strong>：A 是 Select。看 B：
<ul>
<li>B 的前 10 个是 Select → 结果 Select 10。</li>
<li>B 的剩余 40 个是 Skip → 结果 Skip 40。</li>
</ul>
</li>
<li><strong>最后 50 行</strong>：A 是 Skip → 结果 Skip 50。</li>
</ol>
<p><strong>结果</strong>：<code>[Skip 100, Select 10, Skip 90]</code>。</p>
<p>下面是代码示例：</p>
<div class="language-rust highlighter-rouge"><div class="highlight"><pre class="highlight"><code data-lang="rust"><span class="c1">// Example: Skip 100 rows, then take the next 10</span>
<span class="k">let</span> <span class="n">a</span><span class="p">:</span> <span class="n">RowSelection</span> <span class="o">=</span> <span class="nd">vec!</span><span class="p">[</span><span class="nn">RowSelector</span><span class="p">::</span><span class="nf">skip</span><span class="p">(</span><span class="mi">100</span><span class="p">),</span> <span class="nn">RowSelector</span><span class="p">::</span><span class="nf">select</span><span class="p">(</span><span class="mi">50</span><span class="p">)]</span><span class="nf">.into</span><span class="p">();</span>
<span class="k">let</span> <span class="n">b</span><span class="p">:</span> <span class="n">RowSelection</span> <span class="o">=</span> <span class="nd">vec!</span><span class="p">[</span><span class="nn">RowSelector</span><span class="p">::</span><span class="nf">select</span><span class="p">(</span><span class="mi">10</span><span class="p">),</span> <span class="nn">RowSelector</span><span class="p">::</span><span class="nf">skip</span><span class="p">(</span><span class="mi">40</span><span class="p">)]</span><span class="nf">.into</span><span class="p">();</span>
<span class="k">let</span> <span class="n">result</span> <span class="o">=</span> <span class="n">a</span><span class="nf">.and_then</span><span class="p">(</span><span class="o">&amp;</span><span class="n">b</span><span class="p">);</span>
<span class="c1">// Result should be: Skip 100, Select 10, Skip 40</span>
<span class="nd">assert_eq!</span><span class="p">(</span>
    <span class="nn">Vec</span><span class="p">::</span><span class="o">&lt;</span><span class="n">RowSelector</span><span class="o">&gt;</span><span class="p">::</span><span class="nf">from</span><span class="p">(</span><span class="n">result</span><span class="p">),</span>
    <span class="nd">vec!</span><span class="p">[</span><span class="nn">RowSelector</span><span class="p">::</span><span class="nf">skip</span><span class="p">(</span><span class="mi">100</span><span class="p">),</span> <span class="nn">RowSelector</span><span class="p">::</span><span class="nf">select</span><span class="p">(</span><span class="mi">10</span><span class="p">),</span> <span class="nn">RowSelector</span><span class="p">::</span><span class="nf">skip</span><span class="p">(</span><span class="mi">40</span><span class="p">)]</span>
<span class="p">);</span>
</code></pre></div></div>
<figure style="text-align: center;">
  <img src="/img/late-materialization/fig3.jpg" alt="RowSelection logical AND walkthrough" width="100%" class="img-responsive">
</figure>
<p>这不断缩小过滤范围，同时只触及轻量级的元数据——没有数据拷贝。目前的 <code>and_then</code> 实现是一个双指针线性扫描；复杂度与选择器段数呈线性关系。谓词收缩选择的越多，后续的扫描就越便宜。</p>
<h3>3. 工程挑战</h3>
<p>延迟物化在理论上听起来很简单，但在像 <code>arrow-rs</code> 这样的生产级系统中实现它绝对是一场<strong>工程噩梦</strong>。历史上，这些技术非常棘手，一直被锁定在专有引擎中。在开源世界中，我们已经为此打磨了多年（看看 <a href="https://github.com/apache/datafusion/issues/3463">DataFusion 的这个 ticket</a> 就知道了），终于，我们可以<strong>大展拳脚</strong>，与全物化一较高下。为了实现这一点，我们需要解决几个严重的工程挑战。</p>
<h3>3.1 自适应 RowSelection 策略（位掩码 vs. RLE）</h3>
<p>一个主要的障碍是为 <code>RowSelection</code> 选择正确的内部表示，因为最佳选择取决于稀疏模式。<a href="https://db.cs.cmu.edu/papers/2021/ngom-damon2021.pdf">这篇论文</a> 揭示了一个关键障碍：对于 <code>RowSelection</code> 来说，不存在“一刀切”的格式。研究人员发现，最佳的内部表示是一个移动的目标，随着数据的“密集”或“稀疏”程度——不断变化。</p>
<ul>
<li><strong>极度稀疏</strong>（例如，每 10,000 行 1 行）：这里使用位掩码很浪费（每行 1 位加起来也不少），而 RLE 非常干净——只需几个选择器就搞定了。</li>
<li><strong>稀疏但有微小间隙</strong>（例如，“读 1，跳 1”）：RLE 会产生碎片化的混乱，让解码器超负荷工作；这里位掩码效率高得多。</li>
</ul>
<p>由于两者各有优缺点，我们决定采用自适应策略来<strong>兼得两者之长</strong>（详情见 <a href="https://github.com/apache/arrow-rs/pull/8733">#arrow-rs/8733</a>）：</p>
<ul>
<li>我们查看选择器的平均游程长度，并将其与阈值（目前为 <code>32</code>）进行比较。如果平均值太小，我们切换到位掩码；否则，我们坚持使用选择器（RLE）。</li>
<li><strong>安全网</strong>：位掩码看起来很棒，直到遇到页修剪（Page Pruning），这可能会导致糟糕的“页丢失”恐慌（panic），因为掩码可能会盲目地试图过滤从未读取过的页中的行。<code>RowSelection</code> 逻辑会提防这种<strong>灾难配方</strong>，并强制切回 RLE 以防止崩溃（见 3.1.2）。</li>
</ul>
<h4>3.1.1 <code>32</code> 这个阈值是怎么来的？</h4>
<p>数字 32 并不是凭空捏造的。它来自于使用各种分布（均匀间隔、指数稀疏、随机噪声）进行的 <a href="https://github.com/apache/arrow-rs/pull/8733#issuecomment-3468441165">数据驱动的“对决”</a>。它在区分“破碎但密集”和“长跳跃区域”方面做得很好。未来，我们可能会基于数据类型采用更复杂的启发式方法。</p>
<p>下图展示了对决中的一个示例运行。蓝线是 <code>read_selector</code> (RLE)，橙线是 <code>read_mask</code> (位掩码)。纵轴是时间（越低越好），横轴是平均游程长度。你可以看到性能曲线在 32 附近交叉。</p>
<figure style="text-align: center;">
  <img src="/img/late-materialization/3.1.1.png" alt="Bitmask vs RLE benchmark threshold" width="100%" class="img-responsive">
</figure>
<h4>3.1.2 位掩码陷阱：丢失的页</h4>
<p>在实现自适应策略时，位掩码在纸面上看起来很完美，但在结合 <strong>页修剪（Page Pruning）</strong> 时隐藏着一个讨厌的陷阱。</p>
<p>在深入细节之前，先快速回顾一下页（更多内容见 3.2 节）：Parquet 文件被切分成页（Page）。如果我们知道一个页在选择中没有行，我们<strong>根本不会触碰它</strong>——不解压，不解码。<code>ArrayReader</code> 甚至不知道它的存在。</p>
<p><strong>案发现场：</strong></p>
<p>想象一下读取一块数据<code>[0,1,2,3,4,5,6]</code>，中间的四行 <code>[1,2,3,4]</code>被过滤掉了。碰巧其中两行 <code>[2,3]</code> 位于它们自己的页中，因此该页被完全修剪掉了。</p>
<figure style="text-align: center;">
  <img src="/img/late-materialization/3.3.2-fig1.jpg" alt="Page pruning example with only first and last rows kept" width="100%" class="img-responsive">
</figure>
<p>如果我们要使用 RLE (<code>RowSelector</code>)，执行 <code>Skip(4)</code> 是一帆风顺的：我们只是跳过间隙。</p>
<figure style="text-align: center;">
  <img src="/img/late-materialization/3.3.2-fig3.jpg" alt="RLE skipping pruned pages safely" width="100%" class="img-responsive">
</figure>
<p><strong>问题：</strong></p>
<p>然而，如果我们使用位掩码，读取器将首先解码所有 6 行，打算稍后过滤它们。但是中间的页不存在！一旦解码器遇到那个间隙，它就会恐慌（panic）。<code>ArrayReader</code> 是一个流处理单元——它不处理 I/O，因此不知道上层决定修剪页，所以它看不到前面的悬崖。</p>
<figure style="text-align: center;">
  <img src="/img/late-materialization/3.3.2-fig2.jpg" alt="Bitmask hitting a missing page panic" width="100%" class="img-responsive">
</figure>
<p><strong>修复：</strong></p>
<p>我们目前的解决方案既保守又稳健：<strong>如果我们检测到页修剪，我们就禁用位掩码并强制回退到 RLE。</strong> 在未来，我们希望扩展位掩码逻辑以使其感知页修剪（见 <a href="https://github.com/apache/arrow-rs/issues/8845">#arrow-rs/8845</a>）。</p>
<div class="language-rust highlighter-rouge"><div class="highlight"><pre class="highlight"><code data-lang="rust"><span class="c1">// Auto prefers bitmask, but... wait, offset_index says page pruning is on.</span>
<span class="k">let</span> <span class="n">policy</span> <span class="o">=</span> <span class="nn">RowSelectionPolicy</span><span class="p">::</span><span class="n">Auto</span> <span class="p">{</span> <span class="n">threshold</span><span class="p">:</span> <span class="mi">32</span> <span class="p">};</span>
<span class="k">let</span> <span class="n">plan_builder</span> <span class="o">=</span> <span class="nn">ReadPlanBuilder</span><span class="p">::</span><span class="nf">new</span><span class="p">(</span><span class="mi">1024</span><span class="p">)</span><span class="nf">.with_row_selection_policy</span><span class="p">(</span><span class="n">policy</span><span class="p">);</span>
<span class="k">let</span> <span class="n">plan_builder</span> <span class="o">=</span> <span class="nf">override_selector_strategy_if_needed</span><span class="p">(</span>
    <span class="n">plan_builder</span><span class="p">,</span>
    <span class="o">&amp;</span><span class="n">projection_mask</span><span class="p">,</span>
    <span class="nf">Some</span><span class="p">(</span><span class="n">offset_index</span><span class="p">),</span> <span class="c1">// page index enables page pruning</span>
<span class="p">);</span>
<span class="c1">// ...so we play it safe and switch to Selectors (RLE).</span>
<span class="nd">assert_eq!</span><span class="p">(</span><span class="n">plan_builder</span><span class="nf">.row_selection_policy</span><span class="p">(),</span> <span class="o">&amp;</span><span class="nn">RowSelectionPolicy</span><span class="p">::</span><span class="n">Selectors</span><span class="p">);</span>
</code></pre></div></div>
<h3>3.2 页修剪（Page Pruning）</h3>
<p>终极的性能胜利是<strong>根本不进行 I/O 或解码</strong>。但是在现实世界中（特别是对象存储），发出一百万个微小的读取请求是<strong>性能杀手</strong>。<code>arrow-rs</code> 使用 Parquet <a href="https://parquet.apache.org/docs/file-format/pageindex/">PageIndex</a> 来精确计算哪些页包含我们实际需要的数据。对于选择性极高的谓词，跳过页可以节省大量的 I/O，即使底层存储客户端合并了相邻的范围请求。另一个主要的胜利是减少了 CPU：<strong>我们完全跳过了对完全修剪页的解压和解码的繁重工作。</strong></p>
<ul>
<li><strong>注意点</strong>：如果 <code>RowSelection</code> 从一个页中哪怕只选择了<strong>一行</strong>，整个页也必须被解压。因此，这一步的效率很大程度上依赖于数据聚类和谓词之间的相关性。</li>
<li><strong>实现</strong>：<a href="https://github.com/apache/arrow-rs/blob/ce4edd53203eb4bca96c10ebf3d2118299dad006/parquet/src/arrow/arrow_reader/selection.rs#L204"><code>RowSelection::scan_ranges</code></a> 使用每个页的元数据（<code>first_row_index</code> 和 <code>compressed_page_size</code>）进行计算，找出哪些范围是完全跳过的，仅返回所需的 <code>(offset, length)</code> 列表。</li>
</ul>
<p>下面的代码示例说明了页跳过：</p>
<div class="language-rust highlighter-rouge"><div class="highlight"><pre class="highlight"><code data-lang="rust"><span class="c1">// Example: two pages; page0 covers 0..100, page1 covers 100..200</span>
<span class="k">let</span> <span class="n">locations</span> <span class="o">=</span> <span class="nd">vec!</span><span class="p">[</span>
    <span class="n">PageLocation</span> <span class="p">{</span> <span class="n">offset</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="n">compressed_page_size</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span> <span class="n">first_row_index</span><span class="p">:</span> <span class="mi">0</span> <span class="p">},</span>
    <span class="n">PageLocation</span> <span class="p">{</span> <span class="n">offset</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span> <span class="n">compressed_page_size</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span> <span class="n">first_row_index</span><span class="p">:</span> <span class="mi">100</span> <span class="p">},</span>
<span class="p">];</span>
<span class="c1">// RowSelection wants 150..160; page0 is total junk, only read page1</span>
<span class="k">let</span> <span class="n">sel</span><span class="p">:</span> <span class="n">RowSelection</span> <span class="o">=</span> <span class="nd">vec!</span><span class="p">[</span>
    <span class="nn">RowSelector</span><span class="p">::</span><span class="nf">skip</span><span class="p">(</span><span class="mi">150</span><span class="p">),</span>
    <span class="nn">RowSelector</span><span class="p">::</span><span class="nf">select</span><span class="p">(</span><span class="mi">10</span><span class="p">),</span>
    <span class="nn">RowSelector</span><span class="p">::</span><span class="nf">skip</span><span class="p">(</span><span class="mi">40</span><span class="p">),</span>
<span class="p">]</span><span class="nf">.into</span><span class="p">();</span>
<span class="k">let</span> <span class="n">ranges</span> <span class="o">=</span> <span class="n">sel</span><span class="nf">.scan_ranges</span><span class="p">(</span><span class="o">&amp;</span><span class="n">locations</span><span class="p">);</span>
<span class="nd">assert_eq!</span><span class="p">(</span><span class="n">ranges</span><span class="nf">.len</span><span class="p">(),</span> <span class="mi">1</span><span class="p">);</span> <span class="c1">// Only request page1</span>
</code></pre></div></div>
<p>下图说明了使用 RLE 选择进行的页跳过。第一页既不读取也不解码，因为没有行被选中。第二页被读取并完全解压（例如，zstd），然后只解码所需的行。第三页被完全解压和解码，因为所有行都被选中。</p>
<figure style="text-align: center;">
  <img src="/img/late-materialization/fig4.jpg" alt="Page-level scan range calculation" width="100%" class="img-responsive">
</figure>
<p>这种机制充当了逻辑行过滤和物理字节获取之间的桥梁。虽然我们无法将文件切分得比单个页更细（由于压缩边界），但页修剪确保了我们永远不会为页支付解压成本，除非它至少为结果贡献了一行。它达成了一种务实的平衡：利用粗粒度的页索引（Page Index）跳过大片数据，同时留给细粒度的 <code>RowSelection</code> 来处理幸存页内的具体行。</p>
<h3>3.3 智能缓存</h3>
<p>延迟物化引入了一个结构性的进退两难（原文是Catch-22，第二十二条军规）：为了有效地跳过数据，我们必须先读取它。考虑像 <code>SELECT A FROM table WHERE A &gt; 10</code> 这样的查询。读取器必须解码列 <code>A</code> 来评估过滤器。在传统的“读取所有内容”的方法中，这不是问题：列 <code>A</code> 只需留在内存中等待投影。然而，在严格的流水线中，“谓词”阶段和“投影”阶段是解耦的。一旦过滤器生成了 <code>RowSelection</code>，投影阶段发现它需要列 <code>A</code>，就会触发对同一数据的第二次读取。</p>
<p>如果不加干预，我们会支付“双重税”：一次解码用于决定保留什么，再一次解码用于实际保留它。在 <a href="https://github.com/apache/arrow-rs/pull/7850">#arrow-rs/7850</a> 中引入的 <a href="https://github.com/apache/arrow-rs/blob/ce4edd53203eb4bca96c10ebf3d2118299dad006/parquet/src/arrow/array_reader/cached_array_reader.rs#L40-L68"><code>CachedArrayReader</code></a> 使用<strong>双层</strong>缓存架构解决了这个难题。它允许我们在第一次看到解码批次时（在过滤期间）将其存储起来，并稍后（在投影期间）重用。</p>
<p>但是为什么要两层？为什么不直接用一个大缓存？</p>
<ul>
<li><strong>共享缓存（乐观重用）：</strong> 这是一个跨所有列和读取器共享的全局缓存。它有一个用户可配置的内存限制（容量）。当一个页因谓词被解码时，它被放置在这里。如果投影步骤紧接着运行，它可以“命中”这个缓存并避免 I/O。然而，因为内存是有限的，<strong>缓存驱逐</strong>随时可能发生。如果我们仅依赖于此，繁重的工作负载可能会在我们再次需要数据之前就将其驱逐。</li>
<li><strong>本地缓存（确定性保证）：</strong> 这是一个特定于单列读取器的私有缓存。它充当<strong>安全网</strong>。当一个列正在被主动读取时，数据被“钉”(Pin)在本地缓存中。这保证了数据在当前操作期间仍然可用，不受全局共享缓存驱逐的影响。</li>
</ul>
<p>读取器在获取页时遵循严格的层级结构：</p>
<ol>
<li><strong>检查本地：</strong> 我已经钉住它了吗？</li>
<li><strong>检查共享：</strong> 流水线的另一部分最近解码过它吗？如果是，将其<strong>提升</strong>到本地（钉住它）。</li>
<li><strong>从源读取：</strong> 执行 I/O 和解码，然后插入到本地和共享缓存中。</li>
</ol>
<p>这种双重策略让我们兼得两者之长：在过滤和投影步骤之间共享数据的<strong>效率</strong>，以及知道必要数据不会因内存压力而在查询中途消失的<strong>稳定性</strong>。</p>
<h3>3.4 最小化拷贝和分配</h3>
<p>arrow-rs 进行重大优化的另一个领域是<strong>避免不必要的拷贝</strong>。Rust 的 <a href="https://doc.rust-lang.org/book/ch04-01-what-is-ownership.html">内存安全</a> 设计使得拷贝变得容易，而每一次额外的分配和拷贝都会浪费 CPU 周期和内存带宽。一种幼稚的实现经常通过将数据解压到临时的 <code>Vec</code> 然后 <code>memcpy</code> 到 Arrow Buffer 而支付**“不必要的税”**。</p>
<p>对于定长类型（如整数或浮点数），这完全是多余的，因为它们的内存布局是相同的。<a href="https://github.com/apache/arrow-rs/blob/ce4edd53203eb4bca96c10ebf3d2118299dad006/parquet/src/arrow/array_reader/primitive_array.rs#L102"><code>PrimitiveArrayReader</code></a> 通过 <a href="https://docs.rs/arrow/latest/arrow/array/struct.PrimitiveArray.html#example-from-a-vec">零拷贝转换</a> 消除了这种开销：它不再拷贝字节，而是简单地将解码后的 <code>Vec&lt;T&gt;</code> 的<strong>所有权直接移交</strong>给底层的 Arrow <code>Buffer</code>。</p>
<h3>3.5 对齐挑战</h3>
<p>链式过滤是坐标系中的一种<strong>令人抓狂</strong>的练习。过滤器 N 中的“第 1 行”实际上可能是文件中的“第 10,001 行”，这是由于之前的过滤器所致。</p>
<ul>
<li><strong>我们如何保持正轨？</strong>：我们对每个 <code>RowSelection</code> 操作（<code>split_off</code>, <code>and_then</code>, <code>trim</code>）进行 <a href="https://github.com/apache/arrow-rs/blob/ce4edd53203eb4bca96c10ebf3d2118299dad006/parquet/src/arrow/arrow_reader/selection.rs#L1309">模糊测试 (fuzz test)</a>。我们需要绝对确定相对偏移量和绝对偏移量之间的转换是精准无误的。这种正确性是保持读取器在批次边界、稀疏选择和页修剪这三重威胁下保持稳定的基石。</li>
</ul>
<h2>4. 结论</h2>
<p><code>arrow-rs</code> 中的 Parquet 读取器不仅仅是一个简单的文件读取器——它是一个伪装的<strong>微型查询引擎</strong>。我们融入了诸如谓词下推和延迟物化等高端特性。读取器只读取需要的内容，只解码必要的内容，在节省资源的同时保持正确性。以前，这些功能仅限于专有或紧密集成的系统。现在，感谢社区的努力，<code>arrow-rs</code> 将高级查询处理技术的好处带给了即使是轻量级的应用程序。</p>
<p>我们邀请您 <a href="https://github.com/apache/arrow-rs?tab=readme-ov-file#arrow-rust-community">加入社区</a>，探索代码，进行实验，并为其不断的演进做出贡献。优化数据访问的旅程永无止境，我们可以一起推动开源数据处理可能性的边界。</p>]]></content><author><name>&lt;a href=&quot;https://github.com/hhhizzz&quot;&gt;Qiwei Huang&lt;/a&gt; and &lt;a href=&quot;https://github.com/alamb&quot;&gt;Andrew Lamb&lt;/a&gt;</name></author><category term="application" /><category term="translation" /><summary type="html"><![CDATA[arrow-rs 如何通过流水线化谓词和投影来最小化 Parquet 扫描过程中的工作量]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">A Practical Dive Into Late Materialization in arrow-rs Parquet Reads</title><link href="https://arrow.apache.org/blog/2025/12/11/parquet-late-materialization-deep-dive/" rel="alternate" type="text/html" title="A Practical Dive Into Late Materialization in arrow-rs Parquet Reads" /><published>2025-12-11T00:00:00-05:00</published><updated>2025-12-11T00:00:00-05:00</updated><id>https://arrow.apache.org/blog/2025/12/11/parquet-late-materialization-deep-dive</id><content type="html" xml:base="https://arrow.apache.org/blog/2025/12/11/parquet-late-materialization-deep-dive/"><![CDATA[<!--

-->
<p>This article dives into the decisions and pitfalls of implementing Late Materialization in the <a href="https://parquet.apache.org/">Apache Parquet</a> reader from <a href="https://github.com/apache/arrow-rs"><code>arrow-rs</code></a> (the reader powering <a href="https://datafusion.apache.org/">Apache DataFusion</a> among other projects). We'll see how a seemingly humble file reader requires complex logic to evaluate predicates—effectively becoming a <strong>tiny query engine</strong> in its own right.</p>
<h2>1. Why Late Materialization?</h2>
<p>Columnar reads are a constant battle between <strong>I/O bandwidth</strong> and <strong>CPU decode costs</strong>. While skipping data is generally good, the act of skipping itself carries a computational cost. The goal of the Parquet reader in <code>arrow-rs</code> is <strong>pipeline-style late materialization</strong>: evaluate predicates first, then access projected columns. For predicates that filter many rows, materializing after evaluation minimizes reads and decode work.</p>
<p>The approach closely mirrors the <strong>LM-pipelined</strong> strategy from <a href="https://www.cs.umd.edu/~abadi/papers/abadiicde2007.pdf">Materialization Strategies in a Column-Oriented DBMS</a> by Abadi et al.: interleaving predicates and data column access instead of reading all columns at once and trying to <strong>stitch them back together</strong> into rows.</p>
<figure style="text-align: center;">
  <img src="/img/late-materialization/fig1.jpg" alt="LM-pipelined late materialization pipeline" width="100%" class="img-responsive">
</figure>
<p>To evaluate a query like <code>SELECT B, C FROM table WHERE A &gt; 10 AND B &lt; 5</code> using late materialization, the reader follows these steps:</p>
<ol>
<li>Read column <code>A</code> and evaluate <code>A &gt; 10</code> to build a <code>RowSelection</code> (a sparse mask) representing the initial set of surviving rows.</li>
<li>Use that <code>RowSelection</code> to read surviving values of column <code>B</code> and evaluate <code>B &lt; 5</code> and update the <code>RowSelection</code> to make it even sparser.</li>
<li>Use the refined <code>RowSelection</code> to read column <code>C</code> (a projection column), decoding only the final surviving rows.</li>
</ol>
<p>The rest of this post zooms in on how the code makes this path work.</p>
<hr />
<h2>2. Late Materialization in the Rust Parquet Reader</h2>
<h3>2.1 LM-pipelined</h3>
<p>&quot;LM-pipelined&quot; might sound like something from a textbook. In <code>arrow-rs</code>, it simply refers to a pipeline that runs sequentially: &quot;read predicate column → generate row selection → read data column&quot;. This contrasts with a <strong>parallel</strong> strategy, where all predicate columns are read simultaneously. While parallelism can maximize multi-core CPU usage, the pipelined approach is often superior in columnar storage because each filtering step drastically reduces the amount of data subsequent steps need to read and parse.</p>
<p>The code is structured into a few core roles:</p>
<ul>
<li><strong><a href="https://github.com/apache/arrow-rs/blob/bab30ae3d61509aa8c73db33010844d440226af2/parquet/src/arrow/arrow_reader/read_plan.rs#L302">ReadPlan</a> / <a href="https://github.com/apache/arrow-rs/blob/bab30ae3d61509aa8c73db33010844d440226af2/parquet/src/arrow/arrow_reader/read_plan.rs#L34">ReadPlanBuilder</a></strong>: Encodes &quot;which columns to read and with what row subset&quot; into a plan. It does not pre-read all predicate columns. It reads one, tightens the selection, and then moves on.</li>
<li><strong><a href="https://github.com/apache/arrow-rs/blob/bab30ae3d61509aa8c73db33010844d440226af2/parquet/src/arrow/arrow_reader/selection.rs#L139">RowSelection</a></strong>: Two implementations: use <a href="https://en.wikipedia.org/wiki/Run-length_encoding">Run-length encoding</a> (RLE) (via <a href="https://github.com/apache/arrow-rs/blob/bab30ae3d61509aa8c73db33010844d440226af2/parquet/src/arrow/arrow_reader/selection.rs#L66"><code>RowSelector</code></a>) to &quot;skip/select N rows&quot;, or use an Arrow <a href="https://github.com/apache/arrow-rs/blob/a67cd19fff65b6c995be9a5eae56845157d95301/arrow-buffer/src/buffer/boolean.rs#L37"><code>BooleanBuffer</code></a> bitmask to filter rows. This is the core mechanism that carries sparsity through the pipeline.</li>
<li><strong><a href="https://github.com/apache/arrow-rs/blob/bab30ae3d61509aa8c73db33010844d440226af2/parquet/src/arrow/array_reader/mod.rs#L85">ArrayReader</a></strong>: Responsible for decoding. It receives a <a href="https://github.com/apache/arrow-rs/blob/ce4edd53203eb4bca96c10ebf3d2118299dad006/parquet/src/arrow/arrow_reader/selection.rs#L139"><code>RowSelection</code></a> and decides which pages to read and which values to decode.</li>
</ul>
<p><a href="https://github.com/apache/arrow-rs/blob/ce4edd53203eb4bca96c10ebf3d2118299dad006/parquet/src/arrow/arrow_reader/selection.rs#L139"><code>RowSelection</code></a> can switch dynamically between RLE and bitmasks. Bitmasks are faster when gaps are tiny and sparsity is high; RLE is friendlier to large, page-level skips. Details on this trade-off appear in section 3.1.</p>
<p>Consider again the query: <code>SELECT B, C FROM table WHERE A &gt; 10 AND B &lt; 5</code>:</p>
<ol>
<li><strong>Initial</strong>: <code>selection = None</code> (equivalent to &quot;select all&quot;).</li>
<li><strong>Read A</strong>: <code>ArrayReader</code> decodes column A in batches; the predicate builds a boolean mask; <a href="https://github.com/apache/arrow-rs/blob/bab30ae3d61509aa8c73db33010844d440226af2/parquet/src/arrow/arrow_reader/selection.rs#L149"><code>RowSelection::from_filters</code></a> turns it into a sparse selection.</li>
<li><strong>Tighten</strong>: <a href="https://github.com/apache/arrow-rs/blob/bab30ae3d61509aa8c73db33010844d440226af2/parquet/src/arrow/arrow_reader/read_plan.rs#L143"><code>ReadPlanBuilder::with_predicate</code></a> chains the new mask via <a href="https://github.com/apache/arrow-rs/blob/bab30ae3d61509aa8c73db33010844d440226af2/parquet/src/arrow/arrow_reader/selection.rs#L345"><code>RowSelection::and_then</code></a>.</li>
<li><strong>Read B</strong>: Build column B's reader with the current <code>selection</code>; the reader only performs I/O and decoding for selected rows, producing an even sparser mask.</li>
<li><strong>Merge</strong>: <code>selection = selection.and_then(selection_b)</code>; projection columns now decode a tiny row set.</li>
</ol>
<p><strong>Code locations and sketch</strong>:</p>
<div class="language-rust highlighter-rouge"><div class="highlight"><pre class="highlight"><code data-lang="rust"><span class="c1">// Close to the flow in read_plan.rs (simplified)</span>
<span class="k">let</span> <span class="k">mut</span> <span class="n">builder</span> <span class="o">=</span> <span class="nn">ReadPlanBuilder</span><span class="p">::</span><span class="nf">new</span><span class="p">(</span><span class="n">batch_size</span><span class="p">);</span>

<span class="c1">// 1) Inject external pruning (e.g., Page Index):</span>
<span class="n">builder</span> <span class="o">=</span> <span class="n">builder</span><span class="nf">.with_selection</span><span class="p">(</span><span class="n">page_index_selection</span><span class="p">);</span>

<span class="c1">// 2) Append predicates serially:</span>
<span class="k">for</span> <span class="n">predicate</span> <span class="k">in</span> <span class="n">predicates</span> <span class="p">{</span>
    <span class="n">builder</span> <span class="o">=</span> <span class="n">builder</span><span class="nf">.with_predicate</span><span class="p">(</span><span class="n">predicate</span><span class="p">);</span> <span class="c1">// internally uses RowSelection::and_then</span>
<span class="p">}</span>

<span class="c1">// 3) Build readers; all ArrayReaders share the final selection strategy</span>
<span class="k">let</span> <span class="n">plan</span> <span class="o">=</span> <span class="n">builder</span><span class="nf">.build</span><span class="p">();</span>
<span class="k">let</span> <span class="n">reader</span> <span class="o">=</span> <span class="nn">ParquetRecordBatchReader</span><span class="p">::</span><span class="nf">new</span><span class="p">(</span><span class="n">array_reader</span><span class="p">,</span> <span class="n">plan</span><span class="p">);</span>
</code></pre></div></div>
<p>I've drawn a simple flowchart that illustrates this flow to help you understand:</p>
<figure style="text-align: center;">
  <img src="/img/late-materialization/fig2.jpg" alt="Predicate-first pipeline flow" width="100%" class="img-responsive">
</figure>
<p>Now that you understand how this pipeline works, the next question is <strong>how to represent and combine these sparse selections</strong> (the <strong>Row Mask</strong> in the diagram), which is where <code>RowSelection</code> comes in.</p>
<h3>2.2 Combining row selectors (<code>RowSelection::and_then</code>)</h3>
<p><a href="https://github.com/apache/arrow-rs/blob/ce4edd53203eb4bca96c10ebf3d2118299dad006/parquet/src/arrow/arrow_reader/selection.rs#L139"><code>RowSelection</code></a> represents the set of rows that will eventually be produced. It currently uses RLE (<code>RowSelector::select/skip(len)</code>) to describe sparse ranges. <a href="https://github.com/apache/arrow-rs/blob/bab30ae3d61509aa8c73db33010844d440226af2/parquet/src/arrow/arrow_reader/selection.rs#L345"><code>RowSelection::and_then</code></a> is the core operator for &quot;apply one selection to another&quot;: the left-hand argument is &quot;rows already passed&quot; and the right-hand argument is &quot;which of the passed rows also pass the second filter.&quot; The output is their boolean AND.</p>
<p><strong>Walkthrough Example</strong>:</p>
<ul>
<li><strong>Input Selection A (already filtered)</strong>: <code>[Skip 100, Select 50, Skip 50]</code> (physical rows 100-150 are selected)</li>
<li><strong>Selection B (filters within A)</strong>: <code>[Select 10, Skip 40]</code> (within the 50 selected rows, only the first 10 survive B)</li>
<li><strong>Result</strong>: <code>[Skip 100, Select 10, Skip 90]</code>.</li>
</ul>
<p><strong>How it runs</strong>:
Think of it like a zipper: we traverse both lists simultaneously, as shown below:</p>
<ol>
<li><strong>First 100 rows</strong>: A is Skip → result is Skip 100.</li>
<li><strong>Next 50 rows</strong>: A is Select. Look at B:
<ul>
<li>B's first 10 are Select → result Select 10.</li>
<li>B's remaining 40 are Skip → result Skip 40.</li>
</ul>
</li>
<li><strong>Final 50 rows</strong>: A is Skip → result Skip 50.</li>
</ol>
<p><strong>Result</strong>: <code>[Skip 100, Select 10, Skip 90]</code>.</p>
<p>Here is an example in code:</p>
<div class="language-rust highlighter-rouge"><div class="highlight"><pre class="highlight"><code data-lang="rust"><span class="c1">// Example: Skip 100 rows, then take the next 10</span>
<span class="k">let</span> <span class="n">a</span><span class="p">:</span> <span class="n">RowSelection</span> <span class="o">=</span> <span class="nd">vec!</span><span class="p">[</span><span class="nn">RowSelector</span><span class="p">::</span><span class="nf">skip</span><span class="p">(</span><span class="mi">100</span><span class="p">),</span> <span class="nn">RowSelector</span><span class="p">::</span><span class="nf">select</span><span class="p">(</span><span class="mi">50</span><span class="p">)]</span><span class="nf">.into</span><span class="p">();</span>
<span class="k">let</span> <span class="n">b</span><span class="p">:</span> <span class="n">RowSelection</span> <span class="o">=</span> <span class="nd">vec!</span><span class="p">[</span><span class="nn">RowSelector</span><span class="p">::</span><span class="nf">select</span><span class="p">(</span><span class="mi">10</span><span class="p">),</span> <span class="nn">RowSelector</span><span class="p">::</span><span class="nf">skip</span><span class="p">(</span><span class="mi">40</span><span class="p">)]</span><span class="nf">.into</span><span class="p">();</span>
<span class="k">let</span> <span class="n">result</span> <span class="o">=</span> <span class="n">a</span><span class="nf">.and_then</span><span class="p">(</span><span class="o">&amp;</span><span class="n">b</span><span class="p">);</span>
<span class="c1">// Result should be: Skip 100, Select 10, Skip 40</span>
<span class="nd">assert_eq!</span><span class="p">(</span>
    <span class="nn">Vec</span><span class="p">::</span><span class="o">&lt;</span><span class="n">RowSelector</span><span class="o">&gt;</span><span class="p">::</span><span class="nf">from</span><span class="p">(</span><span class="n">result</span><span class="p">),</span>
    <span class="nd">vec!</span><span class="p">[</span><span class="nn">RowSelector</span><span class="p">::</span><span class="nf">skip</span><span class="p">(</span><span class="mi">100</span><span class="p">),</span> <span class="nn">RowSelector</span><span class="p">::</span><span class="nf">select</span><span class="p">(</span><span class="mi">10</span><span class="p">),</span> <span class="nn">RowSelector</span><span class="p">::</span><span class="nf">skip</span><span class="p">(</span><span class="mi">40</span><span class="p">)]</span>
<span class="p">);</span>
</code></pre></div></div>
<figure style="text-align: center;">
  <img src="/img/late-materialization/fig3.jpg" alt="RowSelection logical AND walkthrough" width="100%" class="img-responsive">
</figure>
<p>This keeps narrowing the filter while touching only lightweight metadata—no data copies. The current implementation of <code>and_then</code> is a two-pointer linear scan; complexity is linear in the number of selector segments. The more predicates shrink the selection, the cheaper later scans become.</p>
<h2>3. Engineering Challenges</h2>
<p>Late Materialization sounds simple enough in theory, but implementing it in a production-grade system like <code>arrow-rs</code> is an absolute <strong>engineering nightmare</strong>. Historically, these techniques are so tricky they have been locked away in proprietary engines. In the open source world, we've been grinding away at this for years (just look at <a href="https://github.com/apache/datafusion/issues/3463">the DataFusion ticket</a>), and finally, we can <strong>flex our muscles</strong> and go toe-to-toe with full materialization. To pull this off, we had to tackle several serious engineering challenges.</p>
<h3>3.1 Adaptive RowSelection Policy (Bitmask vs. RLE)</h3>
<p>One major hurdle is choosing the right internal representation for <code>RowSelection</code> because the best choice depends on the sparsity pattern. <a href="https://db.cs.cmu.edu/papers/2021/ngom-damon2021.pdf">This paper</a> reveals a critical hurdle: there is no 'one-size-fits-all' format for <code>RowSelection</code>. The researchers found that the optimal internal representation is a moving target, shifting constantly depending on the sparsity pattern—essentially, how 'dense' or 'sparse' the surviving data is at any given moment.</p>
<ul>
<li><strong>Ultra sparse</strong> (e.g., 1 row every 10,000): Using a bitmask here is just wasteful (1 bit per row adds up), whereas RLE is super clean—just a few selectors and you're done.</li>
<li><strong>Sparse but with tiny gaps</strong> (e.g., &quot;read 1, skip 1&quot;): RLE creates a fragmented mess that makes the decoder work overtime; here, bitmasks are way more efficient.</li>
</ul>
<p>Since both have their pros and cons, we decided to get the <strong>best of both worlds</strong> with an adaptive strategy (see <a href="https://github.com/apache/arrow-rs/pull/8733">#arrow-rs/8733</a> for more details):</p>
<ul>
<li>We look at the average run length of the selectors and compare it to a threshold (currently <code>32</code>). If the average is too small, we switch to bitmasks; otherwise, we stick with selectors (RLE).</li>
<li><strong>The Safety Net</strong>: Bitmasks look great until you hit Page Pruning, which can cause a nasty &quot;missing page&quot; panic because the mask might blindly try to filter rows from pages that were never even read. The <code>RowSelection</code> logic watches out for this <strong>recipe for disaster</strong> and forces a switch back to RLE to keep things from crashing (see 3.1.2).</li>
</ul>
<h4>3.1.1 Where did the threshold of <code>32</code> come from?</h4>
<p>The number 32 wasn't just pulled out of thin air. It came from a <a href="https://github.com/apache/arrow-rs/pull/8733#issuecomment-3468441165">data-driven &quot;face-off&quot;</a> using various distributions (even spacing, exponential sparsity, random noise). It does a solid job of distinguishing between &quot;choppy but dense&quot; and &quot;long skip regions.&quot; In the future, we might get even fancier with heuristics based on data types.</p>
<p>The chart below shows an example run from the showdown. Blue lines are <code>read_selector</code> (RLE) and orange lines are <code>read_mask</code> (bitmasks). The vertical axis is time (lower is better), and the horizontal axis is average run length. You can see the performance curves cross around 32.</p>
<figure style="text-align: center;">
  <img src="/img/late-materialization/3.1.1.png" alt="Bitmask vs RLE benchmark threshold" width="100%" class="img-responsive">
</figure>
<h4>3.1.2 The Bitmask Trap: Missing Pages</h4>
<p>When implementing the adaptive strategy, bitmasks seem perfect on paper, but they hide a nasty trap when combined with <strong>Page Pruning</strong>.</p>
<p>Before we get into the weeds, a quick refresher on pages (more in Section 3.2): Parquet files are sliced into pages. If we know a page has no rows in the selection, we <strong>don't even touch it</strong>—no decompression, no decoding. The <code>ArrayReader</code> doesn't even know it exists.</p>
<p><strong>The Scene of the Crime:</strong></p>
<p>Imagine reading a chunk of data and the middle four rows<code>[0,1,2,3,4,5,6]</code>, <code>[1,2,3,4]</code>, are filtered out. It just so happens that two of those rows, <code>[2,3]</code> sit in their own page, so that page gets completely pruned.</p>
<figure style="text-align: center;">
  <img src="/img/late-materialization/3.3.2-fig1.jpg" alt="Page pruning example with only first and last rows kept" width="100%" class="img-responsive">
</figure>
<p>If we use RLE (<code>RowSelector</code>), executing <code>Skip(4)</code> is smooth sailing: we just jump over the gap.</p>
<figure style="text-align: center;">
  <img src="/img/late-materialization/3.3.2-fig3.jpg" alt="RLE skipping pruned pages safely" width="100%" class="img-responsive">
</figure>
<p><strong>The Problem:</strong></p>
<p>If we use a bitmask, however, the reader will decode all 6 rows first, intending to filter them later. But that middle page isn't there! As soon as the decoder hits that gap, it panics. The <code>ArrayReader</code> is a stream processing unit—it doesn't handle I/O and thus doesn't know the layer above decided to prune a page, so it can't see the cliff coming.</p>
<figure style="text-align: center;">
  <img src="/img/late-materialization/3.3.2-fig2.jpg" alt="Bitmask hitting a missing page panic" width="100%" class="img-responsive">
</figure>
<p><strong>The Fix:</strong></p>
<p>Our current solution is conservative but bulletproof: <strong>if we detect Page Pruning, we ban bitmasks and force a fallback to RLE.</strong> In the future, we hope to extend the bitmask logic to be Page Pruning-aware (see <a href="https://github.com/apache/arrow-rs/issues/8845">#arrow-rs/8845</a>).</p>
<div class="language-rust highlighter-rouge"><div class="highlight"><pre class="highlight"><code data-lang="rust"><span class="c1">// Auto prefers bitmask, but... wait, offset_index says page pruning is on.</span>
<span class="k">let</span> <span class="n">policy</span> <span class="o">=</span> <span class="nn">RowSelectionPolicy</span><span class="p">::</span><span class="n">Auto</span> <span class="p">{</span> <span class="n">threshold</span><span class="p">:</span> <span class="mi">32</span> <span class="p">};</span>
<span class="k">let</span> <span class="n">plan_builder</span> <span class="o">=</span> <span class="nn">ReadPlanBuilder</span><span class="p">::</span><span class="nf">new</span><span class="p">(</span><span class="mi">1024</span><span class="p">)</span><span class="nf">.with_row_selection_policy</span><span class="p">(</span><span class="n">policy</span><span class="p">);</span>
<span class="k">let</span> <span class="n">plan_builder</span> <span class="o">=</span> <span class="nf">override_selector_strategy_if_needed</span><span class="p">(</span>
    <span class="n">plan_builder</span><span class="p">,</span>
    <span class="o">&amp;</span><span class="n">projection_mask</span><span class="p">,</span>
    <span class="nf">Some</span><span class="p">(</span><span class="n">offset_index</span><span class="p">),</span> <span class="c1">// page index enables page pruning</span>
<span class="p">);</span>
<span class="c1">// ...so we play it safe and switch to Selectors (RLE).</span>
<span class="nd">assert_eq!</span><span class="p">(</span><span class="n">plan_builder</span><span class="nf">.row_selection_policy</span><span class="p">(),</span> <span class="o">&amp;</span><span class="nn">RowSelectionPolicy</span><span class="p">::</span><span class="n">Selectors</span><span class="p">);</span>
</code></pre></div></div>
<h3>3.2 Page Pruning</h3>
<p>The ultimate performance win is <strong>not doing I/O or decoding at all</strong>. In the real world (especially with object storage), firing off a million tiny read requests is a <strong>performance killer</strong>. <code>arrow-rs</code> uses the Parquet <a href="https://parquet.apache.org/docs/file-format/pageindex/">PageIndex</a> to calculate exactly which pages contain data we actually need. For very selective predicates, skipping pages can result in substantial I/O savings, even if the underlying storage client merges adjacent range requests. Another major win is reduced CPU: <strong>we completely skip the heavy lifting of decompressing and decoding entirely pruned pages.</strong></p>
<ul>
<li><strong>The Catch</strong>: If the <code>RowSelection</code> selects even a <strong>single row</strong> from a page, the whole page must be decompressed. Therefore, the efficiency of this step relies heavily on the correlation between data clustering and the predicates.</li>
<li><strong>Implementation</strong>: <a href="https://github.com/apache/arrow-rs/blob/ce4edd53203eb4bca96c10ebf3d2118299dad006/parquet/src/arrow/arrow_reader/selection.rs#L204"><code>RowSelection::scan_ranges</code></a> crunches the numbers using each page's metadata (<code>first_row_index</code> and <code>compressed_page_size</code>) to figure out which ranges are total skips, returning only the required <code>(offset, length)</code> list.</li>
</ul>
<p>Page skipping is illustrated in the following code example:</p>
<div class="language-rust highlighter-rouge"><div class="highlight"><pre class="highlight"><code data-lang="rust"><span class="c1">// Example: two pages; page0 covers 0..100, page1 covers 100..200</span>
<span class="k">let</span> <span class="n">locations</span> <span class="o">=</span> <span class="nd">vec!</span><span class="p">[</span>
    <span class="n">PageLocation</span> <span class="p">{</span> <span class="n">offset</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="n">compressed_page_size</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span> <span class="n">first_row_index</span><span class="p">:</span> <span class="mi">0</span> <span class="p">},</span>
    <span class="n">PageLocation</span> <span class="p">{</span> <span class="n">offset</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span> <span class="n">compressed_page_size</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span> <span class="n">first_row_index</span><span class="p">:</span> <span class="mi">100</span> <span class="p">},</span>
<span class="p">];</span>
<span class="c1">// RowSelection wants 150..160; page0 is total junk, only read page1</span>
<span class="k">let</span> <span class="n">sel</span><span class="p">:</span> <span class="n">RowSelection</span> <span class="o">=</span> <span class="nd">vec!</span><span class="p">[</span>
    <span class="nn">RowSelector</span><span class="p">::</span><span class="nf">skip</span><span class="p">(</span><span class="mi">150</span><span class="p">),</span>
    <span class="nn">RowSelector</span><span class="p">::</span><span class="nf">select</span><span class="p">(</span><span class="mi">10</span><span class="p">),</span>
    <span class="nn">RowSelector</span><span class="p">::</span><span class="nf">skip</span><span class="p">(</span><span class="mi">40</span><span class="p">),</span>
<span class="p">]</span><span class="nf">.into</span><span class="p">();</span>
<span class="k">let</span> <span class="n">ranges</span> <span class="o">=</span> <span class="n">sel</span><span class="nf">.scan_ranges</span><span class="p">(</span><span class="o">&amp;</span><span class="n">locations</span><span class="p">);</span>
<span class="nd">assert_eq!</span><span class="p">(</span><span class="n">ranges</span><span class="nf">.len</span><span class="p">(),</span> <span class="mi">1</span><span class="p">);</span> <span class="c1">// Only request page1</span>
</code></pre></div></div>
<p>The following figure illustrates page skipping with RLE selections. The
first page is neither read nor decoded, as no rows are selected. The second page
is read and fully decompressed (e.g., zstd), and then only the needed rows are decoded.
The third page is decompressed and decoded in full, as all rows are selected.</p>
<figure style="text-align: center;">
  <img src="/img/late-materialization/fig4.jpg" alt="Page-level scan range calculation" width="100%" class="img-responsive">
</figure>
<p>This mechanism acts as the bridge between logical row filtering and physical byte fetching. While we cannot slice the file thinner than a single page (due to compression boundaries), Page Pruning ensures that we never pay the decompression cost for a page unless it contributes at least one row to the result. It strikes a pragmatic balance: utilizing the coarse-grained Page Index to skip large swathes of data, while leaving the fine-grained <code>RowSelection</code> to handle the specific rows within the surviving pages.</p>
<h3>3.3 Smart Caching</h3>
<p>Late materialization introduces a structural Catch-22: to efficiently skip data, we must first read it. Consider a query like <code>SELECT A FROM table WHERE A &gt; 10</code>. The reader must decode column <code>A</code> to evaluate the filter. In a traditional &quot;read-everything&quot; approach, this wouldn't be an issue: column A would simply sit in memory, waiting to be projected. However, in a strict pipeline, the &quot;Predicate&quot; stage and the &quot;Projection&quot; stage are decoupled. Once the filter produces a RowSelection, the projection stage sees that it needs column <code>A</code> and triggers a second read of the same data.</p>
<p>Without intervention, we pay a &quot;double tax&quot;: decoding once to decide what to keep, and decoding again to actually keep it.<a href="https://github.com/apache/arrow-rs/blob/ce4edd53203eb4bca96c10ebf3d2118299dad006/parquet/src/arrow/array_reader/cached_array_reader.rs#L40-L68"><code>CachedArrayReader</code></a>, introduced in <a href="https://github.com/apache/arrow-rs/pull/7850">#arrow-rs/7850</a>, solves this dilemma using a <strong>Dual-Layer</strong> Cache architecture. It allows us to stash the decoded batch the first time we see it (during filtering) and reuse it later (during projection).</p>
<p>But why two layers? Why not just one big cache?</p>
<ul>
<li><strong>The Shared Cache (Optimistic Reuse):</strong> This is a global cache shared across all columns and readers. It has a user-configurable memory limit (capacity). When a page is decoded for a predicate, it is placed here. If the projection step runs soon after, it can &quot;hit&quot; this cache and avoid I/O. However, because memory is finite, <strong>cache eviction</strong> can happen at any moment. If we relied solely on this, a heavy workload could evict our data right before we need it again.</li>
<li><strong>The Local Cache (Deterministic Guarantee):</strong> This is a private cache specific to a single column's reader. It acts as a <strong>safety net</strong>. When a column is being actively read, the data is &quot;pinned&quot; in the Local Cache. This guarantees that the data remains available for the duration of the current operation, immune to eviction from the global Shared Cache.</li>
</ul>
<p>The reader follows a strict hierarchy when fetching a page:</p>
<ol>
<li><strong>Check Local:</strong> Do I already have it pinned?</li>
<li><strong>Check Shared:</strong> Did another part of the pipeline decode this recently? If yes, <strong>promote</strong> it to Local (pin it).</li>
<li><strong>Read from Source:</strong> Perform the I/O and decoding, then insert into both Local and Shared.</li>
</ol>
<p>This dual strategy gives us the best of both worlds: the <strong>efficiency</strong> of sharing data between filter and projection steps, and the <strong>stability</strong> of knowing that necessary data won't vanish mid-query due to memory pressure.</p>
<h3>3.4 Minimizing Copies and Allocations</h3>
<p>Another area where arrow-rs has significant optimization is <strong>avoiding unnecessary copies</strong>. Rust's <a href="https://doc.rust-lang.org/book/ch04-01-what-is-ownership.html">memory safe</a> design makes it easy to copy, but every extra allocation wastes CPU cycles and memory bandwidth. A naive implementation often pays an <strong>&quot;unnecessary tax&quot;</strong> by decompressing data into a temporary <code>Vec</code> and then <code>memcpy</code>-ing it into an Arrow Buffer.</p>
<p>For fixed-width types (like integers or floats), this is completely redundant because their memory layouts are identical. <a href="https://github.com/apache/arrow-rs/blob/ce4edd53203eb4bca96c10ebf3d2118299dad006/parquet/src/arrow/array_reader/primitive_array.rs#L102"><code>PrimitiveArrayReader</code></a> eliminates this overhead via <a href="https://docs.rs/arrow/latest/arrow/array/struct.PrimitiveArray.html#example-from-a-vec">zero-copy conversions</a>: instead of copying bytes, it simply <strong>hands over ownership</strong> of the decoded <code>Vec&lt;T&gt;</code> directly to the underlying Arrow <code>Buffer</code>.</p>
<h3>3.5 The Alignment Gauntlet</h3>
<p>Chained filtering is a <strong>hair-pulling</strong> exercise in coordinate systems. &quot;Row 1&quot; in filter N might actually be &quot;Row 10,001&quot; in the file due to prior filters.</p>
<ul>
<li><strong>How do we keep the train on the rails?</strong>: We <a href="https://github.com/apache/arrow-rs/blob/ce4edd53203eb4bca96c10ebf3d2118299dad006/parquet/src/arrow/arrow_reader/selection.rs#L1309">fuzz test</a> every <code>RowSelection</code> operation (<code>split_off</code>, <code>and_then</code>, <code>trim</code>). We need absolute certainty that our translation between relative and absolute offsets is pixel-perfect. This correctness is the bedrock that keeps the Reader stable under the triple threat of batch boundaries, sparse selections, and page pruning.</li>
</ul>
<h2>4. Conclusion</h2>
<p>The Parquet reader in <code>arrow-rs</code> isn't just a humble file reader—it's a <strong>mini query engine</strong> in disguise. We've baked in high-end features like predicate pushdown and late materialization. The reader reads only what's needed and decodes only what's necessary, saving resources while maintaining correctness. Previously, these features were restricted to proprietary or tightly integrated systems. Now, thanks to the community's efforts, <code>arrow-rs</code> brings the benefits of advanced query processing techniques to even lightweight applications.</p>
<p>We invite you to <a href="https://github.com/apache/arrow-rs?tab=readme-ov-file#arrow-rust-community">join the community</a>, explore the code, experiment with it, and contribute to its ongoing evolution. The journey of optimizing data access is never-ending, and together, we can push the boundaries of what's possible in open-source data processing.</p>]]></content><author><name>&lt;a href=&quot;https://github.com/hhhizzz&quot;&gt;Qiwei Huang&lt;/a&gt; and &lt;a href=&quot;https://github.com/alamb&quot;&gt;Andrew Lamb&lt;/a&gt;</name></author><category term="application" /><summary type="html"><![CDATA[How arrow-rs pipelines predicates and projections to minimize work during Parquet scans]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Apache Arrow ADBC 21 (Libraries) Release</title><link href="https://arrow.apache.org/blog/2025/11/07/adbc-21-release/" rel="alternate" type="text/html" title="Apache Arrow ADBC 21 (Libraries) Release" /><published>2025-11-07T00:00:00-05:00</published><updated>2025-11-07T00:00:00-05:00</updated><id>https://arrow.apache.org/blog/2025/11/07/adbc-21-release</id><content type="html" xml:base="https://arrow.apache.org/blog/2025/11/07/adbc-21-release/"><![CDATA[<!--

-->
<p>The Apache Arrow team is pleased to announce the version 21 release of
the Apache Arrow ADBC libraries. This release includes <a href="https://github.com/apache/arrow-adbc/milestone/25"><strong>30
resolved issues</strong></a> from <a href="#contributors"><strong>23 distinct contributors</strong></a>.</p>
<p>This is a release of the <strong>libraries</strong>, which are at version 21.  The
<a href="https://arrow.apache.org/adbc/21/format/specification.html"><strong>API specification</strong></a> is versioned separately and is at
version 1.1.0.</p>
<p>The subcomponents are versioned independently:</p>
<ul>
<li>C/C++/GLib/Go/Python/Ruby: 1.9.0</li>
<li>C#: 0.21.0</li>
<li>Java: 0.21.0</li>
<li>R: 0.21.0</li>
<li>Rust: 0.21.0</li>
</ul>
<p>The release notes below are not exhaustive and only expose selected
highlights of the release. Many other bugfixes and improvements have
been made: we refer you to the <a href="https://github.com/apache/arrow-adbc/blob/apache-arrow-adbc-21/CHANGELOG.md">complete changelog</a>.</p>
<h2>Release Highlights</h2>
<p>Language bindings:</p>
<ul>
<li>The driver manager reports better errors when it fails to find a driver (<a href="https://github.com/apache/arrow-adbc/pull/3646">#3646</a>).</li>
<li>The Python driver manager now searches for manifests in the proper location when inside a Python virtual environment (<a href="https://github.com/apache/arrow-adbc/pull/3490">#3490</a>).</li>
<li>Added convenience methods to the driver manager on top of the standard DBAPI-2.0/PEP249 APIs (<a href="https://github.com/apache/arrow-adbc/pull/3539">#3539</a>).</li>
<li>The signature for <code>connect</code> has been simplified, so you can connect without having to repeat the driver name or specify explicit keyword arguments depending on the driver (<a href="https://github.com/apache/arrow-adbc/pull/3537">#3537</a>).</li>
<li>Support for Python 3.9 has been dropped (<a href="https://github.com/apache/arrow-adbc/pull/3573">#3573</a>, <a href="https://github.com/apache/arrow-adbc/pull/3663">#3663</a>).</li>
<li>Support for Python 3.14 (including the free-threading variant) has been added (<a href="https://github.com/apache/arrow-adbc/pull/3575">#3575</a>, <a href="https://github.com/apache/arrow-adbc/pull/3620">#3620</a>, <a href="https://github.com/apache/arrow-adbc/pull/3663">#3663</a>).</li>
<li>The R bindings now support <code>replace</code> and <code>create_append</code> ingest modes.</li>
</ul>
<p>Drivers:</p>
<ul>
<li>The Go BigQuery driver now supports service account impersonation (<a href="https://github.com/apache/arrow-adbc/pull/3488">#3488</a>) and setting a quota project (<a href="https://github.com/apache/arrow-adbc/pull/3622">#3622</a>).</li>
<li>The Go BigQuery driver now returns more detailed type metadata in result sets (<a href="https://github.com/apache/arrow-adbc/pull/3604">#3604</a>).</li>
<li>The C# BigQuery driver allows setting a location (<a href="https://github.com/apache/arrow-adbc/pull/3494">#3494</a>).</li>
<li>The C# Databricks driver adjusted default settings to make small queries faster (<a href="https://github.com/apache/arrow-adbc/pull/3489">#3489</a>).</li>
<li>Memory usage of the C# Databricks driver was improved (<a href="https://github.com/apache/arrow-adbc/pull/3652">#3652</a>, <a href="https://github.com/apache/arrow-adbc/pull/3656">#3656</a>).</li>
<li>All C# HiveServer2-based drivers (Hive, Impala, Spark, Databricks) throw Unauthorized exceptions when appropriate (<a href="https://github.com/apache/arrow-adbc/pull/3551">#3551</a>).</li>
<li>JNI bindings to the C++ driver manager are now released, making it possible to use non-Java drivers from a Java application (in a very limited fashion) (<a href="https://github.com/apache/arrow-adbc/pull/3429">#3429</a>).  Binary artifacts are available for amd64/arm64 Linux, arm64 macOS, and amd64 Windows.</li>
<li>The PostgreSQL driver can now return the schema of any bind parameters in a prepared query ([#3579[(https://github.com/apache/arrow-adbc/pull/3579)]]).</li>
<li>The PostgreSQL driver properly batches result sets with large string/binary values now (<a href="https://github.com/apache/arrow-adbc/pull/3616">#3616</a>).</li>
<li>The Snowflake driver now returns float64 for numeric columns when use_high_precision is false and scale is nonzero (<a href="https://github.com/apache/arrow-adbc/pull/3295">#3295</a>).  (Previously it incorrectly truncated to int64.)</li>
<li>Added an option to disable the &quot;vectorized&quot; scanner when ingesting data into Snowflake, which sometimes appeared to cause performance issues (<a href="https://github.com/apache/arrow-adbc/pull/3555">#3555</a>).</li>
</ul>
<p>Packaging:</p>
<ul>
<li>Added AlmaLinux 10 (<a href="https://github.com/apache/arrow-adbc/pull/3514">#3514</a>).</li>
<li>Added Debian Trixie (<a href="https://github.com/apache/arrow-adbc/pull/3513">#3513</a>).</li>
</ul>
<h2>Contributors</h2>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ git shortlog --perl-regexp --author='^((?!dependabot\[bot\]).*)$' -sn apache-arrow-adbc-20..apache-arrow-adbc-21
    28	David Li
     9	Bruce Irschick
     7	eric-wang-1990
     6	Curt Hagenlocher
     5	eitsupi
     4	msrathore-db
     3	Bryce Mecum
     3	Mandukhai Alimaa
     3	Sutou Kouhei
     3	davidhcoe
     2	Anna Lee
     2	Jason Lin
     2	Kevin Liu
     1	Dewey Dunnington
     1	Ian Cook
     1	Jacky Hu
     1	Jade Wang
     1	Kristin Cowalcijk
     1	Lucas Valente
     1	Matthijs Brobbel
     1	bruceNu1l
     1	praveentandra
     1	rnowacoski
</code></pre></div></div>
<h2>Roadmap</h2>
<p>We are starting work on async interfaces and other API enhancements, and
welcome comments or contributions from anyone interested.  See the initial
pull requests:</p>
<ul>
<li><a href="https://github.com/apache/arrow-adbc/pull/3607">https://github.com/apache/arrow-adbc/pull/3607</a></li>
<li><a href="https://github.com/apache/arrow-adbc/pull/3623">https://github.com/apache/arrow-adbc/pull/3623</a></li>
</ul>
<h2>Getting Involved</h2>
<p>We welcome questions and contributions from all interested.  Issues
can be filed on <a href="https://github.com/apache/arrow-adbc/issues">GitHub</a>, and questions can be directed to GitHub
or the <a href="/community/">Arrow mailing lists</a>.</p>]]></content><author><name>pmc</name></author><category term="release" /><summary type="html"><![CDATA[The Apache Arrow team is pleased to announce the version 21 release of the Apache Arrow ADBC libraries. This release includes 30 resolved issues from 23 distinct contributors. This is a release of the libraries, which are at version 21. The API specification is versioned separately and is at version 1.1.0. The subcomponents are versioned independently: C/C++/GLib/Go/Python/Ruby: 1.9.0 C#: 0.21.0 Java: 0.21.0 R: 0.21.0 Rust: 0.21.0 The release notes below are not exhaustive and only expose selected highlights of the release. Many other bugfixes and improvements have been made: we refer you to the complete changelog. Release Highlights Language bindings: The driver manager reports better errors when it fails to find a driver (#3646). The Python driver manager now searches for manifests in the proper location when inside a Python virtual environment (#3490). Added convenience methods to the driver manager on top of the standard DBAPI-2.0/PEP249 APIs (#3539). The signature for connect has been simplified, so you can connect without having to repeat the driver name or specify explicit keyword arguments depending on the driver (#3537). Support for Python 3.9 has been dropped (#3573, #3663). Support for Python 3.14 (including the free-threading variant) has been added (#3575, #3620, #3663). The R bindings now support replace and create_append ingest modes. Drivers: The Go BigQuery driver now supports service account impersonation (#3488) and setting a quota project (#3622). The Go BigQuery driver now returns more detailed type metadata in result sets (#3604). The C# BigQuery driver allows setting a location (#3494). The C# Databricks driver adjusted default settings to make small queries faster (#3489). Memory usage of the C# Databricks driver was improved (#3652, #3656). All C# HiveServer2-based drivers (Hive, Impala, Spark, Databricks) throw Unauthorized exceptions when appropriate (#3551). JNI bindings to the C++ driver manager are now released, making it possible to use non-Java drivers from a Java application (in a very limited fashion) (#3429). Binary artifacts are available for amd64/arm64 Linux, arm64 macOS, and amd64 Windows. The PostgreSQL driver can now return the schema of any bind parameters in a prepared query ([#3579[(https://github.com/apache/arrow-adbc/pull/3579)]]). The PostgreSQL driver properly batches result sets with large string/binary values now (#3616). The Snowflake driver now returns float64 for numeric columns when use_high_precision is false and scale is nonzero (#3295). (Previously it incorrectly truncated to int64.) Added an option to disable the &quot;vectorized&quot; scanner when ingesting data into Snowflake, which sometimes appeared to cause performance issues (#3555). Packaging: Added AlmaLinux 10 (#3514). Added Debian Trixie (#3513). Contributors $ git shortlog --perl-regexp --author='^((?!dependabot\[bot\]).*)$' -sn apache-arrow-adbc-20..apache-arrow-adbc-21 28 David Li 9 Bruce Irschick 7 eric-wang-1990 6 Curt Hagenlocher 5 eitsupi 4 msrathore-db 3 Bryce Mecum 3 Mandukhai Alimaa 3 Sutou Kouhei 3 davidhcoe 2 Anna Lee 2 Jason Lin 2 Kevin Liu 1 Dewey Dunnington 1 Ian Cook 1 Jacky Hu 1 Jade Wang 1 Kristin Cowalcijk 1 Lucas Valente 1 Matthijs Brobbel 1 bruceNu1l 1 praveentandra 1 rnowacoski Roadmap We are starting work on async interfaces and other API enhancements, and welcome comments or contributions from anyone interested. See the initial pull requests: https://github.com/apache/arrow-adbc/pull/3607 https://github.com/apache/arrow-adbc/pull/3623 Getting Involved We welcome questions and contributions from all interested. Issues can be filed on GitHub, and questions can be directed to GitHub or the Arrow mailing lists.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Apache Arrow Rust 57.0.0 Release</title><link href="https://arrow.apache.org/blog/2025/10/30/arrow-rs-57.0.0/" rel="alternate" type="text/html" title="Apache Arrow Rust 57.0.0 Release" /><published>2025-10-30T00:00:00-04:00</published><updated>2025-10-30T00:00:00-04:00</updated><id>https://arrow.apache.org/blog/2025/10/30/arrow-rs-57.0.0</id><content type="html" xml:base="https://arrow.apache.org/blog/2025/10/30/arrow-rs-57.0.0/"><![CDATA[<!--

-->
<p>The Apache Arrow team is pleased to announce that the v57.0.0 release of Apache Arrow
Rust is now available on crates.io (<a href="https://crates.io/crates/arrow">arrow</a> and <a href="https://crates.io/crates/parquet">parquet</a>) and as <a href="https://dist.apache.org/repos/dist/release/arrow/arrow-rs-57.0.0">source download</a>.</p>
<p>See the <a href="https://github.com/apache/arrow-rs/blob/57.0.0/CHANGELOG.md">57.0.0 changelog</a> for a full list of changes.</p>
<h2>New Features</h2>
<p>Note: Arrow Rust hosts the development of the <a href="https://crates.io/crates/parquet">parquet</a> crate, a high
performance Rust implementation of <a href="https://parquet.apache.org/">Apache Parquet</a>.</p>
<h3>Performance: 4x Faster Parquet Metadata Parsing 🚀</h3>
<p>Ed Seidl (<a href="https://github.com/etseidl">@etseidl</a>) and Jörn Horstmann (<a href="https://github.com/jhorstmann">@jhorstmann</a>) contributed a rewritten
thrift metadata parser for Parquet files which is almost 4x faster than the
previous parser based on the <code>thrift</code> crate. This is especially exciting for low
latency use cases and reading Parquet files with large amounts of metadata (e.g.
many row groups or columns).
See the <a href="https://arrow.apache.org/blog/2025/10/23/rust-parquet-metadata/">blog post about the new Parquet metadata parser</a> for more details.</p>
<div style="display: flex; gap: 16px; justify-content: center; align-items: flex-start;">
  <img src="/img/rust-parquet-metadata/results.png" width="100%" class="img-responsive" alt="" aria-hidden="true">
</div>
<p><em>Figure 1:</em> Performance improvements of <a href="https://parquet.apache.org/">Apache Parquet</a> metadata parsing between version <code>56.2.0</code> and <code>57.0.0</code>.</p>
<h3>New <code>arrow-avro</code> Crate</h3>
<p>The <code>57.0.0</code> release introduces a new <a href="https://crates.io/crates/arrow-avro"><code>arrow-avro</code></a> crate contributed by <a href="https://github.com/jecsand838">@jecsand838</a>
and <a href="https://github.com/nathaniel-d-ef">@nathaniel-d-ef</a> that provides much more efficient conversion between
<a href="https://avro.apache.org/">Apache Avro</a> and Arrow <code>RecordBatch</code>es, as well as broader feature support.</p>
<p>Previously, Arrow‑based systems that read or wrote Avro data
typically used the general‑purpose <a href="https://crates.io/crates/apache-avro">apache-avro</a> crate. While mature and
feature‑complete, its row-oriented API does not support features such as
projection pushdown or vectorized execution. The new <code>arrow-avro</code> crate supports
these features efficiently by converting Avro data directly into Arrow's
columnar format.</p>
<p>See the <a href="https://arrow.apache.org/blog/2025/10/23/introducing-arrow-avro/">blog post about adding arrow-avro</a> for more details.</p>
<div style="display: flex; gap: 16px; justify-content: center; align-items: flex-start; padding: 20px 15px;">
<img src="/img/introducing-arrow-avro/arrow-avro-architecture.svg"
        width="100%"
        alt="High-level `arrow-avro` architecture"
        style="background:#fff">
</div>
<p><em>Figure 2:</em> Architecture of the <code>arrow-avro</code> crate.</p>
<h3>Parquet Variant Support 🧬</h3>
<p>The Apache Parquet project recently added a <a href="https://github.com/apache/parquet-format/blob/master/VariantEncoding.md">new <code>Variant</code> type</a> for
representing semi-structured data. The <code>57.0.0</code> release includes support for reading and
writing both normal and shredded <code>Variant</code> values to and from Parquet files. It
also includes <a href="https://crates.io/crates/parquet-variant">parquet-variant</a>, a complete library for working with <code>Variant</code>
values, <a href="https://docs.rs/parquet/latest/parquet/variant/struct.VariantArray.html"><code>VariantArray</code></a> for working with arrays of <code>Variant</code> values in Apache
Arrow, computation kernels for converting to/from JSON and Arrow types,
extracting paths, and shredding values.</p>
<div class="language-rust highlighter-rouge"><div class="highlight"><pre class="highlight"><code data-lang="rust"> <span class="c1">// Use the VariantArrayBuilder to build a VariantArray</span>
<span class="k">let</span> <span class="k">mut</span> <span class="n">builder</span> <span class="o">=</span> <span class="nn">VariantArrayBuilder</span><span class="p">::</span><span class="nf">new</span><span class="p">(</span><span class="mi">3</span><span class="p">);</span>
<span class="n">builder</span><span class="nf">.new_object</span><span class="p">()</span><span class="nf">.with_field</span><span class="p">(</span><span class="s">"name"</span><span class="p">,</span> <span class="s">"Alice"</span><span class="p">)</span><span class="nf">.finish</span><span class="p">();</span> <span class="c1">// row 1: {"name": "Alice"}</span>
<span class="n">builder</span><span class="nf">.append_value</span><span class="p">(</span><span class="s">"such wow"</span><span class="p">);</span> <span class="c1">// row 2: "such wow" (a string)</span>
<span class="k">let</span> <span class="n">array</span> <span class="o">=</span> <span class="n">builder</span><span class="nf">.build</span><span class="p">();</span>

<span class="c1">// Since VariantArray is an ExtensionType, it needs to be converted</span>
<span class="c1">// to an ArrayRef and Field with the appropriate metadata</span>
<span class="c1">// before it can be written to a Parquet file</span>
<span class="k">let</span> <span class="n">field</span> <span class="o">=</span> <span class="n">array</span><span class="nf">.field</span><span class="p">(</span><span class="s">"data"</span><span class="p">);</span>
<span class="k">let</span> <span class="n">array</span> <span class="o">=</span> <span class="nn">ArrayRef</span><span class="p">::</span><span class="nf">from</span><span class="p">(</span><span class="n">array</span><span class="p">);</span>
<span class="c1">// create a RecordBatch with the VariantArray</span>
<span class="k">let</span> <span class="n">schema</span> <span class="o">=</span> <span class="nn">Schema</span><span class="p">::</span><span class="nf">new</span><span class="p">(</span><span class="nd">vec!</span><span class="p">[</span><span class="n">field</span><span class="p">]);</span>
<span class="k">let</span> <span class="n">batch</span> <span class="o">=</span> <span class="nn">RecordBatch</span><span class="p">::</span><span class="nf">try_new</span><span class="p">(</span><span class="nn">Arc</span><span class="p">::</span><span class="nf">new</span><span class="p">(</span><span class="n">schema</span><span class="p">),</span> <span class="nd">vec!</span><span class="p">[</span><span class="n">array</span><span class="p">])</span><span class="o">?</span><span class="p">;</span>

<span class="c1">// Now you can write the RecordBatch to the Parquet file, as normal</span>
<span class="k">let</span> <span class="n">file</span> <span class="o">=</span> <span class="nn">std</span><span class="p">::</span><span class="nn">fs</span><span class="p">::</span><span class="nn">File</span><span class="p">::</span><span class="nf">create</span><span class="p">(</span><span class="s">"variant.parquet"</span><span class="p">)</span><span class="o">?</span><span class="p">;</span>
<span class="k">let</span> <span class="k">mut</span> <span class="n">writer</span> <span class="o">=</span> <span class="nn">ArrowWriter</span><span class="p">::</span><span class="nf">try_new</span><span class="p">(</span><span class="n">file</span><span class="p">,</span> <span class="n">batch</span><span class="nf">.schema</span><span class="p">(),</span> <span class="nb">None</span><span class="p">)</span><span class="o">?</span><span class="p">;</span>
<span class="n">writer</span><span class="nf">.write</span><span class="p">(</span><span class="o">&amp;</span><span class="n">batch</span><span class="p">)</span><span class="o">?</span><span class="p">;</span>
<span class="n">writer</span><span class="nf">.close</span><span class="p">()</span><span class="o">?</span><span class="p">;</span>
</code></pre></div></div>
<p>This support is being integrated into query engines, such as
<a href="https://github.com/friendlymatthew">@friendlymatthew</a>'s <a href="https://github.com/datafusion-contrib/datafusion-variant"><code>datafusion-variant</code></a> crate to integrate into DataFusion
and <a href="https://github.com/delta-io/delta-rs/issues/3637">delta-rs</a>. While this support is still experimental, we believe the APIs
are mostly complete and do not expect major changes. Please consider trying
it out and providing feedback and improvements.</p>
<p>Thanks to the many contributors who made this possible, including:</p>
<ul>
<li>Ryan Johnson (<a href="https://github.com/scovich">@scovich</a>), Congxian Qiu (<a href="https://github.com/klion26">@klion26</a>), and Liam Bao (<a href="https://github.com/liamzwbao">@liamzwbao</a>) for completing the implementation</li>
<li>Li Jiaying (<a href="https://github.com/PinkCrow007">@PinkCrow007</a>), Aditya Bhatnagar (<a href="https://github.com/carpecodeum">@carpecodeum</a>), and Malthe Karbo (<a href="https://github.com/mkarbo">@mkarbo</a>) for
initiating the work</li>
<li>Everyone else who has contributed, including <a href="https://github.com/superserious-dev">@superserious-dev</a>, <a href="https://github.com/friendlymatthew">@friendlymatthew</a>, <a href="https://github.com/micoo227">@micoo227</a>, <a href="https://github.com/Weijun-H">@Weijun-H</a>,
<a href="https://github.com/harshmotw-db">@harshmotw-db</a>, <a href="https://github.com/odysa">@odysa</a>, <a href="https://github.com/viirya">@viirya</a>, <a href="https://github.com/adriangb">@adriangb</a>, <a href="https://github.com/kosiew">@kosiew</a>, <a href="https://github.com/codephage2020">@codephage2020</a>,
<a href="https://github.com/ding-young">@ding-young</a>, <a href="https://github.com/mbrobbel">@mbrobbel</a>, <a href="https://github.com/petern48">@petern48</a>, <a href="https://github.com/sdf-jkl">@sdf-jkl</a>, <a href="https://github.com/abacef">@abacef</a>, and <a href="https://github.com/mprammer">@mprammer</a>.</li>
</ul>
<p>See the ticket <a href="https://github.com/apache/arrow-rs/issues/6736">Variant type support in Parquet #6736</a> for more details</p>
<h3>Parquet Geometry Support 🗺️</h3>
<p>The <code>57.0.0</code> release also includes support for reading and writing <a href="https://github.com/apache/parquet-format/blob/master/Geospatial.md">Parquet Geometry
types</a>, <code>GEOMETRY</code> and <code>GEOGRAPHY</code>, including <code>GeospatialStatistics</code>
contributed by Kyle Barron (<a href="https://github.com/kylebarron">@kylebarron</a>), Dewey Dunnington (<a href="https://github.com/paleolimbot">@paleolimbot</a>),
Kaushik Srinivasan (<a href="https://github.com/kaushiksrini">@kaushiksrini</a>), and Blake Orth (<a href="https://github.com/BlakeOrth">@BlakeOrth</a>).</p>
<p>Please see the <a href="https://github.com/apache/arrow-rs/issues/8373">Implement Geometry and Geography type support in Parquet</a> tracking ticket for more details.</p>
<h2>Thanks to Our Contributors</h2>
<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code data-lang="console"><span class="gp">$</span><span class="w"> </span>git shortlog <span class="nt">-sn</span> 56.0.0..57.0.0
<span class="go">    36  Matthijs Brobbel
    20  Andrew Lamb
    13  Ryan Johnson
    11  Ed Seidl
    10  Connor Sanders
     8  Alex Huang
     5  Emil Ernerfeldt
     5  Liam Bao
     5  Matthew Kim
     4  nathaniel-d-ef
     3  Raz Luvaton
     3  albertlockett
     3  dependabot[bot]
     3  mwish
     2  Ben Ye
     2  Congxian Qiu
     2  Dewey Dunnington
     2  Kyle Barron
     2  Lilian Maurel
     2  Mark Nash
     2  Nuno Faria
     2  Pepijn Van Eeckhoudt
     2  Tobias Schwarzinger
     2  lichuang
     1  Adam Gutglick
     1  Adam Reeve
     1  Alex Stephen
     1  Chen Chongchen
     1  Jack
     1  Jeffrey Vo
     1  Jörn Horstmann
     1  Kaushik Srinivasan
     1  Li Jiaying
     1  Lin Yihai
     1  Marco Neumann
     1  Piotr Findeisen
     1  Piotr Srebrny
     1  Samuele Resca
     1  Van De Bio
     1  Yan Tingwang
     1  ding-young
     1  kosiew
     1  张林伟
</span></code></pre></div></div>]]></content><author><name>pmc</name></author><category term="release" /><summary type="html"><![CDATA[The Apache Arrow team is pleased to announce that the v57.0.0 release of Apache Arrow Rust is now available on crates.io (arrow and parquet) and as source download. See the 57.0.0 changelog for a full list of changes. New Features Note: Arrow Rust hosts the development of the parquet crate, a high performance Rust implementation of Apache Parquet. Performance: 4x Faster Parquet Metadata Parsing 🚀 Ed Seidl (@etseidl) and Jörn Horstmann (@jhorstmann) contributed a rewritten thrift metadata parser for Parquet files which is almost 4x faster than the previous parser based on the thrift crate. This is especially exciting for low latency use cases and reading Parquet files with large amounts of metadata (e.g. many row groups or columns). See the blog post about the new Parquet metadata parser for more details. Figure 1: Performance improvements of Apache Parquet metadata parsing between version 56.2.0 and 57.0.0. New arrow-avro Crate The 57.0.0 release introduces a new arrow-avro crate contributed by @jecsand838 and @nathaniel-d-ef that provides much more efficient conversion between Apache Avro and Arrow RecordBatches, as well as broader feature support. Previously, Arrow‑based systems that read or wrote Avro data typically used the general‑purpose apache-avro crate. While mature and feature‑complete, its row-oriented API does not support features such as projection pushdown or vectorized execution. The new arrow-avro crate supports these features efficiently by converting Avro data directly into Arrow's columnar format. See the blog post about adding arrow-avro for more details. Figure 2: Architecture of the arrow-avro crate. Parquet Variant Support 🧬 The Apache Parquet project recently added a new Variant type for representing semi-structured data. The 57.0.0 release includes support for reading and writing both normal and shredded Variant values to and from Parquet files. It also includes parquet-variant, a complete library for working with Variant values, VariantArray for working with arrays of Variant values in Apache Arrow, computation kernels for converting to/from JSON and Arrow types, extracting paths, and shredding values. // Use the VariantArrayBuilder to build a VariantArray let mut builder = VariantArrayBuilder::new(3); builder.new_object().with_field("name", "Alice").finish(); // row 1: {"name": "Alice"} builder.append_value("such wow"); // row 2: "such wow" (a string) let array = builder.build(); // Since VariantArray is an ExtensionType, it needs to be converted // to an ArrayRef and Field with the appropriate metadata // before it can be written to a Parquet file let field = array.field("data"); let array = ArrayRef::from(array); // create a RecordBatch with the VariantArray let schema = Schema::new(vec![field]); let batch = RecordBatch::try_new(Arc::new(schema), vec![array])?; // Now you can write the RecordBatch to the Parquet file, as normal let file = std::fs::File::create("variant.parquet")?; let mut writer = ArrowWriter::try_new(file, batch.schema(), None)?; writer.write(&amp;batch)?; writer.close()?; This support is being integrated into query engines, such as @friendlymatthew's datafusion-variant crate to integrate into DataFusion and delta-rs. While this support is still experimental, we believe the APIs are mostly complete and do not expect major changes. Please consider trying it out and providing feedback and improvements. Thanks to the many contributors who made this possible, including: Ryan Johnson (@scovich), Congxian Qiu (@klion26), and Liam Bao (@liamzwbao) for completing the implementation Li Jiaying (@PinkCrow007), Aditya Bhatnagar (@carpecodeum), and Malthe Karbo (@mkarbo) for initiating the work Everyone else who has contributed, including @superserious-dev, @friendlymatthew, @micoo227, @Weijun-H, @harshmotw-db, @odysa, @viirya, @adriangb, @kosiew, @codephage2020, @ding-young, @mbrobbel, @petern48, @sdf-jkl, @abacef, and @mprammer. See the ticket Variant type support in Parquet #6736 for more details Parquet Geometry Support 🗺️ The 57.0.0 release also includes support for reading and writing Parquet Geometry types, GEOMETRY and GEOGRAPHY, including GeospatialStatistics contributed by Kyle Barron (@kylebarron), Dewey Dunnington (@paleolimbot), Kaushik Srinivasan (@kaushiksrini), and Blake Orth (@BlakeOrth). Please see the Implement Geometry and Geography type support in Parquet tracking ticket for more details. Thanks to Our Contributors $ git shortlog -sn 56.0.0..57.0.0 36 Matthijs Brobbel 20 Andrew Lamb 13 Ryan Johnson 11 Ed Seidl 10 Connor Sanders 8 Alex Huang 5 Emil Ernerfeldt 5 Liam Bao 5 Matthew Kim 4 nathaniel-d-ef 3 Raz Luvaton 3 albertlockett 3 dependabot[bot] 3 mwish 2 Ben Ye 2 Congxian Qiu 2 Dewey Dunnington 2 Kyle Barron 2 Lilian Maurel 2 Mark Nash 2 Nuno Faria 2 Pepijn Van Eeckhoudt 2 Tobias Schwarzinger 2 lichuang 1 Adam Gutglick 1 Adam Reeve 1 Alex Stephen 1 Chen Chongchen 1 Jack 1 Jeffrey Vo 1 Jörn Horstmann 1 Kaushik Srinivasan 1 Li Jiaying 1 Lin Yihai 1 Marco Neumann 1 Piotr Findeisen 1 Piotr Srebrny 1 Samuele Resca 1 Van De Bio 1 Yan Tingwang 1 ding-young 1 kosiew 1 张林伟]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Apache Arrow 22.0.0 Release</title><link href="https://arrow.apache.org/blog/2025/10/24/22.0.0-release/" rel="alternate" type="text/html" title="Apache Arrow 22.0.0 Release" /><published>2025-10-24T00:00:00-04:00</published><updated>2025-10-24T00:00:00-04:00</updated><id>https://arrow.apache.org/blog/2025/10/24/22.0.0-release</id><content type="html" xml:base="https://arrow.apache.org/blog/2025/10/24/22.0.0-release/"><![CDATA[<!--

-->
<p>The Apache Arrow team is pleased to announce the 22.0.0 release. This release
covers over 3 months of development work and includes <a href="https://github.com/apache/arrow/milestone/70?closed=1"><strong>213 resolved
issues</strong></a> on <a href="/release/22.0.0.html#contributors"><strong>255 distinct commits</strong></a> from <a href="/release/22.0.0.html#contributors"><strong>60 distinct
contributors</strong></a>. See the <a href="https://arrow.apache.org/install/">Install Page</a> to
learn how to get the libraries for your platform.</p>
<p>The release notes below are not exhaustive and only expose selected highlights
of the release. Many other bugfixes and improvements have been made: we refer
you to the <a href="/release/22.0.0.html#changelog">complete changelog</a>.</p>
<h2>Community</h2>
<p>Since the 21.0.0 release, Kyle Barron has been invited to be committer.</p>
<p>Matthijs Brobbel, Adam Reeve and Rossi Sun have been joined the
Project Management Committee (PMC).</p>
<p>Thanks for your contributions and participation in the project!</p>
<p>The first Apache Arrow Summit was held on October 2nd 2025 in Paris, France
as part of <a href="https://pydata.org/paris2025">PyData Paris</a>.
Program details and agenda can be found here: <a href="https://www.meetup.com/pydata-paris/events/310646396/">https://www.meetup.com/pydata-paris/events/310646396/</a></p>
<p>There were around 35 attendees, of which ~20 were existing core developers or PMC members.
The Summit was overwhelmingly described as a success, with a friendly atmosphere between all participants.
Unfortunately, no Audio / Video recording system was available for this event.</p>
<h2>Arrow Flight RPC Notes</h2>
<p>Support for dictionary replacement and dictionary encoding has been added to the DoGet and DoExchange methods. (<a href="https://github.com/apache/arrow/issues/45056">GH-45056</a>, <a href="https://github.com/apache/arrow/issues/45055">GH-45055</a> and <a href="https://github.com/apache/arrow/issues/26727">GH-26727</a>).</p>
<p>As part of supporting dictionary replacement we have also exposed the <code>ipc::ReadStats</code> on the <code>FlightStreamReader</code> in order to facilitate debugging. (<a href="https://github.com/apache/arrow/issues/47422">GH-47422</a>)</p>
<h2>C++ Notes</h2>
<h3>Compute</h3>
<p>Timezone aware kernels can now handle timezone offset strings. (<a href="https://github.com/apache/arrow/issues/30036">GH-30036</a>)</p>
<p>Better decimal support has been added introducing a structure <code>MatchConstraint</code> for applying extra (and optional) matching constraint for kernel signature matching. (<a href="https://github.com/apache/arrow/issues/47287">GH-47287</a>, <a href="https://github.com/apache/arrow/issues/41336">GH-41336</a>)</p>
<p>The scatter function has been moved to Arrow core from Arrow Compute. (<a href="https://github.com/apache/arrow/issues/47375">GH-47375</a>)</p>
<h3>Filesystems</h3>
<p>The Request ID has been added when the AWS client raises an error. (<a href="https://github.com/apache/arrow/issues/47349">GH-47349</a>)</p>
<h3>Format</h3>
<p>Several improvements around Half Float (Float16) support. (<a href="https://github.com/apache/arrow/issues/46860">GH-46860</a>, <a href="https://github.com/apache/arrow/issues/46739">GH-46739</a>)</p>
<h3>Parquet</h3>
<p>Better Fuzzing support for Parquet and several related fixes. (<a href="https://github.com/apache/arrow/issues/47803">GH-47803</a>, <a href="https://github.com/apache/arrow/issues/47740">GH-47740</a>, <a href="https://github.com/apache/arrow/issues/47655">GH-47655</a>, <a href="https://github.com/apache/arrow/issues/47597">GH-47597</a>, <a href="https://github.com/apache/arrow/issues/47184">GH-47184</a>)</p>
<p>Rework around the RLE decoder in order to extract a RLE parser to drive further optimisations. (<a href="https://github.com/apache/arrow/issues/47112">GH-47112</a>)</p>
<p>Dynamic dispatch support has been added to Byte Stream Split. (<a href="https://github.com/apache/arrow/issues/46962">GH-46962</a>)</p>
<p>Now some statistics, i.e. null count. will not be discarded when the sort order of the column is unknown. (<a href="https://github.com/apache/arrow/issues/47449">GH-47449</a>)</p>
<p><code>is_min_value_exact</code> and <code>is_max_value_exact</code> now are exposed in Parquet Statistics if present when reading. (<a href="https://github.com/apache/arrow/issues/46905">GH-46905</a>)</p>
<p>We now reserve values correctly when reading <code>BYTE_ARRAY</code> and <code>FLBA</code>. (<a href="https://github.com/apache/arrow/issues/47012">GH-47012</a>)</p>
<h4>Encryption</h4>
<p>String based Parquet encrption methods have been deprecated. (<a href="https://github.com/apache/arrow/issues/47338">GH-47338</a>)</p>
<p>Memory usage required by decryption buffers when reading encrypted Parquet has been reduced. (<a href="https://github.com/apache/arrow/issues/46971">GH-46971</a>)</p>
<h4>Type support</h4>
<p>Improvements on the Parquet Variant type support. (<a href="https://github.com/apache/arrow/issues/47241">GH-47241</a>, <a href="https://github.com/apache/arrow/issues/47838">GH-47838</a>)</p>
<p>Better support for Decimal32 and Decimal64. (<a href="https://github.com/apache/arrow/issues/44345">GH-44345</a>)</p>
<h3>Gandiva</h3>
<p>Support for LLVM 21.1.0 has been added. (<a href="https://github.com/apache/arrow/issues/47469">GH-47469</a>)</p>
<h3>Miscellaneous C++ changes</h3>
<p>Add support for further Arrow Statistics. (<a href="https://github.com/apache/arrow/issues/47102">GH-47102</a>, <a href="https://github.com/apache/arrow/issues/47101">GH-47101</a>)</p>
<p>Support for shared memory comparison in <code>arrow::RecordBatch</code> has been added. (<a href="https://github.com/apache/arrow/pull/47149">GH-47149</a>)</p>
<p><code>arrow::Table::Equals</code> now allows an optional <code>arrow::EqualOptions</code> argument. (<a href="https://github.com/apache/arrow/issues/46937">GH-46937</a>)</p>
<p>Skyhook integration has been removed from the main repository and has been moved to its own repository. (<a href="https://github.com/apache/arrow/issues/47225">GH-47225</a>)</p>
<h2>Linux Packaging Notes</h2>
<p>Support for Debian forky has been added. (<a href="https://github.com/apache/arrow/issues/47312">GH-47312</a>)</p>
<h2>MATLAB Notes</h2>
<ul>
<li><code>NumNulls</code> property was added to <code>arrow.array.Array</code> and <code>arrow.arrray.ChunkedArray</code>. (<a href="https://github.com/apache/arrow/issues/47263">GH-47263</a>, <a href="https://github.com/apache/arrow/issues/38422">GH-38422</a>)</li>
</ul>
<h2>Python Notes</h2>
<p>Compatibility notes:</p>
<ul>
<li>Support for Python 3.9 has been dropped (<a href="https://github.com/apache/arrow/issues/47443">GH-47443</a>) and support for Python 3.14, regular and free-threaded has been added, (<a href="https://github.com/apache/arrow/issues/47438">GH-47438</a>).</li>
<li>Cython 3.1 is now required build-time dependency (<a href="https://github.com/apache/arrow/issues/47370">GH-47370</a>).</li>
<li><code>project.optional-dependencies</code> has been replaced with <code>dependency-groups</code> (<a href="https://github.com/apache/arrow/issues/47137">GH-47137</a>).</li>
</ul>
<p>New features:</p>
<ul>
<li>CSV writer option <code>quoting_header</code> is now exposed (<a href="https://github.com/apache/arrow/issues/47575">GH-47575</a>).</li>
</ul>
<p>Other improvements:</p>
<ul>
<li>Support for pandas <code>DataFrame.attrs</code> during conversion between a dataframe and a Parquet file has been added (<a href="https://github.com/apache/arrow/issues/45382">GH-45382</a>).</li>
<li>A utility function to create Arrow table instead of pandas dataframe has been added (<a href="https://github.com/apache/arrow/issues/47172">GH-47172</a>).</li>
<li>IPC and Flight options now have a nice repr/str methods (<a href="https://github.com/apache/arrow/issues/47358">GH-47358</a>).</li>
<li>Access to Request ID in AWS client error is now available from Python (<a href="https://github.com/apache/arrow/issues/47349">GH-47349</a>).</li>
<li>Public Type Enums are added (<a href="https://github.com/apache/arrow/issues/47123">GH-47123</a>).</li>
<li>Python Development documentation section has been restructured in order to make it easier for contributors to build and develop PyArrow (<a href="https://github.com/apache/arrow/issues/20125">GH-20125</a>.</li>
</ul>
<p>Relevant bug fixes:</p>
<ul>
<li>Schema is now hashable when metadata is set (<a href="https://github.com/apache/arrow/issues/47602">GH-47602</a>).</li>
<li><code>MapScalar.as_py(maps_as_pydicts=&quot;strict&quot;)</code> option now works for nested maps (<a href="https://github.com/apache/arrow/issues/47380">GH-47380</a>).</li>
<li><code>FileFragment.open()</code> no longer segfaults on file-like objects (<a href="https://github.com/apache/arrow/issues/47301">GH-47301</a>).</li>
<li><code>pa.compute.fill_null</code> regression on Windows due to a compiler bug has been fixed (<a href="https://github.com/apache/arrow/issues/47234">GH-47234</a>).</li>
<li>Integer dictionary bitwidth preservation no longer breaks multi-file read behaviour as <code>DatasetFactory.inspect</code> method now accepts <code>promote_options</code> and <code>fragments</code> parameters (<a href="https://github.com/apache/arrow/issues/46629">GH-46629</a>).</li>
<li><code>FileSystem.from_uri</code> is reverted to be a staticmethod again (<a href="https://github.com/apache/arrow/issues/47179">GH-47179</a>).</li>
</ul>
<h2>Java, JavaScript, Go, .NET, Swift and Rust Notes</h2>
<p>The Java, JavaScript, Go, .NET, Swift and Rust projects have moved to separate
repositories outside the main Arrow <a href="https://github.com/apache/arrow">monorepo</a>.</p>
<ul>
<li>For notes on the latest release of the <a href="https://github.com/apache/arrow-java">Java
implementation</a>, see the latest <a href="https://github.com/apache/arrow-java/releases">Arrow
Java changelog</a>.</li>
<li>For notes on the latest release of the <a href="https://github.com/apache/arrow-js">JavaScript
implementation</a>, see the latest <a href="https://github.com/apache/arrow-js/releases">Arrow
JavaScript changelog</a>.</li>
<li>For notes on the latest release of the <a href="https://github.com/apache/arrow-rs">Rust
implementation</a> see the latest <a href="https://github.com/apache/arrow-rs/blob/main/CHANGELOG.md">Arrow Rust
changelog</a>.</li>
<li>For notes on the latest release of the <a href="https://github.com/apache/arrow-go">Go
implementation</a>, see the latest <a href="https://github.com/apache/arrow-go/releases">Arrow Go
changelog</a>.</li>
<li>For notes on the latest release of the <a href="https://github.com/apache/arrow-dotnet">.NET
implementation</a>, see the latest <a href="https://github.com/apache/arrow-dotnet/releases">Arrow  .NET changelog</a>.</li>
<li>For notes on the latest release of the <a href="https://github.com/apache/arrow-swift">Swift implementation</a>, see the latest <a href="https://github.com/apache/arrow-swift/releases">Arrow Swift changelog</a>.</li>
</ul>]]></content><author><name>pmc</name></author><category term="release" /><summary type="html"><![CDATA[The Apache Arrow team is pleased to announce the 22.0.0 release. This release covers over 3 months of development work and includes 213 resolved issues on 255 distinct commits from 60 distinct contributors. See the Install Page to learn how to get the libraries for your platform. The release notes below are not exhaustive and only expose selected highlights of the release. Many other bugfixes and improvements have been made: we refer you to the complete changelog. Community Since the 21.0.0 release, Kyle Barron has been invited to be committer. Matthijs Brobbel, Adam Reeve and Rossi Sun have been joined the Project Management Committee (PMC). Thanks for your contributions and participation in the project! The first Apache Arrow Summit was held on October 2nd 2025 in Paris, France as part of PyData Paris. Program details and agenda can be found here: https://www.meetup.com/pydata-paris/events/310646396/ There were around 35 attendees, of which ~20 were existing core developers or PMC members. The Summit was overwhelmingly described as a success, with a friendly atmosphere between all participants. Unfortunately, no Audio / Video recording system was available for this event. Arrow Flight RPC Notes Support for dictionary replacement and dictionary encoding has been added to the DoGet and DoExchange methods. (GH-45056, GH-45055 and GH-26727). As part of supporting dictionary replacement we have also exposed the ipc::ReadStats on the FlightStreamReader in order to facilitate debugging. (GH-47422) C++ Notes Compute Timezone aware kernels can now handle timezone offset strings. (GH-30036) Better decimal support has been added introducing a structure MatchConstraint for applying extra (and optional) matching constraint for kernel signature matching. (GH-47287, GH-41336) The scatter function has been moved to Arrow core from Arrow Compute. (GH-47375) Filesystems The Request ID has been added when the AWS client raises an error. (GH-47349) Format Several improvements around Half Float (Float16) support. (GH-46860, GH-46739) Parquet Better Fuzzing support for Parquet and several related fixes. (GH-47803, GH-47740, GH-47655, GH-47597, GH-47184) Rework around the RLE decoder in order to extract a RLE parser to drive further optimisations. (GH-47112) Dynamic dispatch support has been added to Byte Stream Split. (GH-46962) Now some statistics, i.e. null count. will not be discarded when the sort order of the column is unknown. (GH-47449) is_min_value_exact and is_max_value_exact now are exposed in Parquet Statistics if present when reading. (GH-46905) We now reserve values correctly when reading BYTE_ARRAY and FLBA. (GH-47012) Encryption String based Parquet encrption methods have been deprecated. (GH-47338) Memory usage required by decryption buffers when reading encrypted Parquet has been reduced. (GH-46971) Type support Improvements on the Parquet Variant type support. (GH-47241, GH-47838) Better support for Decimal32 and Decimal64. (GH-44345) Gandiva Support for LLVM 21.1.0 has been added. (GH-47469) Miscellaneous C++ changes Add support for further Arrow Statistics. (GH-47102, GH-47101) Support for shared memory comparison in arrow::RecordBatch has been added. (GH-47149) arrow::Table::Equals now allows an optional arrow::EqualOptions argument. (GH-46937) Skyhook integration has been removed from the main repository and has been moved to its own repository. (GH-47225) Linux Packaging Notes Support for Debian forky has been added. (GH-47312) MATLAB Notes NumNulls property was added to arrow.array.Array and arrow.arrray.ChunkedArray. (GH-47263, GH-38422) Python Notes Compatibility notes: Support for Python 3.9 has been dropped (GH-47443) and support for Python 3.14, regular and free-threaded has been added, (GH-47438). Cython 3.1 is now required build-time dependency (GH-47370). project.optional-dependencies has been replaced with dependency-groups (GH-47137). New features: CSV writer option quoting_header is now exposed (GH-47575). Other improvements: Support for pandas DataFrame.attrs during conversion between a dataframe and a Parquet file has been added (GH-45382). A utility function to create Arrow table instead of pandas dataframe has been added (GH-47172). IPC and Flight options now have a nice repr/str methods (GH-47358). Access to Request ID in AWS client error is now available from Python (GH-47349). Public Type Enums are added (GH-47123). Python Development documentation section has been restructured in order to make it easier for contributors to build and develop PyArrow (GH-20125. Relevant bug fixes: Schema is now hashable when metadata is set (GH-47602). MapScalar.as_py(maps_as_pydicts=&quot;strict&quot;) option now works for nested maps (GH-47380). FileFragment.open() no longer segfaults on file-like objects (GH-47301). pa.compute.fill_null regression on Windows due to a compiler bug has been fixed (GH-47234). Integer dictionary bitwidth preservation no longer breaks multi-file read behaviour as DatasetFactory.inspect method now accepts promote_options and fragments parameters (GH-46629). FileSystem.from_uri is reverted to be a staticmethod again (GH-47179). Java, JavaScript, Go, .NET, Swift and Rust Notes The Java, JavaScript, Go, .NET, Swift and Rust projects have moved to separate repositories outside the main Arrow monorepo. For notes on the latest release of the Java implementation, see the latest Arrow Java changelog. For notes on the latest release of the JavaScript implementation, see the latest Arrow JavaScript changelog. For notes on the latest release of the Rust implementation see the latest Arrow Rust changelog. For notes on the latest release of the Go implementation, see the latest Arrow Go changelog. For notes on the latest release of the .NET implementation, see the latest Arrow .NET changelog. For notes on the latest release of the Swift implementation, see the latest Arrow Swift changelog.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Announcing arrow-avro in Arrow Rust</title><link href="https://arrow.apache.org/blog/2025/10/23/introducing-arrow-avro/" rel="alternate" type="text/html" title="Announcing arrow-avro in Arrow Rust" /><published>2025-10-23T00:00:00-04:00</published><updated>2025-10-23T00:00:00-04:00</updated><id>https://arrow.apache.org/blog/2025/10/23/introducing-arrow-avro</id><content type="html" xml:base="https://arrow.apache.org/blog/2025/10/23/introducing-arrow-avro/"><![CDATA[<!--

-->
<p><a href="https://crates.io/crates/arrow-avro"><code>arrow-avro</code></a>, a newly rewritten Rust crate that reads and writes <a href="https://avro.apache.org/">Apache Avro</a> data directly as Arrow <code>RecordBatch</code>es, is now available. It supports <a href="https://avro.apache.org/docs/1.11.1/specification/#object-container-files">Avro Object Container Files</a> (OCF), <a href="https://avro.apache.org/docs/1.11.1/specification/#single-object-encoding">Single‑Object Encoding</a> (SOE), the <a href="https://docs.confluent.io/platform/current/schema-registry/fundamentals/serdes-develop/index.html#wire-format">Confluent Schema Registry wire format</a>, and the <a href="https://www.apicur.io/registry/docs/apicurio-registry/1.3.3.Final/getting-started/assembly-using-kafka-client-serdes.html#registry-serdes-types-avro-registry">Apicurio Registry wire format</a>, with projection/evolution, tunable batch sizing, and optional <code>StringViewArray</code> support for faster strings. Its vectorized design reduces copies and cache misses, making both batch and streaming pipelines simpler and faster.</p>
<h2>Motivation</h2>
<p>Apache Avro’s row‑oriented design is effective for encoding one record at a time, while Apache Arrow’s columnar layout is optimized for vectorized analytics. A major challenge lies in converting between these formats without reintroducing row‑wise overhead. Decoding Avro a row at a time and then building Arrow arrays incurs extra allocations and cache‑unfriendly access (the very costs Arrow is designed to avoid). In the real world, this overhead commonly shows up in analytical hot paths. For instance, <a href="https://github.com/apache/datafusion/tree/main/datafusion/datasource-avro">DataFusion’s Avro data source</a> currently ships with its own row‑centric Avro‑to‑Arrow layer. This implementation has led to an open issue for <a href="https://github.com/apache/datafusion/issues/14097">using an upstream arrow-avro reader</a> to simplify the code and speed up scans. Additionally, DataFusion has another open issue for <a href="https://github.com/apache/datafusion/issues/7679#issuecomment-3412302891">supporting Avro format writes</a> that is predicated on the development of an upstream <code>arrow-avro</code> writer.</p>
<h3>Why not use the existing <code>apache-avro</code> crate?</h3>
<p>Rust already has a mature, general‑purpose Avro crate, <a href="https://crates.io/crates/apache-avro">apache-avro</a>. It reads and writes Avro records as Avro value types and provides Object Container File readers and writers. What it does not do is decode directly into Arrow arrays, so any Arrow integration must materialize rows and then build columns.</p>
<p>What’s needed is a complementary approach that decodes column‑by‑column straight into Arrow builders and emits <code>RecordBatch</code>es. This would enable projection pushdown while keeping execution vectorized end to end. For projects such as <a href="https://datafusion.apache.org/">Apache DataFusion</a>, access to a mature, upstream Arrow‑native reader and writer would help simplify the code path and reduce duplication.</p>
<p>Modern pipelines heighten this need because <a href="https://www.confluent.io/blog/avro-kafka-data/">Avro is also used on the wire</a>, not just in files. Kafka ecosystems commonly use Confluent’s Schema Registry framing, and many services adopt the Avro Single‑Object Encoding format. An approach that enables decoding straight into Arrow batches (rather than through per‑row values) would let downstream compute remain vectorized at streaming rates.</p>
<h3>Why this matters</h3>
<p>Apache Avro is a first‑class format across stream processors and cloud services:</p>
<ul>
<li>Confluent Schema Registry supports <a href="https://docs.confluent.io/platform/current/schema-registry/fundamentals/serdes-develop/serdes-avro.html">Avro across multiple languages and tooling</a>.</li>
<li>Apache Flink exposes an <a href="https://nightlies.apache.org/flink/flink-docs-release-1.19/docs/connectors/table/formats/avro-confluent/"><code>avro-confluent</code> format for Kafka</a>.</li>
<li>AWS Lambda <a href="https://aws.amazon.com/about-aws/whats-new/2025/06/aws-lambda-native-support-avro-protobuf-kafka-events/">(June 2025) added native handling for Avro‑formatted Kafka events</a> with Glue and Confluent Schema Registry integrations.</li>
<li>Azure Event Hubs provides a <a href="https://learn.microsoft.com/en-us/azure/event-hubs/schema-registry-overview">Schema Registry with Avro support</a> for Kafka‑compatible clients.</li>
</ul>
<p>In short: Arrow users encounter Avro both on disk (OCF) and on the wire (SOE). An Arrow‑first, vectorized reader/writer for OCF, SOE, and Confluent framing removes a pervasive bottleneck and keeps pipelines columnar end‑to‑end.</p>
<h2>Introducing <code>arrow-avro</code></h2>
<p><a href="https://github.com/apache/arrow-rs/tree/main/arrow-avro"><code>arrow-avro</code></a> is a high-performance Rust crate that converts between Avro and Arrow with a column‑first, batch‑oriented design. On the read side, it decodes Avro Object Container Files (OCF), Single‑Object Encoding (SOE), and the Confluent Schema Registry wire format directly into Arrow <code>RecordBatch</code>es. Meanwhile, the write path provides formats for encoding to OCF and SOE as well.</p>
<p>The crate exposes two primary read APIs: a high-level <code>Reader</code> for OCF inputs and a low-level <code>Decoder</code> for streaming SOE frames. For SOE and Confluent/Apicurio frames, a <code>SchemaStore</code> is provided that resolves fingerprints or schema IDs to full Avro writer schemas, enabling schema evolution while keeping the decode path vectorized.</p>
<p>On the write side, <code>AvroWriter</code> produces OCF (including container‑level compression), while <code>AvroStreamWriter</code> produces framed Avro messages for Single‑Object or Confluent/Apicurio encodings, as configured via the <code>WriterBuilder::with_fingerprint_strategy(...)</code> knob.</p>
<p>Configuration is intentionally minimal but practical. For instance, the <code>ReaderBuilder</code> exposes knobs covering both batch file ingestion and streaming systems without forcing format‑specific code paths.</p>
<h3>How this mirrors Parquet in Arrow‑rs</h3>
<p>If you have used Parquet with Arrow‑rs, you already know the pattern. The <code>parquet</code> crate exposes a <a href="https://docs.rs/parquet/latest/parquet/arrow/index.html">parquet::arrow module</a> that reads and writes Arrow <code>RecordBatch</code>es directly. Most users reach for <code>ParquetRecordBatchReaderBuilder</code> when reading and <code>ArrowWriter</code> when writing. You choose columns up front, set a batch size, and the reader gives you Arrow batches that flow straight into vectorized operators. This is the widely adopted &quot;format crate + Arrow‑native bridge&quot; approach in Rust.</p>
<p><code>arrow‑avro</code> brings that same bridge to Avro. You get a single <code>ReaderBuilder</code> that can produce a <code>Reader</code> for OCF, or a streaming <code>Decoder</code> for on‑the‑wire frames. Both return Arrow <code>RecordBatch</code>es, which means engines can keep projection and filtering close to the reader and avoid building rows only to reassemble them back into columns later. For evolving streams, a small <code>SchemaStore</code> resolves fingerprints or ids before decoding, so the batches that come out are already shaped for vectorized execution.</p>
<p>The reason this pattern matters is straightforward. Arrow’s columnar format is designed for vectorized work and good cache locality. When a format reader produces Arrow batches directly, copies and branchy per‑row work are minimized, keeping downstream operators fast. That is the same story that made <code>parquet::arrow</code> popular in Rust, and it is what <code>arrow‑avro</code> now enables for Avro.</p>
<h2>Architecture &amp; Technical Overview</h2>
<div style="display: flex; gap: 16px; justify-content: center; align-items: flex-start; padding: 20px 15px;">
<img src="/img/introducing-arrow-avro/arrow-avro-architecture.svg"
        width="100%"
        alt="High-level `arrow-avro` architecture"
        style="background:#fff">
</div>
<p>At a high level, <a href="https://arrow.apache.org/rust/arrow_avro/index.html">arrow-avro</a> splits cleanly into read and write paths built around Arrow <code>RecordBatch</code>es. The read side turns Avro (OCF files or framed byte streams) into batched Arrow arrays, while the write side takes Arrow batches and produces OCF files or streaming frames. When using an <code>AvroStreamWriter</code>, the framing (SOE or Confluent) is part of the stream output based on the configured fingerprint strategy; thus no separate framing work is required. The public API and module layout are intentionally small, so most applications only touch a builder, a reader/decoder, and (optionally) a schema store for schema evolution.</p>
<p>On the <a href="https://arrow.apache.org/rust/arrow_avro/reader/index.html">read</a> path, everything starts with the <a href="https://arrow.apache.org/rust/arrow_avro/reader/struct.ReaderBuilder.html">ReaderBuilder</a>. A single builder can create a <a href="https://arrow.apache.org/rust/arrow_avro/reader/struct.Reader.html">Reader</a> for Object Container Files (OCF) or a streaming <a href="https://arrow.apache.org/rust/arrow_avro/reader/struct.Decoder.html">Decoder</a> for SOE/Confluent/Apicurio frames. The <code>Reader</code> pulls OCF blocks and yields Arrow <code>RecordBatch</code>es while the <code>Decoder</code> is push‑based, i.e., bytes are fed in as they arrive and then drained as completed batches once <code>flush</code> is called. Both use the same schema‑driven decoding logic (per‑column decoders with projection/union/nullability handling), so file and streaming inputs produce batches using fewer per‑row allocations and minimal branching/redundancy. Additionally, the streaming <code>Decoder</code> maintains a cache of per‑schema record decoders keyed by fingerprint to avoid re‑planning when a stream interleaves schema versions. This keeps steady‑state decode fast even as schemas evolve.</p>
<p>When reading an OCF, the <code>Reader</code> parses a header and then iterates over blocks of encoded data. The header contains a metadata map with the embedded Avro schema and optional compression (i.e., <code>deflate</code>, <code>snappy</code>, <code>zstd</code>, <code>bzip2</code>, <code>xz</code>), plus a 16‑byte sync marker used to delimit blocks. Each subsequent OCF block then carries a row count and the encoded payload. The parsed OCF header and block structures are also encoded with variable‑length integers that use zig‑zag encoding for signed values. <code>arrow-avro</code> implements a small <code>vlq</code> (variable‑length quantity) module, which is used during both header parsing and block iteration. Efficient <code>vlq</code> decode is part of why the <code>Reader</code> and <code>Decoder</code> can stay vectorized and avoid unnecessary per‑row overhead.</p>
<p>On the <a href="https://arrow.apache.org/rust/arrow_avro/writer/index.html">write</a> path, the <a href="https://arrow.apache.org/rust/arrow_avro/writer/struct.WriterBuilder.html">WriterBuilder</a> produces either an <a href="https://arrow.apache.org/rust/arrow_avro/writer/type.AvroWriter.html">AvroWriter</a> (OCF) or an <a href="https://arrow.apache.org/rust/arrow_avro/writer/type.AvroStreamWriter.html">AvroStreamWriter</a> (SOE/Message). The <code>with_compression(...)</code> knob is used for OCF block compression while <code>with_fingerprint_strategy(...)</code> selects the streaming frame, i.e., Rabin for SOE, a 32‑bit schema ID for Confluent, or a 64-bit schema ID for Apicurio. The <code>AvroStreamWriter</code> also adds the appropriate prefix automatically while encoding, thus eliminating the need for potentially expensive post‑processing steps to wrap output Avro SOEs.</p>
<p>Schema handling is centralized in the <a href="https://arrow.apache.org/rust/arrow_avro/schema/index.html">schema</a> module. <a href="https://arrow.apache.org/rust/arrow_avro/schema/struct.AvroSchema.html">AvroSchema</a> wraps a valid Avro Schema JSON string, supports computing a <code>Fingerprint</code>, and can be loaded into a <a href="https://arrow.apache.org/rust/arrow_avro/schema/struct.SchemaStore.html">SchemaStore</a> as a writer schema. At runtime, the <code>Reader</code>/<code>Decoder</code> can use a <code>SchemaStore</code> to resolve fingerprints before decoding, enabling <a href="https://avro.apache.org/docs/1.11.1/specification/#schema-resolution">schema resolution</a>. The <code>FingerprintAlgorithm</code> captures how fingerprints are derived (i.e., CRC‑64‑AVRO Rabin, MD5, SHA‑256, or a registry ID), and <code>FingerprintStrategy</code> configures how the <code>Writer</code> prefixes each record while encoding SOE streams. This schema module is the glue that enables SOE and Confluent/Apicurio support without coupling to a specific registry client.</p>
<p>At the heart of <code>arrow-avro</code> is a type‑mapping <code>Codec</code> that the library uses to construct both encoders and decoders. The <code>Codec</code> captures, for every Avro field, how it maps to Arrow and how it should be encoded or decoded. The <code>Reader</code> logic builds a <code>Codec</code> per <em>(writer, reader)</em> schema pair, which the decoder later uses to vectorize parsing of Avro values directly into the correct Arrow builders. The <code>Writer</code> logic uses the same <code>Codec</code> mappings to drive pre-computed record encoding plans which enable fast serialization of Arrow arrays to the correct Avro physical representation (i.e., decimals as bytes vs fixed, enum symbol handling, union branch tagging, etc.). Because the <code>Codec</code> informs union and nullable decisions in both the encoder and decoder, the common Avro pattern <code>[&quot;null&quot;, T]</code> seamlessly maps to and from an Arrow optional field, while Avro unions map to Arrow unions using an 8‑bit type‑id with minimal overhead. Meanwhile, enabling <code>strict_mode</code> applies tighter Avro resolution rules in the <code>Codec</code> to help surface ambiguous unions early.</p>
<p>Finally, by keeping container and stream framing (OCF vs. SOE) separate from encoding and decoding, the crate composes naturally with the rest of Arrow‑rs: you read or write Arrow <code>RecordBatch</code>es, pick OCF or SOE streams as needed, and wire up fingerprints only when you're on a streaming path. This results in a compact API surface that covers both batch files and high‑throughput streams without sacrificing columnar, vectorized execution.</p>
<h2>Examples</h2>
<h3>Decoding a Confluent-framed Kafka Stream</h3>
<div class="language-rust highlighter-rouge"><div class="highlight"><pre class="highlight"><code data-lang="rust"><span class="k">use</span> <span class="nn">arrow_avro</span><span class="p">::</span><span class="nn">reader</span><span class="p">::</span><span class="n">ReaderBuilder</span><span class="p">;</span>
<span class="k">use</span> <span class="nn">arrow_avro</span><span class="p">::</span><span class="nn">schema</span><span class="p">::{</span>
    <span class="n">SchemaStore</span><span class="p">,</span> <span class="n">AvroSchema</span><span class="p">,</span> <span class="n">Fingerprint</span><span class="p">,</span> <span class="n">FingerprintAlgorithm</span><span class="p">,</span> <span class="n">CONFLUENT_MAGIC</span>
<span class="p">};</span>

<span class="k">fn</span> <span class="nf">main</span><span class="p">()</span> <span class="k">-&gt;</span> <span class="nb">Result</span><span class="o">&lt;</span><span class="p">(),</span> <span class="nb">Box</span><span class="o">&lt;</span><span class="k">dyn</span> <span class="nn">std</span><span class="p">::</span><span class="nn">error</span><span class="p">::</span><span class="n">Error</span><span class="o">&gt;&gt;</span> <span class="p">{</span>
    <span class="c1">// Register writer schema under Confluent id=1.</span>
    <span class="k">let</span> <span class="k">mut</span> <span class="n">store</span> <span class="o">=</span> <span class="nn">SchemaStore</span><span class="p">::</span><span class="nf">new_with_type</span><span class="p">(</span><span class="nn">FingerprintAlgorithm</span><span class="p">::</span><span class="n">Id</span><span class="p">);</span>
    <span class="n">store</span><span class="nf">.set</span><span class="p">(</span>
        <span class="nn">Fingerprint</span><span class="p">::</span><span class="nf">Id</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span>
        <span class="nn">AvroSchema</span><span class="p">::</span><span class="nf">new</span><span class="p">(</span><span class="s">r#"{"type":"record","name":"T","fields":[{"name":"x","type":"long"}]}"#</span><span class="nf">.into</span><span class="p">()),</span>
    <span class="p">)</span><span class="o">?</span><span class="p">;</span>

    <span class="c1">// Define reader schema to enable projection/schema evolution.</span>
    <span class="k">let</span> <span class="n">reader_schema</span> <span class="o">=</span> <span class="nn">AvroSchema</span><span class="p">::</span><span class="nf">new</span><span class="p">(</span><span class="s">r#"{"type":"record","name":"T","fields":[{"name":"x","type":"long"}]}"#</span><span class="nf">.into</span><span class="p">());</span>

    <span class="c1">// Build Decoder using reader and writer schemas</span>
    <span class="k">let</span> <span class="k">mut</span> <span class="n">decoder</span> <span class="o">=</span> <span class="nn">ReaderBuilder</span><span class="p">::</span><span class="nf">new</span><span class="p">()</span>
        <span class="nf">.with_reader_schema</span><span class="p">(</span><span class="n">reader_schema</span><span class="p">)</span>
        <span class="nf">.with_writer_schema_store</span><span class="p">(</span><span class="n">store</span><span class="p">)</span>
        <span class="nf">.build_decoder</span><span class="p">()</span><span class="o">?</span><span class="p">;</span>

    <span class="c1">// Simulate one frame: magic 0x00 + 4‑byte big‑endian schema ID + Avro body (x=1 encoded as zig‑zag/VLQ).</span>
    <span class="k">let</span> <span class="k">mut</span> <span class="n">frame</span> <span class="o">=</span> <span class="nn">Vec</span><span class="p">::</span><span class="nf">from</span><span class="p">(</span><span class="n">CONFLUENT_MAGIC</span><span class="p">);</span> <span class="n">frame</span><span class="nf">.extend_from_slice</span><span class="p">(</span><span class="o">&amp;</span><span class="mi">1u32</span><span class="nf">.to_be_bytes</span><span class="p">());</span> <span class="n">frame</span><span class="nf">.extend_from_slice</span><span class="p">(</span><span class="o">&amp;</span><span class="p">[</span><span class="mi">2</span><span class="p">]);</span>

    <span class="c1">// Consume from decoder</span>
    <span class="k">let</span> <span class="n">_consumed</span> <span class="o">=</span> <span class="n">decoder</span><span class="nf">.decode</span><span class="p">(</span><span class="o">&amp;</span><span class="n">frame</span><span class="p">)</span><span class="o">?</span><span class="p">;</span>
    <span class="k">while</span> <span class="k">let</span> <span class="nf">Some</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span> <span class="o">=</span> <span class="n">decoder</span><span class="nf">.flush</span><span class="p">()</span><span class="o">?</span> <span class="p">{</span>
        <span class="nd">println!</span><span class="p">(</span><span class="s">"rows={}, cols={}"</span><span class="p">,</span> <span class="n">batch</span><span class="nf">.num_rows</span><span class="p">(),</span> <span class="n">batch</span><span class="nf">.num_columns</span><span class="p">());</span>
    <span class="p">}</span>
    <span class="nf">Ok</span><span class="p">(())</span>
<span class="p">}</span>
</code></pre></div></div>
<p>The <code>SchemaStore</code> maps the incoming schema ID to the correct Avro writer schema so the decoder can perform projection/evolution against the reader schema. Confluent's wire format prefixes each message with a magic byte <code>0x00</code> followed by a big‑endian 4‑byte schema ID. After decoding Avro messages, the <code>Decoder::flush()</code> method yields Arrow <code>RecordBatch</code>es suitable for vectorized processing.</p>
<p>A more advanced example can be found <a href="https://github.com/apache/arrow-rs/blob/main/arrow-avro/examples/decode_kafka_stream.rs">here</a>.</p>
<h3>Writing a Snappy Compressed Avro OCF file</h3>
<div class="language-rust highlighter-rouge"><div class="highlight"><pre class="highlight"><code data-lang="rust"><span class="k">use</span> <span class="nn">arrow_array</span><span class="p">::{</span><span class="n">Int64Array</span><span class="p">,</span> <span class="n">RecordBatch</span><span class="p">};</span>
<span class="k">use</span> <span class="nn">arrow_schema</span><span class="p">::{</span><span class="n">Schema</span><span class="p">,</span> <span class="n">Field</span><span class="p">,</span> <span class="n">DataType</span><span class="p">};</span>
<span class="k">use</span> <span class="nn">arrow_avro</span><span class="p">::</span><span class="nn">writer</span><span class="p">::{</span><span class="n">Writer</span><span class="p">,</span> <span class="n">WriterBuilder</span><span class="p">};</span>
<span class="k">use</span> <span class="nn">arrow_avro</span><span class="p">::</span><span class="nn">writer</span><span class="p">::</span><span class="nn">format</span><span class="p">::</span><span class="n">AvroOcfFormat</span><span class="p">;</span>
<span class="k">use</span> <span class="nn">arrow_avro</span><span class="p">::</span><span class="nn">compression</span><span class="p">::</span><span class="n">CompressionCodec</span><span class="p">;</span>
<span class="k">use</span> <span class="nn">std</span><span class="p">::{</span><span class="nn">sync</span><span class="p">::</span><span class="nb">Arc</span><span class="p">,</span> <span class="nn">fs</span><span class="p">::</span><span class="n">File</span><span class="p">,</span> <span class="nn">io</span><span class="p">::</span><span class="n">BufWriter</span><span class="p">};</span>

<span class="k">fn</span> <span class="nf">main</span><span class="p">()</span> <span class="k">-&gt;</span> <span class="nb">Result</span><span class="o">&lt;</span><span class="p">(),</span> <span class="nb">Box</span><span class="o">&lt;</span><span class="k">dyn</span> <span class="nn">std</span><span class="p">::</span><span class="nn">error</span><span class="p">::</span><span class="n">Error</span><span class="o">&gt;&gt;</span> <span class="p">{</span>
  <span class="k">let</span> <span class="n">schema</span> <span class="o">=</span> <span class="nn">Schema</span><span class="p">::</span><span class="nf">new</span><span class="p">(</span><span class="nd">vec!</span><span class="p">[</span><span class="nn">Field</span><span class="p">::</span><span class="nf">new</span><span class="p">(</span><span class="s">"id"</span><span class="p">,</span> <span class="nn">DataType</span><span class="p">::</span><span class="n">Int64</span><span class="p">,</span> <span class="kc">false</span><span class="p">)]);</span>
  <span class="k">let</span> <span class="n">batch</span> <span class="o">=</span> <span class="nn">RecordBatch</span><span class="p">::</span><span class="nf">try_new</span><span class="p">(</span>
    <span class="nn">Arc</span><span class="p">::</span><span class="nf">new</span><span class="p">(</span><span class="n">schema</span><span class="nf">.clone</span><span class="p">()),</span>
    <span class="nd">vec!</span><span class="p">[</span><span class="nn">Arc</span><span class="p">::</span><span class="nf">new</span><span class="p">(</span><span class="nn">Int64Array</span><span class="p">::</span><span class="nf">from</span><span class="p">(</span><span class="nd">vec!</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">]))],</span>
  <span class="p">)</span><span class="o">?</span><span class="p">;</span>
  <span class="k">let</span> <span class="n">file</span> <span class="o">=</span> <span class="nn">File</span><span class="p">::</span><span class="nf">create</span><span class="p">(</span><span class="s">"target/example.avro"</span><span class="p">)</span><span class="o">?</span><span class="p">;</span>

  <span class="c1">// Choose OCF block compression (e.g., None, Deflate, Snappy, Zstd)</span>
  <span class="k">let</span> <span class="k">mut</span> <span class="n">writer</span><span class="p">:</span> <span class="n">Writer</span><span class="o">&lt;</span><span class="n">_</span><span class="p">,</span> <span class="n">AvroOcfFormat</span><span class="o">&gt;</span> <span class="o">=</span> <span class="nn">WriterBuilder</span><span class="p">::</span><span class="nf">new</span><span class="p">(</span><span class="n">schema</span><span class="p">)</span>
      <span class="nf">.with_compression</span><span class="p">(</span><span class="nf">Some</span><span class="p">(</span><span class="nn">CompressionCodec</span><span class="p">::</span><span class="n">Snappy</span><span class="p">))</span>
      <span class="nf">.build</span><span class="p">(</span><span class="nn">BufWriter</span><span class="p">::</span><span class="nf">new</span><span class="p">(</span><span class="n">file</span><span class="p">))</span><span class="o">?</span><span class="p">;</span>
  <span class="n">writer</span><span class="nf">.write</span><span class="p">(</span><span class="o">&amp;</span><span class="n">batch</span><span class="p">)</span><span class="o">?</span><span class="p">;</span>
  <span class="n">writer</span><span class="nf">.finish</span><span class="p">()</span><span class="o">?</span><span class="p">;</span>
  <span class="nf">Ok</span><span class="p">(())</span>
<span class="p">}</span>
</code></pre></div></div>
<p>The example above configures an Avro OCF <code>Writer</code>. It constructs a <code>Writer&lt;_, AvroOcfFormat&gt;</code> using <code>WriterBuilder::new(schema)</code> and wraps a <code>File</code> in a <code>BufWriter</code> for efficient I/O. The call to <code>.with_compression(Some(CompressionCodec::Snappy))</code> enables block‑level snappy compression. Finally, <code>writer.write(&amp;batch)?</code> serializes the batch as an Avro encoded block, and <code>writer.finish()?</code> flushes and finalizes the outputted file.</p>
<h2>Alternatives &amp; Benchmarks</h2>
<p>There are fundamentally two different approaches for bringing Avro into Arrow:</p>
<ol>
<li>Row‑centric approach, typical of general Avro libraries such as <code>apache-avro</code>, deserializes one record at a time into native Rust values (i.e., <code>Value</code> or Serde types) and then builds Arrow arrays from those values.</li>
<li>Vectorized approach, what <code>arrow-avro</code> provides, decodes directly into Arrow builders/arrays and emits <code>RecordBatch</code>es, avoiding most per‑row overhead.</li>
</ol>
<p>This section compares the performance of both approaches using these <a href="https://github.com/jecsand838/arrow-rs/tree/blog-benches/arrow-avro/benches">Criterion benchmarks</a>.</p>
<h3>Read performance (1M)</h3>
<div style="display: flex; gap: 16px; justify-content: center; align-items: flex-start; padding: 5px 5px;">
<img src="/img/introducing-arrow-avro/read_violin_1m.svg"
        width="100%"
        alt="1M Row Read Violin Plot"
        style="background:#fff">
</div>
<h3>Read performance (10K)</h3>
<div style="display: flex; gap: 16px; justify-content: center; align-items: flex-start; padding: 5px 5px;">
<img src="/img/introducing-arrow-avro/read_violin_10k.svg"
        width="100%"
        alt="10K Row Read Violin Plot"
        style="background:#fff">
</div>
<h3>Write performance (1M)</h3>
<div style="display: flex; gap: 16px; justify-content: center; align-items: flex-start; padding: 5px 5px;">
<img src="/img/introducing-arrow-avro/write_violin_1m.svg"
        width="100%"
        alt="1M Row Write Violin Plot"
        style="background:#fff">
</div>
<h3>Write performance (10K)</h3>
<div style="display: flex; gap: 16px; justify-content: center; align-items: flex-start; padding: 5px 5px;">
<img src="/img/introducing-arrow-avro/write_violin_10k.svg"
        width="100%"
        alt="10K Row Write Violin Plot"
        style="background:#fff">
</div>
<p>Across benchmarks, the violin plots show lower medians and tighter spreads for <code>arrow-avro</code> on both read and write paths. The gap widens when per‑row work dominates (i.e., 10K‑row scenarios). At 1M rows, the distributions remain favorable to <code>arrow-avro</code>, reflecting better cache locality and fewer copies once decoding goes straight to Arrow arrays. The general behavior is consistent with <code>apache-avro</code>'s record‑by‑record iteration and <code>arrow-avro</code>'s batch‑oriented design.</p>
<p>The table below lists the cases we report in the figures:</p>
<ul>
<li>10K vs 1M rows for multiple data shapes.</li>
<li><strong>Read cases:</strong>
<ul>
<li><code>f8</code>: <em>Full schema, 8K batch size.</em>
Decode all four columns with batch_size = 8192.</li>
<li><code>f1</code>: <em>Full schema, 1K batch size.</em>
Decode all four columns with batch_size = 1024.</li>
<li><code>p8</code>: <em>Projected <code>{id,name}</code>, 8K batch size (pushdown).</em>
Decode only <code>id</code> and <code>name</code> with batch_size = 8192`.
<em>How projection is applied:</em>
<ul>
<li><code>arrow-avro/p8</code>: projection via reader schema (<code>ReaderBuilder::with_reader_schema(...)</code>) so decoding is column‑pushed down in the Arrow‑first reader.</li>
<li><code>apache-avro/p8</code>: projection via Avro reader schema (<code>AvroReader::with_schema(...)</code>) so the Avro library decodes only the projected fields.</li>
</ul>
</li>
<li><code>np</code>: <em>Projected <code>{id,name}</code>, no pushdown, 8K batch size.</em>
Both readers decode the full record (all four columns), materialize all arrays, then project down to <code>{id,name}</code> after decode. This models systems that can't push projection into the file/codec reader.</li>
</ul>
</li>
<li><strong>Write cases:</strong>
<ul>
<li><code>c</code> (cold): <em>Schema conversion each iteration.</em></li>
<li><code>h</code> (hot): <em>Avro JSON &quot;hot&quot; path.</em></li>
</ul>
</li>
<li>The resulting Apache‑Avro vs Arrow‑Avro medians with the computed speedup.</li>
</ul>
<h3>Benchmark Median Time Results (Apple Silicon Mac)</h3>
<table>
<thead>
<tr>
<th>Case</th>
<th align="right">apache-avro median</th>
<th align="right">arrow-avro median</th>
<th align="right">speedup</th>
</tr>
</thead>
<tbody>
<tr>
<td>R/f8/10K</td>
<td align="right">2.60 ms</td>
<td align="right">0.24 ms</td>
<td align="right">10.83x</td>
</tr>
<tr>
<td>R/p8/10K</td>
<td align="right">7.91 ms</td>
<td align="right">0.24 ms</td>
<td align="right">32.95x</td>
</tr>
<tr>
<td>R/f1/10K</td>
<td align="right">2.65 ms</td>
<td align="right">0.25 ms</td>
<td align="right">10.60x</td>
</tr>
<tr>
<td>R/np/10K</td>
<td align="right">2.62 ms</td>
<td align="right">0.25 ms</td>
<td align="right">10.48x</td>
</tr>
<tr>
<td>R/f8/1M</td>
<td align="right">267.21 ms</td>
<td align="right">27.91 ms</td>
<td align="right">9.57x</td>
</tr>
<tr>
<td>R/p8/1M</td>
<td align="right">791.79 ms</td>
<td align="right">26.28 ms</td>
<td align="right">30.13x</td>
</tr>
<tr>
<td>R/f1/1M</td>
<td align="right">262.93 ms</td>
<td align="right">28.25 ms</td>
<td align="right">9.31x</td>
</tr>
<tr>
<td>R/np/1M</td>
<td align="right">268.79 ms</td>
<td align="right">27.69 ms</td>
<td align="right">9.71x</td>
</tr>
<tr>
<td>W/c/10K</td>
<td align="right">4.78 ms</td>
<td align="right">0.27 ms</td>
<td align="right">17.70x</td>
</tr>
<tr>
<td>W/h/10K</td>
<td align="right">0.82 ms</td>
<td align="right">0.28 ms</td>
<td align="right">2.93x</td>
</tr>
<tr>
<td>W/c/1M</td>
<td align="right">485.58 ms</td>
<td align="right">36.97 ms</td>
<td align="right">13.13x</td>
</tr>
<tr>
<td>W/h/1M</td>
<td align="right">83.58 ms</td>
<td align="right">36.75 ms</td>
<td align="right">2.27x</td>
</tr>
</tbody>
</table>
<h2>Closing</h2>
<p><code>arrow-avro</code> brings a purpose‑built, vectorized bridge connecting Arrow-rs and Avro that covers Object Container Files (OCF), Single‑Object Encoding (SOE), and the Confluent/Apicurio Schema Registry wire formats. This means you can now keep your ingestion paths columnar for both batch files and streaming systems. The reader and writer APIs shown above are now available for you to use with the v57.0.0 release of <code>arrow-rs</code>.</p>
<p>This work is part of the ongoing Arrow‑rs effort to implement first-class Avro support in Rust. We'd love your feedback on real‑world use-cases, workloads, and integrations. We also welcome contributions, whether that's issues, benchmarks, or PRs. To follow along or help, open an <a href="https://github.com/apache/arrow-rs/issues">issue on GitHub</a> and/or track <a href="https://github.com/apache/arrow-rs/issues/4886">Add Avro Support</a> in <code>apache/arrow-rs</code>.</p>
<h3>Acknowledgments</h3>
<p>Special thanks to:</p>
<ul>
<li><a href="https://github.com/tustvold">tustvold</a> for laying an incredible zero-copy foundation.</li>
<li><a href="https://github.com/nathaniel-d-ef">nathaniel-d-ef</a> and <a href="https://github.com/elastiflow">ElastiFlow</a> for their numerous and invaluable project-wide contributions.</li>
<li><a href="https://github.com/veronica-m-ef">veronica-m-ef</a> for making Impala‑related contributions to the <code>Reader</code>.</li>
<li><a href="https://github.com/Supermetal-Inc">Supermetal</a> for contributions related to Apicurio Registry and Run-End Encoding type support.</li>
<li><a href="https://github.com/kumarlokesh">kumarlokesh</a> for contributing <code>Utf8View</code> support.</li>
<li><a href="https://github.com/alamb">alamb</a>, <a href="https://github.com/scovich">scovich</a>, <a href="https://github.com/mbrobbel">mbrobbel</a>, and <a href="https://github.com/klion26">klion26</a> for their thoughtful reviews, detailed feedback, and support throughout the development of <code>arrow-avro</code>.</li>
</ul>
<p>If you have any questions about this blog post, please feel free to contact the author, <a href="mailto:jecs838@gmail.com">Connor Sanders</a>.</p>]]></content><author><name>jecsand838</name></author><category term="application" /><summary type="html"><![CDATA[A new native Rust vectorized reader/writer for Avro to Arrow, with OCF, Single‑Object, and Confluent wire format support.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">3x-9x Faster Apache Parquet Footer Metadata Using a Custom Thrift Parser in Rust</title><link href="https://arrow.apache.org/blog/2025/10/23/rust-parquet-metadata/" rel="alternate" type="text/html" title="3x-9x Faster Apache Parquet Footer Metadata Using a Custom Thrift Parser in Rust" /><published>2025-10-23T00:00:00-04:00</published><updated>2025-10-23T00:00:00-04:00</updated><id>https://arrow.apache.org/blog/2025/10/23/rust-parquet-metadata</id><content type="html" xml:base="https://arrow.apache.org/blog/2025/10/23/rust-parquet-metadata/"><![CDATA[<!--

-->
<p><em>Editor’s Note: While <a href="https://arrow.apache.org/">Apache Arrow</a> and <a href="https://parquet.apache.org/">Apache Parquet</a> are separate projects,
the Arrow <a href="https://github.com/apache/arrow-rs">arrow-rs</a> repository hosts the development of the <a href="https://crates.io/crates/parquet">parquet</a> Rust
crate, a widely used and high-performance Parquet implementation.</em></p>
<h2>Summary</h2>
<p>Version <a href="https://crates.io/crates/parquet/57.0.0">57.0.0</a> of the <a href="https://crates.io/crates/parquet">parquet</a> Rust crate decodes metadata more than three times
faster than previous versions thanks to a new custom <a href="https://thrift.apache.org/">Apache Thrift</a> parser. The new
parser is both faster in all cases and enables further performance improvements not
possible with generated parsers, such as skipping unnecessary fields and selective parsing.</p>
<!-- Image source: https://docs.google.com/presentation/d/1WjX4t7YVj2kY14SqCpenGqNl_swjdHvPg86UeBT3IcY -->
<div style="display: flex; gap: 16px; justify-content: center; align-items: flex-start;">
  <img src="/img/rust-parquet-metadata/results.png" width="100%" class="img-responsive" alt="" aria-hidden="true">
</div>
<p><em>Figure 1:</em> Performance comparison of <a href="https://parquet.apache.org/">Apache Parquet</a> metadata parsing using a generated
Thrift parser (versions <code>56.2.0</code> and earlier) and the new
<a href="https://github.com/apache/arrow-rs/issues/5854">custom Thrift parser</a> in <a href="https://github.com/apache/arrow-rs">arrow-rs</a> version <a href="https://crates.io/crates/parquet/57.0.0">57.0.0</a>. No
changes are needed to the Parquet format itself.
See the <a href="https://github.com/alamb/parquet_footer_parsing">benchmark page</a> for more details.</p>
<!-- Image source: https://docs.google.com/presentation/d/1WjX4t7YVj2kY14SqCpenGqNl_swjdHvPg86UeBT3IcY -->
<div style="display: flex; gap: 16px; justify-content: center; align-items: flex-start;">
  <img src="/img/rust-parquet-metadata/scaling.png" width="100%" class="img-responsive" alt="Scaling behavior of custom Thrift parser" aria-hidden="true">
</div>
<p><em>Figure 2:</em> Speedup of the [custom Thrift decoder] for string and floating-point data types,
for <code>100</code>, <code>1000</code>, <code>10,000</code>, and <code>100,000</code> columns. The new parser is faster in all cases,
and the speedup is similar regardless of the number of columns. See the <a href="https://github.com/alamb/parquet_footer_parsing">benchmark page</a> for more details.</p>
<h2>Introduction: Parquet and the Importance of Metadata Parsing</h2>
<p><a href="https://parquet.apache.org/">Apache Parquet</a> is a popular columnar storage format
designed to be efficient for both storage and query processing. Parquet
files consist of a series of data pages, and a footer, as shown in Figure 3. The footer
contains metadata about the file, including schema, statistics, and other
information needed to decode the data pages.</p>
<!-- Image source: https://docs.google.com/presentation/d/1WjX4t7YVj2kY14SqCpenGqNl_swjdHvPg86UeBT3IcY -->
<div style="display: flex; gap: 16px; justify-content: center; align-items: flex-start;">
  <img src="/img/rust-parquet-metadata/parquet.png" width="100%" class="img-responsive" alt="Physical File Structure of Parquet" aria-hidden="true">
</div>
<p><em>Figure 3:</em> Structure of a Parquet file showing the header, data pages, and footer metadata.</p>
<p>Getting information stored in the footer is typically the first step in reading
a Parquet file, as it is required to interpret the data pages. <em>Parsing</em> the
footer is often performance critical:</p>
<ul>
<li>When reading from fast local storage, such as modern NVMe SSDs, footer parsing
must be completed to know what data pages to read, placing it directly on the critical
I/O path.</li>
<li>Footer parsing scales linearly with the number of columns and row groups in a
Parquet file and thus can be a bottleneck for tables with many columns or files
with many row groups.</li>
<li>Even in systems that cache the parsed footer in memory (see <a href="https://datafusion.apache.org/blog/2025/08/15/external-parquet-indexes/">Using
External Indexes, Metadata Stores, Catalogs and Caches to Accelerate Queries
on Apache Parquet</a>), the footer must still be parsed on cache miss.</li>
</ul>
<!-- Image source: https://docs.google.com/presentation/d/1WjX4t7YVj2kY14SqCpenGqNl_swjdHvPg86UeBT3IcY -->
<div style="display: flex; gap: 16px; justify-content: center; align-items: flex-start;">
  <img src="/img/rust-parquet-metadata/flow.png" width="100%" class="img-responsive" alt="Typical Parquet processing flow" aria-hidden="true">
</div>
<p><em>Figure 4:</em> Typical processing flow for Parquet files for stateless and stateful
systems. Stateless engines read the footer on every query, so the time taken to
parse the footer directly adds to query latency. Stateful systems cache some or
all of the parsed footer in advance of queries.</p>
<p>The speed of parsing metadata has grown even more important as Parquet spreads
throughout the data ecosystem and is used for more latency-sensitive workloads such
as observability, interactive analytics, and single-point
lookups for Retrieval-Augmented Generation (RAG) applications feeding LLMs.
As overall query times decrease, the proportion spent on footer parsing increases.</p>
<h2>Background: Apache Thrift</h2>
<p>Parquet stores metadata using <a href="https://thrift.apache.org/">Apache Thrift</a>, a framework for
network data types and service interfaces. It includes a <a href="https://thrift.apache.org/docs/idl">data definition
language</a> similar to <a href="https://developers.google.com/protocol-buffers">Protocol Buffers</a>. Thrift definition files describe data
types in a language-neutral way, and systems typically use code generators to
automatically create code for a specific programming language to read and write
those data types.</p>
<p>The <a href="https://github.com/apache/parquet-format/blob/master/src/main/thrift/parquet.thrift">parquet.thrift</a> file defines the format of the metadata
serialized at the end of each Parquet file in the <a href="https://github.com/apache/thrift/blob/master/doc/specs/thrift-compact-protocol.md">Thrift Compact
protocol</a>, as shown below in Figure 5. The binary encoding is &quot;variable-length&quot;,
meaning that the length of each element depends on its content, not
just its type. Smaller-valued primitive types are encoded in fewer bytes than
larger values, and strings and lists are stored inline, prefixed with their
length.</p>
<p>This encoding is space-efficient but, due to being variable-length, does not
support random access: it is not possible to locate a particular field without
scanning all previous fields. Other formats such as <a href="https://google.github.io/flatbuffers/">FlatBuffers</a> provide
random-access parsing and have been <a href="https://lists.apache.org/thread/j9qv5vyg0r4jk6tbm6sqthltly4oztd3">proposed as alternatives</a> given their
theoretical performance advantages. However, changing the Parquet format is a
significant undertaking, requires buy-in from the community and ecosystem,
and would likely take years to be adopted.</p>
<!-- Image source: https://docs.google.com/presentation/d/1WjX4t7YVj2kY14SqCpenGqNl_swjdHvPg86UeBT3IcY -->
<div style="display: flex; gap: 16px; justify-content: center; align-items: flex-start;">
  <img src="/img/rust-parquet-metadata/thrift-compact-encoding.png" width="100%" class="img-responsive" alt="Thrift Compact Encoding Illustration" aria-hidden="true">
</div>
<p><em>Figure 5:</em> Parquet metadata is serialized using the <a href="https://github.com/apache/thrift/blob/master/doc/specs/thrift-compact-protocol.md">Thrift Compact protocol</a>.
Each field is stored using a variable number of bytes that depends on its value.
Primitive types use a variable-length encoding and strings and lists are
prefixed with their lengths.</p>
<p>Despite Thrift's very real disadvantage due to lack of random access, software
optimizations are much easier to deploy than format changes. <a href="https://xiangpeng.systems/">Xiangpeng Hao</a>'s
previous analysis theorized significant (2x–4x) potential performance
improvements simply by optimizing the implementation of Parquet footer parsing
(see <a href="https://www.influxdata.com/blog/how-good-parquet-wide-tables/">How Good is Parquet for Wide Tables (Machine Learning
Workloads) Really?</a> for more details).</p>
<h2>Processing Thrift Using Generated Parsers</h2>
<p><em>Parsing</em> Parquet metadata is the process of decoding the Thrift-encoded bytes
into in-memory structures that can be used for computation. Most Parquet
implementations use one of the existing <a href="https://thrift.apache.org/lib/">Thrift compilers</a> to generate a parser
that converts Thrift binary data into generated code structures, and then copy
relevant portions of those generated structures into API-level structures.
For example, the <a href="https://github.com/apache/arrow/blob/e1f727cbb447d2385949a54d8f4be2fdc6cefe29/cpp/src/parquet">C/C++ Parquet implementation</a> includes a <a href="https://github.com/apache/arrow/blob/e1f727cbb447d2385949a54d8f4be2fdc6cefe29/cpp/build-support/update-thrift.sh#L23">two</a>-<a href="https://github.com/apache/arrow/blob/e1f727cbb447d2385949a54d8f4be2fdc6cefe29/cpp/src/parquet/thrift_internal.h#L56">step</a> process,
as does <a href="https://github.com/apache/parquet-java/blob/0fea3e1e22fffb0a25193e3efb9a5d090899458a/parquet-format-structures/pom.xml#L69-L88">parquet-java</a>. <a href="https://github.com/duckdb/duckdb/blob/8f512187537c65d36ce6d6f562b75a37e8d4ee54/third_party/parquet/parquet_types.h#L1-L6">DuckDB</a> also contains a Thrift compiler–generated
parser.</p>
<p>In versions <code>56.2.0</code> and earlier, the Apache Arrow Rust implementation used the
same pattern. The <a href="https://docs.rs/parquet/56.2.0/parquet/format/index.html">format</a> module contains a parser generated by the <a href="https://crates.io/crates/thrift">thrift
crate</a> and the <a href="https://github.com/apache/parquet-format/blob/master/src/main/thrift/parquet.thrift">parquet.thrift</a> definition. Parsing metadata involves:</p>
<ol>
<li>Invoke the generated parser on the Thrift binary data, producing
generated in-memory structures (e.g., <a href="https://docs.rs/parquet/56.2.0/parquet/format/struct.FileMetaData.html"><code>struct FileMetaData</code></a>), then</li>
<li>Copy the relevant fields into a more user-friendly representation,
<a href="https://docs.rs/parquet/56.2.0/parquet/file/metadata/struct.ParquetMetaData.html"><code>ParquetMetadata</code></a>.</li>
</ol>
<!-- Image source: https://docs.google.com/presentation/d/1WjX4t7YVj2kY14SqCpenGqNl_swjdHvPg86UeBT3IcY -->
<div style="display: flex; gap: 16px; justify-content: center; align-items: flex-start;">
  <img src="/img/rust-parquet-metadata/original-pipeline.png" width="100%" class="img-responsive" alt="Original Parquet Parsing Pipeline" aria-hidden="true">
</div>
<p><em>Figure 6:</em> Two-step process to read Parquet metadata: A parser created with the
<code>thrift</code> crate and <code>parquet.thrift</code> parses the metadata bytes
into generated in-memory structures. These structures are then converted into
API objects.</p>
<p>The parsers generated by standard Thrift compilers typically parse <em>all</em> fields
in a single pass over the Thrift-encoded bytes, copying data into in-memory,
heap-allocated structures (e.g., Rust <a href="https://doc.rust-lang.org/std/vec/struct.Vec.html"><code>Vec</code></a>, or C++ <a href="https://en.cppreference.com/w/cpp/container/vector.html"><code>std::vector</code></a>) as shown
in Figure 7 below.</p>
<p>Parsing all fields is straightforward and a good default
choice given Thrift's original design goal of encoding network messages.
Network messages typically don't contain extra information irrelevant for receivers;
however, Parquet metadata often <em>does</em> contain information
that is not needed for a particular query. In such cases, parsing the entire
metadata into in-memory structures is wasteful.</p>
<p>For example, a query on a file with 1,000 columns that reads
only 10 columns and has a single column predicate
(e.g., <code>time &gt; now() - '1 minute'</code>) only needs</p>
<ol>
<li><a href="https://github.com/apache/parquet-format/blob/9fd57b59e0ce1a82a69237dcf8977d3e72a2965d/src/main/thrift/parquet.thrift#L912"><code>Statistics</code></a> (or <a href="https://github.com/apache/parquet-format/blob/9fd57b59e0ce1a82a69237dcf8977d3e72a2965d/src/main/thrift/parquet.thrift#L1163"><code>ColumnIndex</code></a>) for the <code>time</code> column</li>
<li><a href="https://github.com/apache/parquet-format/blob/9fd57b59e0ce1a82a69237dcf8977d3e72a2965d/src/main/thrift/parquet.thrift#L958"><code>ColumnChunk</code></a> information for the 10 selected columns</li>
</ol>
<p>The default strategy to parse (allocating and copying) all statistics and all
<code>ColumnChunks</code> results in creating 999 more statistics and 990 more <code>ColumnChunks</code>
than necessary. As discussed above, given the
variable encoding used for the metadata, all metadata bytes must still be
fetched and scanned; however, CPUs are (very) fast at scanning data, and
skipping <em>parsing</em> of unneeded fields speeds up overall metadata performance
significantly.</p>
<!-- Image source: https://docs.google.com/presentation/d/1WjX4t7YVj2kY14SqCpenGqNl_swjdHvPg86UeBT3IcY -->
<div style="display: flex; gap: 16px; justify-content: center; align-items: flex-start;">
  <img src="/img/rust-parquet-metadata/thrift-parsing-allocations.png" width="100%" class="img-responsive" alt="Thrift Parsing Allocations" aria-hidden="true">
</div>
<p><em>Figure 7:</em> Generated Thrift parsers typically parse encoded bytes into
structures requiring many small heap allocations, which are expensive.</p>
<h2>New Design: Custom Thrift Parser</h2>
<p>As is typical of generated code, opportunities for specializing
the behavior of generated Thrift parsers is limited:</p>
<ol>
<li>It is not easy to modify (it is re-generated from the
Thrift definitions when they change and carries the warning
<code>/* DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING */</code>).</li>
<li>It typically maps one-to-one with Thrift definitions, limiting
additional optimizations such as zero-copy parsing, field
skipping, and amortized memory allocation strategies.</li>
<li>Its API is very stable (hard to change), which is important for easy maintenance when a large number
of projects are built using the <a href="https://crates.io/crates/thrift">thrift crate</a>. For example, the
<a href="https://crates.io/crates/thrift/0.17.0">last release of the Rust <code>thrift</code> crate</a> was almost three years ago at
the time of this writing.</li>
</ol>
<p>These limitations are a consequence of the Thrift project's design goals: general purpose
code that is easy to embed in a wide variety of other projects, rather than
any fundamental limitation of the Thrift format.
Given our goal of fast Parquet metadata parsing, we needed
a custom, easier to optimize parser, to convert Thrift binary directly into the needed
structures (Figure 8). Since arrow-rs already did some postprocessing on the generated code
and included a custom implementation of the compact protocol api, this change
to a completely custom parser was a natural next step.</p>
<!-- Image source: https://docs.google.com/presentation/d/1WjX4t7YVj2kY14SqCpenGqNl_swjdHvPg86UeBT3IcY -->
<div style="display: flex; gap: 16px; justify-content: center; align-items: flex-start;">
  <img src="/img/rust-parquet-metadata/new-pipeline.png" width="100%" class="img-responsive" alt="New Parquet Parsing Pipeline" aria-hidden="true">
</div>
<p><em>Figure 8:</em> One-step Parquet metadata parsing using a custom Thrift parser. The
Thrift binary is parsed directly into the desired in-memory representation with
highly optimized code.</p>
<p>Our new custom parser is optimized for the specific subset of Thrift used by
Parquet and contains various performance optimizations, such as careful
memory allocation. The largest initial speedup came from removing
intermediate structures and directly creating the needed in-memory representation.
We also carefully hand-optimized several performance-critical code paths (see <a href="https://github.com/apache/arrow-rs/pull/8574">#8574</a>,
<a href="https://github.com/apache/arrow-rs/pull/8587">#8587</a>, and <a href="https://github.com/apache/arrow-rs/pull/8599">#8599</a>).</p>
<h3>Maintainability</h3>
<p>The largest concern with a custom parser is that it is more difficult
to maintain than generated parsers because the custom parser must be updated to
reflect any changes to <a href="https://github.com/apache/parquet-format/blob/master/src/main/thrift/parquet.thrift">parquet.thrift</a>. This is a growing concern given the
resurgent interest in Parquet and the recent addition of new features such as
<a href="https://github.com/apache/parquet-format/blob/master/Geospatial.md">Geospatial</a> and <a href="https://github.com/apache/parquet-format/blob/master/VariantEncoding.md">Variant</a> types.</p>
<p>Thankfully, after discussions with the community, <a href="https://github.com/jhorstmann">Jörn Horstmann</a> developed
a <a href="https://github.com/jhorstmann/compact-thrift">Rust macro based approach</a> for generating code with annotated Rust structs
that closely resemble the Thrift definitions while permitting additional hand
optimization where necessary. This approach is similar to the <a href="https://serde.rs/">serde</a> crate
where generic implementations can be generated with <code>#[derive]</code> annotations and
specialized serialization is written by hand where needed. <a href="https://github.com/etseidl">Ed Seidl</a> then
rewrote the metadata parsing code in the <a href="https://crates.io/crates/parquet">parquet</a> crate using these macros.
Please see the <a href="https://github.com/apache/arrow-rs/pull/8530">final PR</a> for details of the level of effort involved.</p>
<p>For example, here is the original Thrift definition of the <a href="https://github.com/apache/parquet-format/blob/9fd57b59e0ce1a82a69237dcf8977d3e72a2965d/src/main/thrift/parquet.thrift#L1254C1-L1314C2"><code>FileMetaData</code></a> structure (comments omitted for brevity):</p>
<div class="language-thrift highlighter-rouge"><div class="highlight"><pre class="highlight"><code data-lang="thrift">struct FileMetaData {
  1: required i32 version
  2: required list&lt;SchemaElement&gt; schema;
  3: required i64 num_rows
  4: required list&lt;RowGroup&gt; row_groups
  5: optional list&lt;KeyValue&gt; key_value_metadata
  6: optional string created_by
  7: optional list&lt;ColumnOrder&gt; column_orders;
  8: optional EncryptionAlgorithm encryption_algorithm
  9: optional binary footer_signing_key_metadata
}
</code></pre></div></div>
<p>And here (<a href="https://github.com/apache/arrow-rs/blob/02fa779a9cb122c5218293be3afb980832701683/parquet/src/file/metadata/thrift_gen.rs#L146-L158">source</a>) is the corresponding Rust structure using the Thrift macros (before Ed wrote a custom version in <a href="https://github.com/apache/arrow-rs/pull/8574">#8574</a>):</p>
<div class="language-rust highlighter-rouge"><div class="highlight"><pre class="highlight"><code data-lang="rust"><span class="nd">thrift_struct!</span><span class="p">(</span>
<span class="k">struct</span> <span class="n">FileMetaData</span><span class="o">&lt;</span><span class="nv">'a</span><span class="o">&gt;</span> <span class="p">{</span>
<span class="mi">1</span><span class="p">:</span> <span class="n">required</span> <span class="nb">i32</span> <span class="n">version</span>
<span class="mi">2</span><span class="p">:</span> <span class="n">required</span> <span class="n">list</span><span class="o">&lt;</span><span class="nv">'a</span><span class="o">&gt;&lt;</span><span class="n">SchemaElement</span><span class="o">&gt;</span> <span class="n">schema</span><span class="p">;</span>
<span class="mi">3</span><span class="p">:</span> <span class="n">required</span> <span class="nb">i64</span> <span class="n">num_rows</span>
<span class="mi">4</span><span class="p">:</span> <span class="n">required</span> <span class="n">list</span><span class="o">&lt;</span><span class="nv">'a</span><span class="o">&gt;&lt;</span><span class="n">RowGroup</span><span class="o">&gt;</span> <span class="n">row_groups</span>
<span class="mi">5</span><span class="p">:</span> <span class="n">optional</span> <span class="n">list</span><span class="o">&lt;</span><span class="n">KeyValue</span><span class="o">&gt;</span> <span class="n">key_value_metadata</span>
<span class="mi">6</span><span class="p">:</span> <span class="n">optional</span> <span class="n">string</span><span class="o">&lt;</span><span class="nv">'a</span><span class="o">&gt;</span> <span class="n">created_by</span>
<span class="mi">7</span><span class="p">:</span> <span class="n">optional</span> <span class="n">list</span><span class="o">&lt;</span><span class="n">ColumnOrder</span><span class="o">&gt;</span> <span class="n">column_orders</span><span class="p">;</span>
<span class="mi">8</span><span class="p">:</span> <span class="n">optional</span> <span class="n">EncryptionAlgorithm</span> <span class="n">encryption_algorithm</span>
<span class="mi">9</span><span class="p">:</span> <span class="n">optional</span> <span class="n">binary</span><span class="o">&lt;</span><span class="nv">'a</span><span class="o">&gt;</span> <span class="n">footer_signing_key_metadata</span>
<span class="p">}</span>
<span class="p">);</span>
</code></pre></div></div>
<p>This system makes it easy to see the correspondence between the Thrift
definition and the Rust structure, and it is straightforward to support newly added
features such as <code>GeospatialStatistics</code>. The carefully hand-
optimized parsers for the most performance-critical structures, such as
<code>RowGroupMetaData</code> and <code>ColumnChunkMetaData</code>, are harder—though still
straightforward—to update (see <a href="https://github.com/apache/arrow-rs/pull/8587">#8587</a>). However, those structures are also less
likely to change frequently.</p>
<h3>Future Improvements</h3>
<p>With the custom parser in place, we are working on additional improvements:</p>
<ul>
<li>Implementing special &quot;skip&quot; indexes to skip directly to the parts of the metadata
that are needed for a particular query, such as the row group offsets.</li>
<li>Selectively decoding only the statistics for columns that are needed for a particular query.</li>
<li>Potentially contributing the macros back to the thrift crate.</li>
</ul>
<h3>Conclusion</h3>
<p>We believe metadata parsing in many open source Parquet
readers is slow primarily because they use parsers automatically generated by Thrift
compilers, which are not optimized for Parquet metadata parsing. By writing a
custom parser, we significantly sped up metadata parsing in the
<a href="https://crates.io/crates/parquet">parquet</a> Rust crate, which is widely used in the <a href="https://arrow.apache.org/">Apache Arrow</a> ecosystem.</p>
<p>While this is not the first open source custom Thrift parser for Parquet
metadata (<a href="https://github.com/rapidsai/cudf/blob/branch-25.12/cpp/src/io/parquet/compact_protocol_reader.hpp">CUDF has had one</a> for many years), we hope that our results will
encourage additional Parquet implementations to consider similar optimizations.
The approach and optimizations we describe in this post are likely applicable to
Parquet implementations in other languages, such as C++ and Java.</p>
<p>Previously, efforts like this were only possible at well-financed commercial
enterprises. On behalf of the arrow-rs and Parquet contributors, we are excited
to share this technology with the community in the upcoming <a href="https://crates.io/crates/parquet/57.0.0">57.0.0</a> release and
invite you to <a href="https://github.com/apache/arrow-rs/blob/main/CONTRIBUTING.md">come join us</a> and help make it even better!</p>]]></content><author><name>alamb</name></author><category term="release" /><summary type="html"><![CDATA[Editor’s Note: While Apache Arrow and Apache Parquet are separate projects, the Arrow arrow-rs repository hosts the development of the parquet Rust crate, a widely used and high-performance Parquet implementation. Summary Version 57.0.0 of the parquet Rust crate decodes metadata more than three times faster than previous versions thanks to a new custom Apache Thrift parser. The new parser is both faster in all cases and enables further performance improvements not possible with generated parsers, such as skipping unnecessary fields and selective parsing. Figure 1: Performance comparison of Apache Parquet metadata parsing using a generated Thrift parser (versions 56.2.0 and earlier) and the new custom Thrift parser in arrow-rs version 57.0.0. No changes are needed to the Parquet format itself. See the benchmark page for more details. Figure 2: Speedup of the [custom Thrift decoder] for string and floating-point data types, for 100, 1000, 10,000, and 100,000 columns. The new parser is faster in all cases, and the speedup is similar regardless of the number of columns. See the benchmark page for more details. Introduction: Parquet and the Importance of Metadata Parsing Apache Parquet is a popular columnar storage format designed to be efficient for both storage and query processing. Parquet files consist of a series of data pages, and a footer, as shown in Figure 3. The footer contains metadata about the file, including schema, statistics, and other information needed to decode the data pages. Figure 3: Structure of a Parquet file showing the header, data pages, and footer metadata. Getting information stored in the footer is typically the first step in reading a Parquet file, as it is required to interpret the data pages. Parsing the footer is often performance critical: When reading from fast local storage, such as modern NVMe SSDs, footer parsing must be completed to know what data pages to read, placing it directly on the critical I/O path. Footer parsing scales linearly with the number of columns and row groups in a Parquet file and thus can be a bottleneck for tables with many columns or files with many row groups. Even in systems that cache the parsed footer in memory (see Using External Indexes, Metadata Stores, Catalogs and Caches to Accelerate Queries on Apache Parquet), the footer must still be parsed on cache miss. Figure 4: Typical processing flow for Parquet files for stateless and stateful systems. Stateless engines read the footer on every query, so the time taken to parse the footer directly adds to query latency. Stateful systems cache some or all of the parsed footer in advance of queries. The speed of parsing metadata has grown even more important as Parquet spreads throughout the data ecosystem and is used for more latency-sensitive workloads such as observability, interactive analytics, and single-point lookups for Retrieval-Augmented Generation (RAG) applications feeding LLMs. As overall query times decrease, the proportion spent on footer parsing increases. Background: Apache Thrift Parquet stores metadata using Apache Thrift, a framework for network data types and service interfaces. It includes a data definition language similar to Protocol Buffers. Thrift definition files describe data types in a language-neutral way, and systems typically use code generators to automatically create code for a specific programming language to read and write those data types. The parquet.thrift file defines the format of the metadata serialized at the end of each Parquet file in the Thrift Compact protocol, as shown below in Figure 5. The binary encoding is &quot;variable-length&quot;, meaning that the length of each element depends on its content, not just its type. Smaller-valued primitive types are encoded in fewer bytes than larger values, and strings and lists are stored inline, prefixed with their length. This encoding is space-efficient but, due to being variable-length, does not support random access: it is not possible to locate a particular field without scanning all previous fields. Other formats such as FlatBuffers provide random-access parsing and have been proposed as alternatives given their theoretical performance advantages. However, changing the Parquet format is a significant undertaking, requires buy-in from the community and ecosystem, and would likely take years to be adopted. Figure 5: Parquet metadata is serialized using the Thrift Compact protocol. Each field is stored using a variable number of bytes that depends on its value. Primitive types use a variable-length encoding and strings and lists are prefixed with their lengths. Despite Thrift's very real disadvantage due to lack of random access, software optimizations are much easier to deploy than format changes. Xiangpeng Hao's previous analysis theorized significant (2x–4x) potential performance improvements simply by optimizing the implementation of Parquet footer parsing (see How Good is Parquet for Wide Tables (Machine Learning Workloads) Really? for more details). Processing Thrift Using Generated Parsers Parsing Parquet metadata is the process of decoding the Thrift-encoded bytes into in-memory structures that can be used for computation. Most Parquet implementations use one of the existing Thrift compilers to generate a parser that converts Thrift binary data into generated code structures, and then copy relevant portions of those generated structures into API-level structures. For example, the C/C++ Parquet implementation includes a two-step process, as does parquet-java. DuckDB also contains a Thrift compiler–generated parser. In versions 56.2.0 and earlier, the Apache Arrow Rust implementation used the same pattern. The format module contains a parser generated by the thrift crate and the parquet.thrift definition. Parsing metadata involves: Invoke the generated parser on the Thrift binary data, producing generated in-memory structures (e.g., struct FileMetaData), then Copy the relevant fields into a more user-friendly representation, ParquetMetadata. Figure 6: Two-step process to read Parquet metadata: A parser created with the thrift crate and parquet.thrift parses the metadata bytes into generated in-memory structures. These structures are then converted into API objects. The parsers generated by standard Thrift compilers typically parse all fields in a single pass over the Thrift-encoded bytes, copying data into in-memory, heap-allocated structures (e.g., Rust Vec, or C++ std::vector) as shown in Figure 7 below. Parsing all fields is straightforward and a good default choice given Thrift's original design goal of encoding network messages. Network messages typically don't contain extra information irrelevant for receivers; however, Parquet metadata often does contain information that is not needed for a particular query. In such cases, parsing the entire metadata into in-memory structures is wasteful. For example, a query on a file with 1,000 columns that reads only 10 columns and has a single column predicate (e.g., time &gt; now() - '1 minute') only needs Statistics (or ColumnIndex) for the time column ColumnChunk information for the 10 selected columns The default strategy to parse (allocating and copying) all statistics and all ColumnChunks results in creating 999 more statistics and 990 more ColumnChunks than necessary. As discussed above, given the variable encoding used for the metadata, all metadata bytes must still be fetched and scanned; however, CPUs are (very) fast at scanning data, and skipping parsing of unneeded fields speeds up overall metadata performance significantly. Figure 7: Generated Thrift parsers typically parse encoded bytes into structures requiring many small heap allocations, which are expensive. New Design: Custom Thrift Parser As is typical of generated code, opportunities for specializing the behavior of generated Thrift parsers is limited: It is not easy to modify (it is re-generated from the Thrift definitions when they change and carries the warning /* DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING */). It typically maps one-to-one with Thrift definitions, limiting additional optimizations such as zero-copy parsing, field skipping, and amortized memory allocation strategies. Its API is very stable (hard to change), which is important for easy maintenance when a large number of projects are built using the thrift crate. For example, the last release of the Rust thrift crate was almost three years ago at the time of this writing. These limitations are a consequence of the Thrift project's design goals: general purpose code that is easy to embed in a wide variety of other projects, rather than any fundamental limitation of the Thrift format. Given our goal of fast Parquet metadata parsing, we needed a custom, easier to optimize parser, to convert Thrift binary directly into the needed structures (Figure 8). Since arrow-rs already did some postprocessing on the generated code and included a custom implementation of the compact protocol api, this change to a completely custom parser was a natural next step. Figure 8: One-step Parquet metadata parsing using a custom Thrift parser. The Thrift binary is parsed directly into the desired in-memory representation with highly optimized code. Our new custom parser is optimized for the specific subset of Thrift used by Parquet and contains various performance optimizations, such as careful memory allocation. The largest initial speedup came from removing intermediate structures and directly creating the needed in-memory representation. We also carefully hand-optimized several performance-critical code paths (see #8574, #8587, and #8599). Maintainability The largest concern with a custom parser is that it is more difficult to maintain than generated parsers because the custom parser must be updated to reflect any changes to parquet.thrift. This is a growing concern given the resurgent interest in Parquet and the recent addition of new features such as Geospatial and Variant types. Thankfully, after discussions with the community, Jörn Horstmann developed a Rust macro based approach for generating code with annotated Rust structs that closely resemble the Thrift definitions while permitting additional hand optimization where necessary. This approach is similar to the serde crate where generic implementations can be generated with #[derive] annotations and specialized serialization is written by hand where needed. Ed Seidl then rewrote the metadata parsing code in the parquet crate using these macros. Please see the final PR for details of the level of effort involved. For example, here is the original Thrift definition of the FileMetaData structure (comments omitted for brevity): struct FileMetaData { 1: required i32 version 2: required list&lt;SchemaElement&gt; schema; 3: required i64 num_rows 4: required list&lt;RowGroup&gt; row_groups 5: optional list&lt;KeyValue&gt; key_value_metadata 6: optional string created_by 7: optional list&lt;ColumnOrder&gt; column_orders; 8: optional EncryptionAlgorithm encryption_algorithm 9: optional binary footer_signing_key_metadata } And here (source) is the corresponding Rust structure using the Thrift macros (before Ed wrote a custom version in #8574): thrift_struct!( struct FileMetaData&lt;'a&gt; { 1: required i32 version 2: required list&lt;'a&gt;&lt;SchemaElement&gt; schema; 3: required i64 num_rows 4: required list&lt;'a&gt;&lt;RowGroup&gt; row_groups 5: optional list&lt;KeyValue&gt; key_value_metadata 6: optional string&lt;'a&gt; created_by 7: optional list&lt;ColumnOrder&gt; column_orders; 8: optional EncryptionAlgorithm encryption_algorithm 9: optional binary&lt;'a&gt; footer_signing_key_metadata } ); This system makes it easy to see the correspondence between the Thrift definition and the Rust structure, and it is straightforward to support newly added features such as GeospatialStatistics. The carefully hand- optimized parsers for the most performance-critical structures, such as RowGroupMetaData and ColumnChunkMetaData, are harder—though still straightforward—to update (see #8587). However, those structures are also less likely to change frequently. Future Improvements With the custom parser in place, we are working on additional improvements: Implementing special &quot;skip&quot; indexes to skip directly to the parts of the metadata that are needed for a particular query, such as the row group offsets. Selectively decoding only the statistics for columns that are needed for a particular query. Potentially contributing the macros back to the thrift crate. Conclusion We believe metadata parsing in many open source Parquet readers is slow primarily because they use parsers automatically generated by Thrift compilers, which are not optimized for Parquet metadata parsing. By writing a custom parser, we significantly sped up metadata parsing in the parquet Rust crate, which is widely used in the Apache Arrow ecosystem. While this is not the first open source custom Thrift parser for Parquet metadata (CUDF has had one for many years), we hope that our results will encourage additional Parquet implementations to consider similar optimizations. The approach and optimizations we describe in this post are likely applicable to Parquet implementations in other languages, such as C++ and Java. Previously, efforts like this were only possible at well-financed commercial enterprises. On behalf of the arrow-rs and Parquet contributors, we are excited to share this technology with the community in the upcoming 57.0.0 release and invite you to come join us and help make it even better!]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Apache Arrow ADBC 20 (Libraries) Release</title><link href="https://arrow.apache.org/blog/2025/09/12/adbc-20-release/" rel="alternate" type="text/html" title="Apache Arrow ADBC 20 (Libraries) Release" /><published>2025-09-12T00:00:00-04:00</published><updated>2025-09-12T00:00:00-04:00</updated><id>https://arrow.apache.org/blog/2025/09/12/adbc-20-release</id><content type="html" xml:base="https://arrow.apache.org/blog/2025/09/12/adbc-20-release/"><![CDATA[<!--

-->
<p>The Apache Arrow team is pleased to announce the version 20 release of
the Apache Arrow ADBC libraries. This release includes <a href="https://github.com/apache/arrow-adbc/milestone/24"><strong>44
resolved issues</strong></a> from <a href="#contributors"><strong>29 distinct contributors</strong></a>.</p>
<p>This is a release of the <strong>libraries</strong>, which are at version 20.  The
<a href="https://arrow.apache.org/adbc/20/format/specification.html"><strong>API specification</strong></a> is versioned separately and is at
version 1.1.0.</p>
<p>The subcomponents are versioned independently:</p>
<ul>
<li>C/C++/GLib/Go/Python/Ruby: 1.8.0</li>
<li>C#: 0.20.0</li>
<li>Java: 0.20.0</li>
<li>R: 0.20.0</li>
<li>Rust: 0.20.0</li>
</ul>
<p>The release notes below are not exhaustive and only expose selected
highlights of the release. Many other bugfixes and improvements have
been made: we refer you to the <a href="https://github.com/apache/arrow-adbc/blob/apache-arrow-adbc-20/CHANGELOG.md">complete changelog</a>.</p>
<h2>Release Highlights</h2>
<p>Driver managers now support loading driver manifests.  To learn more about
this feature, please see the <a href="https://arrow.apache.org/adbc/current/format/driver_manifests.html#driver-manifests">documentation</a>.</p>
<p>The Rust crates were reorganized.  <strong>This is a breaking change.</strong>  Now,
FFI-related code is part of <code>adbc_ffi</code> and the driver manager is part of
<code>adbc_driver_manager</code>.  Previously these were features of a single crate
<code>adbc_core</code>, which now only contains API definitions
(<a href="https://github.com/apache/arrow-adbc/pull/3381">#3381</a>,
<a href="https://github.com/apache/arrow-adbc/pull/3197">#3197</a>).  Also, some enums
are no longer marked as <code>#[non_exhaustive]</code>
(<a href="https://github.com/apache/arrow-adbc/pull/3245">#3245</a>).</p>
<p>The Java JNI bindings support a few more features
(<a href="https://github.com/apache/arrow-adbc/pull/3373">#3373</a>,
<a href="https://github.com/apache/arrow-adbc/pull/3372">#3372</a>,
<a href="https://github.com/apache/arrow-adbc/pull/3370">#3370</a>,
<a href="https://github.com/apache/arrow-adbc/pull/3348">#3348</a>).</p>
<p>The BigQuery driver properly uses microsecond timestamps
(<a href="https://github.com/apache/arrow-adbc/pull/3364">#3364</a>), has an improved
error message if your user account lacks the proper permissions
(<a href="https://github.com/apache/arrow-adbc/pull/3297">#3297</a>), properly handles
nested data (<a href="https://github.com/apache/arrow-adbc/pull/3240">#3240</a>), and
supports service account impersonation
(<a href="https://github.com/apache/arrow-adbc/pull/3174">#3174</a>).  The C#
Databricks/HiveServer2 Thrift-protocol drivers continues to expand their
featureset, such as support for cancelling statements, token exchange, and
better tracing (<a href="https://github.com/apache/arrow-adbc/pull/3304">#3304</a>,
<a href="https://github.com/apache/arrow-adbc/pull/3302">#3302</a>,
<a href="https://github.com/apache/arrow-adbc/pull/3301">#3301</a>,
<a href="https://github.com/apache/arrow-adbc/pull/3224">#3224</a>,
<a href="https://github.com/apache/arrow-adbc/pull/3218">#3218</a>,
<a href="https://github.com/apache/arrow-adbc/pull/3192">#3192</a>,
<a href="https://github.com/apache/arrow-adbc/pull/3177">#3177</a>,
<a href="https://github.com/apache/arrow-adbc/pull/3137">#3137</a>,
<a href="https://github.com/apache/arrow-adbc/pull/3127">#3127</a>).  The PostgreSQL
driver will properly bind <code>arrow.json</code> extension arrays as JSON parameters
(<a href="https://github.com/apache/arrow-adbc/pull/3333">#3333</a>).  The Snowflake
driver supports more authentication methods
(<a href="https://github.com/apache/arrow-adbc/pull/3366">#3366</a>).  The SQLite driver
can bind parameters by name instead of position
(<a href="https://github.com/apache/arrow-adbc/pull/3362">#3362</a>).</p>
<p>The C# library has been upgraded to .NET 8
(<a href="https://github.com/apache/arrow-adbc/pull/3120">#3120</a>).</p>
<p>GLib has more bindings to ADBC functions
(<a href="https://github.com/apache/arrow-adbc/pull/3118">#3118</a>).</p>
<p>The Go library has some experimental helpers to simplify getting driver
metadata (<a href="https://github.com/apache/arrow-adbc/pull/3239">#3239</a>) and
ingesting Arrow data
(<a href="https://github.com/apache/arrow-adbc/pull/3150">#3150</a>).  The <code>database/sql</code>
adapter handles <code>time.Time</code> values for bind parameters now
(<a href="https://github.com/apache/arrow-adbc/pull/3109">#3109</a>).  Drivers will
forward SQLSTATE and other error metadata across the FFI boundary
(<a href="https://github.com/apache/arrow-adbc/pull/2801">#2801</a>).</p>
<h2>Contributors</h2>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ git shortlog --perl-regexp --author='^((?!dependabot\[bot\]).*)$' -sn apache-arrow-adbc-19..apache-arrow-adbc-20
    28	David Li
    14	Todd Meng
    13	Bryce Mecum
    13	eitsupi
    12	Jacky Hu
    12	Matt Topol
     8	Bruce Irschick
     7	Matthijs Brobbel
     6	davidhcoe
     5	eric-wang-1990
     4	Alex Guo
     3	Daijiro Fukuda
     3	Felipe Oliveira Carvalho
     3	Sutou Kouhei
     2	Curt Hagenlocher
     2	Jade Wang
     2	Mandukhai Alimaa
     2	amangoyal
     1	Arseny Tsypushkin
     1	Dewey Dunnington
     1	Even Rouault
     1	Ian Cook
     1	Jordan E
     1	Lucas Valente
     1	Mila Page
     1	Ryan Syed
     1	Sudhir Reddy Emmadi
     1	Xuliang (Harry) Sun
     1	Yu Ishikawa
</code></pre></div></div>
<h2>Roadmap</h2>
<p>A Go-based driver for Databricks is in the works from a contributor.</p>
<h2>Getting Involved</h2>
<p>We welcome questions and contributions from all interested.  Issues
can be filed on <a href="https://github.com/apache/arrow-adbc/issues">GitHub</a>, and questions can be directed to GitHub
or the <a href="/community/">Arrow mailing lists</a>.</p>]]></content><author><name>pmc</name></author><category term="release" /><summary type="html"><![CDATA[The Apache Arrow team is pleased to announce the version 20 release of the Apache Arrow ADBC libraries. This release includes 44 resolved issues from 29 distinct contributors. This is a release of the libraries, which are at version 20. The API specification is versioned separately and is at version 1.1.0. The subcomponents are versioned independently: C/C++/GLib/Go/Python/Ruby: 1.8.0 C#: 0.20.0 Java: 0.20.0 R: 0.20.0 Rust: 0.20.0 The release notes below are not exhaustive and only expose selected highlights of the release. Many other bugfixes and improvements have been made: we refer you to the complete changelog. Release Highlights Driver managers now support loading driver manifests. To learn more about this feature, please see the documentation. The Rust crates were reorganized. This is a breaking change. Now, FFI-related code is part of adbc_ffi and the driver manager is part of adbc_driver_manager. Previously these were features of a single crate adbc_core, which now only contains API definitions (#3381, #3197). Also, some enums are no longer marked as #[non_exhaustive] (#3245). The Java JNI bindings support a few more features (#3373, #3372, #3370, #3348). The BigQuery driver properly uses microsecond timestamps (#3364), has an improved error message if your user account lacks the proper permissions (#3297), properly handles nested data (#3240), and supports service account impersonation (#3174). The C# Databricks/HiveServer2 Thrift-protocol drivers continues to expand their featureset, such as support for cancelling statements, token exchange, and better tracing (#3304, #3302, #3301, #3224, #3218, #3192, #3177, #3137, #3127). The PostgreSQL driver will properly bind arrow.json extension arrays as JSON parameters (#3333). The Snowflake driver supports more authentication methods (#3366). The SQLite driver can bind parameters by name instead of position (#3362). The C# library has been upgraded to .NET 8 (#3120). GLib has more bindings to ADBC functions (#3118). The Go library has some experimental helpers to simplify getting driver metadata (#3239) and ingesting Arrow data (#3150). The database/sql adapter handles time.Time values for bind parameters now (#3109). Drivers will forward SQLSTATE and other error metadata across the FFI boundary (#2801). Contributors $ git shortlog --perl-regexp --author='^((?!dependabot\[bot\]).*)$' -sn apache-arrow-adbc-19..apache-arrow-adbc-20 28 David Li 14 Todd Meng 13 Bryce Mecum 13 eitsupi 12 Jacky Hu 12 Matt Topol 8 Bruce Irschick 7 Matthijs Brobbel 6 davidhcoe 5 eric-wang-1990 4 Alex Guo 3 Daijiro Fukuda 3 Felipe Oliveira Carvalho 3 Sutou Kouhei 2 Curt Hagenlocher 2 Jade Wang 2 Mandukhai Alimaa 2 amangoyal 1 Arseny Tsypushkin 1 Dewey Dunnington 1 Even Rouault 1 Ian Cook 1 Jordan E 1 Lucas Valente 1 Mila Page 1 Ryan Syed 1 Sudhir Reddy Emmadi 1 Xuliang (Harry) Sun 1 Yu Ishikawa Roadmap A Go-based driver for Databricks is in the works from a contributor. Getting Involved We welcome questions and contributions from all interested. Issues can be filed on GitHub, and questions can be directed to GitHub or the Arrow mailing lists.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Apache Arrow Go 18.4.1 Release</title><link href="https://arrow.apache.org/blog/2025/09/04/arrow-go-18.4.1/" rel="alternate" type="text/html" title="Apache Arrow Go 18.4.1 Release" /><published>2025-09-04T00:00:00-04:00</published><updated>2025-09-04T00:00:00-04:00</updated><id>https://arrow.apache.org/blog/2025/09/04/arrow-go-18.4.1</id><content type="html" xml:base="https://arrow.apache.org/blog/2025/09/04/arrow-go-18.4.1/"><![CDATA[<!--

-->
<p>The Apache Arrow team is pleased to announce the v18.4.1 release of Apache Arrow Go.
This patch release covers 15 commits from 7 distinct contributors.</p>
<h2>Contributors</h2>
<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code data-lang="console"><span class="gp">$</span><span class="w"> </span>git shortlog <span class="nt">-sn</span> v18.4.0..v18.4.1
<span class="go">     7	Matt Topol
     4	Mandukhai Alimaa
     1	Chromo-residuum-opec
     1	Ryan Schneider
     1	Travis Patterson
     1	daniel-adam-tfs
     1	secfree
</span></code></pre></div></div>
<h2>Highlights</h2>
<ul>
<li>The <code>Record</code> interface type has been renamed to <code>RecordBatch</code> to align with
other Arrow implementations and to avoid confusion. The old <code>Record</code> type is
aliased to the new <code>RecordBatch</code> type so existing code works but users may wish
to update references now. This work was contributed by a first-time contributor,
@Mandukhai-Alimaa. See <a href="https://github.com/apache/arrow-go/pull/466">#466</a>,
<a href="https://github.com/apache/arrow-go/pull/473">#473</a>,
<a href="https://github.com/apache/arrow-go/pull/478">#478</a>, and
<a href="https://github.com/apache/arrow-go/pull/486">#486</a>.</li>
</ul>
<h3>Important Note</h3>
<ul>
<li>A side effect of the above was an unintentional breaking change by introducing a new
method to the <code>RecordReader</code> interface. This shouldn't affect the majority of consumers,
but is a breaking change for any who implemented their own concrete <code>RecordReader</code> type.
The solution is simply to add a <code>RecordBatch()</code> method to the type when upgrading to v18.4.1</li>
</ul>
<h2>Changelog</h2>
<h3>What's Changed</h3>
<ul>
<li>fix(arrow/compute/exprs): Handle large types in expr handling by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/440">https://github.com/apache/arrow-go/pull/440</a></li>
<li>fix(arrow/compute/exprs): fix literalToDatum for precision types by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/447">https://github.com/apache/arrow-go/pull/447</a></li>
<li>fix(arrow/array): Fix RecordFromJSON perf by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/449">https://github.com/apache/arrow-go/pull/449</a></li>
<li>fix(arrow/array): update timestamp json format by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/450">https://github.com/apache/arrow-go/pull/450</a></li>
<li>refactor: switch golang.org/x/exp to standard library packages by @ufUNnxagpM in <a href="https://github.com/apache/arrow-go/pull/453">https://github.com/apache/arrow-go/pull/453</a></li>
<li>fix(parquet/pqarrow): supress io.EOF in RecordReader.Err() by @ryanschneider in <a href="https://github.com/apache/arrow-go/pull/452">https://github.com/apache/arrow-go/pull/452</a></li>
<li>fix(array): add nil checks in Data.Release() for childData by @secfree in <a href="https://github.com/apache/arrow-go/pull/456">https://github.com/apache/arrow-go/pull/456</a></li>
<li>fix(arrow/compute): Fix scalar comparison batches by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/465">https://github.com/apache/arrow-go/pull/465</a></li>
<li>refactor(arrow): rename Record to RecordBatch and add deprecated alias by @Mandukhai-Alimaa in <a href="https://github.com/apache/arrow-go/pull/466">https://github.com/apache/arrow-go/pull/466</a></li>
<li>refactor(arrow): migrate leaf packages to use RecordBatch by @Mandukhai-Alimaa in <a href="https://github.com/apache/arrow-go/pull/473">https://github.com/apache/arrow-go/pull/473</a></li>
<li>refactor(arrow): third increment of the Record -&gt; RecordBatch migration by @Mandukhai-Alimaa in <a href="https://github.com/apache/arrow-go/pull/478">https://github.com/apache/arrow-go/pull/478</a></li>
<li>ci(parquet/pqarrow): integration tests for reading shredded variants by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/455">https://github.com/apache/arrow-go/pull/455</a></li>
<li>Implement RLE dictionary decoder using generics by @daniel-adam-tfs in <a href="https://github.com/apache/arrow-go/pull/477">https://github.com/apache/arrow-go/pull/477</a></li>
<li>fix(parquet/internal/encoding): Fix typed dictionary encoding by @MasslessParticle in <a href="https://github.com/apache/arrow-go/pull/479">https://github.com/apache/arrow-go/pull/479</a></li>
<li>refactor(arrow): fourth increment of the Record -&gt; RecordBatch migration by @Mandukhai-Alimaa in <a href="https://github.com/apache/arrow-go/pull/486">https://github.com/apache/arrow-go/pull/486</a></li>
<li>chore: bump version number by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/487">https://github.com/apache/arrow-go/pull/487</a></li>
</ul>
<h3>New Contributors</h3>
<ul>
<li>@ufUNnxagpM made their first contribution in <a href="https://github.com/apache/arrow-go/pull/453">https://github.com/apache/arrow-go/pull/453</a></li>
<li>@ryanschneider made their first contribution in <a href="https://github.com/apache/arrow-go/pull/452">https://github.com/apache/arrow-go/pull/452</a></li>
<li>@secfree made their first contribution in <a href="https://github.com/apache/arrow-go/pull/456">https://github.com/apache/arrow-go/pull/456</a></li>
<li>@Mandukhai-Alimaa made their first contribution in <a href="https://github.com/apache/arrow-go/pull/466">https://github.com/apache/arrow-go/pull/466</a></li>
<li>@daniel-adam-tfs made their first contribution in <a href="https://github.com/apache/arrow-go/pull/477">https://github.com/apache/arrow-go/pull/477</a></li>
</ul>
<p><strong>Full Changelog</strong>: <a href="https://github.com/apache/arrow-go/compare/v18.4.0...v18.4.1">https://github.com/apache/arrow-go/compare/v18.4.0...v18.4.1</a></p>]]></content><author><name>pmc</name></author><category term="release" /><summary type="html"><![CDATA[The Apache Arrow team is pleased to announce the v18.4.1 release of Apache Arrow Go. This patch release covers 15 commits from 7 distinct contributors. Contributors $ git shortlog -sn v18.4.0..v18.4.1 7 Matt Topol 4 Mandukhai Alimaa 1 Chromo-residuum-opec 1 Ryan Schneider 1 Travis Patterson 1 daniel-adam-tfs 1 secfree Highlights The Record interface type has been renamed to RecordBatch to align with other Arrow implementations and to avoid confusion. The old Record type is aliased to the new RecordBatch type so existing code works but users may wish to update references now. This work was contributed by a first-time contributor, @Mandukhai-Alimaa. See #466, #473, #478, and #486. Important Note A side effect of the above was an unintentional breaking change by introducing a new method to the RecordReader interface. This shouldn't affect the majority of consumers, but is a breaking change for any who implemented their own concrete RecordReader type. The solution is simply to add a RecordBatch() method to the type when upgrading to v18.4.1 Changelog What's Changed fix(arrow/compute/exprs): Handle large types in expr handling by @zeroshade in https://github.com/apache/arrow-go/pull/440 fix(arrow/compute/exprs): fix literalToDatum for precision types by @zeroshade in https://github.com/apache/arrow-go/pull/447 fix(arrow/array): Fix RecordFromJSON perf by @zeroshade in https://github.com/apache/arrow-go/pull/449 fix(arrow/array): update timestamp json format by @zeroshade in https://github.com/apache/arrow-go/pull/450 refactor: switch golang.org/x/exp to standard library packages by @ufUNnxagpM in https://github.com/apache/arrow-go/pull/453 fix(parquet/pqarrow): supress io.EOF in RecordReader.Err() by @ryanschneider in https://github.com/apache/arrow-go/pull/452 fix(array): add nil checks in Data.Release() for childData by @secfree in https://github.com/apache/arrow-go/pull/456 fix(arrow/compute): Fix scalar comparison batches by @zeroshade in https://github.com/apache/arrow-go/pull/465 refactor(arrow): rename Record to RecordBatch and add deprecated alias by @Mandukhai-Alimaa in https://github.com/apache/arrow-go/pull/466 refactor(arrow): migrate leaf packages to use RecordBatch by @Mandukhai-Alimaa in https://github.com/apache/arrow-go/pull/473 refactor(arrow): third increment of the Record -&gt; RecordBatch migration by @Mandukhai-Alimaa in https://github.com/apache/arrow-go/pull/478 ci(parquet/pqarrow): integration tests for reading shredded variants by @zeroshade in https://github.com/apache/arrow-go/pull/455 Implement RLE dictionary decoder using generics by @daniel-adam-tfs in https://github.com/apache/arrow-go/pull/477 fix(parquet/internal/encoding): Fix typed dictionary encoding by @MasslessParticle in https://github.com/apache/arrow-go/pull/479 refactor(arrow): fourth increment of the Record -&gt; RecordBatch migration by @Mandukhai-Alimaa in https://github.com/apache/arrow-go/pull/486 chore: bump version number by @zeroshade in https://github.com/apache/arrow-go/pull/487 New Contributors @ufUNnxagpM made their first contribution in https://github.com/apache/arrow-go/pull/453 @ryanschneider made their first contribution in https://github.com/apache/arrow-go/pull/452 @secfree made their first contribution in https://github.com/apache/arrow-go/pull/456 @Mandukhai-Alimaa made their first contribution in https://github.com/apache/arrow-go/pull/466 @daniel-adam-tfs made their first contribution in https://github.com/apache/arrow-go/pull/477 Full Changelog: https://github.com/apache/arrow-go/compare/v18.4.0...v18.4.1]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>