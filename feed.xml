<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://arrow.apache.org/feed.xml" rel="self" type="application/atom+xml" /><link href="https://arrow.apache.org/" rel="alternate" type="text/html" /><updated>2025-09-25T02:10:14-04:00</updated><id>https://arrow.apache.org/feed.xml</id><title type="html">Apache Arrow</title><subtitle>Apache Arrow is the universal columnar format and multi-language toolbox for fast data interchange and in-memory analytics. It specifies a standardized language-independent column-oriented memory format for flat and nested data, organized for efficient analytic operations on modern hardware. It also provides computational libraries and zero-copy streaming messaging and interprocess communication. Languages currently supported include C, C++, C#, Go, Java, JavaScript, MATLAB, Python, R, Ruby, and Rust.</subtitle><entry><title type="html">Apache Arrow ADBC 20 (Libraries) Release</title><link href="https://arrow.apache.org/blog/2025/09/12/adbc-20-release/" rel="alternate" type="text/html" title="Apache Arrow ADBC 20 (Libraries) Release" /><published>2025-09-12T00:00:00-04:00</published><updated>2025-09-12T00:00:00-04:00</updated><id>https://arrow.apache.org/blog/2025/09/12/adbc-20-release</id><content type="html" xml:base="https://arrow.apache.org/blog/2025/09/12/adbc-20-release/"><![CDATA[<!--

-->
<p>The Apache Arrow team is pleased to announce the version 20 release of
the Apache Arrow ADBC libraries. This release includes <a href="https://github.com/apache/arrow-adbc/milestone/24"><strong>44
resolved issues</strong></a> from <a href="#contributors"><strong>29 distinct contributors</strong></a>.</p>
<p>This is a release of the <strong>libraries</strong>, which are at version 20.  The
<a href="https://arrow.apache.org/adbc/20/format/specification.html"><strong>API specification</strong></a> is versioned separately and is at
version 1.1.0.</p>
<p>The subcomponents are versioned independently:</p>
<ul>
<li>C/C++/GLib/Go/Python/Ruby: 1.8.0</li>
<li>C#: 0.20.0</li>
<li>Java: 0.20.0</li>
<li>R: 0.20.0</li>
<li>Rust: 0.20.0</li>
</ul>
<p>The release notes below are not exhaustive and only expose selected
highlights of the release. Many other bugfixes and improvements have
been made: we refer you to the <a href="https://github.com/apache/arrow-adbc/blob/apache-arrow-adbc-20/CHANGELOG.md">complete changelog</a>.</p>
<h2>Release Highlights</h2>
<p>Driver managers now support loading driver manifests.  To learn more about
this feature, please see the <a href="https://arrow.apache.org/adbc/current/format/driver_manifests.html#driver-manifests">documentation</a>.</p>
<p>The Rust crates were reorganized.  <strong>This is a breaking change.</strong>  Now,
FFI-related code is part of <code>adbc_ffi</code> and the driver manager is part of
<code>adbc_driver_manager</code>.  Previously these were features of a single crate
<code>adbc_core</code>, which now only contains API definitions
(<a href="https://github.com/apache/arrow-adbc/pull/3381">#3381</a>,
<a href="https://github.com/apache/arrow-adbc/pull/3197">#3197</a>).  Also, some enums
are no longer marked as <code>#[non_exhaustive]</code>
(<a href="https://github.com/apache/arrow-adbc/pull/3245">#3245</a>).</p>
<p>The Java JNI bindings support a few more features
(<a href="https://github.com/apache/arrow-adbc/pull/3373">#3373</a>,
<a href="https://github.com/apache/arrow-adbc/pull/3372">#3372</a>,
<a href="https://github.com/apache/arrow-adbc/pull/3370">#3370</a>,
<a href="https://github.com/apache/arrow-adbc/pull/3348">#3348</a>).</p>
<p>The BigQuery driver properly uses microsecond timestamps
(<a href="https://github.com/apache/arrow-adbc/pull/3364">#3364</a>), has an improved
error message if your user account lacks the proper permissions
(<a href="https://github.com/apache/arrow-adbc/pull/3297">#3297</a>), properly handles
nested data (<a href="https://github.com/apache/arrow-adbc/pull/3240">#3240</a>), and
supports service account impersonation
(<a href="https://github.com/apache/arrow-adbc/pull/3174">#3174</a>).  The C#
Databricks/HiveServer2 Thrift-protocol drivers continues to expand their
featureset, such as support for cancelling statements, token exchange, and
better tracing (<a href="https://github.com/apache/arrow-adbc/pull/3304">#3304</a>,
<a href="https://github.com/apache/arrow-adbc/pull/3302">#3302</a>,
<a href="https://github.com/apache/arrow-adbc/pull/3301">#3301</a>,
<a href="https://github.com/apache/arrow-adbc/pull/3224">#3224</a>,
<a href="https://github.com/apache/arrow-adbc/pull/3218">#3218</a>,
<a href="https://github.com/apache/arrow-adbc/pull/3192">#3192</a>,
<a href="https://github.com/apache/arrow-adbc/pull/3177">#3177</a>,
<a href="https://github.com/apache/arrow-adbc/pull/3137">#3137</a>,
<a href="https://github.com/apache/arrow-adbc/pull/3127">#3127</a>).  The PostgreSQL
driver will properly bind <code>arrow.json</code> extension arrays as JSON parameters
(<a href="https://github.com/apache/arrow-adbc/pull/3333">#3333</a>).  The Snowflake
driver supports more authentication methods
(<a href="https://github.com/apache/arrow-adbc/pull/3366">#3366</a>).  The SQLite driver
can bind parameters by name instead of position
(<a href="https://github.com/apache/arrow-adbc/pull/3362">#3362</a>).</p>
<p>The C# library has been upgraded to .NET 8
(<a href="https://github.com/apache/arrow-adbc/pull/3120">#3120</a>).</p>
<p>GLib has more bindings to ADBC functions
(<a href="https://github.com/apache/arrow-adbc/pull/3118">#3118</a>).</p>
<p>The Go library has some experimental helpers to simplify getting driver
metadata (<a href="https://github.com/apache/arrow-adbc/pull/3239">#3239</a>) and
ingesting Arrow data
(<a href="https://github.com/apache/arrow-adbc/pull/3150">#3150</a>).  The <code>database/sql</code>
adapter handles <code>time.Time</code> values for bind parameters now
(<a href="https://github.com/apache/arrow-adbc/pull/3109">#3109</a>).  Drivers will
forward SQLSTATE and other error metadata across the FFI boundary
(<a href="https://github.com/apache/arrow-adbc/pull/2801">#2801</a>).</p>
<h2>Contributors</h2>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ git shortlog --perl-regexp --author='^((?!dependabot\[bot\]).*)$' -sn apache-arrow-adbc-19..apache-arrow-adbc-20
    28	David Li
    14	Todd Meng
    13	Bryce Mecum
    13	eitsupi
    12	Jacky Hu
    12	Matt Topol
     8	Bruce Irschick
     7	Matthijs Brobbel
     6	davidhcoe
     5	eric-wang-1990
     4	Alex Guo
     3	Daijiro Fukuda
     3	Felipe Oliveira Carvalho
     3	Sutou Kouhei
     2	Curt Hagenlocher
     2	Jade Wang
     2	Mandukhai Alimaa
     2	amangoyal
     1	Arseny Tsypushkin
     1	Dewey Dunnington
     1	Even Rouault
     1	Ian Cook
     1	Jordan E
     1	Lucas Valente
     1	Mila Page
     1	Ryan Syed
     1	Sudhir Reddy Emmadi
     1	Xuliang (Harry) Sun
     1	Yu Ishikawa
</code></pre></div></div>
<h2>Roadmap</h2>
<p>A Go-based driver for Databricks is in the works from a contributor.</p>
<h2>Getting Involved</h2>
<p>We welcome questions and contributions from all interested.  Issues
can be filed on <a href="https://github.com/apache/arrow-adbc/issues">GitHub</a>, and questions can be directed to GitHub
or the <a href="/community/">Arrow mailing lists</a>.</p>]]></content><author><name>pmc</name></author><category term="release" /><summary type="html"><![CDATA[The Apache Arrow team is pleased to announce the version 20 release of the Apache Arrow ADBC libraries. This release includes 44 resolved issues from 29 distinct contributors. This is a release of the libraries, which are at version 20. The API specification is versioned separately and is at version 1.1.0. The subcomponents are versioned independently: C/C++/GLib/Go/Python/Ruby: 1.8.0 C#: 0.20.0 Java: 0.20.0 R: 0.20.0 Rust: 0.20.0 The release notes below are not exhaustive and only expose selected highlights of the release. Many other bugfixes and improvements have been made: we refer you to the complete changelog. Release Highlights Driver managers now support loading driver manifests. To learn more about this feature, please see the documentation. The Rust crates were reorganized. This is a breaking change. Now, FFI-related code is part of adbc_ffi and the driver manager is part of adbc_driver_manager. Previously these were features of a single crate adbc_core, which now only contains API definitions (#3381, #3197). Also, some enums are no longer marked as #[non_exhaustive] (#3245). The Java JNI bindings support a few more features (#3373, #3372, #3370, #3348). The BigQuery driver properly uses microsecond timestamps (#3364), has an improved error message if your user account lacks the proper permissions (#3297), properly handles nested data (#3240), and supports service account impersonation (#3174). The C# Databricks/HiveServer2 Thrift-protocol drivers continues to expand their featureset, such as support for cancelling statements, token exchange, and better tracing (#3304, #3302, #3301, #3224, #3218, #3192, #3177, #3137, #3127). The PostgreSQL driver will properly bind arrow.json extension arrays as JSON parameters (#3333). The Snowflake driver supports more authentication methods (#3366). The SQLite driver can bind parameters by name instead of position (#3362). The C# library has been upgraded to .NET 8 (#3120). GLib has more bindings to ADBC functions (#3118). The Go library has some experimental helpers to simplify getting driver metadata (#3239) and ingesting Arrow data (#3150). The database/sql adapter handles time.Time values for bind parameters now (#3109). Drivers will forward SQLSTATE and other error metadata across the FFI boundary (#2801). Contributors $ git shortlog --perl-regexp --author='^((?!dependabot\[bot\]).*)$' -sn apache-arrow-adbc-19..apache-arrow-adbc-20 28 David Li 14 Todd Meng 13 Bryce Mecum 13 eitsupi 12 Jacky Hu 12 Matt Topol 8 Bruce Irschick 7 Matthijs Brobbel 6 davidhcoe 5 eric-wang-1990 4 Alex Guo 3 Daijiro Fukuda 3 Felipe Oliveira Carvalho 3 Sutou Kouhei 2 Curt Hagenlocher 2 Jade Wang 2 Mandukhai Alimaa 2 amangoyal 1 Arseny Tsypushkin 1 Dewey Dunnington 1 Even Rouault 1 Ian Cook 1 Jordan E 1 Lucas Valente 1 Mila Page 1 Ryan Syed 1 Sudhir Reddy Emmadi 1 Xuliang (Harry) Sun 1 Yu Ishikawa Roadmap A Go-based driver for Databricks is in the works from a contributor. Getting Involved We welcome questions and contributions from all interested. Issues can be filed on GitHub, and questions can be directed to GitHub or the Arrow mailing lists.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Apache Arrow Go 18.4.1 Release</title><link href="https://arrow.apache.org/blog/2025/09/04/arrow-go-18.4.1/" rel="alternate" type="text/html" title="Apache Arrow Go 18.4.1 Release" /><published>2025-09-04T00:00:00-04:00</published><updated>2025-09-04T00:00:00-04:00</updated><id>https://arrow.apache.org/blog/2025/09/04/arrow-go-18.4.1</id><content type="html" xml:base="https://arrow.apache.org/blog/2025/09/04/arrow-go-18.4.1/"><![CDATA[<!--

-->
<p>The Apache Arrow team is pleased to announce the v18.4.1 release of Apache Arrow Go.
This patch release covers 15 commits from 7 distinct contributors.</p>
<h2>Contributors</h2>
<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code data-lang="console"><span class="gp">$</span><span class="w"> </span>git shortlog <span class="nt">-sn</span> v18.4.0..v18.4.1
<span class="go">     7	Matt Topol
     4	Mandukhai Alimaa
     1	Chromo-residuum-opec
     1	Ryan Schneider
     1	Travis Patterson
     1	daniel-adam-tfs
     1	secfree
</span></code></pre></div></div>
<h2>Highlights</h2>
<ul>
<li>The <code>Record</code> interface type has been renamed to <code>RecordBatch</code> to align with
other Arrow implementations and to avoid confusion. The old <code>Record</code> type is
aliased to the new <code>RecordBatch</code> type so existing code works but users may wish
to update references now. This work was contributed by a first-time contributor,
@Mandukhai-Alimaa. See <a href="https://github.com/apache/arrow-go/pull/466">#466</a>,
<a href="https://github.com/apache/arrow-go/pull/473">#473</a>,
<a href="https://github.com/apache/arrow-go/pull/478">#478</a>, and
<a href="https://github.com/apache/arrow-go/pull/486">#486</a>.</li>
</ul>
<h3>Important Note</h3>
<ul>
<li>A side effect of the above was an unintentional breaking change by introducing a new
method to the <code>RecordReader</code> interface. This shouldn't affect the majority of consumers,
but is a breaking change for any who implemented their own concrete <code>RecordReader</code> type.
The solution is simply to add a <code>RecordBatch()</code> method to the type when upgrading to v18.4.1</li>
</ul>
<h2>Changelog</h2>
<h3>What's Changed</h3>
<ul>
<li>fix(arrow/compute/exprs): Handle large types in expr handling by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/440">https://github.com/apache/arrow-go/pull/440</a></li>
<li>fix(arrow/compute/exprs): fix literalToDatum for precision types by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/447">https://github.com/apache/arrow-go/pull/447</a></li>
<li>fix(arrow/array): Fix RecordFromJSON perf by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/449">https://github.com/apache/arrow-go/pull/449</a></li>
<li>fix(arrow/array): update timestamp json format by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/450">https://github.com/apache/arrow-go/pull/450</a></li>
<li>refactor: switch golang.org/x/exp to standard library packages by @ufUNnxagpM in <a href="https://github.com/apache/arrow-go/pull/453">https://github.com/apache/arrow-go/pull/453</a></li>
<li>fix(parquet/pqarrow): supress io.EOF in RecordReader.Err() by @ryanschneider in <a href="https://github.com/apache/arrow-go/pull/452">https://github.com/apache/arrow-go/pull/452</a></li>
<li>fix(array): add nil checks in Data.Release() for childData by @secfree in <a href="https://github.com/apache/arrow-go/pull/456">https://github.com/apache/arrow-go/pull/456</a></li>
<li>fix(arrow/compute): Fix scalar comparison batches by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/465">https://github.com/apache/arrow-go/pull/465</a></li>
<li>refactor(arrow): rename Record to RecordBatch and add deprecated alias by @Mandukhai-Alimaa in <a href="https://github.com/apache/arrow-go/pull/466">https://github.com/apache/arrow-go/pull/466</a></li>
<li>refactor(arrow): migrate leaf packages to use RecordBatch by @Mandukhai-Alimaa in <a href="https://github.com/apache/arrow-go/pull/473">https://github.com/apache/arrow-go/pull/473</a></li>
<li>refactor(arrow): third increment of the Record -&gt; RecordBatch migration by @Mandukhai-Alimaa in <a href="https://github.com/apache/arrow-go/pull/478">https://github.com/apache/arrow-go/pull/478</a></li>
<li>ci(parquet/pqarrow): integration tests for reading shredded variants by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/455">https://github.com/apache/arrow-go/pull/455</a></li>
<li>Implement RLE dictionary decoder using generics by @daniel-adam-tfs in <a href="https://github.com/apache/arrow-go/pull/477">https://github.com/apache/arrow-go/pull/477</a></li>
<li>fix(parquet/internal/encoding): Fix typed dictionary encoding by @MasslessParticle in <a href="https://github.com/apache/arrow-go/pull/479">https://github.com/apache/arrow-go/pull/479</a></li>
<li>refactor(arrow): fourth increment of the Record -&gt; RecordBatch migration by @Mandukhai-Alimaa in <a href="https://github.com/apache/arrow-go/pull/486">https://github.com/apache/arrow-go/pull/486</a></li>
<li>chore: bump version number by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/487">https://github.com/apache/arrow-go/pull/487</a></li>
</ul>
<h3>New Contributors</h3>
<ul>
<li>@ufUNnxagpM made their first contribution in <a href="https://github.com/apache/arrow-go/pull/453">https://github.com/apache/arrow-go/pull/453</a></li>
<li>@ryanschneider made their first contribution in <a href="https://github.com/apache/arrow-go/pull/452">https://github.com/apache/arrow-go/pull/452</a></li>
<li>@secfree made their first contribution in <a href="https://github.com/apache/arrow-go/pull/456">https://github.com/apache/arrow-go/pull/456</a></li>
<li>@Mandukhai-Alimaa made their first contribution in <a href="https://github.com/apache/arrow-go/pull/466">https://github.com/apache/arrow-go/pull/466</a></li>
<li>@daniel-adam-tfs made their first contribution in <a href="https://github.com/apache/arrow-go/pull/477">https://github.com/apache/arrow-go/pull/477</a></li>
</ul>
<p><strong>Full Changelog</strong>: <a href="https://github.com/apache/arrow-go/compare/v18.4.0...v18.4.1">https://github.com/apache/arrow-go/compare/v18.4.0...v18.4.1</a></p>]]></content><author><name>pmc</name></author><category term="release" /><summary type="html"><![CDATA[The Apache Arrow team is pleased to announce the v18.4.1 release of Apache Arrow Go. This patch release covers 15 commits from 7 distinct contributors. Contributors $ git shortlog -sn v18.4.0..v18.4.1 7 Matt Topol 4 Mandukhai Alimaa 1 Chromo-residuum-opec 1 Ryan Schneider 1 Travis Patterson 1 daniel-adam-tfs 1 secfree Highlights The Record interface type has been renamed to RecordBatch to align with other Arrow implementations and to avoid confusion. The old Record type is aliased to the new RecordBatch type so existing code works but users may wish to update references now. This work was contributed by a first-time contributor, @Mandukhai-Alimaa. See #466, #473, #478, and #486. Important Note A side effect of the above was an unintentional breaking change by introducing a new method to the RecordReader interface. This shouldn't affect the majority of consumers, but is a breaking change for any who implemented their own concrete RecordReader type. The solution is simply to add a RecordBatch() method to the type when upgrading to v18.4.1 Changelog What's Changed fix(arrow/compute/exprs): Handle large types in expr handling by @zeroshade in https://github.com/apache/arrow-go/pull/440 fix(arrow/compute/exprs): fix literalToDatum for precision types by @zeroshade in https://github.com/apache/arrow-go/pull/447 fix(arrow/array): Fix RecordFromJSON perf by @zeroshade in https://github.com/apache/arrow-go/pull/449 fix(arrow/array): update timestamp json format by @zeroshade in https://github.com/apache/arrow-go/pull/450 refactor: switch golang.org/x/exp to standard library packages by @ufUNnxagpM in https://github.com/apache/arrow-go/pull/453 fix(parquet/pqarrow): supress io.EOF in RecordReader.Err() by @ryanschneider in https://github.com/apache/arrow-go/pull/452 fix(array): add nil checks in Data.Release() for childData by @secfree in https://github.com/apache/arrow-go/pull/456 fix(arrow/compute): Fix scalar comparison batches by @zeroshade in https://github.com/apache/arrow-go/pull/465 refactor(arrow): rename Record to RecordBatch and add deprecated alias by @Mandukhai-Alimaa in https://github.com/apache/arrow-go/pull/466 refactor(arrow): migrate leaf packages to use RecordBatch by @Mandukhai-Alimaa in https://github.com/apache/arrow-go/pull/473 refactor(arrow): third increment of the Record -&gt; RecordBatch migration by @Mandukhai-Alimaa in https://github.com/apache/arrow-go/pull/478 ci(parquet/pqarrow): integration tests for reading shredded variants by @zeroshade in https://github.com/apache/arrow-go/pull/455 Implement RLE dictionary decoder using generics by @daniel-adam-tfs in https://github.com/apache/arrow-go/pull/477 fix(parquet/internal/encoding): Fix typed dictionary encoding by @MasslessParticle in https://github.com/apache/arrow-go/pull/479 refactor(arrow): fourth increment of the Record -&gt; RecordBatch migration by @Mandukhai-Alimaa in https://github.com/apache/arrow-go/pull/486 chore: bump version number by @zeroshade in https://github.com/apache/arrow-go/pull/487 New Contributors @ufUNnxagpM made their first contribution in https://github.com/apache/arrow-go/pull/453 @ryanschneider made their first contribution in https://github.com/apache/arrow-go/pull/452 @secfree made their first contribution in https://github.com/apache/arrow-go/pull/456 @Mandukhai-Alimaa made their first contribution in https://github.com/apache/arrow-go/pull/466 @daniel-adam-tfs made their first contribution in https://github.com/apache/arrow-go/pull/477 Full Changelog: https://github.com/apache/arrow-go/compare/v18.4.0...v18.4.1]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Apache Arrow Go 18.4.0 Release</title><link href="https://arrow.apache.org/blog/2025/07/21/arrow-go-18.4.0/" rel="alternate" type="text/html" title="Apache Arrow Go 18.4.0 Release" /><published>2025-07-21T00:00:00-04:00</published><updated>2025-07-21T00:00:00-04:00</updated><id>https://arrow.apache.org/blog/2025/07/21/arrow-go-18.4.0</id><content type="html" xml:base="https://arrow.apache.org/blog/2025/07/21/arrow-go-18.4.0/"><![CDATA[<!--

-->
<p>The Apache Arrow team is pleased to announce the v18.4.0 release of Apache Arrow Go.
This minor release covers 25 commits from 11 distinct contributors.</p>
<h2>Contributors</h2>
<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code data-lang="console"><span class="gp">$</span><span class="w"> </span>git shortlog <span class="nt">-sn</span> v18.3.1..v18.4.0
<span class="go">    16	Matt Topol
     1	Alvaro Viebrantz
     1	Arnold Wakim
     1	Daniil Mileev
     1	Kristofer Gaudel
     1	Marcin Bojanczyk
     1	Raúl Cumplido
     1	Saurabh Singh
     1	Sutou Kouhei
     1	Victor Perez
     1	Willem Jan
</span></code></pre></div></div>
<h2>Changelog</h2>
<h3>What's Changed</h3>
<ul>
<li>feat(arrow/cdata): Add ReleaseCArrowArrayStream function by @karsov in <a href="https://github.com/apache/arrow-go/pull/373">https://github.com/apache/arrow-go/pull/373</a></li>
<li>fix: TestDeltaByteArray implementation and fix by @MetalBlueberry in <a href="https://github.com/apache/arrow-go/pull/369">https://github.com/apache/arrow-go/pull/369</a></li>
<li>chore: move .github/ISSUE_TEMPLATE/config.yaml to config.yml as currently does not work by @raulcd in <a href="https://github.com/apache/arrow-go/pull/383">https://github.com/apache/arrow-go/pull/383</a></li>
<li>fix: list_columns.parquet testing by @MetalBlueberry in <a href="https://github.com/apache/arrow-go/pull/378">https://github.com/apache/arrow-go/pull/378</a></li>
<li>feat: Extend arrow csv writter by @MetalBlueberry in <a href="https://github.com/apache/arrow-go/pull/375">https://github.com/apache/arrow-go/pull/375</a></li>
<li>feat(parquet/pqarrow): parallelize SeekToRow by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/380">https://github.com/apache/arrow-go/pull/380</a></li>
<li>chore: Use apache/arrow-js for JS in integration test by @kou in <a href="https://github.com/apache/arrow-go/pull/389">https://github.com/apache/arrow-go/pull/389</a></li>
<li>feat(parquet): add variant encoder/decoder by @sfc-gh-mbojanczyk in <a href="https://github.com/apache/arrow-go/pull/344">https://github.com/apache/arrow-go/pull/344</a></li>
<li>chore: remove extra binary by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/391">https://github.com/apache/arrow-go/pull/391</a></li>
<li>CI: add benchmark workflow and script by @singh1203 in <a href="https://github.com/apache/arrow-go/pull/250">https://github.com/apache/arrow-go/pull/250</a></li>
<li>feat(arrow/extensions): Add Variant extension type, array, and builder by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/395">https://github.com/apache/arrow-go/pull/395</a></li>
<li>fix(parquet/pqarrow): Fix propagation of field-ids for Lists by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/397">https://github.com/apache/arrow-go/pull/397</a></li>
<li>feat(arrow/_examples): enhance library examples by @kris-gaudel in <a href="https://github.com/apache/arrow-go/pull/394">https://github.com/apache/arrow-go/pull/394</a></li>
<li>chore: bump Windows GitHub hosted runner to windows-2022 by @raulcd in <a href="https://github.com/apache/arrow-go/pull/407">https://github.com/apache/arrow-go/pull/407</a></li>
<li>ci(benchmark): Fix benchmark runs by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/409">https://github.com/apache/arrow-go/pull/409</a></li>
<li>ci: fix flaky test by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/411">https://github.com/apache/arrow-go/pull/411</a></li>
<li>ci: make additional checks to prevent flaky EOF by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/414">https://github.com/apache/arrow-go/pull/414</a></li>
<li>refactor: update linter and run it by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/419">https://github.com/apache/arrow-go/pull/419</a></li>
<li>feat: expose Payload.WritePayload to allow serializing into IPC format by @alvarowolfx in <a href="https://github.com/apache/arrow-go/pull/421">https://github.com/apache/arrow-go/pull/421</a></li>
<li>feat(parquet/variant): Parse JSON into variant by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/426">https://github.com/apache/arrow-go/pull/426</a></li>
<li>refactor(parquet/internal/encoding): Refactor parquet logic to use generics by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/428">https://github.com/apache/arrow-go/pull/428</a></li>
<li>feat(arrrow/compute/expr): support substrait timestamp and decimal properly by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/418">https://github.com/apache/arrow-go/pull/418</a></li>
<li>feat(parquet/examples): enhance library examples by @milden6 in <a href="https://github.com/apache/arrow-go/pull/429">https://github.com/apache/arrow-go/pull/429</a></li>
<li>feat(arrow/compute): support some float16 casts by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/430">https://github.com/apache/arrow-go/pull/430</a></li>
<li>feat(parquet/pqarrow): Correctly handle Variant types in schema by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/433">https://github.com/apache/arrow-go/pull/433</a></li>
<li>fix(arrow/avro-reader): bunch of types that didn't work by @Willem-J-an in <a href="https://github.com/apache/arrow-go/pull/416">https://github.com/apache/arrow-go/pull/416</a></li>
<li>feat(parquet/pqarrow): read/write variant by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/434">https://github.com/apache/arrow-go/pull/434</a></li>
<li>build(deps): update to substrait-go v4.3.0 by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/438">https://github.com/apache/arrow-go/pull/438</a></li>
<li>fix(arrow/flight/flightsql): drain channel in flightSqlServer.DoGet by @arnoldwakim in <a href="https://github.com/apache/arrow-go/pull/437">https://github.com/apache/arrow-go/pull/437</a></li>
<li>chore(arrow): Update PkgVersion by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/439">https://github.com/apache/arrow-go/pull/439</a></li>
</ul>
<h3>New Contributors</h3>
<ul>
<li>@karsov made their first contribution in <a href="https://github.com/apache/arrow-go/pull/373">https://github.com/apache/arrow-go/pull/373</a></li>
<li>@MetalBlueberry made their first contribution in <a href="https://github.com/apache/arrow-go/pull/369">https://github.com/apache/arrow-go/pull/369</a></li>
<li>@sfc-gh-mbojanczyk made their first contribution in <a href="https://github.com/apache/arrow-go/pull/344">https://github.com/apache/arrow-go/pull/344</a></li>
<li>@kris-gaudel made their first contribution in <a href="https://github.com/apache/arrow-go/pull/394">https://github.com/apache/arrow-go/pull/394</a></li>
<li>@alvarowolfx made their first contribution in <a href="https://github.com/apache/arrow-go/pull/421">https://github.com/apache/arrow-go/pull/421</a></li>
<li>@milden6 made their first contribution in <a href="https://github.com/apache/arrow-go/pull/429">https://github.com/apache/arrow-go/pull/429</a></li>
<li>@Willem-J-an made their first contribution in <a href="https://github.com/apache/arrow-go/pull/416">https://github.com/apache/arrow-go/pull/416</a></li>
<li>@arnoldwakim made their first contribution in <a href="https://github.com/apache/arrow-go/pull/437">https://github.com/apache/arrow-go/pull/437</a></li>
</ul>
<p><strong>Full Changelog</strong>: <a href="https://github.com/apache/arrow-go/compare/v18.3.0...v18.4.0">https://github.com/apache/arrow-go/compare/v18.3.0...v18.4.0</a></p>]]></content><author><name>pmc</name></author><category term="release" /><summary type="html"><![CDATA[The Apache Arrow team is pleased to announce the v18.4.0 release of Apache Arrow Go. This minor release covers 25 commits from 11 distinct contributors. Contributors $ git shortlog -sn v18.3.1..v18.4.0 16 Matt Topol 1 Alvaro Viebrantz 1 Arnold Wakim 1 Daniil Mileev 1 Kristofer Gaudel 1 Marcin Bojanczyk 1 Raúl Cumplido 1 Saurabh Singh 1 Sutou Kouhei 1 Victor Perez 1 Willem Jan Changelog What's Changed feat(arrow/cdata): Add ReleaseCArrowArrayStream function by @karsov in https://github.com/apache/arrow-go/pull/373 fix: TestDeltaByteArray implementation and fix by @MetalBlueberry in https://github.com/apache/arrow-go/pull/369 chore: move .github/ISSUE_TEMPLATE/config.yaml to config.yml as currently does not work by @raulcd in https://github.com/apache/arrow-go/pull/383 fix: list_columns.parquet testing by @MetalBlueberry in https://github.com/apache/arrow-go/pull/378 feat: Extend arrow csv writter by @MetalBlueberry in https://github.com/apache/arrow-go/pull/375 feat(parquet/pqarrow): parallelize SeekToRow by @zeroshade in https://github.com/apache/arrow-go/pull/380 chore: Use apache/arrow-js for JS in integration test by @kou in https://github.com/apache/arrow-go/pull/389 feat(parquet): add variant encoder/decoder by @sfc-gh-mbojanczyk in https://github.com/apache/arrow-go/pull/344 chore: remove extra binary by @zeroshade in https://github.com/apache/arrow-go/pull/391 CI: add benchmark workflow and script by @singh1203 in https://github.com/apache/arrow-go/pull/250 feat(arrow/extensions): Add Variant extension type, array, and builder by @zeroshade in https://github.com/apache/arrow-go/pull/395 fix(parquet/pqarrow): Fix propagation of field-ids for Lists by @zeroshade in https://github.com/apache/arrow-go/pull/397 feat(arrow/_examples): enhance library examples by @kris-gaudel in https://github.com/apache/arrow-go/pull/394 chore: bump Windows GitHub hosted runner to windows-2022 by @raulcd in https://github.com/apache/arrow-go/pull/407 ci(benchmark): Fix benchmark runs by @zeroshade in https://github.com/apache/arrow-go/pull/409 ci: fix flaky test by @zeroshade in https://github.com/apache/arrow-go/pull/411 ci: make additional checks to prevent flaky EOF by @zeroshade in https://github.com/apache/arrow-go/pull/414 refactor: update linter and run it by @zeroshade in https://github.com/apache/arrow-go/pull/419 feat: expose Payload.WritePayload to allow serializing into IPC format by @alvarowolfx in https://github.com/apache/arrow-go/pull/421 feat(parquet/variant): Parse JSON into variant by @zeroshade in https://github.com/apache/arrow-go/pull/426 refactor(parquet/internal/encoding): Refactor parquet logic to use generics by @zeroshade in https://github.com/apache/arrow-go/pull/428 feat(arrrow/compute/expr): support substrait timestamp and decimal properly by @zeroshade in https://github.com/apache/arrow-go/pull/418 feat(parquet/examples): enhance library examples by @milden6 in https://github.com/apache/arrow-go/pull/429 feat(arrow/compute): support some float16 casts by @zeroshade in https://github.com/apache/arrow-go/pull/430 feat(parquet/pqarrow): Correctly handle Variant types in schema by @zeroshade in https://github.com/apache/arrow-go/pull/433 fix(arrow/avro-reader): bunch of types that didn't work by @Willem-J-an in https://github.com/apache/arrow-go/pull/416 feat(parquet/pqarrow): read/write variant by @zeroshade in https://github.com/apache/arrow-go/pull/434 build(deps): update to substrait-go v4.3.0 by @zeroshade in https://github.com/apache/arrow-go/pull/438 fix(arrow/flight/flightsql): drain channel in flightSqlServer.DoGet by @arnoldwakim in https://github.com/apache/arrow-go/pull/437 chore(arrow): Update PkgVersion by @zeroshade in https://github.com/apache/arrow-go/pull/439 New Contributors @karsov made their first contribution in https://github.com/apache/arrow-go/pull/373 @MetalBlueberry made their first contribution in https://github.com/apache/arrow-go/pull/369 @sfc-gh-mbojanczyk made their first contribution in https://github.com/apache/arrow-go/pull/344 @kris-gaudel made their first contribution in https://github.com/apache/arrow-go/pull/394 @alvarowolfx made their first contribution in https://github.com/apache/arrow-go/pull/421 @milden6 made their first contribution in https://github.com/apache/arrow-go/pull/429 @Willem-J-an made their first contribution in https://github.com/apache/arrow-go/pull/416 @arnoldwakim made their first contribution in https://github.com/apache/arrow-go/pull/437 Full Changelog: https://github.com/apache/arrow-go/compare/v18.3.0...v18.4.0]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Recent Improvements to Hash Join in Arrow C++</title><link href="https://arrow.apache.org/blog/2025/07/18/recent-improvements-to-hash-join/" rel="alternate" type="text/html" title="Recent Improvements to Hash Join in Arrow C++" /><published>2025-07-18T00:00:00-04:00</published><updated>2025-07-18T00:00:00-04:00</updated><id>https://arrow.apache.org/blog/2025/07/18/recent-improvements-to-hash-join</id><content type="html" xml:base="https://arrow.apache.org/blog/2025/07/18/recent-improvements-to-hash-join/"><![CDATA[<!--

-->
<p><em>Editor’s Note: Apache Arrow is an expansive project, ranging from the Arrow columnar format itself, to its numerous specifications, and a long list of implementations. Arrow is also an expansive project in terms of its community of contributors. In this blog post, we’d like to highlight recent work by Apache Arrow Committer Rossi Sun on improving the performance and stability of Arrow’s embeddable query execution engine: Acero.</em></p>
<h1>Introduction</h1>
<p>Hash join is a fundamental operation in analytical processing engines — it matches rows from two tables based on key values using a hash table for fast lookup. In the C++ implementation of Apache Arrow, the hash join is implemented in the C++ engine Acero, which powers query execution in bindings like PyArrow and the R Arrow package. Even if you haven't used Acero directly, your code may already be benefiting from it under the hood.</p>
<p>For example, this simple PyArrow example uses Acero:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code data-lang="python"><span class="kn">import</span> <span class="n">pyarrow</span> <span class="k">as</span> <span class="n">pa</span>

<span class="n">t1</span> <span class="o">=</span> <span class="n">pa</span><span class="p">.</span><span class="nf">table</span><span class="p">({</span><span class="sh">'</span><span class="s">id</span><span class="sh">'</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
               <span class="sh">'</span><span class="s">year</span><span class="sh">'</span><span class="p">:</span> <span class="p">[</span><span class="mi">2020</span><span class="p">,</span> <span class="mi">2022</span><span class="p">,</span> <span class="mi">2019</span><span class="p">]})</span>
<span class="n">t2</span> <span class="o">=</span> <span class="n">pa</span><span class="p">.</span><span class="nf">table</span><span class="p">({</span><span class="sh">'</span><span class="s">id</span><span class="sh">'</span><span class="p">:</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span>
               <span class="sh">'</span><span class="s">n_legs</span><span class="sh">'</span><span class="p">:</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">100</span><span class="p">],</span>
               <span class="sh">'</span><span class="s">animal</span><span class="sh">'</span><span class="p">:</span> <span class="p">[</span><span class="sh">"</span><span class="s">Brittle stars</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Centipede</span><span class="sh">"</span><span class="p">]})</span>

<span class="n">t1</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">t2</span><span class="p">,</span> <span class="sh">'</span><span class="s">id</span><span class="sh">'</span><span class="p">).</span><span class="nf">combine_chunks</span><span class="p">().</span><span class="nf">sort_by</span><span class="p">(</span><span class="sh">'</span><span class="s">year</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div>
<p>Acero was originally created in 2019 to demonstrate that the ever-growing library of compute kernels in Arrow C++ could be linked together into realistic workflows and also to take advantage of the emerging Datasets API to give these workflows access to data. Rather than aiming to compete with full query engines like DuckDB, Acero focuses on enabling flexible, composable, and embeddable query execution — serving as a building block for tools and systems that need fast, modular analytics capabilities — including those built atop Arrow C++, or integrating via bindings like PyArrow, Substrait, or ADBC.</p>
<p>Across several recent Arrow C++ releases, we've made substantial improvements to the hash join implementation to address common user pain points. These changes improve stability, memory efficiency, and parallel performance, with a focus on making joins more usable and scalable out of the box. If you've had trouble using Arrow’s hash join in the past, now is a great time to try again.</p>
<h1>Scaling Safely: Improvements to Stability</h1>
<p>In earlier versions of Arrow C++, the hash join implementation used internal data structures that weren’t designed for very large datasets and lacked safeguards in some of the underlying memory operations. These limitations rarely surfaced in small to medium workloads but became problematic at scale, manifesting as crashes or subtle correctness issues.</p>
<p>At the core of Arrow’s join implementation is a compact, row-oriented structure known as the “row table”. While Arrow’s data model is columnar, its hash join implementation operates in a row-wise fashion — similar to modern engines like DuckDB and Meta’s Velox. This layout minimizes CPU cache misses during hash table lookups by collocating keys, payloads, and null bits in memory so they can be accessed together.</p>
<p>In previous versions, the row table used 32-bit offsets to reference packed rows. This capped each table’s size to 4GB and introduced risks of overflow when working with large datasets or wide rows. Several reported issues — <a href="https://github.com/apache/arrow/issues/34474">GH-34474</a>, <a href="https://github.com/apache/arrow/issues/41813">GH-41813</a>, and <a href="https://github.com/apache/arrow/issues/43202">GH-43202</a> — highlighted the limitations of this design. In response, PR <a href="https://github.com/apache/arrow/pull/43389">GH-43389</a> widened the internal offset type to 64-bit, reworking key parts of the row table infrastructure to support larger data sizes more safely and scalably.</p>
<p>Besides the offset limitation, earlier versions of Arrow C++ also included overflow-prone logic in the buffer indexing paths used throughout the hash join implementation. Many internal calculations assumed that 32-bit integers were sufficient for addressing memory — a fragile assumption when working with large datasets or wide rows. These issues appeared not only in conventional C++ indexing code but also in Arrow’s SIMD-accelerated paths — Arrow includes heavy SIMD specializations, used to speed up operations like hash table probing and row comparison. Together, these assumptions led to subtle overflows and incorrect behavior, as documented in issues like <a href="https://github.com/apache/arrow/issues/44513">GH-44513</a>, <a href="https://github.com/apache/arrow/issues/45334">GH-45334</a>, and <a href="https://github.com/apache/arrow/issues/45506">GH-45506</a>.</p>
<p>Two representative examples:</p>
<ul>
<li>Row-wise buffer access in C++</li>
</ul>
<p>The aforementioned row table stores fixed-length data in tightly packed buffers. Accessing a particular row (and optionally a column within it) typically involves <a href="https://github.com/apache/arrow/blob/12f62653c825fbf305bfde61c112d2aa69203c62/cpp/src/arrow/acero/swiss_join_internal.h#L120">pointer arithmetic</a>:</p>
<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code data-lang="cpp"><span class="k">const</span> <span class="kt">uint8_t</span><span class="o">*</span> <span class="n">row_ptr</span> <span class="o">=</span> <span class="n">row_ptr_base</span> <span class="o">+</span> <span class="n">row_length</span> <span class="o">*</span> <span class="n">row_id</span><span class="p">;</span>
</code></pre></div></div>
<p>When both <code>row_length</code> and <code>row_id</code> are large 32-bit integers, their product can overflow.</p>
<p>Similarly, accessing null masks involves <a href="https://github.com/apache/arrow/blob/12f62653c825fbf305bfde61c112d2aa69203c62/cpp/src/arrow/acero/swiss_join_internal.h#L150">null-bit indexing arithmetic</a>:</p>
<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code data-lang="cpp"><span class="kt">int64_t</span> <span class="n">bit_id</span> <span class="o">=</span> <span class="n">row_id</span> <span class="o">*</span> <span class="n">null_mask_num_bytes</span> <span class="o">*</span> <span class="mi">8</span> <span class="o">+</span> <span class="n">pos_after_encoding</span><span class="p">;</span>
</code></pre></div></div>
<p>The intermediate multiplication is performed using 32-bit arithmetic and can overflow even though the final result is stored in a 64-bit variable.</p>
<ul>
<li>SIMD gathers with 32-bit offsets</li>
</ul>
<p>One essential SIMD instruction is the AVX2 intrinsic <code>__m256i _mm256_i32gather_epi32(int const * base, __m256i vindex, const int scale);</code>, which performs a parallel memory gather of eight 32-bit integers based on eight 32-bit signed offsets. It was extensively used in Arrow for hash table operations, for example, <a href="https://github.com/apache/arrow/blob/0a00e25f2f6fb927fb555b69038d0be9b9d9f265/cpp/src/arrow/compute/key_map_internal_avx2.cc#L404">fetching 8 group IDs</a> (hash table slots) in parallel during hash table probing:</p>
<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code data-lang="cpp"><span class="n">__m256i</span> <span class="n">group_id</span> <span class="o">=</span> <span class="n">_mm256_i32gather_epi32</span><span class="p">(</span><span class="n">elements</span><span class="p">,</span> <span class="n">pos</span><span class="p">,</span> <span class="mi">1</span><span class="p">);</span>
</code></pre></div></div>
<p>and <a href="https://github.com/apache/arrow/blob/69e8a78c018da88b60f9eb2b3b45703f81f3c93d/cpp/src/arrow/compute/row/compare_internal_avx2.cc#L284">loading 8 corresponding key values</a> from the right-side input in parallel for comparison:</p>
<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code data-lang="cpp"><span class="n">__m256i</span> <span class="n">right</span> <span class="o">=</span> <span class="n">_mm256_i32gather_epi32</span><span class="p">((</span><span class="k">const</span> <span class="kt">int</span><span class="o">*</span><span class="p">)</span><span class="n">right_base</span><span class="p">,</span> <span class="n">offset_right</span><span class="p">,</span> <span class="mi">1</span><span class="p">);</span>
</code></pre></div></div>
<p>If any of the computed offsets exceed <code>2^31 - 1</code>, they wrap into the negative range, which can lead to invalid memory access (i.e., a crash) or, more subtly, fetch data from a valid but incorrect location — producing silently wrong results (trust me, you don’t want to debug that).</p>
<p>To mitigate these risks, PR <a href="https://github.com/apache/arrow/pull/45108">GH-45108</a>, <a href="https://github.com/apache/arrow/pull/45336">GH-45336</a>, and <a href="https://github.com/apache/arrow/pull/45515">GH-45515</a> promoted critical arithmetic to 64-bit and reworked SIMD logic to use safer indexing. Buffer access logic was also encapsulated in safer abstractions to avoid repeated manual casting or unchecked offset math. These examples are not unique to Arrow — they reflect common pitfalls in building data-intensive systems, where unchecked assumptions about integer sizes can silently compromise correctness.</p>
<p>Together, these changes make Arrow’s hash join implementation significantly more robust and better equipped for modern data workloads. These foundations not only resolve known issues but also reduce the risk of similar bugs in future development.</p>
<h1>Leaner Memory Usage</h1>
<p>While refining overflow-prone parts of the hash join implementation, I ended up examining most of the code path for potential pitfalls. When doing this kind of work, one sits down quietly and interrogates every line — asking not just whether an intermediate value might overflow, but whether it even needs to exist at all. And during that process, I came across something unrelated to overflow — but even more impactful.</p>
<p>In a textbook hash join algorithm, once the right-side table (the build-side) is fully accumulated, a hash table is constructed to support probing the left-side table (the probe-side) for matches. To parallelize this build step, Arrow C++’s implementation partitions the build-side into <code>N</code> partitions — typically matching the number of available CPU cores — and builds a separate hash table for each partition in parallel. These are then merged into a final, unified hash table used during the probe phase.</p>
<p>The issue? The memory footprint. The total size of the partitioned hash tables is roughly equal to that of the final hash table, but they were being held in memory even after merging. Once the final hash table was built, these temporary structures had no further use — yet they persisted through the entire join operation. There were no crashes, no warnings, no visible red flags — just silent overhead.</p>
<p>Once spotted, the fix was straightforward: restructure the join process to release these buffers immediately after the merge. The change was implemented in PR <a href="https://github.com/apache/arrow/issues/45552">GH-45552</a>. The memory profiles below illustrate its impact.</p>
<div style="display: flex; gap: 16px; justify-content: center; align-items: flex-start;">
  <img src="/img/recent-improvements-to-hash-join/memory-profile-baseline.png" width="50%" class="img-responsive" alt="Memory profile before" aria-hidden="true">
  <img src="/img/recent-improvements-to-hash-join/memory-profile-opt.png" width="50%" class="img-responsive" alt="Memory profile after" aria-hidden="true">
</div>
<p>At <code>A</code>, memory usage rises steadily as the join builds partitioned hash tables in parallel. <code>B</code> marks the merge point, where these partitions are combined into a final, unified hash table. <code>C</code> represents the start of the probe phase, where the left-side table is scanned and matched against the final hash table. Memory begins to rise again as join results are materialized. <code>D</code> is the peak of the join operation, just before memory begins to drop as processing completes. The “leap of faith” occurs at the star on the right profile, where the partitioned hash tables are released immediately after merging. This early release frees up substantial memory and makes room for downstream processing — reducing the overall peak memory observed at <code>D</code>.</p>
<p>This improvement already benefits real-world scenarios — for example, the <a href="https://duckdblabs.github.io/db-benchmark/">DuckDB Labs DB Benchmark</a>. Some benchmark queries that previously failed with out-of-memory (OOM) errors can now complete successfully — as shown in the comparison below.</p>
<div style="display: flex; gap: 16px; justify-content: center; align-items: flex-start;">
  <img src="/img/recent-improvements-to-hash-join/duckdb-bench-oom-baseline.png" width="50%" class="img-responsive" alt="DuckDB benchmark OOM before" aria-hidden="true">
  <img src="/img/recent-improvements-to-hash-join/duckdb-bench-oom-opt.png" width="50%" class="img-responsive" alt="DuckDB benchmark OOM after" aria-hidden="true">
</div>
<p>As one reviewer noted in the PR, this was a “low-hanging fruit.” And sometimes, meaningful performance gains don’t come from tuning hot loops or digging through flame graphs — they come from noticing something that doesn’t feel right and asking: why are we still keeping this around?</p>
<h1>Faster Execution Through Better Parallelism</h1>
<p>Not every improvement comes from poring over flame graphs — but some definitely do. Performance is, after all, the most talked-about aspect of any query engine. So, how about a nice cup of flame graph?</p>
<img src="/img/recent-improvements-to-hash-join/a-nice-cup-of-flame-graph.png" width="100%" class="img-responsive" alt="A nice cup of flame graph" aria-hidden="true">
<p>It’s hard not to notice the long, flat bar dominating the middle — especially with the rather alarming word “Lock” in it. That’s our red flag.</p>
<p>We’ve mentioned that in the build phase, we build partitioned hash tables in parallel. In earlier versions of Arrow C++, this parallelism was implemented on a batch basis — each thread processed a build-side batch concurrently. Since each batch contained arbitrary data that could fall into any partition, threads had to synchronize when accessing shared partitions. This was managed through <a href="https://github.com/apache/arrow/blob/196cde38c112d32a944afe978b6da9c7ce935ef7/cpp/src/arrow/acero/partition_util.h#L93">locks on partitions</a>. Although we introduced some randomness in the locking order to reduce contention, it remained high — clearly visible in the flame graph.</p>
<p>To mitigate this contention, we restructured the build phase in PR <a href="https://github.com/apache/arrow/issues/45612">GH-45612</a>. Instead of having all threads partition and insert at once — each thread touching every hash table — we split the work into two distinct stages. In the first partition stage, <code>M</code> threads take their assigned batches and only partition them, recording which rows belong to which partition. No insertion happens yet — just classification. Then comes the second, newly separated build stage. Here, <code>N</code> threads take over, and each thread is responsible for building just one of the <code>N</code> partitioned hash tables. Every thread scans all the relevant partitions across all batches but inserts only the rows belonging to its assigned partition. This restructuring eliminates the need for locking between threads during insertion — each thread now has exclusive access to its partitioned hash table. By decoupling the work this way, we turned a highly contentious operation into a clean, embarrassingly parallel one. As a result, we saw performance improve by up to 10x in dedicated build benchmarks. The <a href="https://github.com/apache/arrow/blob/196cde38c112d32a944afe978b6da9c7ce935ef7/cpp/src/arrow/acero/hash_join_benchmark.cc#L302">example</a> below is from a more typical, general-purpose workload — not especially build-heavy — but it still shows a solid 2x speedup. In the chart, the leap of faith — marked by the purple icons 🟣⬇️ — represents results with this improvement applied, while the gray and black ones show earlier runs before the change.</p>
<img src="/img/recent-improvements-to-hash-join/internal-benchmark.png" width="100%" class="img-responsive" alt="Internal benchmark" aria-hidden="true">
<p>Also in real-world scenarios like the <a href="https://duckdblabs.github.io/db-benchmark/">DuckDB Labs DB Benchmark</a>, we’ve observed similar gains. The comparison below shows around a 2x improvement in query performance after this change was applied.</p>
<div style="display: flex; gap: 16px; justify-content: center; align-items: flex-start;">
  <img src="/img/recent-improvements-to-hash-join/duckdb-bench-perf-baseline.png" width="50%" class="img-responsive" alt="DuckDB benchmark perf before" aria-hidden="true">
  <img src="/img/recent-improvements-to-hash-join/duckdb-bench-perf-opt.png" width="50%" class="img-responsive" alt="DuckDB benchmark perf after" aria-hidden="true">
</div>
<p>Additional improvements include <a href="https://github.com/apache/arrow/pull/43832">GH-43832</a>, which extends AVX2 acceleration to more probing code paths, and <a href="https://github.com/apache/arrow/pull/45918">GH-45918</a>, which introduces parallelism to a previously sequential task phase. These target more specialized scenarios and edge cases.</p>
<h2>Closing</h2>
<p>These improvements reflect ongoing investment in Arrow C++’s execution engine and a commitment to delivering fast, robust building blocks for analytic workloads. They are available in recent Arrow C++ releases and exposed through higher-level bindings like PyArrow and the Arrow R package — starting from version 18.0.0, with the most significant improvements landing in 20.0.0. If joins were a blocker for you before — due to memory, scale, or correctness — recent changes may offer a very different experience.</p>
<p>The Arrow C++ engine is not just alive — it’s improving in meaningful, user-visible ways. We’re also actively monitoring for further issues and open to expanding the design based on user feedback and real-world needs. If you’ve tried joins in the past and run into performance or stability issues, we encourage you to give them another try and file an <a href="https://github.com/apache/arrow/issues">issue on GitHub</a> if you run into any issues.</p>
<p>If you have any questions about this blog post, please feel free to contact the author, <a href="mailto:zanmato1984@gmail.com">Rossi Sun</a>.</p>]]></content><author><name>zanmato</name></author><category term="application" /><summary type="html"><![CDATA[A deep dive into recent improvements to Apache Arrow’s hash join implementation — enhancing stability, memory efficiency, and parallel performance for modern analytic workloads.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Apache Arrow 21.0.0 Release</title><link href="https://arrow.apache.org/blog/2025/07/17/21.0.0-release/" rel="alternate" type="text/html" title="Apache Arrow 21.0.0 Release" /><published>2025-07-17T00:00:00-04:00</published><updated>2025-07-17T00:00:00-04:00</updated><id>https://arrow.apache.org/blog/2025/07/17/21.0.0-release</id><content type="html" xml:base="https://arrow.apache.org/blog/2025/07/17/21.0.0-release/"><![CDATA[<!--

-->
<p>The Apache Arrow team is pleased to announce the 21.0.0 release. This release
covers over 2 months of development work and includes <a href="https://github.com/apache/arrow/milestone/69?closed=1"><strong>339 resolved
issues</strong></a> on <a href="/release/21.0.0.html#contributors"><strong>400 distinct commits</strong></a> from <a href="/release/21.0.0.html#contributors"><strong>82 distinct
contributors</strong></a>. See the <a href="https://arrow.apache.org/install/">Install Page</a> to
learn how to get the libraries for your platform.</p>
<p>The release notes below are not exhaustive and only expose selected highlights
of the release. Many other bugfixes and improvements have been made: we refer
you to the <a href="/release/21.0.0.html#changelog">complete changelog</a>.</p>
<h2>Community</h2>
<p>Since the 20.0.0 release, Alenka Frim has been invited to join the Project
Management Committee (PMC).</p>
<p>Thanks for your contributions and participation in the project!</p>
<p>The <a href="https://sessionize.com/arrow-summit-2025/">Call for Speakers</a> for the
Apache Arrow Summit 2025 is now open! The Summit will take place on October 2nd,
2025 in Paris, France as part of <a href="https://pydata.org/paris2025">PyData Paris</a>.
The call will be open until July 26, 2025. Please see the <a href="https://sessionize.com/arrow-summit-2025/">Call for
Speakers</a> link to submit a talk or
the <a href="https://lists.apache.org/thread/f0vcbtpzg6rntzbvmjstyd27bd9qfhl0">developer mailing
list</a> for more
information.</p>
<h2>Arrow Flight RPC Notes</h2>
<p>In C++ and Python, a new IPC reader option was added to force data buffers to be
aligned based on the data type, making it easier to work with systems that
expected alignment (<a href="https://github.com/apache/arrow/issues/32276">GH-32276</a>).
While this is not a Flight-specific option, it tended to occur with Flight due
to implementation details. Also, C++ and Python are now consistent with other
Flight implementations in allowing the schema of a <code>FlightInfo</code> to be omitted
(<a href="https://github.com/apache/arrow/issues/37677">GH-37677</a>).</p>
<p>We have accepted a donation of an ODBC driver for Flight SQL from Dremio
(<a href="https://github.com/apache/arrow/issues/46522">GH-46522</a>). Note that the driver
is not usable in its current state and contributors are working on implementing
the rest of the driver.</p>
<h2>C++ Notes</h2>
<h3>Compute</h3>
<p>The Cast function is now able to reorder fields when casting from one struct
type to another; the fields are matched by name, not by index
(<a href="https://github.com/apache/arrow/issues/45028">GH-45028</a>).</p>
<p>Many compute kernels have been moved into a separate, optional, shared library
(<a href="https://github.com/apache/arrow/issues/25025">GH-25025</a>). This improves
modularity for dependency management in the application and reduces the Arrow
C++ distribution size when the compute functionality is not being used. Note
that some compute functions, such as the Cast function, will still be built for
internal use in various Arrow components.</p>
<p>Better half-float support has been added to some compute functions: <code>is_nan</code>,
<code>is_inf</code>, <code>is_finite</code>, <code>negate</code>, <code>negate_checked</code>, <code>sign</code>
(<a href="https://github.com/apache/arrow/issues/45083">GH-45083</a>); <code>if_else</code>,
<code>case_when</code>, <code>coalesce</code>, <code>choose</code>, <code>replace_with_mask</code>, <code>fill_null_forward</code>,
<code>fill_null_backward</code> (<a href="https://github.com/apache/arrow/issues/37027">GH-37027</a>);
<code>run_end_encode</code>, <code>run_end_decode</code>
(<a href="https://github.com/apache/arrow/issues/46285">GH-46285</a>).</p>
<p>Better decimal32 and decimal64 support has been added to some compute functions:
<code>run_end_encode</code>, <code>run_end_decode</code>
(<a href="https://github.com/apache/arrow/issues/46285">GH-46285</a>).</p>
<p>A new function <code>utf8_zero_fill</code> acts like Python's <code>str.zfill</code> method by
providing a left-padding function that preserves the optional leading plus/minus
sign (<a href="https://github.com/apache/arrow/issues/46683">GH-46683</a>).</p>
<p>Decimal sum aggregation now produces a decimal result with an increased
precision in order to reduce the risk of overflowing the result type
(<a href="https://github.com/apache/arrow/issues/35166">GH-35166</a>).</p>
<h3>CSV</h3>
<p>Reading Duration columns is now supported
(<a href="https://github.com/apache/arrow/issues/40278">GH-40278</a>).</p>
<h3>Dataset</h3>
<p>It is now possible to preserve order when writing a dataset multi-threaded. The
feature is disabled by default
(<a href="https://github.com/apache/arrow/issues/26818">GH-26818</a>).</p>
<h3>Filesystems</h3>
<p>The S3 filesystem can optionally be built into a separate DLL
(<a href="https://github.com/apache/arrow/issues/40343">GH-40343</a>).</p>
<h3>Parquet</h3>
<h4>Encryption</h4>
<p>A new <code>SecureString</code> class must now be used to communicate sensitive data (such
as secret keys) with Parquet encryption APIs. This class automatically wipes its
contents from memory when destroyed, unlike regular <code>std::string</code>
(<a href="https://github.com/apache/arrow/issues/31603">GH-31603</a>).</p>
<h4>Type support</h4>
<p>The new VARIANT logical type is supported at a low level, and an extension type
<code>parquet.variant</code> is added to reflect such columns when reading them to Arrow
(<a href="https://github.com/apache/arrow/issues/45937">GH-45937</a>).</p>
<p>The UUID logical type is automatically converted to/from the <code>arrow.uuid</code>
canonical extension type when reading or writing Parquet data, respectively.</p>
<p>The GEOMETRY and GEOGRAPHY logical types are supported
(<a href="https://github.com/apache/arrow/issues/45522">GH-45522</a>). They are
automatically converted to/from the corresponding GeoArrow extension type, if it
has been registered by GeoArrow. Geospatial column statistics are also
supported.</p>
<p>It is now possible to read BYTE_ARRAY columns directly as LargeBinary or
BinaryView, without any intermediate conversion from Binary. Similarly, those
types can be written directly to Parquet
(<a href="https://github.com/apache/arrow/issues/43041">GH-43041</a>). This allows
bypassing the 2 GiB data per chunk limitation of the Binary type, and can also
improve performance. This also applies to String types when a Parquet column has
the STRING logical type.</p>
<p>Similarly, LIST columns can now be read directly as LargeList rather than List.
This allows bypassing the 2^31 values per chunk limitation of regular List types
(<a href="https://github.com/apache/arrow/issues/46676">GH-46676</a>).</p>
<h4>Other Parquet improvements</h4>
<p>A new feature named Content-Defined Chunking improves deduplication of Parquet
files with mostly identical contents, by choosing data page boundaries based on
actual contents rather than a number of values. For that, it uses a rolling hash
function, and the min and max chunk size can be chosen. The feature is disabled
by default and can be enabled on a per-file basis in the Parquet
<code>WriterProperties</code> (<a href="https://github.com/apache/arrow/issues/45750">GH-45750</a>).</p>
<p>The <code>EncodedStatistics</code> of a column chunk are publicly exposed in
<code>ColumnChunkMetaData</code> and can be read faster than if decoded as <code>Statistics</code>
(<a href="https://github.com/apache/arrow/issues/46462">GH-46462</a>).</p>
<p>SIMD optimizations for the BYTE_STREAM_SPLIT have been improved
(<a href="https://github.com/apache/arrow/issues/46788">GH-46788</a>).</p>
<p>Reading FIXED_LEN_BYTE_ARRAY data has been made significantly faster (up to 3x
faster on some benchmarks). This benefits logical types such as FLOAT16
(<a href="https://github.com/apache/arrow/issues/43891">GH-43891</a>).</p>
<h3>Miscellaneous C++ changes</h3>
<p>The <code>ARROW_USE_PRECOMPILED_HEADERS</code> build option was removed, as
<code>CMAKE_UNITY_BUILD</code> usually provides more benefits while requiring less
maintenance.</p>
<p>New data creation helpers <code>ArrayFromJSONString</code>, <code>ChunkedArrayFromJSONString</code>,
<code>DictArrayFromJSONString</code>, <code>ScalarFromJSONString</code> and <code>DictScalarFromJSONString</code>
are now exposed publicly. While not as high-performing as <code>BufferBuilder</code> and
the concrete <code>ArrayBuilder</code> subclasses, they allow easy creation of test or
example data, for example:</p>
<div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code data-lang="c++">  <span class="n">ARROW_ASSIGN_OR_RAISE</span><span class="p">(</span>
      <span class="k">auto</span> <span class="n">string_array</span><span class="p">,</span>
      <span class="n">arrow</span><span class="o">::</span><span class="n">ArrayFromJSONString</span><span class="p">(</span><span class="n">arrow</span><span class="o">::</span><span class="n">utf8</span><span class="p">(),</span> <span class="s">R"(["Hello", "World", null])"</span><span class="p">));</span>
  <span class="n">ARROW_ASSIGN_OR_RAISE</span><span class="p">(</span>
      <span class="k">auto</span> <span class="n">list_array</span><span class="p">,</span>
      <span class="n">arrow</span><span class="o">::</span><span class="n">ArrayFromJSONString</span><span class="p">(</span><span class="n">arrow</span><span class="o">::</span><span class="n">list</span><span class="p">(</span><span class="n">arrow</span><span class="o">::</span><span class="n">int32</span><span class="p">()),</span>
                                 <span class="s">"[[1, null, 2], [], [3]]"</span><span class="p">));</span>
</code></pre></div></div>
<p>Some APIs were changed to accept <code>std::string_view</code> instead of <code>const std::string&amp;</code>. Most uses of those APIs should not be affected
(<a href="https://github.com/apache/arrow/issues/46551">GH-46551</a>).</p>
<p>A new pretty-print option allows limiting element size when printing string or
binary data (<a href="https://github.com/apache/arrow/issues/46403">GH-46403</a>).</p>
<p>It is now possible to export <code>Tensor</code> data using
<a href="https://dmlc.github.io/dlpack/latest/">DLPack</a>
(<a href="https://github.com/apache/arrow/issues/39294">GH-39294</a>).</p>
<p>Half-float arrays can be properly diff'ed and pretty-printed
(<a href="https://github.com/apache/arrow/issues/36753">GH-36753</a>).</p>
<p>Some header files in <code>arrow/util</code> that were not supposed to be exposed are
now made internal (<a href="https://github.com/apache/arrow/issues/46459">GH-46459</a>).</p>
<h2>C# Notes</h2>
<p>The C# Arrow implementation is being extracted from the <a href="https://github.com/apache/arrow">Arrow
monorepo</a> into a standalone repository to allow
it to have its own release cadence. See <a href="https://lists.apache.org/thread/0vj7hlzbzrv0lcrm92tgtfdh9gsj4dqb">the mailing
list</a> for more
information. This is the final release of the Arrow monorepo that will include
the the C# implementation.</p>
<h2>Java, JavaScript, Go, and Rust Notes</h2>
<p>The Java, JavaScript, Go, and Rust Go projects have moved to separate
repositories outside the main Arrow <a href="https://github.com/apache/arrow">monorepo</a>.</p>
<ul>
<li>For notes on the latest release of the <a href="https://github.com/apache/arrow-java">Java
implementation</a>, see the latest <a href="https://github.com/apache/arrow-java/releases">Arrow
Java changelog</a>.</li>
<li>For notes on the latest release of the <a href="https://github.com/apache/arrow-js">JavaScript
implementation</a>, see the latest <a href="https://github.com/apache/arrow-js/releases">Arrow
JavaScript changelog</a>.</li>
<li>For notes on the latest release of the <a href="https://github.com/apache/arrow-rs">Rust
implementation</a> see the latest <a href="https://github.com/apache/arrow-rs/blob/main/CHANGELOG.md">Arrow Rust
changelog</a>.</li>
<li>For notes on the latest release of the <a href="https://github.com/apache/arrow-go">Go
implementation</a>, see the latest <a href="https://github.com/apache/arrow-go/releases">Arrow Go
changelog</a>.</li>
</ul>
<h2>Linux Packaging Notes</h2>
<p>We added support for AlmaLinux 10. You can use AlmaLinux 10 packages on Red Hat
Enterprise Linux 10 like distributions too.</p>
<p>We dropped support for CentOS Stream 8 because it reached EOL on 2024-05-31.</p>
<h2>MATLAB Notes</h2>
<h3>New Features</h3>
<p>Added support for creating an <code>arrow.tabular.Table</code> from a list of
<code>arrow.tabular.RecordBatch</code> instances
(<a href="https://github.com/apache/arrow/issues/46877">GH-46877</a>)</p>
<h3>Packaging</h3>
<p>The MLTBX available in apache/arrow's GitHub Releases area was built against
MATLAB R2025a.</p>
<h2>Python Notes</h2>
<p>Compatibility notes:</p>
<ul>
<li>Deprecated <code>PyExtensionType</code> has been removed
(<a href="https://github.com/apache/arrow/issues/46198">GH-46198</a>).</li>
<li>Deprecated <code>use_legacy_format</code>has been removed in favour of setting
<code>IpcWriteOptions</code> (<a href="https://github.com/apache/arrow/issues/46130">GH-46130</a>).</li>
<li>Due to SciPy 1.15's stricter sparse code changes are made to
<code>pa.SparseCXXMatrix</code> constructors and <code>pa.SparseCXXMatrix.to_scipy</code> methods
with migrating from <code>scipy.spmatrix</code> to <code>scipy.sparray</code>
(<a href="https://github.com/apache/arrow/issues/45229">GH-45229</a>).</li>
</ul>
<p>New features:</p>
<ul>
<li>PyArrow does not require NumPy anymore to generate <code>float16</code> scalars and
arrays (<a href="https://github.com/apache/arrow/issues/46611">GH-46611</a>).</li>
<li><code>pc.utf8_zero_fill</code> is now available in the compute module imitating
Python’s `str.zfill`` (<a href="https://github.com/apache/arrow/issues/46683">GH-46683</a>).</li>
<li><code>pa.arange</code> utility function is now available which creates an array of
evenly spaced values within a given interval
(<a href="https://github.com/apache/arrow/issues/46771">GH-46771</a>).</li>
<li>Scalar subclasses are now implementing Python protocols
(<a href="https://github.com/apache/arrow/issues/45653">GH-45653</a>).</li>
<li>It is possible now to specify footer metadata when opening IPC file for
writing using metadata keyword in <code>pa.ipc.new_file()</code>
(<a href="https://github.com/apache/arrow/issues/46222">GH-46222</a>).</li>
<li>DLPack is now implemented (export) on the Tensor class in C++ and available in
Python (<a href="https://github.com/apache/arrow/issues/39294">GH-39294</a>).</li>
</ul>
<p>Other improvements:</p>
<ul>
<li>Couple of improvements have been included in the Filesystems module:
Filesystem operations are more convenient to users by supporting explicit
<code>fsspec+{protocol}</code> and <code>hf://</code> filesystem URIs
(<a href="https://github.com/apache/arrow/issues/44900">GH-44900</a>),
<code>ConfigureManagedIdentityCredential</code> and <code>ConfigureClientSecretCredential</code>
have been exposed to <code>AzureFileSystem</code>
(<a href="https://github.com/apache/arrow/issues/46833">GH-46833</a>), <code>allow_delayed_open</code>
(<a href="https://github.com/apache/arrow/issues/45957">GH-45957</a>) and
<code>tls_ca_file_path</code> (<a href="https://github.com/apache/arrow/issues/40754">GH-40754</a>)
have been exposed to <code>S3FileSystem</code>.</li>
<li>Parquet module improvements include: mapping of logical types to Arrow
extension types by default
(<a href="https://github.com/apache/arrow/issues/44500">GH-44500</a>), UUID extension type
conversion support when writing or reading to/from Parquet
(<a href="https://github.com/apache/arrow/issues/43807">GH-43807</a>) and support for uniform
encryption when writing parquet files by exposing
<code>EncryptionConfiguration.uniform_encryption</code>
(<a href="https://github.com/apache/arrow/issues/38914">GH-38914</a>).</li>
<li><code>filter_expression</code> is exposed in <code>Table.join</code> and <code>Dataset.join</code> to
support filtering rows when performing hash joins with Acero
(<a href="https://github.com/apache/arrow/issues/46572">GH-46572</a>).</li>
<li><code>dim_names</code> argument can now be passed to <code>from_numpy_ndarray</code> constructor
(<a href="https://github.com/apache/arrow/issues/45531">GH-45531</a>).</li>
</ul>
<p>Relevant bug fixes:</p>
<ul>
<li><code>pyarrow.Table.to_struct_array</code> failure when the table is empty has been
fixed (<a href="https://github.com/apache/arrow/issues/46355">GH-46355</a>).</li>
<li>Filtering all rows with <code>RecordBatch.filter</code> using an expression now returns
empty table with same schema instead of erroring
(<a href="https://github.com/apache/arrow/issues/44366">GH-44366</a>).</li>
</ul>
<h2>Ruby and C GLib Notes</h2>
<p>A number of changes were made in the 21.0.0 release which affect both Ruby and C GLib:</p>
<ul>
<li>Added support for fixed shape tensor extension data type.</li>
<li>Added support for UUID extension data type.</li>
<li>Added support for fixed size list data type.</li>
<li>Added support for <a href="https://arrow.apache.org/docs/format/CDataInterface.html">the Arrow C data
interface</a> for
chunked array.</li>
<li>Added support for distinct count in array statistics.</li>
</ul>
<h3>Ruby</h3>
<p>There were no update only for Ruby.</p>
<h3>C GLib</h3>
<p>You must call <code>garrow_compute_initialize()</code> explicitly before you use
computation related features.</p>]]></content><author><name>pmc</name></author><category term="release" /><summary type="html"><![CDATA[The Apache Arrow team is pleased to announce the 21.0.0 release. This release covers over 2 months of development work and includes 339 resolved issues on 400 distinct commits from 82 distinct contributors. See the Install Page to learn how to get the libraries for your platform. The release notes below are not exhaustive and only expose selected highlights of the release. Many other bugfixes and improvements have been made: we refer you to the complete changelog. Community Since the 20.0.0 release, Alenka Frim has been invited to join the Project Management Committee (PMC). Thanks for your contributions and participation in the project! The Call for Speakers for the Apache Arrow Summit 2025 is now open! The Summit will take place on October 2nd, 2025 in Paris, France as part of PyData Paris. The call will be open until July 26, 2025. Please see the Call for Speakers link to submit a talk or the developer mailing list for more information. Arrow Flight RPC Notes In C++ and Python, a new IPC reader option was added to force data buffers to be aligned based on the data type, making it easier to work with systems that expected alignment (GH-32276). While this is not a Flight-specific option, it tended to occur with Flight due to implementation details. Also, C++ and Python are now consistent with other Flight implementations in allowing the schema of a FlightInfo to be omitted (GH-37677). We have accepted a donation of an ODBC driver for Flight SQL from Dremio (GH-46522). Note that the driver is not usable in its current state and contributors are working on implementing the rest of the driver. C++ Notes Compute The Cast function is now able to reorder fields when casting from one struct type to another; the fields are matched by name, not by index (GH-45028). Many compute kernels have been moved into a separate, optional, shared library (GH-25025). This improves modularity for dependency management in the application and reduces the Arrow C++ distribution size when the compute functionality is not being used. Note that some compute functions, such as the Cast function, will still be built for internal use in various Arrow components. Better half-float support has been added to some compute functions: is_nan, is_inf, is_finite, negate, negate_checked, sign (GH-45083); if_else, case_when, coalesce, choose, replace_with_mask, fill_null_forward, fill_null_backward (GH-37027); run_end_encode, run_end_decode (GH-46285). Better decimal32 and decimal64 support has been added to some compute functions: run_end_encode, run_end_decode (GH-46285). A new function utf8_zero_fill acts like Python's str.zfill method by providing a left-padding function that preserves the optional leading plus/minus sign (GH-46683). Decimal sum aggregation now produces a decimal result with an increased precision in order to reduce the risk of overflowing the result type (GH-35166). CSV Reading Duration columns is now supported (GH-40278). Dataset It is now possible to preserve order when writing a dataset multi-threaded. The feature is disabled by default (GH-26818). Filesystems The S3 filesystem can optionally be built into a separate DLL (GH-40343). Parquet Encryption A new SecureString class must now be used to communicate sensitive data (such as secret keys) with Parquet encryption APIs. This class automatically wipes its contents from memory when destroyed, unlike regular std::string (GH-31603). Type support The new VARIANT logical type is supported at a low level, and an extension type parquet.variant is added to reflect such columns when reading them to Arrow (GH-45937). The UUID logical type is automatically converted to/from the arrow.uuid canonical extension type when reading or writing Parquet data, respectively. The GEOMETRY and GEOGRAPHY logical types are supported (GH-45522). They are automatically converted to/from the corresponding GeoArrow extension type, if it has been registered by GeoArrow. Geospatial column statistics are also supported. It is now possible to read BYTE_ARRAY columns directly as LargeBinary or BinaryView, without any intermediate conversion from Binary. Similarly, those types can be written directly to Parquet (GH-43041). This allows bypassing the 2 GiB data per chunk limitation of the Binary type, and can also improve performance. This also applies to String types when a Parquet column has the STRING logical type. Similarly, LIST columns can now be read directly as LargeList rather than List. This allows bypassing the 2^31 values per chunk limitation of regular List types (GH-46676). Other Parquet improvements A new feature named Content-Defined Chunking improves deduplication of Parquet files with mostly identical contents, by choosing data page boundaries based on actual contents rather than a number of values. For that, it uses a rolling hash function, and the min and max chunk size can be chosen. The feature is disabled by default and can be enabled on a per-file basis in the Parquet WriterProperties (GH-45750). The EncodedStatistics of a column chunk are publicly exposed in ColumnChunkMetaData and can be read faster than if decoded as Statistics (GH-46462). SIMD optimizations for the BYTE_STREAM_SPLIT have been improved (GH-46788). Reading FIXED_LEN_BYTE_ARRAY data has been made significantly faster (up to 3x faster on some benchmarks). This benefits logical types such as FLOAT16 (GH-43891). Miscellaneous C++ changes The ARROW_USE_PRECOMPILED_HEADERS build option was removed, as CMAKE_UNITY_BUILD usually provides more benefits while requiring less maintenance. New data creation helpers ArrayFromJSONString, ChunkedArrayFromJSONString, DictArrayFromJSONString, ScalarFromJSONString and DictScalarFromJSONString are now exposed publicly. While not as high-performing as BufferBuilder and the concrete ArrayBuilder subclasses, they allow easy creation of test or example data, for example: ARROW_ASSIGN_OR_RAISE( auto string_array, arrow::ArrayFromJSONString(arrow::utf8(), R"(["Hello", "World", null])")); ARROW_ASSIGN_OR_RAISE( auto list_array, arrow::ArrayFromJSONString(arrow::list(arrow::int32()), "[[1, null, 2], [], [3]]")); Some APIs were changed to accept std::string_view instead of const std::string&amp;. Most uses of those APIs should not be affected (GH-46551). A new pretty-print option allows limiting element size when printing string or binary data (GH-46403). It is now possible to export Tensor data using DLPack (GH-39294). Half-float arrays can be properly diff'ed and pretty-printed (GH-36753). Some header files in arrow/util that were not supposed to be exposed are now made internal (GH-46459). C# Notes The C# Arrow implementation is being extracted from the Arrow monorepo into a standalone repository to allow it to have its own release cadence. See the mailing list for more information. This is the final release of the Arrow monorepo that will include the the C# implementation. Java, JavaScript, Go, and Rust Notes The Java, JavaScript, Go, and Rust Go projects have moved to separate repositories outside the main Arrow monorepo. For notes on the latest release of the Java implementation, see the latest Arrow Java changelog. For notes on the latest release of the JavaScript implementation, see the latest Arrow JavaScript changelog. For notes on the latest release of the Rust implementation see the latest Arrow Rust changelog. For notes on the latest release of the Go implementation, see the latest Arrow Go changelog. Linux Packaging Notes We added support for AlmaLinux 10. You can use AlmaLinux 10 packages on Red Hat Enterprise Linux 10 like distributions too. We dropped support for CentOS Stream 8 because it reached EOL on 2024-05-31. MATLAB Notes New Features Added support for creating an arrow.tabular.Table from a list of arrow.tabular.RecordBatch instances (GH-46877) Packaging The MLTBX available in apache/arrow's GitHub Releases area was built against MATLAB R2025a. Python Notes Compatibility notes: Deprecated PyExtensionType has been removed (GH-46198). Deprecated use_legacy_formathas been removed in favour of setting IpcWriteOptions (GH-46130). Due to SciPy 1.15's stricter sparse code changes are made to pa.SparseCXXMatrix constructors and pa.SparseCXXMatrix.to_scipy methods with migrating from scipy.spmatrix to scipy.sparray (GH-45229). New features: PyArrow does not require NumPy anymore to generate float16 scalars and arrays (GH-46611). pc.utf8_zero_fill is now available in the compute module imitating Python’s `str.zfill`` (GH-46683). pa.arange utility function is now available which creates an array of evenly spaced values within a given interval (GH-46771). Scalar subclasses are now implementing Python protocols (GH-45653). It is possible now to specify footer metadata when opening IPC file for writing using metadata keyword in pa.ipc.new_file() (GH-46222). DLPack is now implemented (export) on the Tensor class in C++ and available in Python (GH-39294). Other improvements: Couple of improvements have been included in the Filesystems module: Filesystem operations are more convenient to users by supporting explicit fsspec+{protocol} and hf:// filesystem URIs (GH-44900), ConfigureManagedIdentityCredential and ConfigureClientSecretCredential have been exposed to AzureFileSystem (GH-46833), allow_delayed_open (GH-45957) and tls_ca_file_path (GH-40754) have been exposed to S3FileSystem. Parquet module improvements include: mapping of logical types to Arrow extension types by default (GH-44500), UUID extension type conversion support when writing or reading to/from Parquet (GH-43807) and support for uniform encryption when writing parquet files by exposing EncryptionConfiguration.uniform_encryption (GH-38914). filter_expression is exposed in Table.join and Dataset.join to support filtering rows when performing hash joins with Acero (GH-46572). dim_names argument can now be passed to from_numpy_ndarray constructor (GH-45531). Relevant bug fixes: pyarrow.Table.to_struct_array failure when the table is empty has been fixed (GH-46355). Filtering all rows with RecordBatch.filter using an expression now returns empty table with same schema instead of erroring (GH-44366). Ruby and C GLib Notes A number of changes were made in the 21.0.0 release which affect both Ruby and C GLib: Added support for fixed shape tensor extension data type. Added support for UUID extension data type. Added support for fixed size list data type. Added support for the Arrow C data interface for chunked array. Added support for distinct count in array statistics. Ruby There were no update only for Ruby. C GLib You must call garrow_compute_initialize() explicitly before you use computation related features.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Apache Arrow ADBC 19 (Libraries) Release</title><link href="https://arrow.apache.org/blog/2025/07/08/adbc-19-release/" rel="alternate" type="text/html" title="Apache Arrow ADBC 19 (Libraries) Release" /><published>2025-07-08T00:00:00-04:00</published><updated>2025-07-08T00:00:00-04:00</updated><id>https://arrow.apache.org/blog/2025/07/08/adbc-19-release</id><content type="html" xml:base="https://arrow.apache.org/blog/2025/07/08/adbc-19-release/"><![CDATA[<!--

-->
<p>The Apache Arrow team is pleased to announce the version 19 release of
the Apache Arrow ADBC libraries. This release includes <a href="https://github.com/apache/arrow-adbc/milestone/23"><strong>60
resolved issues</strong></a> from <a href="#contributors"><strong>27 distinct contributors</strong></a>.</p>
<p>This is a release of the <strong>libraries</strong>, which are at version 19.  The
<a href="https://arrow.apache.org/adbc/19/format/specification.html"><strong>API specification</strong></a> is versioned separately and is at
version 1.1.0.</p>
<p>The subcomponents are versioned independently:</p>
<ul>
<li>C/C++/GLib/Go/Python/Ruby: 1.7.0</li>
<li>C#: 0.19.0</li>
<li>Java: 0.19.0</li>
<li>R: 0.19.0</li>
<li>Rust: 0.19.0</li>
</ul>
<p>The release notes below are not exhaustive and only expose selected
highlights of the release. Many other bugfixes and improvements have
been made: we refer you to the <a href="https://github.com/apache/arrow-adbc/blob/apache-arrow-adbc-19/CHANGELOG.md">complete changelog</a>.</p>
<h2>Release Highlights</h2>
<ul>
<li>Apache Hive/Impala/Spark, Databricks: these drivers have received a plethora of improvements,
optimizations, and bug fixes.</li>
<li>DataFusion: the arrow crate version requirement is now independent from that
of the <code>adbc_core</code> crate, to make it easier to use older versions of the
dependency when not using the DataFusion driver
(<a href="https://github.com/apache/arrow-adbc/pull/3017">#3017</a>).</li>
<li>Driver Manager: drivers can now be loaded by searching configuration
directories (or on Windows, the registry) for 'manifest' files describing
where the driver is located
(<a href="https://github.com/apache/arrow-adbc/pull/2918">#2918</a>,
<a href="https://github.com/apache/arrow-adbc/pull/3018">#3018</a>,
<a href="https://github.com/apache/arrow-adbc/pull/3021">#3021</a>,
<a href="https://github.com/apache/arrow-adbc/pull/3036">#3036</a>,
<a href="https://github.com/apache/arrow-adbc/pull/3041">#3041</a>).  Add freethreaded
wheels for the driver manager in Python 3.13.  These are still experimental;
please <a href="https://github.com/apache/arrow-adbc/issues">file a bug report</a> with
any feedback (<a href="https://github.com/apache/arrow-adbc/pull/3063">#3063</a>).
Make it easier to use the DB-API layer in Python without depending on
PyArrow, to make it easier for users of polars and other libraries
(<a href="https://github.com/apache/arrow-adbc/pull/2839">#2839</a>).</li>
<li>Flight SQL (Go): the last release unintentionally renamed the entrypoint
symbol.  Both the old and 'new' names are now present
(<a href="https://github.com/apache/arrow-adbc/pull/3056">#3056</a>).
Use custom certificates (when present) for OAuth
(<a href="https://github.com/apache/arrow-adbc/pull/2829">#2829</a>).</li>
<li>PostgreSQL: ingest zoned timestamps as <code>TIMESTAMP WITH TIME ZONE</code>
(<a href="https://github.com/apache/arrow-adbc/pull/2904">#2904</a>) and support
reading the <code>int2vector</code> type
(<a href="https://github.com/apache/arrow-adbc/pull/2919">#2919</a>).</li>
<li>Snowflake: fix issues with COPY concurrency options
(<a href="https://github.com/apache/arrow-adbc/pull/2805">#2805</a>), logging spam
(<a href="https://github.com/apache/arrow-adbc/pull/2807">#2807</a>), and boolean
result columns (<a href="https://github.com/apache/arrow-adbc/pull/2854">#2854</a>).
Add an option to return timestamps in microseconds to avoid overflow with
extreme values (<a href="https://github.com/apache/arrow-adbc/pull/2917">#2917</a>).</li>
<li>Rust: make a breaking change from <code>&amp;mut self</code> to <code>&amp;mut</code> in one API to enable
fearless concurrency
(<a href="https://github.com/apache/arrow-adbc/pull/2788">#2788</a>).</li>
<li>Add experimental support for integrating with
<a href="https://opentelemetry.io/">OpenTelemetry</a>, starting with the Snowflake
driver.  (<a href="https://github.com/apache/arrow-adbc/pull/2729">#2729</a>,
<a href="https://github.com/apache/arrow-adbc/pull/2825">#2825</a>,
<a href="https://github.com/apache/arrow-adbc/pull/2847">#2847</a>,
<a href="https://github.com/apache/arrow-adbc/pull/2951">#2951</a>).</li>
<li>Improve the build experience when using Meson
(<a href="https://github.com/apache/arrow-adbc/pull/2848">#2848</a>,
<a href="https://github.com/apache/arrow-adbc/pull/2849">#2849</a>).
Make it easier to statically link drivers
(<a href="https://github.com/apache/arrow-adbc/pull/2738">#2738</a>).</li>
</ul>
<h2>Contributors</h2>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ git shortlog --perl-regexp --author='^((?!dependabot\[bot\]).*)$' -sn apache-arrow-adbc-18..apache-arrow-adbc-19
    32	David Li
    20	Daijiro Fukuda
    16	Todd Meng
    10	eric-wang-1990
     7	eitsupi
     6	Matt Topol
     6	davidhcoe
     5	Bruce Irschick
     5	Sutou Kouhei
     3	Dewey Dunnington
     3	Jacky Hu
     2	Alex Guo
     2	Bryce Mecum
     2	Jade Wang
     2	James Thompson
     2	William Ayd
     2	qifanzhang-ms
     1	Arseny Tsypushkin
     1	Felipe Oliveira Carvalho
     1	Hiroyuki Sato
     1	Hélder Gregório
     1	Jan-Hendrik Zab
     1	Jarro van Ginkel
     1	Jolan Rensen
     1	Sergei Grebnov
     1	Sudhir Reddy Emmadi
     1	amangoyal
</code></pre></div></div>
<h2>Roadmap</h2>
<p>We plan to continue expanding support for features like OpenTelemetry that
have been introduced experimentally.</p>
<p>There is some discussion on a potential second revision of ADBC to include
more missing functionality and asynchronous API support.  For more, see the
<a href="https://github.com/apache/arrow-adbc/milestone/8">milestone</a>.  We would
welcome suggestions on APIs that could be added or extended.  Some of the
contributors are planning to begin work on a proposal in the eventual future.</p>
<h2>Getting Involved</h2>
<p>We welcome questions and contributions from all interested.  Issues
can be filed on <a href="https://github.com/apache/arrow-adbc/issues">GitHub</a>, and questions can be directed to GitHub
or the <a href="/community/">Arrow mailing lists</a>.</p>]]></content><author><name>pmc</name></author><category term="release" /><summary type="html"><![CDATA[The Apache Arrow team is pleased to announce the version 19 release of the Apache Arrow ADBC libraries. This release includes 60 resolved issues from 27 distinct contributors. This is a release of the libraries, which are at version 19. The API specification is versioned separately and is at version 1.1.0. The subcomponents are versioned independently: C/C++/GLib/Go/Python/Ruby: 1.7.0 C#: 0.19.0 Java: 0.19.0 R: 0.19.0 Rust: 0.19.0 The release notes below are not exhaustive and only expose selected highlights of the release. Many other bugfixes and improvements have been made: we refer you to the complete changelog. Release Highlights Apache Hive/Impala/Spark, Databricks: these drivers have received a plethora of improvements, optimizations, and bug fixes. DataFusion: the arrow crate version requirement is now independent from that of the adbc_core crate, to make it easier to use older versions of the dependency when not using the DataFusion driver (#3017). Driver Manager: drivers can now be loaded by searching configuration directories (or on Windows, the registry) for 'manifest' files describing where the driver is located (#2918, #3018, #3021, #3036, #3041). Add freethreaded wheels for the driver manager in Python 3.13. These are still experimental; please file a bug report with any feedback (#3063). Make it easier to use the DB-API layer in Python without depending on PyArrow, to make it easier for users of polars and other libraries (#2839). Flight SQL (Go): the last release unintentionally renamed the entrypoint symbol. Both the old and 'new' names are now present (#3056). Use custom certificates (when present) for OAuth (#2829). PostgreSQL: ingest zoned timestamps as TIMESTAMP WITH TIME ZONE (#2904) and support reading the int2vector type (#2919). Snowflake: fix issues with COPY concurrency options (#2805), logging spam (#2807), and boolean result columns (#2854). Add an option to return timestamps in microseconds to avoid overflow with extreme values (#2917). Rust: make a breaking change from &amp;mut self to &amp;mut in one API to enable fearless concurrency (#2788). Add experimental support for integrating with OpenTelemetry, starting with the Snowflake driver. (#2729, #2825, #2847, #2951). Improve the build experience when using Meson (#2848, #2849). Make it easier to statically link drivers (#2738). Contributors $ git shortlog --perl-regexp --author='^((?!dependabot\[bot\]).*)$' -sn apache-arrow-adbc-18..apache-arrow-adbc-19 32 David Li 20 Daijiro Fukuda 16 Todd Meng 10 eric-wang-1990 7 eitsupi 6 Matt Topol 6 davidhcoe 5 Bruce Irschick 5 Sutou Kouhei 3 Dewey Dunnington 3 Jacky Hu 2 Alex Guo 2 Bryce Mecum 2 Jade Wang 2 James Thompson 2 William Ayd 2 qifanzhang-ms 1 Arseny Tsypushkin 1 Felipe Oliveira Carvalho 1 Hiroyuki Sato 1 Hélder Gregório 1 Jan-Hendrik Zab 1 Jarro van Ginkel 1 Jolan Rensen 1 Sergei Grebnov 1 Sudhir Reddy Emmadi 1 amangoyal Roadmap We plan to continue expanding support for features like OpenTelemetry that have been introduced experimentally. There is some discussion on a potential second revision of ADBC to include more missing functionality and asynchronous API support. For more, see the milestone. We would welcome suggestions on APIs that could be added or extended. Some of the contributors are planning to begin work on a proposal in the eventual future. Getting Involved We welcome questions and contributions from all interested. Issues can be filed on GitHub, and questions can be directed to GitHub or the Arrow mailing lists.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Apache Arrow nanoarrow 0.7.0 Release</title><link href="https://arrow.apache.org/blog/2025/07/02/nanoarrow-0.7.0-release/" rel="alternate" type="text/html" title="Apache Arrow nanoarrow 0.7.0 Release" /><published>2025-07-02T00:00:00-04:00</published><updated>2025-07-02T00:00:00-04:00</updated><id>https://arrow.apache.org/blog/2025/07/02/nanoarrow-0.7.0-release</id><content type="html" xml:base="https://arrow.apache.org/blog/2025/07/02/nanoarrow-0.7.0-release/"><![CDATA[<!--

-->
<p>The Apache Arrow team is pleased to announce the 0.7.0 release of
Apache Arrow nanoarrow. This release covers 117 resolved issues from
12 contributors.</p>
<h2>Release Highlights</h2>
<ul>
<li>Migrate Python bindings to Meson Python</li>
<li>Better support for shared linkage</li>
<li>ZSTD Decompression support in IPC reader</li>
<li>Decimal32, Decimal64, ListView and LargeListView support</li>
<li>Support for vcpkg</li>
</ul>
<p>See the
<a href="https://github.com/apache/arrow-nanoarrow/blob/apache-arrow-nanoarrow-0.7.0-rc1/CHANGELOG.md">Changelog</a>
for a detailed list of contributions to this release.</p>
<h2>Features</h2>
<h3>Meson Python</h3>
<p>The Python bindings now use <a href="https://mesonbuild.com/meson-python/">Meson Python</a> as
the build backend. The main benefit is that adding C or C++ library dependencies
like ZSTD is much simpler than with setuptools which was needed to add the new
decompression support to the Python bindings.</p>
<p>Thanks to <a href="https://github.com/WillAyd">@WillAyd</a> for this contribution and continued
maintenance of the Python build infrastructure!</p>
<h3>Shared Linkage</h3>
<p>The nanoarrow C library is generally designed to be statically linked into an
application or library; however, there were some applications that did want
shared linkage and on Windows some extra work was needed to ensure this worked
as intended. Version 0.7.0 includes the appropriate DLL import/export attributes
and adds dedicated <code>nanoarrow_shared</code> and <code>nanoarrow_static</code> targets to the CMake
configuration to explicitly choose a strategy (linking to <code>nanoarrow</code> will continue
to use the CMake default as it did in previous versions).</p>
<p>Thanks to <a href="https://github.com/m-kuhn">@m-kuhn</a> for authoring the initial vcpkg
configuration that brought this to our attention!</p>
<h3>ZSTD Decompression Support</h3>
<p>The Arrow IPC reader included in the nanoarrow C library supports most features
of the Arrow IPC format; however, decompression support was missing which made
the library and its bindings unusable for some common use cases. In 0.7.0,
decompression support was added to the C library and R and Python bindings.</p>
<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code data-lang="r"><span class="n">library</span><span class="p">(</span><span class="n">nanoarrow</span><span class="p">)</span><span class="w">

</span><span class="n">url</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="s2">"https://github.com/geoarrow/geoarrow-data/releases/download/v0.2.0/ns-water_water-point.arrows"</span><span class="w">
</span><span class="n">read_nanoarrow</span><span class="p">(</span><span class="n">url</span><span class="p">)</span><span class="w"> </span><span class="o">|&gt;</span><span class="w">
  </span><span class="n">tibble</span><span class="o">::</span><span class="n">as_tibble</span><span class="p">()</span><span class="w">
</span><span class="c1">#&gt; # A tibble: 44,690 × 8</span><span class="w">
</span><span class="c1">#&gt;    OBJECTID FEAT_CODE ZVALUE PT_CLASS NAMEID_1 NAME_1 HID             geometry$x</span><span class="w">
</span><span class="c1">#&gt;       &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt;    &lt;int&gt; &lt;chr&gt;    &lt;chr&gt;  &lt;chr&gt;                &lt;dbl&gt;</span><span class="w">
</span><span class="c1">#&gt;  1     1055 WARK60      -0.5        4 &lt;NA&gt;     &lt;NA&gt;   252C345D59374D…    258976.</span><span class="w">
</span><span class="c1">#&gt;  2     1023 WARK60       0.6        4 &lt;NA&gt;     &lt;NA&gt;   1DAB1D800FB84E…    258341.</span><span class="w">
</span><span class="c1">#&gt;  3     1021 WARK60       0.5        4 &lt;NA&gt;     &lt;NA&gt;   838438F1BBE745…    258338.</span><span class="w">
</span><span class="c1">#&gt;  4      985 WARK60       0          4 &lt;NA&gt;     &lt;NA&gt;   0A4BE2AB03D845…    258527.</span><span class="w">
</span><span class="c1">#&gt;  5      994 WARK60       1.9        4 &lt;NA&gt;     &lt;NA&gt;   6ACD71128B6B49…    258499.</span><span class="w">
</span><span class="c1">#&gt;  6      995 WARK60       1.4        4 &lt;NA&gt;     &lt;NA&gt;   B10B26FA32FB44…    258502.</span><span class="w">
</span><span class="c1">#&gt;  7      997 WARK60       1.1        4 &lt;NA&gt;     &lt;NA&gt;   28E47E22D71549…    258498.</span><span class="w">
</span><span class="c1">#&gt;  8      993 WARK60       1.9        4 &lt;NA&gt;     &lt;NA&gt;   FC9A29123BEF4A…    258499.</span><span class="w">
</span><span class="c1">#&gt;  9     1003 WARK60       0.7        4 &lt;NA&gt;     &lt;NA&gt;   3C7CA3CD0E8840…    258528.</span><span class="w">
</span><span class="c1">#&gt; 10     1001 WARK60       0.7        4 &lt;NA&gt;     &lt;NA&gt;   A6F508B066DC4A…    258511.</span><span class="w">
</span><span class="c1">#&gt; # ℹ 44,680 more rows</span><span class="w">
</span><span class="c1">#&gt; # ℹ 2 more variables: geometry$y &lt;dbl&gt;, $z &lt;dbl&gt;</span><span class="w">
</span></code></pre></div></div>
<p>Users of the C library will need to configure CMake with <code>-DNANOARROW_IPC_WITH_ZSTD=ON</code>
and <code>-DNANOARROW_IPC=ON</code> to use CMake-resolved ZSTD; however, client libraries
can also use an existing ZSTD or LZ4 implementation using callbacks.</p>
<h3>New Type Support</h3>
<p>While the nanoarrow C library is a minimal library, we do strive to support the full
specification and several new types were not supported by the C library. Version 0.7.0
includes support in the C library for Decimal32, Decimal64, ListView, and LargeListView
and improved support for support for decimal types in the nanoarrow R bindings.</p>
<p>Thanks to <a href="https://github.com/zeroshade">@zeroshade</a> for contributing Decimal32/Decimal64
support and <a href="https://github.com/WillAyd">@WillAyd</a> for contributing</p>
<h3>nanoarrow on vcpkg</h3>
<p>The nanoarrow C library can now be installed using
<a href="https://github.com/microsoft/vcpkg">vcpkg</a>!</p>
<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code data-lang="shell">git clone https://github.com/microsoft/vcpkg.git
<span class="nb">cd </span>vcpkg <span class="o">&amp;&amp;</span> ./bootstrap-vcpkg.sh
./vcpkg <span class="nb">install </span>nanoarrow
</code></pre></div></div>
<p>CMake projects can then use <code>find_package(nanoarrow)</code> when using the vcpkg
toolchain (i.e., <code>-DCMAKE_TOOLCHAIN_FILE=path/to/vcpkg/scripts/buildsystems/vcpkg.cmake</code>).
This also allows other vcpkg ports to use nanoarrow as a dependency in addition
to a convenience for projects already using vcpkg.</p>
<p>Thanks to <a href="https://github.com/m-kuhn">@m-kuhn</a> for contributing the nanoarrow port to
vcpkg!</p>
<h2>Contributors</h2>
<p>This release consists of contributions from 12 contributors in addition
to the invaluable advice and support of the Apache Arrow community.</p>
<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code data-lang="console"><span class="gp">$</span><span class="w"> </span>git shortlog <span class="nt">-sn</span> apache-arrow-nanoarrow-0.7.0.dev..apache-arrow-nanoarrow-0.7.0-rc1
<span class="go">    53  Dewey Dunnington
    27  William Ayd
     3  Michael Chirico
     2  Sutou Kouhei
     1  Bryce Mecum
     1  David Li
     1  Gang Wu
     1  Ilya Verbin
     1  Jacob Wujciak-Jens
     1  Matt Topol
     1  Matthias Kuhn
     1  eitsupi
</span></code></pre></div></div>]]></content><author><name>pmc</name></author><category term="release" /><summary type="html"><![CDATA[The Apache Arrow team is pleased to announce the 0.7.0 release of Apache Arrow nanoarrow. This release covers 117 resolved issues from 12 contributors. Release Highlights Migrate Python bindings to Meson Python Better support for shared linkage ZSTD Decompression support in IPC reader Decimal32, Decimal64, ListView and LargeListView support Support for vcpkg See the Changelog for a detailed list of contributions to this release. Features Meson Python The Python bindings now use Meson Python as the build backend. The main benefit is that adding C or C++ library dependencies like ZSTD is much simpler than with setuptools which was needed to add the new decompression support to the Python bindings. Thanks to @WillAyd for this contribution and continued maintenance of the Python build infrastructure! Shared Linkage The nanoarrow C library is generally designed to be statically linked into an application or library; however, there were some applications that did want shared linkage and on Windows some extra work was needed to ensure this worked as intended. Version 0.7.0 includes the appropriate DLL import/export attributes and adds dedicated nanoarrow_shared and nanoarrow_static targets to the CMake configuration to explicitly choose a strategy (linking to nanoarrow will continue to use the CMake default as it did in previous versions). Thanks to @m-kuhn for authoring the initial vcpkg configuration that brought this to our attention! ZSTD Decompression Support The Arrow IPC reader included in the nanoarrow C library supports most features of the Arrow IPC format; however, decompression support was missing which made the library and its bindings unusable for some common use cases. In 0.7.0, decompression support was added to the C library and R and Python bindings. library(nanoarrow) url &lt;- "https://github.com/geoarrow/geoarrow-data/releases/download/v0.2.0/ns-water_water-point.arrows" read_nanoarrow(url) |&gt; tibble::as_tibble() #&gt; # A tibble: 44,690 × 8 #&gt; OBJECTID FEAT_CODE ZVALUE PT_CLASS NAMEID_1 NAME_1 HID geometry$x #&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 1055 WARK60 -0.5 4 &lt;NA&gt; &lt;NA&gt; 252C345D59374D… 258976. #&gt; 2 1023 WARK60 0.6 4 &lt;NA&gt; &lt;NA&gt; 1DAB1D800FB84E… 258341. #&gt; 3 1021 WARK60 0.5 4 &lt;NA&gt; &lt;NA&gt; 838438F1BBE745… 258338. #&gt; 4 985 WARK60 0 4 &lt;NA&gt; &lt;NA&gt; 0A4BE2AB03D845… 258527. #&gt; 5 994 WARK60 1.9 4 &lt;NA&gt; &lt;NA&gt; 6ACD71128B6B49… 258499. #&gt; 6 995 WARK60 1.4 4 &lt;NA&gt; &lt;NA&gt; B10B26FA32FB44… 258502. #&gt; 7 997 WARK60 1.1 4 &lt;NA&gt; &lt;NA&gt; 28E47E22D71549… 258498. #&gt; 8 993 WARK60 1.9 4 &lt;NA&gt; &lt;NA&gt; FC9A29123BEF4A… 258499. #&gt; 9 1003 WARK60 0.7 4 &lt;NA&gt; &lt;NA&gt; 3C7CA3CD0E8840… 258528. #&gt; 10 1001 WARK60 0.7 4 &lt;NA&gt; &lt;NA&gt; A6F508B066DC4A… 258511. #&gt; # ℹ 44,680 more rows #&gt; # ℹ 2 more variables: geometry$y &lt;dbl&gt;, $z &lt;dbl&gt; Users of the C library will need to configure CMake with -DNANOARROW_IPC_WITH_ZSTD=ON and -DNANOARROW_IPC=ON to use CMake-resolved ZSTD; however, client libraries can also use an existing ZSTD or LZ4 implementation using callbacks. New Type Support While the nanoarrow C library is a minimal library, we do strive to support the full specification and several new types were not supported by the C library. Version 0.7.0 includes support in the C library for Decimal32, Decimal64, ListView, and LargeListView and improved support for support for decimal types in the nanoarrow R bindings. Thanks to @zeroshade for contributing Decimal32/Decimal64 support and @WillAyd for contributing nanoarrow on vcpkg The nanoarrow C library can now be installed using vcpkg! git clone https://github.com/microsoft/vcpkg.git cd vcpkg &amp;&amp; ./bootstrap-vcpkg.sh ./vcpkg install nanoarrow CMake projects can then use find_package(nanoarrow) when using the vcpkg toolchain (i.e., -DCMAKE_TOOLCHAIN_FILE=path/to/vcpkg/scripts/buildsystems/vcpkg.cmake). This also allows other vcpkg ports to use nanoarrow as a dependency in addition to a convenience for projects already using vcpkg. Thanks to @m-kuhn for contributing the nanoarrow port to vcpkg! Contributors This release consists of contributions from 12 contributors in addition to the invaluable advice and support of the Apache Arrow community. $ git shortlog -sn apache-arrow-nanoarrow-0.7.0.dev..apache-arrow-nanoarrow-0.7.0-rc1 53 Dewey Dunnington 27 William Ayd 3 Michael Chirico 2 Sutou Kouhei 1 Bryce Mecum 1 David Li 1 Gang Wu 1 Ilya Verbin 1 Jacob Wujciak-Jens 1 Matt Topol 1 Matthias Kuhn 1 eitsupi]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Apache Arrow Java 18.3.0 Release</title><link href="https://arrow.apache.org/blog/2025/05/13/arrow-java-18.3.0/" rel="alternate" type="text/html" title="Apache Arrow Java 18.3.0 Release" /><published>2025-05-13T00:00:00-04:00</published><updated>2025-05-13T00:00:00-04:00</updated><id>https://arrow.apache.org/blog/2025/05/13/arrow-java-18.3.0</id><content type="html" xml:base="https://arrow.apache.org/blog/2025/05/13/arrow-java-18.3.0/"><![CDATA[<!--

-->
<p>The Apache Arrow team is pleased to announce the <a href="https://github.com/apache/arrow-java/releases/tag/v18.3.0">v18.3.0</a> release of Apache Arrow Java.
This is a minor release since the last release <a href="https://github.com/apache/arrow-java/releases/tag/v18.2.0">v18.2.0</a>.</p>
<h2>Changelog</h2>
<h3>New Features and Enhancements</h3>
<ul>
<li>MINOR: ZstdCompressionCodec should use decompressedSize to get error name by @libenchao in <a href="https://github.com/apache/arrow-java/pull/619">#619</a></li>
<li>MINOR: Add explicit exception when no more buffer can be read when loading buffers by @viirya in <a href="https://github.com/apache/arrow-java/pull/649">#649</a></li>
<li>GH-81: [Flight] Expose gRPC in Flight client builder by @lidavidm in <a href="https://github.com/apache/arrow-java/pull/660">#660</a></li>
<li>GH-615: Produce Avro core data types out of Arrow VSR by @martin-traverse in <a href="https://github.com/apache/arrow-java/pull/638">#638</a></li>
<li>GH-494: [Flight] Allow configuring connect timeout in JDBC by @lidavidm in <a href="https://github.com/apache/arrow-java/pull/495">#495</a></li>
<li>GH-87: [Vector] Add ExtensionWriter by @xxlaykxx in <a href="https://github.com/apache/arrow-java/pull/697">#697</a></li>
<li>GH-698: Improve and fix Avro read consumers by @martin-traverse in <a href="https://github.com/apache/arrow-java/pull/718">#718</a></li>
<li>GH-737: [FlightSQL] Allow returning column remarks in FlightSQL's CommandGetTables by @mateuszrzeszutek in <a href="https://github.com/apache/arrow-java/pull/727">#727</a></li>
<li>GH-661: [Flight] JDBC: Cache failed locations by @lidavidm in <a href="https://github.com/apache/arrow-java/pull/662">#662</a></li>
</ul>
<h3>Bug Fixes</h3>
<ul>
<li>GH-601: [Gandiva] Synchronize some methods on the Projector by @lriggs in <a href="https://github.com/apache/arrow-java/pull/602">#602</a></li>
<li>GH-625: Map MinorType getNewFieldWriter returns UnionMapWriter by @wsuppiger in <a href="https://github.com/apache/arrow-java/pull/627">#627</a></li>
<li>GH-653: Nullify fieldReader when invalidating parent object by @lriggs in <a href="https://github.com/apache/arrow-java/pull/654">#654</a></li>
<li>GH-655: Failure in UnionReader.read after DecimalVector promotion to UnionVector by @lriggs in <a href="https://github.com/apache/arrow-java/pull/656">#656</a></li>
<li>GH-692: Preserve nullability information while transfering DecimalVector and Decimal256Vector by @bodduv in <a href="https://github.com/apache/arrow-java/pull/693">#693</a></li>
<li>GH-704: Fix initialization of offset buffer when exporting VarChar vectors through C Data Interface by @Kontinuation in <a href="https://github.com/apache/arrow-java/pull/705">#705</a></li>
<li>GH-709: Correct length calculation of value buffers of variable-sized arrays by @pepijnve in <a href="https://github.com/apache/arrow-java/pull/707">#707</a></li>
<li>GH-721: Allow using 1GB+ data buffers in variable width vectors by @gszadovszky in <a href="https://github.com/apache/arrow-java/pull/722">#722</a></li>
<li>GH-463: Improve TZ support for JDBC driver by @aiguofer in <a href="https://github.com/apache/arrow-java/pull/464">#464</a></li>
<li>GH-729: [JDBC] Fix BinaryConsumer consuming null value by @hnwyllmm in <a href="https://github.com/apache/arrow-java/pull/730">#730</a></li>
</ul>
<p><strong>Full Changelog</strong>: <a href="https://github.com/apache/arrow-java/commits/v18.3.0">changelog</a></p>]]></content><author><name>pmc</name></author><category term="release" /><summary type="html"><![CDATA[The Apache Arrow team is pleased to announce the v18.3.0 release of Apache Arrow Java. This is a minor release since the last release v18.2.0. Changelog New Features and Enhancements MINOR: ZstdCompressionCodec should use decompressedSize to get error name by @libenchao in #619 MINOR: Add explicit exception when no more buffer can be read when loading buffers by @viirya in #649 GH-81: [Flight] Expose gRPC in Flight client builder by @lidavidm in #660 GH-615: Produce Avro core data types out of Arrow VSR by @martin-traverse in #638 GH-494: [Flight] Allow configuring connect timeout in JDBC by @lidavidm in #495 GH-87: [Vector] Add ExtensionWriter by @xxlaykxx in #697 GH-698: Improve and fix Avro read consumers by @martin-traverse in #718 GH-737: [FlightSQL] Allow returning column remarks in FlightSQL's CommandGetTables by @mateuszrzeszutek in #727 GH-661: [Flight] JDBC: Cache failed locations by @lidavidm in #662 Bug Fixes GH-601: [Gandiva] Synchronize some methods on the Projector by @lriggs in #602 GH-625: Map MinorType getNewFieldWriter returns UnionMapWriter by @wsuppiger in #627 GH-653: Nullify fieldReader when invalidating parent object by @lriggs in #654 GH-655: Failure in UnionReader.read after DecimalVector promotion to UnionVector by @lriggs in #656 GH-692: Preserve nullability information while transfering DecimalVector and Decimal256Vector by @bodduv in #693 GH-704: Fix initialization of offset buffer when exporting VarChar vectors through C Data Interface by @Kontinuation in #705 GH-709: Correct length calculation of value buffers of variable-sized arrays by @pepijnve in #707 GH-721: Allow using 1GB+ data buffers in variable width vectors by @gszadovszky in #722 GH-463: Improve TZ support for JDBC driver by @aiguofer in #464 GH-729: [JDBC] Fix BinaryConsumer consuming null value by @hnwyllmm in #730 Full Changelog: changelog]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Apache Arrow Go 18.3.0 Release</title><link href="https://arrow.apache.org/blog/2025/05/09/arrow-go-18.3.0/" rel="alternate" type="text/html" title="Apache Arrow Go 18.3.0 Release" /><published>2025-05-09T00:00:00-04:00</published><updated>2025-05-09T00:00:00-04:00</updated><id>https://arrow.apache.org/blog/2025/05/09/arrow-go-18.3.0</id><content type="html" xml:base="https://arrow.apache.org/blog/2025/05/09/arrow-go-18.3.0/"><![CDATA[<!--

-->
<p>The Apache Arrow team is pleased to announce the v18.3.0 release of Apache Arrow Go.
This minor release covers 21 commits from 8 distinct contributors.</p>
<h2>Contributors</h2>
<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code data-lang="console"><span class="gp">$</span><span class="w"> </span>git shortlog <span class="nt">-sn</span> v18.2.0..v18.3.0
<span class="go">    13	Matt Topol
     2	Chris Pahl
     1	Ashish Negi
     1	David Li
     1	Jeroen Demeyer
     1	Mateusz Rzeszutek
     1	Raúl Cumplido
     1	Saurabh Singh
</span></code></pre></div></div>
<h2>Highlights</h2>
<ul>
<li>Fix alignment of atomic refcount handling for ARM <a href="https://github.com/apache/arrow-go/pull/323">#323</a></li>
</ul>
<h3>Arrow</h3>
<ul>
<li>Functions to convert RecordReader to Go iter.Seq and vice versa <a href="https://github.com/apache/arrow-go/pull/314">#314</a></li>
<li>New &quot;is_in&quot; function for Arrow compute package</li>
<li>Allow returning column remarks for FlightSQL CommandGetTables <a href="https://github.com/apache/arrow-go/pull/361">#361</a></li>
</ul>
<h3>Parquet</h3>
<ul>
<li>Added new <code>SeekToRow</code> function for pqarrow.RecordReader <a href="https://github.com/apache/arrow-go/pull/321">#321</a></li>
<li>Bloom filters can now be read and written, then utilized for skipping <a href="https://github.com/apache/arrow-go/pull/341">#341</a> <a href="https://github.com/apache/arrow-go/pull/336">#336</a></li>
<li>Fix a panic when <code>WriteDataPage</code> fails <a href="https://github.com/apache/arrow-go/pull/366">#366</a></li>
</ul>
<h2>Changelog</h2>
<h3>What's Changed</h3>
<ul>
<li>feat(arrow/array): convert RecordReader and iterators by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/314">#314</a></li>
<li>refactor(arrow/array): replace some codegen with generics by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/315">#315</a></li>
<li>feat(parquet/pqarrow): Add SeekToRow for RecordReader by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/321">#321</a></li>
<li>fix: go's atomic operations require 64bit alignment in structs on ARM by @sahib in <a href="https://github.com/apache/arrow-go/pull/323">#323</a></li>
<li>feat(arrow/compute): implement &quot;is_in&quot; function by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/319">#319</a></li>
<li>fix(parquet/pqarrow): fix propagation of FieldIds for nested fields by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/324">#324</a></li>
<li>Fix: Handle null values in PlainFixedLenByteArrayEncoder gracefully by @singh1203 in <a href="https://github.com/apache/arrow-go/pull/320">#320</a></li>
<li>fix(parquet/pqarrow): fix definition levels with non-nullable lists by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/325">#325</a></li>
<li>chore: fix macOS Go 1.24 CI by @lidavidm in <a href="https://github.com/apache/arrow-go/pull/334">#334</a></li>
<li>feat(parquet/metadata): bloom filter implementation by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/336">#336</a></li>
<li>feat(parquet): Write/Read bloom filters from files by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/341">#341</a></li>
<li>fix: move from atomic.(Add|Load|Store) to atomic.Int64{} by @sahib in <a href="https://github.com/apache/arrow-go/pull/326">#326</a></li>
<li>fix(parquet/file): restore goroutine safety for reader by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/343">#343</a></li>
<li>chore: Enable GitHub discussions on arrow-go repository by @raulcd in <a href="https://github.com/apache/arrow-go/pull/353">#353</a></li>
<li>Compress: add MarshalText and UnmarshalText by @jdemeyer in <a href="https://github.com/apache/arrow-go/pull/357">#357</a></li>
<li>fix(arrow/array): optional struct array with required field by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/359">#359</a></li>
<li>feat(parquet/schema): initial variant logical type by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/352">#352</a></li>
<li>chore(arrow): remove most lock copies by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/362">#362</a></li>
<li>Fix panic when WriteDataPage fails by @ashishnegi in <a href="https://github.com/apache/arrow-go/pull/366">#366</a></li>
<li>GH-46087: [FlightSQL] Allow returning column remarks in FlightSQL's CommandGetTables by @mateuszrzeszutek in <a href="https://github.com/apache/arrow-go/pull/361">#361</a></li>
</ul>
<h3>New Contributors</h3>
<ul>
<li>@sahib made their first contribution in <a href="https://github.com/apache/arrow-go/pull/323">#323</a></li>
<li>@jdemeyer made their first contribution in <a href="https://github.com/apache/arrow-go/pull/357">#357</a></li>
<li>@ashishnegi made their first contribution in <a href="https://github.com/apache/arrow-go/pull/366">#366</a></li>
<li>@mateuszrzeszutek made their first contribution in <a href="https://github.com/apache/arrow-go/pull/361">#361</a></li>
</ul>
<p><strong>Full Changelog</strong>: <a href="https://github.com/apache/arrow-go/compare/v18.2.0...v18.3.0">https://github.com/apache/arrow-go/compare/v18.2.0...v18.3.0</a></p>]]></content><author><name>pmc</name></author><category term="release" /><summary type="html"><![CDATA[The Apache Arrow team is pleased to announce the v18.3.0 release of Apache Arrow Go. This minor release covers 21 commits from 8 distinct contributors. Contributors $ git shortlog -sn v18.2.0..v18.3.0 13 Matt Topol 2 Chris Pahl 1 Ashish Negi 1 David Li 1 Jeroen Demeyer 1 Mateusz Rzeszutek 1 Raúl Cumplido 1 Saurabh Singh Highlights Fix alignment of atomic refcount handling for ARM #323 Arrow Functions to convert RecordReader to Go iter.Seq and vice versa #314 New &quot;is_in&quot; function for Arrow compute package Allow returning column remarks for FlightSQL CommandGetTables #361 Parquet Added new SeekToRow function for pqarrow.RecordReader #321 Bloom filters can now be read and written, then utilized for skipping #341 #336 Fix a panic when WriteDataPage fails #366 Changelog What's Changed feat(arrow/array): convert RecordReader and iterators by @zeroshade in #314 refactor(arrow/array): replace some codegen with generics by @zeroshade in #315 feat(parquet/pqarrow): Add SeekToRow for RecordReader by @zeroshade in #321 fix: go's atomic operations require 64bit alignment in structs on ARM by @sahib in #323 feat(arrow/compute): implement &quot;is_in&quot; function by @zeroshade in #319 fix(parquet/pqarrow): fix propagation of FieldIds for nested fields by @zeroshade in #324 Fix: Handle null values in PlainFixedLenByteArrayEncoder gracefully by @singh1203 in #320 fix(parquet/pqarrow): fix definition levels with non-nullable lists by @zeroshade in #325 chore: fix macOS Go 1.24 CI by @lidavidm in #334 feat(parquet/metadata): bloom filter implementation by @zeroshade in #336 feat(parquet): Write/Read bloom filters from files by @zeroshade in #341 fix: move from atomic.(Add|Load|Store) to atomic.Int64{} by @sahib in #326 fix(parquet/file): restore goroutine safety for reader by @zeroshade in #343 chore: Enable GitHub discussions on arrow-go repository by @raulcd in #353 Compress: add MarshalText and UnmarshalText by @jdemeyer in #357 fix(arrow/array): optional struct array with required field by @zeroshade in #359 feat(parquet/schema): initial variant logical type by @zeroshade in #352 chore(arrow): remove most lock copies by @zeroshade in #362 Fix panic when WriteDataPage fails by @ashishnegi in #366 GH-46087: [FlightSQL] Allow returning column remarks in FlightSQL's CommandGetTables by @mateuszrzeszutek in #361 New Contributors @sahib made their first contribution in #323 @jdemeyer made their first contribution in #357 @ashishnegi made their first contribution in #366 @mateuszrzeszutek made their first contribution in #361 Full Changelog: https://github.com/apache/arrow-go/compare/v18.2.0...v18.3.0]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Apache Arrow ADBC 18 (Libraries) Release</title><link href="https://arrow.apache.org/blog/2025/05/06/adbc-18-release/" rel="alternate" type="text/html" title="Apache Arrow ADBC 18 (Libraries) Release" /><published>2025-05-06T00:00:00-04:00</published><updated>2025-05-06T00:00:00-04:00</updated><id>https://arrow.apache.org/blog/2025/05/06/adbc-18-release</id><content type="html" xml:base="https://arrow.apache.org/blog/2025/05/06/adbc-18-release/"><![CDATA[<!--

-->
<p>The Apache Arrow team is pleased to announce the version 18 release of
the Apache Arrow ADBC libraries. This release includes <a href="https://github.com/apache/arrow-adbc/milestone/22"><strong>28
resolved issues</strong></a> from <a href="#contributors"><strong>22 distinct contributors</strong></a>.</p>
<p>This is a release of the <strong>libraries</strong>, which are at version 18.  The
<a href="https://arrow.apache.org/adbc/18/format/specification.html"><strong>API specification</strong></a> is versioned separately and is at
version 1.1.0.</p>
<p>The subcomponents are versioned independently:</p>
<ul>
<li>C/C++/GLib/Go/Python/Ruby: 1.6.0</li>
<li>C#: 0.18.0</li>
<li>Java: 0.18.0</li>
<li>R: 0.18.0</li>
<li>Rust: 0.18.0</li>
</ul>
<p>The release notes below are not exhaustive and only expose selected
highlights of the release. Many other bugfixes and improvements have
been made: we refer you to the <a href="https://github.com/apache/arrow-adbc/blob/apache-arrow-adbc-18/CHANGELOG.md">complete changelog</a>.</p>
<h2>Release Highlights</h2>
<p>Using Meson to build the project has been improved (#2735, #2746).</p>
<p>The C# bindings and its drivers have seen a lot of activity in this release.  A Databricks Spark driver is now available (#2672, #2737, #2743, #2692), with support for features like CloudFetch (#2634, #2678, #2691).  The general Spark driver now has better retry behavior for 503 responses (#2664), supports LZ4 compression applied outside of the Arrow IPC format (#2669), and supports OAuth (#2579), among other improvements.  The &quot;Apache&quot; driver for various Thrift-based systems now supports Apache Hive in addition to Apache Spark and Apache Impala (#2540), among other improvements.  The BigQuery driver adds more authentication and other configuration settings (#2655, #2566, #2541, #2698).</p>
<p>The Flight SQL driver supports OAuth (#2651).</p>
<p>The Java bindings experimentally support a JNI wrapper around drivers exposing the ADBC C API (#2401).  These are not currently distributed via Maven and must be built by hand.</p>
<p>The Go bindings now support union types in the <code>database/sql</code> wrapper (#2637).  The Golang-based BigQuery driver returns more metadata about tables (#2697).</p>
<p>The PostgreSQL driver now avoids spurious commit/rollback commands (#2685).  It also handles improper usage more gracefully (#2653).</p>
<p>The Python bindings now make it easier to pass options in various places (#2589, #2700).  Also, the DB-API layer can be minimally used without PyArrow installed, making it easier for users of libraries like polars that don't need or want a second Arrow implementation (#2609).</p>
<p>The Rust bindings now avoid locking the driver on every operation, allowing concurrent usage (#2736).</p>
<h2>Contributors</h2>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ git shortlog --perl-regexp --author='^((?!dependabot\[bot\]).*)$' -sn apache-arrow-adbc-17..apache-arrow-adbc-18
    20	David Li
     6	William Ayd
     5	Curt Hagenlocher
     5	davidhcoe
     4	Alex Guo
     4	Felipe Oliveira Carvalho
     4	Jade Wang
     4	Matthijs Brobbel
     4	Sutou Kouhei
     4	eric-wang-1990
     3	Bruce Irschick
     2	Milos Gligoric
     2	Sudhir Reddy Emmadi
     2	Todd Meng
     1	Bryce Mecum
     1	Dewey Dunnington
     1	Filip Wojciechowski
     1	Hiroaki Yutani
     1	Hélder Gregório
     1	Marin Nozhchev
     1	amangoyal
     1	qifanzhang-ms
</code></pre></div></div>
<h2>Roadmap</h2>
<p>There is some discussion on a potential second revision of ADBC to include more missing functionality and asynchronous API support.  For more, see the <a href="https://github.com/apache/arrow-adbc/milestone/8">milestone</a>.  We would welcome suggestions on APIs that could be added or extended.  Some of the contributors are planning to begin work on a proposal in the near future.</p>
<h2>Getting Involved</h2>
<p>We welcome questions and contributions from all interested.  Issues
can be filed on <a href="https://github.com/apache/arrow-adbc/issues">GitHub</a>, and questions can be directed to GitHub
or the <a href="/community/">Arrow mailing lists</a>.</p>]]></content><author><name>pmc</name></author><category term="release" /><summary type="html"><![CDATA[The Apache Arrow team is pleased to announce the version 18 release of the Apache Arrow ADBC libraries. This release includes 28 resolved issues from 22 distinct contributors. This is a release of the libraries, which are at version 18. The API specification is versioned separately and is at version 1.1.0. The subcomponents are versioned independently: C/C++/GLib/Go/Python/Ruby: 1.6.0 C#: 0.18.0 Java: 0.18.0 R: 0.18.0 Rust: 0.18.0 The release notes below are not exhaustive and only expose selected highlights of the release. Many other bugfixes and improvements have been made: we refer you to the complete changelog. Release Highlights Using Meson to build the project has been improved (#2735, #2746). The C# bindings and its drivers have seen a lot of activity in this release. A Databricks Spark driver is now available (#2672, #2737, #2743, #2692), with support for features like CloudFetch (#2634, #2678, #2691). The general Spark driver now has better retry behavior for 503 responses (#2664), supports LZ4 compression applied outside of the Arrow IPC format (#2669), and supports OAuth (#2579), among other improvements. The &quot;Apache&quot; driver for various Thrift-based systems now supports Apache Hive in addition to Apache Spark and Apache Impala (#2540), among other improvements. The BigQuery driver adds more authentication and other configuration settings (#2655, #2566, #2541, #2698). The Flight SQL driver supports OAuth (#2651). The Java bindings experimentally support a JNI wrapper around drivers exposing the ADBC C API (#2401). These are not currently distributed via Maven and must be built by hand. The Go bindings now support union types in the database/sql wrapper (#2637). The Golang-based BigQuery driver returns more metadata about tables (#2697). The PostgreSQL driver now avoids spurious commit/rollback commands (#2685). It also handles improper usage more gracefully (#2653). The Python bindings now make it easier to pass options in various places (#2589, #2700). Also, the DB-API layer can be minimally used without PyArrow installed, making it easier for users of libraries like polars that don't need or want a second Arrow implementation (#2609). The Rust bindings now avoid locking the driver on every operation, allowing concurrent usage (#2736). Contributors $ git shortlog --perl-regexp --author='^((?!dependabot\[bot\]).*)$' -sn apache-arrow-adbc-17..apache-arrow-adbc-18 20 David Li 6 William Ayd 5 Curt Hagenlocher 5 davidhcoe 4 Alex Guo 4 Felipe Oliveira Carvalho 4 Jade Wang 4 Matthijs Brobbel 4 Sutou Kouhei 4 eric-wang-1990 3 Bruce Irschick 2 Milos Gligoric 2 Sudhir Reddy Emmadi 2 Todd Meng 1 Bryce Mecum 1 Dewey Dunnington 1 Filip Wojciechowski 1 Hiroaki Yutani 1 Hélder Gregório 1 Marin Nozhchev 1 amangoyal 1 qifanzhang-ms Roadmap There is some discussion on a potential second revision of ADBC to include more missing functionality and asynchronous API support. For more, see the milestone. We would welcome suggestions on APIs that could be added or extended. Some of the contributors are planning to begin work on a proposal in the near future. Getting Involved We welcome questions and contributions from all interested. Issues can be filed on GitHub, and questions can be directed to GitHub or the Arrow mailing lists.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>