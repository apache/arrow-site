<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://arrow.apache.org/feed.xml" rel="self" type="application/atom+xml" /><link href="https://arrow.apache.org/" rel="alternate" type="text/html" /><updated>2025-04-10T10:34:00-04:00</updated><id>https://arrow.apache.org/feed.xml</id><title type="html">Apache Arrow</title><subtitle>Apache Arrow is the universal columnar format and multi-language toolbox for fast data interchange and in-memory analytics. It specifies a standardized language-independent column-oriented memory format for flat and nested data, organized for efficient analytic operations on modern hardware. It also provides computational libraries and zero-copy streaming messaging and interprocess communication. Languages currently supported include C, C++, C#, Go, Java, JavaScript, MATLAB, Python, R, Ruby, and Rust.</subtitle><entry><title type="html">Apache Arrow Go 18.2.0 Release</title><link href="https://arrow.apache.org/blog/2025/03/16/arrow-go-18.2.0/" rel="alternate" type="text/html" title="Apache Arrow Go 18.2.0 Release" /><published>2025-03-16T00:00:00-04:00</published><updated>2025-03-16T00:00:00-04:00</updated><id>https://arrow.apache.org/blog/2025/03/16/arrow-go-18.2.0</id><content type="html" xml:base="https://arrow.apache.org/blog/2025/03/16/arrow-go-18.2.0/"><![CDATA[<!--

-->

<p>The Apache Arrow team is pleased to announce the <a href="https://github.com/apache/arrow-go/releases/tag/v18.2.0">v18.2.0</a> release of Apache Arrow Go. 
This minor release covers 21 commits from 7 distinct contributors.</p>

<h2 id="highlights">Highlights</h2>

<h3 id="arrow">Arrow</h3>

<ul>
  <li>Fixed bitmap ops on 32-bit platforms <a href="https://github.com/apache/arrow-go/pull/277">#277</a></li>
  <li>Allocations by arrow/memory will always be aligned even from the Mallocator <a href="https://github.com/apache/arrow-go/pull/289">#289</a></li>
  <li>Sped up overflow checks for small integers in compute library <a href="https://github.com/apache/arrow-go/pull/303">#303</a></li>
</ul>

<h3 id="parquet">Parquet</h3>

<ul>
  <li>The <code class="language-plaintext highlighter-rouge">parquet_reader</code> CLI now has an option to dump the column and index offsets <a href="https://github.com/apache/arrow-go/pull/281">#281</a></li>
  <li>Column readers now have a <code class="language-plaintext highlighter-rouge">SeekToRow</code> method that will leverage column/index offsets if they exist <a href="https://github.com/apache/arrow-go/pull/283">#283</a></li>
</ul>

<h2 id="full-changelog">Full Changelog</h2>

<h3 id="whats-changed">What’s Changed</h3>
<ul>
  <li>fix(release): fix wrong upload path by @kou in <a href="https://github.com/apache/arrow-go/pull/243">#243</a></li>
  <li>fix(release): fix svn add command and add script to generate release notes by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/249">#249</a></li>
  <li>fix(arrow/cdata): move headers into parent Go package by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/251">#251</a></li>
  <li>chore: Add NOTICE.txt file by @singh1203 in <a href="https://github.com/apache/arrow-go/pull/257">#257</a></li>
  <li>chore: Update self-hosted arm runner to ubuntu-24.04-arm by @singh1203 in <a href="https://github.com/apache/arrow-go/pull/268">#268</a></li>
  <li>chore: Drop executable file bit of source files by @jas4711 in <a href="https://github.com/apache/arrow-go/pull/274">#274</a></li>
  <li>ci: update actions/cache by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/276">#276</a></li>
  <li>fix(arrow/bitutil): fix bitmap ops on 32-bit platforms by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/277">#277</a></li>
  <li>feat(internal/encoding): add Discard method to decoders by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/280">#280</a></li>
  <li>feat(parquet/cmd/parquet_reader): Add command to dump the column and offset indices by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/281">#281</a></li>
  <li>fix(internal/utils): fix clobbering BP by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/288">#288</a></li>
  <li>docs(license): update LICENSE.txt by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/290">#290</a></li>
  <li>feat(parquet/file): Add SeekToRow for Column readers by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/283">#283</a></li>
  <li>fix(parquet/pqarrow): propagate field id metadata for lists/maps by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/293">#293</a></li>
  <li>feat: Add arrayApproxEqualString to handle null characters in string. by @singh1203 in <a href="https://github.com/apache/arrow-go/pull/291">#291</a></li>
  <li>feat(parquet): update comments for <code class="language-plaintext highlighter-rouge">BufferedStreamEnabled</code> by @joechenrh in <a href="https://github.com/apache/arrow-go/pull/295">#295</a></li>
  <li>fix(arrow/memory): Align allocations always by @lidavidm in <a href="https://github.com/apache/arrow-go/pull/289">#289</a></li>
  <li>feat(parquet): add byte buffer when disable buffered stream by @joechenrh in <a href="https://github.com/apache/arrow-go/pull/302">#302</a></li>
  <li>perf(overflow): Speed up overflow checks for small integers by @cbandy in <a href="https://github.com/apache/arrow-go/pull/303">#303</a></li>
  <li>chore: bump version by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/305">#305</a></li>
  <li>fix(pqarrow): respect list element nullability during conversion by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/311">#311</a></li>
  <li>chore(testing): Update testing submodules by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/312">#312</a></li>
</ul>

<h3 id="new-contributors">New Contributors</h3>
<ul>
  <li>@singh1203 made their first contribution in <a href="https://github.com/apache/arrow-go/pull/257">#257</a></li>
  <li>@jas4711 made their first contribution in <a href="https://github.com/apache/arrow-go/pull/274">#274</a></li>
  <li>@lidavidm made their first contribution in <a href="https://github.com/apache/arrow-go/pull/289">#289</a></li>
  <li>@cbandy made their first contribution in <a href="https://github.com/apache/arrow-go/pull/303">#303</a></li>
</ul>

<p><strong>Full Changelog</strong>: https://github.com/apache/arrow-go/compare/v18.1.0…v18.2.0</p>

<h2 id="contributors">Contributors</h2>
<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">$</span><span class="w"> </span>git shortlog <span class="nt">-sn</span> v18.1.0..v18.2.0
<span class="go">    13	Matt Topol
     3	Saurabh Singh
     2	Ruihao Chen
     1	Chris Bandy
     1	David Li
     1	Simon Josefsson
     1	Sutou Kouhei
</span></code></pre></div></div>]]></content><author><name>pmc</name></author><category term="release" /><summary type="html"><![CDATA[The Apache Arrow team is pleased to announce the v18.2.0 release of Apache Arrow Go. This minor release covers 21 commits from 7 distinct contributors. Highlights Arrow Fixed bitmap ops on 32-bit platforms #277 Allocations by arrow/memory will always be aligned even from the Mallocator #289 Sped up overflow checks for small integers in compute library #303 Parquet The parquet_reader CLI now has an option to dump the column and index offsets #281 Column readers now have a SeekToRow method that will leverage column/index offsets if they exist #283 Full Changelog What’s Changed fix(release): fix wrong upload path by @kou in #243 fix(release): fix svn add command and add script to generate release notes by @zeroshade in #249 fix(arrow/cdata): move headers into parent Go package by @zeroshade in #251 chore: Add NOTICE.txt file by @singh1203 in #257 chore: Update self-hosted arm runner to ubuntu-24.04-arm by @singh1203 in #268 chore: Drop executable file bit of source files by @jas4711 in #274 ci: update actions/cache by @zeroshade in #276 fix(arrow/bitutil): fix bitmap ops on 32-bit platforms by @zeroshade in #277 feat(internal/encoding): add Discard method to decoders by @zeroshade in #280 feat(parquet/cmd/parquet_reader): Add command to dump the column and offset indices by @zeroshade in #281 fix(internal/utils): fix clobbering BP by @zeroshade in #288 docs(license): update LICENSE.txt by @zeroshade in #290 feat(parquet/file): Add SeekToRow for Column readers by @zeroshade in #283 fix(parquet/pqarrow): propagate field id metadata for lists/maps by @zeroshade in #293 feat: Add arrayApproxEqualString to handle null characters in string. by @singh1203 in #291 feat(parquet): update comments for BufferedStreamEnabled by @joechenrh in #295 fix(arrow/memory): Align allocations always by @lidavidm in #289 feat(parquet): add byte buffer when disable buffered stream by @joechenrh in #302 perf(overflow): Speed up overflow checks for small integers by @cbandy in #303 chore: bump version by @zeroshade in #305 fix(pqarrow): respect list element nullability during conversion by @zeroshade in #311 chore(testing): Update testing submodules by @zeroshade in #312 New Contributors @singh1203 made their first contribution in #257 @jas4711 made their first contribution in #274 @lidavidm made their first contribution in #289 @cbandy made their first contribution in #303 Full Changelog: https://github.com/apache/arrow-go/compare/v18.1.0…v18.2.0 Contributors $ git shortlog -sn v18.1.0..v18.2.0 13 Matt Topol 3 Saurabh Singh 2 Ruihao Chen 1 Chris Bandy 1 David Li 1 Simon Josefsson 1 Sutou Kouhei]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Fast Streaming Inserts in DuckDB with ADBC</title><link href="https://arrow.apache.org/blog/2025/03/10/fast-streaming-inserts-in-duckdb-with-adbc/" rel="alternate" type="text/html" title="Fast Streaming Inserts in DuckDB with ADBC" /><published>2025-03-10T00:00:00-04:00</published><updated>2025-03-10T00:00:00-04:00</updated><id>https://arrow.apache.org/blog/2025/03/10/fast-streaming-inserts-in-duckdb-with-adbc</id><content type="html" xml:base="https://arrow.apache.org/blog/2025/03/10/fast-streaming-inserts-in-duckdb-with-adbc/"><![CDATA[<!--

-->
<style>
.a-header {
  color: #984EA3;
  font-weight: bold;
}
.a-data {
  color: #377EB8;
  font-weight: bold;
}
.a-length {
  color: #FF7F00;
  font-weight: bold;
}
.a-padding {
  color: #E41A1C;
  font-weight: bold;
}
</style>

<p><img src="/img/adbc-duckdb/adbc-duckdb.png" width="100%" class="img-responsive" alt="" aria-hidden="true" /></p>
<h1 id="tldr">TL;DR</h1>

<p>DuckDB is rapidly becoming an essential part of data practitioners’ toolbox, finding use cases in data engineering, machine learning, and local analytics. In many cases DuckDB has been used to query and process data that has already been saved to storage (file-based or external database) by another process. Arrow Database Connectivity APIs enable high-throughput data processing using DuckDB as the engine.</p>

<h1 id="how-it-started">How it started</h1>

<p>The company I work for is the leading digital out-of-home marketing platform, including a programmatic ad tech stack. For several years, my technical operations team was making use of logs emitted by the real-time programmatic auction system in the <a href="http://avro.apache.org/">Apache Avro</a> format. Over time we’ve built an entire operations and analytics back end using this data. Avro files are row-based which is less than ideal for analytics at scale, in fact it’s downright painful. So much so that I developed and contributed an Avro reader feature to the <a href="https://github.com/apache/arrow-go">Apache Arrow  Go</a> library to be able to convert Avro files to parquet. This data pipeline is now humming along transforming hundreds of GB/day from Avro to Parquet.</p>

<p>Since “any problem in computer science can be solved with another layer of indirection”, the original system has grown layers (like an onion) and started to emit other logs, this time in <a href="https://parquet.apache.org/">Apache Parquet</a> format…</p>
<figure style="text-align: center;">
  <img src="/img/adbc-duckdb/muchrejoicing.webp" width="80%" class="img-responsive" alt="Figure 1: And there was much rejoicing" />
  <figcaption>Figure 1: A pseudo-medieval tapestry displaying intrepid data practitioners rejoicing due to a columnar data storage format.</figcaption>
</figure>
<p>As we learned in Shrek, onions are like ogres: they’re green, they have layers and they make you cry, so this rejoicing was rather short-lived, as the mechanism chosen to emit the parquet files was rather inefficient:</p>

<ul>
  <li>the new onion-layer (ahem…system component) sends Protobuf encoded messages to Kafka topics</li>
  <li>a Kafka Connect cluster with the S3 sink connector consumes topics and saves the parquet files to object storage</li>
</ul>

<p>Due to the firehose of data, the cluster size over time grew to &gt; 25 nodes and was producing thousands of small Parquet files (13 MB or smaller) an hour. This led to ever-increasing query latency, in some cases breaking our tools due to query timeouts (aka <a href="https://www.dremio.com/blog/compaction-in-apache-iceberg-fine-tuning-your-iceberg-tables-data-files/#h-the-small-files-problem">the small files problem</a>). Not to mention that running aggregations on the raw data in our data warehouse wasn’t fast or cheap.</p>

<h1 id="duckdb-to-the-rescue-i-think">DuckDB to the rescue… I think</h1>

<p>I’d used DuckDB to process and analyse Parquet data so I knew it could do that very quickly. Then I came across this post on LinkedIn (<a href="https://www.linkedin.com/posts/shubham-dhal-349626ba_real-time-analytics-with-kafka-and-duckdb-activity-7258424841538555904-xfU6">Real-Time Analytics using Kafka and DuckDB</a>), where someone has built a system for near-realtime analytics in Go using DuckDB.</p>

<p>The slides listed DuckDB’s limitations:<br />
<img src="/img/adbc-duckdb/duckdb.png" width="100%" class="img-responsive" alt="DuckDB limitations: Single Pod, *Data should fit in memory, *Low Query Concurrency, *Low Ingest Rate - *Solvable with some efforts" aria-hidden="true" /> 
The poster’s solution batches data at the application layer managing to scale up ingestion 100x to ~20k inserts/second, noting that they thought that using the DuckDB Appender API could possibly increase this 10x. So, potentially ~200k inserts/second. Yayyyyy…</p>

<figure style="text-align: center;">
  <img src="/img/adbc-duckdb/Yay.gif" width="40%" class="img-responsive" alt="Figure 2: Yay" />
</figure>

<p>Then I noticed the data schema in the slides was flat and had only 4 fields (vs. <a href="https://github.com/InteractiveAdvertisingBureau/openrtb2.x/blob/main/2.6.md#31---object-model-">OpenRTB</a> schema with deeply nested Lists and Structs); and then looked at our monitoring dashboards whereupon I realized that at peak our system was emitting &gt;250k events/second. [cue sad trombone]</p>

<p>Undeterred (and not particularly enamored with the idea of setting up/running/maintaining a Spark cluster), I suspected that Apache Arrow’s columnar memory representation might still make DuckDB viable since it has an Arrow API; getting Parquet files would be as easy as running <code class="language-plaintext highlighter-rouge">COPY...TO (format parquet)</code>.</p>

<p>Using a pattern found in a Github issue, I wrote a POC using <a href="http://github.com/marcboeker/go-duckdb">github.com/marcboeker/go-duckdb</a> to connect to a DB, retrieve an Arrow, create an Arrow Reader, register a view on the reader, then run an INSERT statement from the view.</p>

<p>This felt a bit like a rabbit pulling itself out of a hat, but no matter, it managed between ~74k and ~110k rows/sec on my laptop.</p>

<p>To make sure this was really the right solution, I also tried out DuckDB’s Appender API (at time of writing the official recommendation for fast inserts) and managed… ~63k rows/sec on my laptop. OK, but… meh.</p>

<h1 id="a-new-hope">A new hope</h1>

<p>In a discussion on the Gopher Slack, Matthew Topol aka <a href="https://github.com/zeroshade">zeroshade</a> suggested using <a href="http://arrow.apache.org/adbc">ADBC</a> with its much simpler API. Who is Matt Topol you ask? Just the guy who <em>literally</em> wrote the book on Apache Arrow, that’s who (<a href="https://www.packtpub.com/en-us/product/in-memory-analytics-with-apache-arrow-9781835461228"><strong><em>In-Memory Analytics with Apache Arrow: Accelerate data analytics for efficient processing of flat and hierarchical data structures 2nd Edition</em></strong></a>). It’s an excellent resource and guide for working with Arrow. <br />
BTW, should you prefer an acronym to remember the name of the book, it’s <strong><em>IMAAA:ADAFEPOFAHDS2E</em></strong>.<br />
<img src="/img/adbc-duckdb/imaaapfedaobfhsd2e.png" width="100%" class="img-responsive" alt="Episode IX: In-Memory Analytics with Apache Arrow: Perform fast and efficient data analytics on both flat and hierarchical structured data 2nd Edition aka IMAAA:PFEDAOBFHSD2E by Matt Topol" aria-hidden="true" /><br />
But I digress. Matt is also a member of the Apache Arrow PMC, a major contributor to the Go implementation of Apache Iceberg and generally a nice, helpful guy.</p>

<h1 id="adbc">ADBC</h1>
<p>ADBC is:</p>
<ul>
  <li>
    <p>A set of <a href="https://arrow.apache.org/adbc/current/format/specification.html">abstract APIs</a> in different languages (C/C++, Go, and Java, with more on the way) for working with databases and Arrow data.</p>

    <p>For example, result sets of queries in ADBC are all returned as streams of Arrow data, not row-by-row.</p>
  </li>
  <li>
    <p>A set of implementations of that API in different languages (C/C++, C#/.NET, Go, Java, Python, and Ruby) that target different databases (e.g. PostgreSQL, SQLite, DuckDB, any database supporting Flight SQL).</p>
  </li>
</ul>

<p>Going back to the drawing board, I created <a href="https://github.com/loicalleyne/quacfka">Quacfka</a>, a Go library built using ADBC and split out my system into 3 worker pools, connected by channels:</p>

<ul>
  <li>Kafka clients consuming topic messages and writing the bytes to a message channel</li>
  <li>Processing routines using the <a href="https://github.com/loicalleyne/bufarrow">Bufarrow</a> library to deserialize Protobuf data and append it to Arrow arrays, writing Arrow Records to a record channel</li>
  <li>DuckDB inserters binding the Arrow Records to ADBC statements and executing insertions</li>
</ul>

<p>I first ran these in series to determine how fast each could run:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight">
<code>2025/01/23 23:39:27 <span class="a-header">kafka read start with 8 readers</span>
2025/01/23 23:39:41 <span class="a-header">read 15728642 kafka records in 14.385530 secs @</span><span class="a-padding">1093365.498477 messages/sec</span>
2025/01/23 23:39:41 <span class="a-length">deserialize []byte to proto, convert to arrow records with 32 goroutines start</span>
2025/01/23 23:40:04 <span class="a-length">deserialize to arrow done - 15728642 records in 22.283532 secs @</span><span class="a-padding"> 705841.509812 messages/sec</span>
2025/01/23 23:40:04 <span class="a-data">ADBC IngestCreateAppend start with 32 connections</span> 
2025/01/23 23:40:25 <span class="a-data">duck ADBC insert 15728642 records in 21.145649535 secs @</span><span class="a-padding"> 743824.007783 rows/sec</span></code></pre></div></div>
<p><img src="/img/adbc-duckdb/holdmybeer.png" width="100%" class="img-responsive" alt="20k rows/sec? Hold my beer" aria-hidden="true" /></p>

<p>With this architecture decided, I then started running the workers concurrently, instrumenting the system, profiling my code to identify performance issues and tweaking the settings to maximize throughput. It seemed to me that there was enough performance headroom to allow for in-flight aggregations.</p>

<p>One issue: Despite DuckDB’s excellent <a href="https://duckdb.org/2022/10/28/lightweight-compression.html">lightweight compression</a>, inserts from this source were making the file size increase at a rate of <strong><em>~8GB/minute</em></strong>. Putting inserts on hold to export the Parquet files and release the storage would reduce the overall throughput to an unacceptable level. I decided to implement a rotation of database files based on a file size threshold.</p>

<p>DuckDB being able to query Hive partitioned parquet on disk or in object storage, the analytics part could be decoupled from the data ingestion pipeline by running a separate querying server pointing at wherever the parquet files would end up.</p>

<p>Iterating, I created several APIs to try to make in-flight aggregations efficient enough to keep the overall throughput above my 250k rows/second target.</p>

<p>The first two either ran into issues of data locality or weren’t optimized enough:</p>

<ul>
  <li><strong>CustomArrows</strong> : functions to run on each Arrow Record to create a new Record to insert along with the original</li>
  <li><strong>DuckRunner</strong> : run a series of queries on the database file before rotation</li>
</ul>

<p>Reasoning that if unnesting deeply nested data in Arrow Record arrays was causing data locality issues:</p>

<ul>
  <li><strong>Normalizer</strong>: a Bufarrow API used in the in the deserialization function to normalize the message data and append it to another Arrow Record, inserted into a separate table</li>
</ul>

<p>This approach allowed throughput to go back to levels almost as high as without Normalizer - flat data is much faster to process and insert.</p>

<h1 id="oh-were-halfway-therelivin-on-a-prayer">Oh, we’re halfway there…livin’ on a prayer</h1>

<p>Next, I tried opening concurrent connections to multiple databases. <strong>BAM!</strong> <strong><em>Segfault</em></strong>. DuckDB concurrency model isn’t <a href="https://duckdb.org/docs/stable/connect/concurrency.html#handling-concurrency">designed</a> that way. From within a process only a single database (in-memory or file) can be opened, then other database files can be <a href="https://duckdb.org/docs/stable/sql/statements/attach.html">attached</a> to the central db’s catalog.</p>

<p>Having already decided to rotate DB files, I decided to make a separate program (<a href="https://github.com/loicalleyne/quacfka-runner">Runner</a>) to process the database files as they were rotated, running aggregations on normalized data and table dumps to parquet. This meant setting up an RPC connection between the two and figuring out a backpressure mechanism to avoid <code class="language-plaintext highlighter-rouge">disk full</code> events.</p>

<p>However having the two running simultaneously was causing memory pressure issues, not to mention massively slowing down the throughput. Upgrading the VM to one with more vCPUs and memory only helped a little, there was clearly some resource contention going on.</p>

<p>Since Go 1.5, the default <code class="language-plaintext highlighter-rouge">GOMAXPROCS</code> value is the number of CPU cores available. What if this was reduced to “sandbox” the ingestion process, along with setting the DuckDB thread count in the Runner? This actually worked so well, it increased the overall throughput. <a href="https://github.com/loicalleyne/quacfka-runner">Runner</a> runs the <code class="language-plaintext highlighter-rouge">COPY...TO...parquet</code> queries, walks the parquet output folder, uploads files to object storage and deletes the uploaded files. Balancing the DuckDB file rotation size threshold in <a href="https://github.com/loicalleyne/quacfka-service">Quafka-Service</a> allows Runner to keep up and avoid a backlog of DB files on disk.</p>

<h1 id="results">Results</h1>

<figure style="text-align: center;">
  <img src="/img/adbc-duckdb/btop.png" width="100%" class="img-responsive" alt="Figure 1: btop utility showing CPU and memory usage of quacfka-service and runner" />
  <figcaption>Figure 2: btop utility showing CPU and memory usage of quacfka-service and runner.</figcaption>
</figure>
<p>Note: both runs with <code class="language-plaintext highlighter-rouge">GOMAXPROCS</code> set to 24 (the number of DuckDB insertion routines)</p>

<p>Ingesting the raw data (14 fields with one deeply nested LIST.STRUCT.LIST field) + normalized data:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight">
<code>
num_cpu:                <span class="a-header">60</span>  
runtime_os:             <span class="a-header">linux</span>  
kafka_clients:          <span class="a-header">5</span>  
kafka_queue_cap:        <span class="a-header">983040</span>  
processor_routines:     <span class="a-header">32</span>  
arrow_queue_cap:        <span class="a-header">4</span>  
duckdb_threshold_mb:    <span class="a-header">4200</span>  
duckdb_connections:     <span class="a-header">24</span>  
normalizer_fields:      <span class="a-header">10</span>  
start_time:             <span class="a-header">2025-02-24T21:06:23Z</span>  
end_time:               <span class="a-header">2025-02-24T21:11:23Z</span>  
records:                <span class="a-header">123_686_901.00</span>  
norm_records:           <span class="a-header">122_212_452.00</span>  
data_transferred:       <span class="a-header">146.53 GB</span>  
duration:               <span class="a-header">4m59.585s</span>  
records_per_second:     <span class="a-padding">398_271.90</span>  
total_rows_per_second:  <span class="a-padding">806_210.41</span>  
transfer_rate:          <span class="a-header">500.86 MB/second</span>  
duckdb_files:           <span class="a-header">9</span>  
duckdb_files_MB:        <span class="a-header">38429</span>
file_avg_duration:      <span class="a-header">33.579s</span></code></pre></div></div>

<p>How many rows/second could we get if we only inserted the flat, normalized data? (Note: original records are still processed, just not inserted):</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight">
<code>
num_cpu:                <span class="a-header">60</span>
runtime_os:             <span class="a-header">linux</span>
kafka_clients:          <span class="a-header">10</span>
kafka_queue_cap:        <span class="a-header">1228800</span>  
processor_routines:     <span class="a-header">32</span>  
arrow_queue_cap:        <span class="a-header">4</span>  
duckdb_threshold_mb:    <span class="a-header">4200</span>  
duckdb_connections:     <span class="a-header">24</span>  
normalizer_fields:      <span class="a-header">10</span>  
start_time:             <span class="a-header">2025-02-25T19:04:33Z</span>  
end_time:               <span class="a-header">2025-02-25T19:09:36Z</span>  
records:                <span class="a-header">231_852_772.00</span>  
norm_records:           <span class="a-header">363_247_327.00</span>  
data_transferred:       <span class="a-header">285.76 GB</span>  
duration:               <span class="a-header">5m3.059s</span>  
records_per_second:     <span class="a-header">0.00</span>  
total_rows_per_second:  <span class="a-padding">1_198_601.39</span> 
transfer_rate:          <span class="a-header">965.54 MB/second</span> 
duckdb_files:           <span class="a-header">5</span>  
duckdb_files_MB:        <span class="a-header">20056</span>  
file_avg_duration:      <span class="a-header">58.975s</span></code></pre></div></div>

<p><img src="/img/adbc-duckdb/onemillionrows.png" width="100%" class="img-responsive" alt="One million rows/second" aria-hidden="true" /></p>

<p>Once deployed, the number of parquet files fell from ~3000 small files per hour to &lt; 20 files per hour. Goodbye small files!</p>

<p><img src="/img/adbc-duckdb/kip_yes.gif" width="25%" class="img-responsive" alt="Yesss" aria-hidden="true" /></p>

<h1 id="challengeslearnings">Challenges/Learnings</h1>

<ul>
  <li>DuckDB insertions are the bottleneck; network speed, Protobuf deserialization, <strong>building Arrow Records are not</strong>.</li>
  <li>For fastest insertion into DuckDB, Arrow Record Batches should contain at least 122880 rows (to align with DuckDB storage row group size).</li>
  <li>DuckDB won’t let you open more than one database at once within the same process (results in a segfault). DuckDB is designed to run only once in a process, with a central database’s catalog having the ability to add connections to other databases.
    <ul>
      <li>Workarounds:
        <ul>
          <li>Use separate processes for writing and reading multiple database files.</li>
          <li>Open a single DuckDB database and use <a href="https://duckdb.org/docs/stable/sql/statements/attach.html">ATTACH</a> to attach other DB files.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Flat data is much, much faster to insert than nested data.</li>
</ul>

<p><img src="/img/adbc-duckdb/whatdoesitallmean.gif" width="100%" class="img-responsive" alt="Whoopdy doo, what does it all mean Basil?" aria-hidden="true" /></p>

<p>ADBC provides DuckDB with a truly high-throughput data ingestion API, unlocking a slew of use cases for using DuckDB with streaming data, making this an ever more useful tool for data practitioners.</p>]]></content><author><name>loicalleyne</name></author><category term="application" /><summary type="html"><![CDATA[ADBC enables high throughput insertion into DuckDB]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/adbc-duckdb/adbc-duckdb.png" /><media:content medium="image" url="https://arrow.apache.org/img/adbc-duckdb/adbc-duckdb.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Apache Arrow ADBC 17 (Libraries) Release</title><link href="https://arrow.apache.org/blog/2025/03/07/adbc-17-release/" rel="alternate" type="text/html" title="Apache Arrow ADBC 17 (Libraries) Release" /><published>2025-03-07T00:00:00-05:00</published><updated>2025-03-07T00:00:00-05:00</updated><id>https://arrow.apache.org/blog/2025/03/07/adbc-17-release</id><content type="html" xml:base="https://arrow.apache.org/blog/2025/03/07/adbc-17-release/"><![CDATA[<!--

-->

<p>The Apache Arrow team is pleased to announce the version 17 release of
the Apache Arrow ADBC libraries. This release includes <a href="https://github.com/apache/arrow-adbc/milestone/21"><strong>18
resolved issues</strong></a> from <a href="#contributors"><strong>13 distinct contributors</strong></a>.</p>

<p>This is a release of the <strong>libraries</strong>, which are at version 17.  The
<a href="https://arrow.apache.org/adbc/17/format/specification.html"><strong>API specification</strong></a> is versioned separately and is at
version 1.1.0.</p>

<p>The subcomponents are versioned independently:</p>

<ul>
  <li>C/C++/GLib/Go/Python/Ruby: 1.5.0</li>
  <li>C#: 0.17.0</li>
  <li>Java: 0.17.0</li>
  <li>R: 0.17.0</li>
  <li>Rust: 0.17.0</li>
</ul>

<p>The release notes below are not exhaustive and only expose selected
highlights of the release. Many other bugfixes and improvements have
been made: we refer you to the <a href="https://github.com/apache/arrow-adbc/blob/apache-arrow-adbc-17/CHANGELOG.md">complete changelog</a>.</p>

<h2 id="release-highlights">Release Highlights</h2>

<p>The CMake config can now build against system dependencies instead of forcing vendored dependencies (<a href="https://github.com/apache/arrow-adbc/pull/2546">#2546</a>).  Also, CMake files are now installed even for the drivers written in Go (<a href="https://github.com/apache/arrow-adbc/issues/2506">#2506</a>).  Packages for Ubuntu 24.04 LTS are now available (<a href="https://github.com/apache/arrow-adbc/pull/2482">#2482</a>).</p>

<p>The performance of <code class="language-plaintext highlighter-rouge">AdbcDataReader</code> and <code class="language-plaintext highlighter-rouge">ValueAt</code> has been improved in C# (<a href="https://github.com/apache/arrow-adbc/pull/2534">#2534</a>).  The C# BigQuery driver will now use a default project ID if one is not specified (<a href="https://github.com/apache/arrow-adbc/pull/2471">#2471</a>).</p>

<p>The Flight SQL and Snowflake drivers allow passing low-level options in Go (gRPC dial options in <a href="https://github.com/apache/arrow-adbc/pull/2563">#2563</a> and gosnowflake options in <a href="https://github.com/apache/arrow-adbc/pull/2558">#2558</a>).  The Flight SQL driver should now provide column-level metadata (<a href="https://github.com/apache/arrow-adbc/pull/2481">#2481</a>).  The Snowflake driver now no longer requires setting the current schema to get metadata (<a href="https://github.com/apache/arrow-adbc/issues/2517">#2517</a>).</p>

<h2 id="contributors">Contributors</h2>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ git shortlog --perl-regexp --author='^((?!dependabot\[bot\]).*)$' -sn apache-arrow-adbc-16..apache-arrow-adbc-17
    15	David Li
     6	Matthijs Brobbel
     2	Hélder Gregório
     2	Matt Topol
     2	Matthias Kuhn
     2	Sutou Kouhei
     2	davidhcoe
     1	Curt Hagenlocher
     1	Felipe Oliveira Carvalho
     1	Felipe Vianna
     1	Marius van Niekerk
     1	Shuoze Li
     1	amangoyal
</code></pre></div></div>

<h2 id="roadmap">Roadmap</h2>

<p>There is some discussion on a potential second revision of ADBC to include more missing functionality and asynchronous API support.  For more, see the <a href="https://github.com/apache/arrow-adbc/milestone/8">milestone</a>.  We would welcome suggestions on APIs that could be added or extended.</p>

<h2 id="getting-involved">Getting Involved</h2>

<p>We welcome questions and contributions from all interested.  Issues
can be filed on <a href="https://github.com/apache/arrow-adbc/issues">GitHub</a>, and questions can be directed to GitHub
or the <a href="/community/">Arrow mailing lists</a>.</p>]]></content><author><name>pmc</name></author><category term="release" /><summary type="html"><![CDATA[The Apache Arrow team is pleased to announce the version 17 release of the Apache Arrow ADBC libraries. This release includes 18 resolved issues from 13 distinct contributors. This is a release of the libraries, which are at version 17. The API specification is versioned separately and is at version 1.1.0. The subcomponents are versioned independently: C/C++/GLib/Go/Python/Ruby: 1.5.0 C#: 0.17.0 Java: 0.17.0 R: 0.17.0 Rust: 0.17.0 The release notes below are not exhaustive and only expose selected highlights of the release. Many other bugfixes and improvements have been made: we refer you to the complete changelog. Release Highlights The CMake config can now build against system dependencies instead of forcing vendored dependencies (#2546). Also, CMake files are now installed even for the drivers written in Go (#2506). Packages for Ubuntu 24.04 LTS are now available (#2482). The performance of AdbcDataReader and ValueAt has been improved in C# (#2534). The C# BigQuery driver will now use a default project ID if one is not specified (#2471). The Flight SQL and Snowflake drivers allow passing low-level options in Go (gRPC dial options in #2563 and gosnowflake options in #2558). The Flight SQL driver should now provide column-level metadata (#2481). The Snowflake driver now no longer requires setting the current schema to get metadata (#2517). Contributors $ git shortlog --perl-regexp --author='^((?!dependabot\[bot\]).*)$' -sn apache-arrow-adbc-16..apache-arrow-adbc-17 15 David Li 6 Matthijs Brobbel 2 Hélder Gregório 2 Matt Topol 2 Matthias Kuhn 2 Sutou Kouhei 2 davidhcoe 1 Curt Hagenlocher 1 Felipe Oliveira Carvalho 1 Felipe Vianna 1 Marius van Niekerk 1 Shuoze Li 1 amangoyal Roadmap There is some discussion on a potential second revision of ADBC to include more missing functionality and asynchronous API support. For more, see the milestone. We would welcome suggestions on APIs that could be added or extended. Getting Involved We welcome questions and contributions from all interested. Issues can be filed on GitHub, and questions can be directed to GitHub or the Arrow mailing lists.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Data Wants to Be Free: Fast Data Exchange with Apache Arrow</title><link href="https://arrow.apache.org/blog/2025/02/28/data-wants-to-be-free/" rel="alternate" type="text/html" title="Data Wants to Be Free: Fast Data Exchange with Apache Arrow" /><published>2025-02-28T00:00:00-05:00</published><updated>2025-02-28T00:00:00-05:00</updated><id>https://arrow.apache.org/blog/2025/02/28/data-wants-to-be-free</id><content type="html" xml:base="https://arrow.apache.org/blog/2025/02/28/data-wants-to-be-free/"><![CDATA[<!--

-->

<style>
.a-header {
  color: #984EA3;
  font-weight: bold;
}
.a-data {
  color: #377EB8;
  font-weight: bold;
}
.a-length {
  color: #FF7F00;
  font-weight: bold;
}
.a-padding {
  color: #E41A1C;
  font-weight: bold;
}
</style>

<p><em>This is the second in a series of posts that aims to demystify the use of
Arrow as a data interchange format for databases and query engines.</em></p>

<!--

-->

<p>Posts in this series:</p>

<ol>
  <li><a href="/blog/2025/01/10/arrow-result-transfer/">How the Apache Arrow Format Accelerates Query Result Transfer</a></li>
  <li><a href="/blog/2025/02/28/data-wants-to-be-free/">Data Wants to Be Free: Fast Data Exchange with Apache Arrow</a></li>
</ol>

<p>As data practitioners, we often find our data “held hostage”. Instead of being
able to use data as soon as we get it, we have to spend time—time to parse and
clean up inefficient and messy CSV files, time to wait for an outdated query
engine to struggle with a few gigabytes of data, and time to wait for the data
to make it across a socket. It’s that last point we’ll focus on today. In an
age of multi-gigabit networks, why is it even a problem in the first place?
And make no mistake, it is a problem—research by Mark Raasveldt and Hannes
Mühleisen in their <a href="https://doi.org/10.14778/3115404.3115408">2017 paper</a><sup id="fnref:freepdf"><a href="#fn:freepdf" class="footnote" rel="footnote" role="doc-noteref">1</a></sup>
found that some systems take over <strong>ten minutes</strong> to transfer a dataset that
should only take ten <em>seconds</em><sup id="fnref:ten"><a href="#fn:ten" class="footnote" rel="footnote" role="doc-noteref">2</a></sup>.</p>

<p>Why are we waiting 60 times as long as we need to? <a href="https://arrow.apache.org/blog/2025/01/10/arrow-result-transfer/">As we’ve argued before,
serialization overheads plague our
tools</a>—and
Arrow can help us here. So let’s make that more concrete: we’ll compare how
PostgreSQL and Arrow encode the same data to illustrate the impact of the data
serialization format. Then we’ll tour various ways to build protocols with
Arrow, like Arrow HTTP and Arrow Flight, and how you might use each of them.</p>

<h2 id="postgresql-vs-arrow-data-serialization">PostgreSQL vs Arrow: Data Serialization</h2>

<p>Let’s compare the <a href="https://www.postgresql.org/docs/current/sql-copy.html#id-1.9.3.55.9.4">PostgreSQL binary
format</a>
and <a href="https://arrow.apache.org/docs/format/Columnar.html#serialization-and-interprocess-communication-ipc">Arrow
IPC</a>
on the same dataset, and show how Arrow (with all the benefit of hindsight)
makes better trade-offs than its predecessors.</p>

<p>When you execute a query with PostgreSQL, the client/driver uses the
PostgreSQL wire protocol to send the query and get back the result.  Inside
that protocol, the result set is encoded with the PostgreSQL binary format<sup id="fnref:textbinary"><a href="#fn:textbinary" class="footnote" rel="footnote" role="doc-noteref">3</a></sup>.</p>

<p>First, we’ll create a table and fill it with data:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>postgres=# CREATE TABLE demo (id BIGINT, val TEXT, val2 BIGINT);
CREATE TABLE
postgres=# INSERT INTO demo VALUES (1, 'foo', 64), (2, 'a longer string', 128), (3, 'yet another string', 10);
INSERT 0 3
</code></pre></div></div>

<p>We can then use the COPY command to dump the raw binary data from PostgreSQL into a file:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>postgres=# COPY demo TO '/tmp/demo.bin' WITH BINARY;
COPY 3
</code></pre></div></div>

<p>Then we can annotate the actual bytes of the data based on the <a href="https://www.postgresql.org/docs/current/sql-copy.html#id-1.9.3.55.9.4">documentation</a>:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>00000000: <span class="a-header">50 47 43 4f 50 59 0a ff  PGCOPY..</span>  <span class="a-header">COPY signature, flags,</span>
00000008: <span class="a-header">0d 0a 00 00 00 00 00 00  ........</span>  <span class="a-header">and extension</span>
00000010: <span class="a-header">00 00 00</span> <span class="a-padding">00 03</span> <span class="a-length">00 00 00</span>  <span class="a-header">...</span><span class="a-padding">..</span><span class="a-length">...</span>  <span class="a-padding">Values in row</span>
00000018: <span class="a-length">08</span> <span class="a-data">00 00 00 00 00 00 00</span>  <span class="a-length">.</span><span class="a-data">.......</span>  <span class="a-length">Length of value</span>
00000020: <span class="a-data">01</span> <span class="a-length">00 00 00 03</span> <span class="a-data">66 6f 6f</span>  <span class="a-data">.</span><span class="a-length">....</span><span class="a-data">foo</span>  <span class="a-data">Data</span>
00000028: <span class="a-length">00 00 00 08</span> <span class="a-data">00 00 00 00</span>  <span class="a-length">....</span><span class="a-data">....</span>
00000030: <span class="a-data">00 00 00 40</span> <span class="a-padding">00 03</span> <span class="a-length">00 00</span>  <span class="a-data">...@</span><span class="a-padding">..</span><span class="a-length">..</span>
00000038: <span class="a-length">00 08</span> <span class="a-data">00 00 00 00 00 00</span>  <span class="a-length">..</span><span class="a-data">......</span>
00000040: <span class="a-data">00 02</span> <span class="a-length">00 00 00 0f</span> <span class="a-data">61 20</span>  <span class="a-data">..</span><span class="a-length">....</span><span class="a-data">a </span>
00000048: <span class="a-data">6c 6f 6e 67 65 72 20 73  longer s</span>
00000050: <span class="a-data">74 72 69 6e 67</span> <span class="a-length">00 00 00</span>  <span class="a-data">tring</span><span class="a-length">...</span>
00000058: <span class="a-length">08</span> <span class="a-data">00 00 00 00 00 00 00</span>  <span class="a-length">.</span><span class="a-data">.......</span>
00000060: <span class="a-data">80</span> <span class="a-padding">00 03</span> <span class="a-length">00 00 00 08</span> <span class="a-data">00</span>  <span class="a-data">.</span><span class="a-padding">..</span><span class="a-length">....</span><span class="a-data">.</span>
00000068: <span class="a-data">00 00 00 00 00 00 03</span> <span class="a-length">00</span>  <span class="a-data">.......</span><span class="a-length">.</span>
00000070: <span class="a-length">00 00 12</span> <span class="a-data">79 65 74 20 61</span>  <span class="a-length">...</span><span class="a-data">yet a</span>
00000078: <span class="a-data">6e 6f 74 68 65 72 20 73  nother s</span>
00000080: <span class="a-data">74 72 69 6e 67</span> <span class="a-length">00 00 00</span>  <span class="a-data">tring</span><span class="a-length">...</span>
00000088: <span class="a-length">08</span> <span class="a-data">00 00 00 00 00 00 00</span>  <span class="a-length">.</span><span class="a-data">.......</span>
00000090: <span class="a-data">0a</span> <span class="a-padding">ff ff</span>                 <span class="a-data">.</span><span class="a-padding">..</span>       <span class="a-padding">End of stream</span></code></pre></div></div>

<p>Honestly, PostgreSQL’s binary format is quite understandable, and compact at first glance. It’s just a series of length-prefixed fields. But a closer look isn’t so favorable. <strong>PostgreSQL has overheads proportional to the number of rows and columns</strong>:</p>

<ul>
  <li>Every row has a 2 byte prefix for the number of values in the row. <em>But the data is tabular—we already know this info, and it doesn’t change!</em></li>
  <li>Every value of every row has a 4 byte prefix for the length of the following data, or -1 if the value is NULL. <em>But we know the data types, and those don’t change—plus, values of most types have a fixed, known length!</em></li>
  <li>All values are big-endian. <em>But most of our devices are little-endian, so the data has to be converted.</em></li>
</ul>

<p>For example, a single column of int32 values would have 4 bytes of data and 6 bytes of overhead per row—<strong>60% is “wasted!”</strong><sup id="fnref:1"><a href="#fn:1" class="footnote" rel="footnote" role="doc-noteref">4</a></sup> The ratio gets a little better with more columns (but not with more rows); in the limit we approach “only” 50% overhead.  And then each of the values has to be converted (even if endian-swapping is trivial).  To PostgreSQL’s credit, its format is at least cheap and easy to parse—<a href="https://protobuf.dev/programming-guides/encoding/">other formats</a> get fancy with tricks like “varint” encoding which are quite expensive.</p>

<p>How does Arrow compare? We can use <a href="https://arrow.apache.org/adbc/current/driver/postgresql.html">ADBC</a> to pull the PostgreSQL table into an Arrow table, then annotate it like before:</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">&gt;</span><span class="o">&gt;&gt;</span> import adbc_driver_postgresql.dbapi
<span class="gp">&gt;</span><span class="o">&gt;&gt;</span> import pyarrow.ipc
<span class="gp">&gt;</span><span class="o">&gt;&gt;</span> conn <span class="o">=</span> adbc_driver_postgresql.dbapi.connect<span class="o">(</span><span class="s2">"..."</span><span class="o">)</span>
<span class="gp">&gt;</span><span class="o">&gt;&gt;</span> cur <span class="o">=</span> conn.cursor<span class="o">()</span>
<span class="gp">&gt;</span><span class="o">&gt;&gt;</span> cur.execute<span class="o">(</span><span class="s2">"SELECT * FROM demo"</span><span class="o">)</span>
<span class="gp">&gt;</span><span class="o">&gt;&gt;</span> data <span class="o">=</span> cur.fetchallarrow<span class="o">()</span>
<span class="gp">&gt;</span><span class="o">&gt;&gt;</span> writer <span class="o">=</span> pyarrow.ipc.new_stream<span class="o">(</span><span class="s2">"demo.arrows"</span>, data.schema<span class="o">)</span>
<span class="gp">&gt;</span><span class="o">&gt;&gt;</span> writer.write_table<span class="o">(</span>data<span class="o">)</span>
<span class="gp">&gt;</span><span class="o">&gt;&gt;</span> writer.close<span class="o">()</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>00000000: <span class="a-length">ff ff ff ff d8 00 00 00  ........  IPC message length</span>
00000008: <span class="a-header">10 00 00 00 00 00 0a 00  ........  IPC schema</span>
&vellip;         <span class="a-header">(208 bytes)</span>
000000e0: <span class="a-length">ff ff ff ff f8 00 00 00  ........  IPC message length</span>
000000e8: <span class="a-header">14 00 00 00 00 00 00 00  ........  IPC record batch</span>
&vellip;         <span class="a-header">(240 bytes)</span>
000001e0: <span class="a-data">01 00 00 00 00 00 00 00  ........  Data for column #1</span>
000001e8: <span class="a-data">02 00 00 00 00 00 00 00  ........</span>
000001f0: <span class="a-data">03 00 00 00 00 00 00 00  ........</span>
000001f8: <span class="a-length">00 00 00 00 03 00 00 00  ........  String offsets</span>
00000200: <span class="a-length">12 00 00 00 24 00 00 00  ....$...</span>
00000208: <span class="a-data">66 6f 6f 61 20 6c 6f 6e  fooa lon  Data for column #2</span>
00000210: <span class="a-data">67 65 72 20 73 74 72 69  ger stri</span>
00000218: <span class="a-data">6e 67 79 65 74 20 61 6e  ngyet an</span>
00000220: <span class="a-data">6f 74 68 65 72 20 73 74  other st</span>
00000228: <span class="a-data">72 69 6e 67</span> <span class="a-padding">00 00 00 00</span>  <span class="a-data">ring</span><span class="a-padding">....  Alignment padding</span>
00000230: <span class="a-data">40 00 00 00 00 00 00 00  @.......  Data for column #3</span>
00000238: <span class="a-data">80 00 00 00 00 00 00 00  ........</span>
00000240: <span class="a-data">0a 00 00 00 00 00 00 00  ........</span>
00000248: <span class="a-length">ff ff ff ff 00 00 00 00  ........  IPC end-of-stream</span></code></pre></div></div>

<p>Arrow looks quite…intimidating…at first glance. There’s a giant header that don’t seem related to our dataset at all, plus mysterious padding that seems to exist solely to take up space. But the important thing is that <strong>the overhead is fixed</strong>. Whether you’re transferring one row or a billion, the overhead doesn’t change. And unlike PostgreSQL, <strong>no per-value parsing is required</strong>.</p>

<p>Instead of putting lengths of values everywhere, Arrow groups values of the same column (and hence same type) together, so it just needs the length of the buffer<sup id="fnref:header"><a href="#fn:header" class="footnote" rel="footnote" role="doc-noteref">5</a></sup>.  Overhead isn’t added where it isn’t otherwise needed.  Strings still have a length per value.  Nullability is instead stored in a bitmap, which is omitted if there aren’t any NULL values (as it is here). Because of that, more rows of data doesn’t increase the overhead; instead, the more data you have, the less you pay.</p>

<p>Even the header isn’t actually the disadvantage it looks like. The header contains the schema, which makes the data stream self-describing. With PostgreSQL, you need to get the schema from somewhere else. So we aren’t making an apples-to-apples comparison in the first place: PostgreSQL still has to transfer the schema, it’s just not part of the “binary format” that we’re looking at here<sup id="fnref:binaryheader"><a href="#fn:binaryheader" class="footnote" rel="footnote" role="doc-noteref">6</a></sup>.</p>

<p>There’s actually another problem with PostgreSQL: alignment. The 2 byte field count at the start of every row means all the 8 byte integers after it are unaligned. And that requires extra effort to handle properly (e.g. explicit unaligned load idioms), lest you suffer <a href="https://port70.net/~nsz/c/c11/n1570.html#6.3.2.3p7">undefined behavior</a>, a performance penalty, or even a runtime error. Arrow, on the other hand, strategically adds some padding to keep data aligned, and lets you use little-endian or big-endian byte order depending on your data. And Arrow doesn’t apply expensive encodings to the data that require further parsing. So generally, <strong>you can use Arrow data as-is without having to parse every value</strong>.</p>

<p>That’s the benefit of Arrow being a standardized data format. By using Arrow for serialization, data coming off the wire is already in Arrow format, and can furthermore be directly passed on to <a href="https://duckdb.org">DuckDB</a>, <a href="https://pandas.pydata.org">pandas</a>, <a href="https://pola.rs">polars</a>, <a href="https://docs.rapids.ai/api/cudf/stable/">cuDF</a>, <a href="https://datafusion.apache.org">DataFusion</a>, or any number of systems. Meanwhile, even if the PostgreSQL format addressed these problems—adding padding to align fields, using little-endian or making endianness configurable, trimming the overhead—you’d still end up having to convert the data to another format (probably Arrow) to use downstream.</p>

<p>Even if you really did want to use the PostgreSQL binary format everywhere<sup id="fnref:3"><a href="#fn:3" class="footnote" rel="footnote" role="doc-noteref">7</a></sup>, the documentation rather unfortunately points you to the C source code as the documentation. Arrow, on the other hand, has a <a href="https://github.com/apache/arrow/tree/main/format">specification</a>, <a href="https://arrow.apache.org/docs/format/Columnar.html">documentation</a>, and multiple <a href="https://arrow.apache.org/docs/#implementations">implementations</a> (including third-party ones) across a dozen languages for you to pick up and use in your own applications.</p>

<p>Now, we don’t mean to pick on PostgreSQL here. Obviously, PostgreSQL is a full-featured database with a storied history, a different set of goals and constraints than Arrow, and many happy users. Arrow isn’t trying to compete in that space. But their domains do intersect. PostgreSQL’s wire protocol has <a href="https://datastation.multiprocess.io/blog/2022-02-08-the-world-of-postgresql-wire-compatibility.html">become a de facto standard</a>, with even brand new products like Google’s AlloyDB using it, and so its design affects many projects<sup id="fnref:4"><a href="#fn:4" class="footnote" rel="footnote" role="doc-noteref">8</a></sup>. In fact, AlloyDB is a great example of a shiny new columnar query engine being locked behind a row-oriented client protocol from the 90s. So <a href="https://en.wikipedia.org/wiki/Amdahl's_law">Amdahl’s law</a> rears its head again—optimizing the “front” and “back” of your data pipeline doesn’t matter when the middle is what’s holding you back.</p>

<h2 id="a-quiver-of-arrow-projects">A quiver of Arrow (projects)</h2>

<p>So if Arrow is so great, how can we actually use it to build our own protocols? Luckily, Arrow comes with a variety of building blocks for different situations.</p>

<ul>
  <li>We just talked about <a href="https://arrow.apache.org/docs/format/Columnar.html#serialization-and-interprocess-communication-ipc"><strong>Arrow IPC</strong></a> before. Where Arrow is the in-memory format defining how arrays of data are laid out, er, in memory, Arrow IPC defines how to serialize and deserialize Arrow data so it can be sent somewhere else—whether that means being written to a file, to a socket, into a shared buffer, or otherwise. Arrow IPC organizes data as a sequence of messages, making it easy to stream over your favorite transport, like WebSockets.</li>
  <li><a href="https://github.com/apache/arrow-experiments/tree/main/http"><strong>Arrow HTTP</strong></a> is “just” streaming Arrow IPC over HTTP. The Arrow community is working on standardizing it, so that different clients agree on how exactly to do this. There’s examples of clients and servers across several languages, how to use HTTP Range requests, using multipart/mixed requests to send combined JSON and Arrow responses, and more. While not a full protocol in and of itself, it’ll fit right in when building REST APIs.</li>
  <li><a href="https://arrow.apache.org/docs/format/DissociatedIPC.html"><strong>Disassociated IPC</strong></a> combines Arrow IPC with advanced network transports like <a href="https://openucx.org/">UCX</a> and <a href="https://ofiwg.github.io/libfabric/">libfabric</a>. For those who require the absolute best performance and have the specialized hardware to boot, this allows you to send Arrow data at full throttle, taking advantage of scatter-gather, Infiniband, and more.</li>
  <li><a href="https://arrow.apache.org/docs/format/FlightSql.html"><strong>Arrow Flight SQL</strong></a> is a fully defined protocol for accessing relational databases. Think of it as an alternative to the full PostgreSQL wire protocol: it defines how to connect to a database, execute queries, fetch results, view the catalog, and so on. For database developers, Flight SQL provides a fully Arrow-native protocol with clients for several programming languages and drivers for ADBC, JDBC, and ODBC—all of which you don’t have to build yourself.</li>
  <li>And finally, <a href="https://arrow.apache.org/docs/format/ADBC.html"><strong>ADBC</strong></a> actually isn’t a protocol. Instead, it’s an API abstraction layer for working with databases (like JDBC and ODBC—bet you didn’t see that coming), that’s Arrow-native and doesn’t require transposing or converting columnar data back and forth. ADBC gives you a single API to access data from multiple databases, whether they use Flight SQL or something else under the hood, and if a conversion is absolutely necessary, ADBC handles the details so that you don’t have to build out a dozen connectors on your own.</li>
</ul>

<p>So to summarize:</p>

<ul>
  <li>If you’re <em>using</em> a database or other data system, you want <a href="https://arrow.apache.org/adbc/"><strong>ADBC</strong></a>.</li>
  <li>If you’re <em>building</em> a database, you want <a href="https://arrow.apache.org/docs/format/FlightSql.html"><strong>Arrow Flight SQL</strong></a>.</li>
  <li>If you’re working with specialized networking hardware (you’ll know if you are—that stuff doesn’t come cheap), you want the <a href="https://arrow.apache.org/docs/format/DissociatedIPC.html"><strong>Disassociated IPC Protocol</strong></a>.</li>
  <li>If you’re <em>designing</em> a REST-ish API, you want <a href="https://github.com/apache/arrow-experiments/tree/main/http"><strong>Arrow HTTP</strong></a>.</li>
  <li>And otherwise, you can roll-your-own with <a href="https://arrow.apache.org/docs/format/Columnar.html#serialization-and-interprocess-communication-ipc"><strong>Arrow IPC</strong></a>.</li>
</ul>

<p><img src="/assets/data_wants_to_be_free/flowchart.png" alt="A flowchart of the decision points." class="img-responsive" width="100%" /></p>

<h2 id="conclusion">Conclusion</h2>

<p>Existing client protocols can be wasteful. Arrow offers better efficiency and avoids design pitfalls from the past. And Arrow makes it easy to build and consume data APIs with a variety of standards like Arrow IPC, Arrow HTTP, and ADBC. By using Arrow serialization in protocols, everyone benefits from easier, faster, and simpler data access, and we can avoid accidentally holding data captive behind slow and inefficient interfaces.</p>

<hr />

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:freepdf">
      <p>The paper is freely available from <a href="https://www.vldb.org/pvldb/vol10/p1022-muehleisen.pdf">VLDB</a>. <a href="#fnref:freepdf" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:ten">
      <p>Figure 1 in the paper shows Hive and MongoDB taking over 600 seconds vs the baseline of 10 seconds for netcat to transfer the CSV file.  Of course, that means the comparison is not entirely fair since the CSV file is not being parsed, but it gives an idea of the magnitudes involved. <a href="#fnref:ten" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:textbinary">
      <p>There is a text format too, and that is often the default used by many clients.  We won’t discuss it here. <a href="#fnref:textbinary" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:1">
      <p>Of course, it’s not fully wasted, as null/not-null is data as well. But for accounting purposes, we’ll be consistent and call lengths, padding, bitmaps, etc. “overhead”. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:header">
      <p>That’s what’s being stored in that ginormous header (among other things)—the lengths of all the buffers. <a href="#fnref:header" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:binaryheader">
      <p>And conversely, the PGCOPY header is specific to the COPY command we executed to get a bulk response. <a href="#fnref:binaryheader" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:3">
      <p><a href="https://datastation.multiprocess.io/blog/2022-02-08-the-world-of-postgresql-wire-compatibility.html">And people do…</a> <a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:4">
      <p><a href="https://github.com/apache/arrow-adbc/blob/ed18b8b221af23c7b32312411da10f6532eb3488/c/driver/postgresql/copy/reader.h">We have some experience with the PostgreSQL wire protocol, too.</a> <a href="#fnref:4" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name>David Li, Ian Cook, Matt Topol</name></author><category term="application" /><summary type="html"><![CDATA[This is the second in a series of posts that aims to demystify the use of Arrow as a data interchange format for databases and query engines. Posts in this series: How the Apache Arrow Format Accelerates Query Result Transfer Data Wants to Be Free: Fast Data Exchange with Apache Arrow As data practitioners, we often find our data “held hostage”. Instead of being able to use data as soon as we get it, we have to spend time—time to parse and clean up inefficient and messy CSV files, time to wait for an outdated query engine to struggle with a few gigabytes of data, and time to wait for the data to make it across a socket. It’s that last point we’ll focus on today. In an age of multi-gigabit networks, why is it even a problem in the first place? And make no mistake, it is a problem—research by Mark Raasveldt and Hannes Mühleisen in their 2017 paper1 found that some systems take over ten minutes to transfer a dataset that should only take ten seconds2. Why are we waiting 60 times as long as we need to? As we’ve argued before, serialization overheads plague our tools—and Arrow can help us here. So let’s make that more concrete: we’ll compare how PostgreSQL and Arrow encode the same data to illustrate the impact of the data serialization format. Then we’ll tour various ways to build protocols with Arrow, like Arrow HTTP and Arrow Flight, and how you might use each of them. PostgreSQL vs Arrow: Data Serialization Let’s compare the PostgreSQL binary format and Arrow IPC on the same dataset, and show how Arrow (with all the benefit of hindsight) makes better trade-offs than its predecessors. When you execute a query with PostgreSQL, the client/driver uses the PostgreSQL wire protocol to send the query and get back the result. Inside that protocol, the result set is encoded with the PostgreSQL binary format3. First, we’ll create a table and fill it with data: postgres=# CREATE TABLE demo (id BIGINT, val TEXT, val2 BIGINT); CREATE TABLE postgres=# INSERT INTO demo VALUES (1, 'foo', 64), (2, 'a longer string', 128), (3, 'yet another string', 10); INSERT 0 3 We can then use the COPY command to dump the raw binary data from PostgreSQL into a file: postgres=# COPY demo TO '/tmp/demo.bin' WITH BINARY; COPY 3 Then we can annotate the actual bytes of the data based on the documentation: 00000000: 50 47 43 4f 50 59 0a ff PGCOPY.. COPY signature, flags, 00000008: 0d 0a 00 00 00 00 00 00 ........ and extension 00000010: 00 00 00 00 03 00 00 00 ........ Values in row 00000018: 08 00 00 00 00 00 00 00 ........ Length of value 00000020: 01 00 00 00 03 66 6f 6f .....foo Data 00000028: 00 00 00 08 00 00 00 00 ........ 00000030: 00 00 00 40 00 03 00 00 ...@.... 00000038: 00 08 00 00 00 00 00 00 ........ 00000040: 00 02 00 00 00 0f 61 20 ......a 00000048: 6c 6f 6e 67 65 72 20 73 longer s 00000050: 74 72 69 6e 67 00 00 00 tring... 00000058: 08 00 00 00 00 00 00 00 ........ 00000060: 80 00 03 00 00 00 08 00 ........ 00000068: 00 00 00 00 00 00 03 00 ........ 00000070: 00 00 12 79 65 74 20 61 ...yet a 00000078: 6e 6f 74 68 65 72 20 73 nother s 00000080: 74 72 69 6e 67 00 00 00 tring... 00000088: 08 00 00 00 00 00 00 00 ........ 00000090: 0a ff ff ... End of stream Honestly, PostgreSQL’s binary format is quite understandable, and compact at first glance. It’s just a series of length-prefixed fields. But a closer look isn’t so favorable. PostgreSQL has overheads proportional to the number of rows and columns: Every row has a 2 byte prefix for the number of values in the row. But the data is tabular—we already know this info, and it doesn’t change! Every value of every row has a 4 byte prefix for the length of the following data, or -1 if the value is NULL. But we know the data types, and those don’t change—plus, values of most types have a fixed, known length! All values are big-endian. But most of our devices are little-endian, so the data has to be converted. For example, a single column of int32 values would have 4 bytes of data and 6 bytes of overhead per row—60% is “wasted!”4 The ratio gets a little better with more columns (but not with more rows); in the limit we approach “only” 50% overhead. And then each of the values has to be converted (even if endian-swapping is trivial). To PostgreSQL’s credit, its format is at least cheap and easy to parse—other formats get fancy with tricks like “varint” encoding which are quite expensive. How does Arrow compare? We can use ADBC to pull the PostgreSQL table into an Arrow table, then annotate it like before: &gt;&gt;&gt; import adbc_driver_postgresql.dbapi &gt;&gt;&gt; import pyarrow.ipc &gt;&gt;&gt; conn = adbc_driver_postgresql.dbapi.connect("...") &gt;&gt;&gt; cur = conn.cursor() &gt;&gt;&gt; cur.execute("SELECT * FROM demo") &gt;&gt;&gt; data = cur.fetchallarrow() &gt;&gt;&gt; writer = pyarrow.ipc.new_stream("demo.arrows", data.schema) &gt;&gt;&gt; writer.write_table(data) &gt;&gt;&gt; writer.close() 00000000: ff ff ff ff d8 00 00 00 ........ IPC message length 00000008: 10 00 00 00 00 00 0a 00 ........ IPC schema &vellip; (208 bytes) 000000e0: ff ff ff ff f8 00 00 00 ........ IPC message length 000000e8: 14 00 00 00 00 00 00 00 ........ IPC record batch &vellip; (240 bytes) 000001e0: 01 00 00 00 00 00 00 00 ........ Data for column #1 000001e8: 02 00 00 00 00 00 00 00 ........ 000001f0: 03 00 00 00 00 00 00 00 ........ 000001f8: 00 00 00 00 03 00 00 00 ........ String offsets 00000200: 12 00 00 00 24 00 00 00 ....$... 00000208: 66 6f 6f 61 20 6c 6f 6e fooa lon Data for column #2 00000210: 67 65 72 20 73 74 72 69 ger stri 00000218: 6e 67 79 65 74 20 61 6e ngyet an 00000220: 6f 74 68 65 72 20 73 74 other st 00000228: 72 69 6e 67 00 00 00 00 ring.... Alignment padding 00000230: 40 00 00 00 00 00 00 00 @....... Data for column #3 00000238: 80 00 00 00 00 00 00 00 ........ 00000240: 0a 00 00 00 00 00 00 00 ........ 00000248: ff ff ff ff 00 00 00 00 ........ IPC end-of-stream Arrow looks quite…intimidating…at first glance. There’s a giant header that don’t seem related to our dataset at all, plus mysterious padding that seems to exist solely to take up space. But the important thing is that the overhead is fixed. Whether you’re transferring one row or a billion, the overhead doesn’t change. And unlike PostgreSQL, no per-value parsing is required. Instead of putting lengths of values everywhere, Arrow groups values of the same column (and hence same type) together, so it just needs the length of the buffer5. Overhead isn’t added where it isn’t otherwise needed. Strings still have a length per value. Nullability is instead stored in a bitmap, which is omitted if there aren’t any NULL values (as it is here). Because of that, more rows of data doesn’t increase the overhead; instead, the more data you have, the less you pay. Even the header isn’t actually the disadvantage it looks like. The header contains the schema, which makes the data stream self-describing. With PostgreSQL, you need to get the schema from somewhere else. So we aren’t making an apples-to-apples comparison in the first place: PostgreSQL still has to transfer the schema, it’s just not part of the “binary format” that we’re looking at here6. There’s actually another problem with PostgreSQL: alignment. The 2 byte field count at the start of every row means all the 8 byte integers after it are unaligned. And that requires extra effort to handle properly (e.g. explicit unaligned load idioms), lest you suffer undefined behavior, a performance penalty, or even a runtime error. Arrow, on the other hand, strategically adds some padding to keep data aligned, and lets you use little-endian or big-endian byte order depending on your data. And Arrow doesn’t apply expensive encodings to the data that require further parsing. So generally, you can use Arrow data as-is without having to parse every value. That’s the benefit of Arrow being a standardized data format. By using Arrow for serialization, data coming off the wire is already in Arrow format, and can furthermore be directly passed on to DuckDB, pandas, polars, cuDF, DataFusion, or any number of systems. Meanwhile, even if the PostgreSQL format addressed these problems—adding padding to align fields, using little-endian or making endianness configurable, trimming the overhead—you’d still end up having to convert the data to another format (probably Arrow) to use downstream. Even if you really did want to use the PostgreSQL binary format everywhere7, the documentation rather unfortunately points you to the C source code as the documentation. Arrow, on the other hand, has a specification, documentation, and multiple implementations (including third-party ones) across a dozen languages for you to pick up and use in your own applications. Now, we don’t mean to pick on PostgreSQL here. Obviously, PostgreSQL is a full-featured database with a storied history, a different set of goals and constraints than Arrow, and many happy users. Arrow isn’t trying to compete in that space. But their domains do intersect. PostgreSQL’s wire protocol has become a de facto standard, with even brand new products like Google’s AlloyDB using it, and so its design affects many projects8. In fact, AlloyDB is a great example of a shiny new columnar query engine being locked behind a row-oriented client protocol from the 90s. So Amdahl’s law rears its head again—optimizing the “front” and “back” of your data pipeline doesn’t matter when the middle is what’s holding you back. A quiver of Arrow (projects) So if Arrow is so great, how can we actually use it to build our own protocols? Luckily, Arrow comes with a variety of building blocks for different situations. We just talked about Arrow IPC before. Where Arrow is the in-memory format defining how arrays of data are laid out, er, in memory, Arrow IPC defines how to serialize and deserialize Arrow data so it can be sent somewhere else—whether that means being written to a file, to a socket, into a shared buffer, or otherwise. Arrow IPC organizes data as a sequence of messages, making it easy to stream over your favorite transport, like WebSockets. Arrow HTTP is “just” streaming Arrow IPC over HTTP. The Arrow community is working on standardizing it, so that different clients agree on how exactly to do this. There’s examples of clients and servers across several languages, how to use HTTP Range requests, using multipart/mixed requests to send combined JSON and Arrow responses, and more. While not a full protocol in and of itself, it’ll fit right in when building REST APIs. Disassociated IPC combines Arrow IPC with advanced network transports like UCX and libfabric. For those who require the absolute best performance and have the specialized hardware to boot, this allows you to send Arrow data at full throttle, taking advantage of scatter-gather, Infiniband, and more. Arrow Flight SQL is a fully defined protocol for accessing relational databases. Think of it as an alternative to the full PostgreSQL wire protocol: it defines how to connect to a database, execute queries, fetch results, view the catalog, and so on. For database developers, Flight SQL provides a fully Arrow-native protocol with clients for several programming languages and drivers for ADBC, JDBC, and ODBC—all of which you don’t have to build yourself. And finally, ADBC actually isn’t a protocol. Instead, it’s an API abstraction layer for working with databases (like JDBC and ODBC—bet you didn’t see that coming), that’s Arrow-native and doesn’t require transposing or converting columnar data back and forth. ADBC gives you a single API to access data from multiple databases, whether they use Flight SQL or something else under the hood, and if a conversion is absolutely necessary, ADBC handles the details so that you don’t have to build out a dozen connectors on your own. So to summarize: If you’re using a database or other data system, you want ADBC. If you’re building a database, you want Arrow Flight SQL. If you’re working with specialized networking hardware (you’ll know if you are—that stuff doesn’t come cheap), you want the Disassociated IPC Protocol. If you’re designing a REST-ish API, you want Arrow HTTP. And otherwise, you can roll-your-own with Arrow IPC. Conclusion Existing client protocols can be wasteful. Arrow offers better efficiency and avoids design pitfalls from the past. And Arrow makes it easy to build and consume data APIs with a variety of standards like Arrow IPC, Arrow HTTP, and ADBC. By using Arrow serialization in protocols, everyone benefits from easier, faster, and simpler data access, and we can avoid accidentally holding data captive behind slow and inefficient interfaces. The paper is freely available from VLDB. &#8617; Figure 1 in the paper shows Hive and MongoDB taking over 600 seconds vs the baseline of 10 seconds for netcat to transfer the CSV file. Of course, that means the comparison is not entirely fair since the CSV file is not being parsed, but it gives an idea of the magnitudes involved. &#8617; There is a text format too, and that is often the default used by many clients. We won’t discuss it here. &#8617; Of course, it’s not fully wasted, as null/not-null is data as well. But for accounting purposes, we’ll be consistent and call lengths, padding, bitmaps, etc. “overhead”. &#8617; That’s what’s being stored in that ginormous header (among other things)—the lengths of all the buffers. &#8617; And conversely, the PGCOPY header is specific to the COPY command we executed to get a bulk response. &#8617; And people do… &#8617; We have some experience with the PostgreSQL wire protocol, too. &#8617;]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-result-transfer/part-1-share-image.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-result-transfer/part-1-share-image.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Apache Arrow Java 18.2.0 Release</title><link href="https://arrow.apache.org/blog/2025/02/19/arrow-java-18.2.0/" rel="alternate" type="text/html" title="Apache Arrow Java 18.2.0 Release" /><published>2025-02-19T00:00:00-05:00</published><updated>2025-02-19T00:00:00-05:00</updated><id>https://arrow.apache.org/blog/2025/02/19/arrow-java-18.2.0</id><content type="html" xml:base="https://arrow.apache.org/blog/2025/02/19/arrow-java-18.2.0/"><![CDATA[<!--

-->

<p>The Apache Arrow team is pleased to announce the <a href="https://github.com/apache/arrow-java/releases/tag/v18.2.0">v18.2.0</a> release of Apache Arrow Java. 
This is the first release since Arrow Java landed in its own <a href="https://github.com/apache/arrow-java">repository</a>.</p>

<h2 id="changelog">Changelog</h2>

<h3 id="whats-changed">What’s Changed</h3>
<ul>
  <li>GH-466: Export namespace from Flight package by @lidavidm in <a href="https://github.com/apache/arrow-java/pull/467">#467</a></li>
  <li>GH-447: Port fix from apache/arrow that was missed by @lidavidm in <a href="https://github.com/apache/arrow-java/pull/475">#475</a></li>
  <li>GH-48: Implement VectorAppender for BaseVariableWidthViewVector by @ViggoC in <a href="https://github.com/apache/arrow-java/pull/454">#454</a></li>
  <li>GH-41: BaseVariableWidthViewVector setZero only if necessary by @ViggoC in <a href="https://github.com/apache/arrow-java/pull/557">#557</a></li>
</ul>

<p><strong>Full Changelog</strong>: <a href="https://github.com/apache/arrow-java/commits/v18.2.0">changelog</a></p>]]></content><author><name>pmc</name></author><category term="release" /><summary type="html"><![CDATA[The Apache Arrow team is pleased to announce the v18.2.0 release of Apache Arrow Java. This is the first release since Arrow Java landed in its own repository. Changelog What’s Changed GH-466: Export namespace from Flight package by @lidavidm in #467 GH-447: Port fix from apache/arrow that was missed by @lidavidm in #475 GH-48: Implement VectorAppender for BaseVariableWidthViewVector by @ViggoC in #454 GH-41: BaseVariableWidthViewVector setZero only if necessary by @ViggoC in #557 Full Changelog: changelog]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Apache Arrow 19.0.1 Release</title><link href="https://arrow.apache.org/blog/2025/02/16/19.0.1-release/" rel="alternate" type="text/html" title="Apache Arrow 19.0.1 Release" /><published>2025-02-16T00:00:00-05:00</published><updated>2025-02-16T00:00:00-05:00</updated><id>https://arrow.apache.org/blog/2025/02/16/19.0.1-release</id><content type="html" xml:base="https://arrow.apache.org/blog/2025/02/16/19.0.1-release/"><![CDATA[<!--

-->

<p>The Apache Arrow team is pleased to announce the 19.0.1 release.</p>

<p>This release primarily addresses a bug in the recent Arrow 19.0.0 release which
prevents Arrow C++ and libraries binding it (e.g., Python, R) from reading
Parquet files created by Arrow Rust v53.0.0 or more recent. See the <a href="/blog/2025/01/16/19.0.0-release/">19.0.0
release blog post</a> for more information.</p>

<p>This release includes <a href="https://github.com/apache/arrow/milestone/68?closed=1"><strong>12 resolved issues</strong></a> from <a href="/release/19.0.1.html#contributors"><strong>9 distinct
contributors</strong></a>. See the <a href="/install">Install Page</a> to learn how to get the libraries
for your platform.</p>

<p>The release notes below are not exhaustive and only expose selected highlights
of the release. Other bugfixes and improvements have been made: we refer you to
the <a href="/release/19.0.1.html#changelog">complete changelog</a>.</p>

<h2 id="c-notes">C++ notes</h2>

<ul>
  <li>A bug has been fixed which caused reading of Parquet files written by Arrow
Rust v53.0.0 or higher to always fail
<a href="https://github.com/apache/arrow/issues/45283">(#45283)</a>.</li>
  <li>A workaround has been added to increase compatibility with newer versions of
the AWS C++ SDK (<a href="https://github.com/apache/arrow/issues/45304">#45304</a>).</li>
  <li>An overflow bug has been fixed which could cause segmentation faults when
joining inputs over a certain size
(<a href="https://github.com/apache/arrow/issues/44513">#44513</a>).</li>
</ul>

<h2 id="python-notes">Python notes</h2>

<ul>
  <li>A bug has been fixed on the integration with Pandas when using
<code class="language-plaintext highlighter-rouge">future.infer_string=True</code> for Pandas 2.2
(<a href="https://github.com/apache/arrow/issues/45296">#45296</a>).</li>
</ul>]]></content><author><name>pmc</name></author><category term="release" /><summary type="html"><![CDATA[The Apache Arrow team is pleased to announce the 19.0.1 release. This release primarily addresses a bug in the recent Arrow 19.0.0 release which prevents Arrow C++ and libraries binding it (e.g., Python, R) from reading Parquet files created by Arrow Rust v53.0.0 or more recent. See the 19.0.0 release blog post for more information. This release includes 12 resolved issues from 9 distinct contributors. See the Install Page to learn how to get the libraries for your platform. The release notes below are not exhaustive and only expose selected highlights of the release. Other bugfixes and improvements have been made: we refer you to the complete changelog. C++ notes A bug has been fixed which caused reading of Parquet files written by Arrow Rust v53.0.0 or higher to always fail (#45283). A workaround has been added to increase compatibility with newer versions of the AWS C++ SDK (#45304). An overflow bug has been fixed which could cause segmentation faults when joining inputs over a certain size (#44513). Python notes A bug has been fixed on the integration with Pandas when using future.infer_string=True for Pandas 2.2 (#45296).]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Apache Arrow ADBC 16 (Libraries) Release</title><link href="https://arrow.apache.org/blog/2025/01/21/adbc-16-release/" rel="alternate" type="text/html" title="Apache Arrow ADBC 16 (Libraries) Release" /><published>2025-01-21T00:00:00-05:00</published><updated>2025-01-21T00:00:00-05:00</updated><id>https://arrow.apache.org/blog/2025/01/21/adbc-16-release</id><content type="html" xml:base="https://arrow.apache.org/blog/2025/01/21/adbc-16-release/"><![CDATA[<!--

-->

<p>The Apache Arrow team is pleased to announce the version 16 release of
the Apache Arrow ADBC libraries. This release includes <a href="https://github.com/apache/arrow-adbc/milestone/20"><strong>15
resolved issues</strong></a> from <a href="#contributors"><strong>11 distinct contributors</strong></a>.</p>

<p>This is a release of the <strong>libraries</strong>, which are at version 16.  The
<a href="https://arrow.apache.org/adbc/16/format/specification.html"><strong>API specification</strong></a> is versioned separately and is at
version 1.1.0.</p>

<p>The subcomponents are versioned independently:</p>

<ul>
  <li>C/C++/GLib/Go/Python/Ruby: 1.4.0</li>
  <li>C#: 0.16.0</li>
  <li>Java: 0.16.0</li>
  <li>R: 0.16.0</li>
  <li>Rust: 0.16.0</li>
</ul>

<p>The release notes below are not exhaustive and only expose selected
highlights of the release. Many other bugfixes and improvements have
been made: we refer you to the <a href="https://github.com/apache/arrow-adbc/blob/apache-arrow-adbc-16/CHANGELOG.md">complete changelog</a>.</p>

<h2 id="release-highlights">Release Highlights</h2>

<p>This release focused mostly on bugfixes.</p>

<p>The C# ADO.NET adapter can now parse connection properties from the connection string (<a href="https://github.com/apache/arrow-adbc/pull/2352">#2352</a>).  The driver for various Thrift-based systems (Hive/Impala/Spark) now supports timeout options (<a href="https://github.com/apache/arrow-adbc/pull/2312">#2312</a>).  A package was added to wrap the Arrow Flight SQL driver (written in Go) from C# (<a href="https://github.com/apache/arrow-adbc/pull/2214">#2214</a>).</p>

<p>The PostgreSQL driver was fixed to properly return unknown types as <code class="language-plaintext highlighter-rouge">arrow.opaque</code> again (<a href="https://github.com/apache/arrow-adbc/pull/2450">#2450</a>) and to avoid issuing an unnecessary <code class="language-plaintext highlighter-rouge">COMMIT</code> which would cause the driver and connection state to get out of sync (<a href="https://github.com/apache/arrow-adbc/pull/2412">#2412</a>).</p>

<p>Python packages only require manylinux2014 again; the baseline glibc requirement was unintentionally raised in the last release and has now been reverted (<a href="https://github.com/apache/arrow-adbc/issues/2350">#2350</a>).</p>

<p>A breaking change was made in the unstable Rust APIs to return a <code class="language-plaintext highlighter-rouge">Result</code> from a fallible function (<a href="https://github.com/apache/arrow-adbc/pull/2334">#2334</a>).  An <code class="language-plaintext highlighter-rouge">adbc_snowflake</code> crate was added to wrap the Snowflake driver (written in Go) into the Rust APIs, though it is not yet being published (<a href="https://github.com/apache/arrow-adbc/pull/2207">#2207</a>).</p>

<h2 id="contributors">Contributors</h2>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ git shortlog --perl-regexp --author='^((?!dependabot\[bot\]).*)$' -sn apache-arrow-adbc-15..apache-arrow-adbc-16
    23	David Li
     8	Matthijs Brobbel
     4	davidhcoe
     3	Bruce Irschick
     2	Matt Topol
     1	Albert LI
     1	Cocoa
     1	Curt Hagenlocher
     1	Jacob Wujciak-Jens
     1	Julian Brandrick
     1	qifanzhang-ms
</code></pre></div></div>

<h2 id="roadmap">Roadmap</h2>

<p>There is some discussion on a potential second revision of ADBC to include more missing functionality and asynchronous API support.  For more, see the <a href="https://github.com/apache/arrow-adbc/milestone/8">milestone</a>; the proposed C Data Interface extensions have been accepted.</p>

<p>We would welcome comments on APIs that could be added or extended, for instance see <a href="https://github.com/apache/arrow-adbc/issues/1704">#1704</a>.</p>

<h2 id="getting-involved">Getting Involved</h2>

<p>We welcome questions and contributions from all interested.  Issues
can be filed on <a href="https://github.com/apache/arrow-adbc/issues">GitHub</a>, and questions can be directed to GitHub
or the <a href="/community/">Arrow mailing lists</a>.</p>]]></content><author><name>pmc</name></author><category term="release" /><summary type="html"><![CDATA[The Apache Arrow team is pleased to announce the version 16 release of the Apache Arrow ADBC libraries. This release includes 15 resolved issues from 11 distinct contributors. This is a release of the libraries, which are at version 16. The API specification is versioned separately and is at version 1.1.0. The subcomponents are versioned independently: C/C++/GLib/Go/Python/Ruby: 1.4.0 C#: 0.16.0 Java: 0.16.0 R: 0.16.0 Rust: 0.16.0 The release notes below are not exhaustive and only expose selected highlights of the release. Many other bugfixes and improvements have been made: we refer you to the complete changelog. Release Highlights This release focused mostly on bugfixes. The C# ADO.NET adapter can now parse connection properties from the connection string (#2352). The driver for various Thrift-based systems (Hive/Impala/Spark) now supports timeout options (#2312). A package was added to wrap the Arrow Flight SQL driver (written in Go) from C# (#2214). The PostgreSQL driver was fixed to properly return unknown types as arrow.opaque again (#2450) and to avoid issuing an unnecessary COMMIT which would cause the driver and connection state to get out of sync (#2412). Python packages only require manylinux2014 again; the baseline glibc requirement was unintentionally raised in the last release and has now been reverted (#2350). A breaking change was made in the unstable Rust APIs to return a Result from a fallible function (#2334). An adbc_snowflake crate was added to wrap the Snowflake driver (written in Go) into the Rust APIs, though it is not yet being published (#2207). Contributors $ git shortlog --perl-regexp --author='^((?!dependabot\[bot\]).*)$' -sn apache-arrow-adbc-15..apache-arrow-adbc-16 23 David Li 8 Matthijs Brobbel 4 davidhcoe 3 Bruce Irschick 2 Matt Topol 1 Albert LI 1 Cocoa 1 Curt Hagenlocher 1 Jacob Wujciak-Jens 1 Julian Brandrick 1 qifanzhang-ms Roadmap There is some discussion on a potential second revision of ADBC to include more missing functionality and asynchronous API support. For more, see the milestone; the proposed C Data Interface extensions have been accepted. We would welcome comments on APIs that could be added or extended, for instance see #1704. Getting Involved We welcome questions and contributions from all interested. Issues can be filed on GitHub, and questions can be directed to GitHub or the Arrow mailing lists.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Apache Arrow 19.0.0 Release</title><link href="https://arrow.apache.org/blog/2025/01/16/19.0.0-release/" rel="alternate" type="text/html" title="Apache Arrow 19.0.0 Release" /><published>2025-01-16T00:00:00-05:00</published><updated>2025-01-16T00:00:00-05:00</updated><id>https://arrow.apache.org/blog/2025/01/16/19.0.0-release</id><content type="html" xml:base="https://arrow.apache.org/blog/2025/01/16/19.0.0-release/"><![CDATA[<!--

-->

<p>The Apache Arrow team is pleased to announce the 19.0.0 release. This release
covers over 2 months of development work and includes <a href="https://github.com/apache/arrow/milestone/66?closed=1"><strong>202 resolved
issues</strong></a> on <a href="/release/19.0.0.html#contributors"><strong>330 distinct commits</strong></a> from <a href="/release/19.0.0.html#contributors"><strong>67 distinct
contributors</strong></a>. See the <a href="https://arrow.apache.org/install/">Install Page</a> to
learn how to get the libraries for your platform.</p>

<p>The release notes below are not exhaustive and only expose selected highlights
of the release. Many other bugfixes and improvements have been made: we refer
you to the <a href="/release/19.0.0.html#changelog">complete changelog</a>.</p>

<h2 id="community">Community</h2>

<p>Since the 18.1.0 release, Adam Reeve and Laurent Goujon have been invited to
become committers. Gang Wu has been invited to join the Project Management
Committee (PMC).</p>

<p>Thanks for your contributions and participation in the project!</p>

<h2 id="release-highlights">Release Highlights</h2>

<p>A <a href="https://github.com/apache/arrow/issues/45283">bug</a> has been identified in the
19.0.0 versions of the C++ and Python libraries which prevents reading Parquet
files written by Arrow Rust v53.0.0 or higher. The files written by Arrow Rust
are correct and the bug was in the patch adding support for Parquet’s
<a href="https://github.com/apache/parquet-format/pull/197">SizeStatistics</a> feature to
Arrow C++ and Python. See <a href="https://github.com/apache/arrow/issues/45283">#45293</a>
for more details including a potential workaround.</p>

<p>As a result, we plan to create a 19.0.1 release to include a fix for this which
should be available in next few weeks.</p>

<h2 id="columnar-format">Columnar Format</h2>

<p>We’ve added a new experimental specification for representing statistics on
Arrow Arrays as Arrow Arrays. This is useful for preserving and exchanging
statistics between systems such as when converting Parquet data to Arrow. See
<a href="https://arrow.apache.org/docs/format/StatisticsSchema.html">the statistics schema
documentation</a> for
details.</p>

<p>We’ve expanded the Arrow C Device Data Interface to include an experimental
Async Device Stream Interface. While the existing Arrow C Device Data Interface
is a pull-oriented API, the Async interface provides a push-oriented design for
other workflows. See the
<a href="https://arrow.apache.org/docs/format/CDeviceDataInterface.html#async-device-stream-interface">documentation</a>
for more information. It currently has implementations in the C++ and Go
libraries.</p>

<h2 id="arrow-flight-rpc-notes">Arrow Flight RPC Notes</h2>

<p>The precision of a Timestamp (used for timeouts) is now nanoseconds on all
platforms; previously it was platform-dependent. This may be a breaking change
depending on your use case.
(<a href="https://github.com/apache/arrow/issues/44679">#44679</a>)</p>

<p>The Python bindings now support various new fields that were added to
FlightEndpoint/FlightInfo (like <code class="language-plaintext highlighter-rouge">expiration_time</code>).
(<a href="https://github.com/apache/arrow/issues/36954">#36954</a>)</p>

<h2 id="c-notes">C++ Notes</h2>

<h3 id="compute">Compute</h3>

<ul>
  <li>It is now possible to cast from a struct type to another struct type with
additional columns, provided the additional columns are nullable
(<a href="https://github.com/apache/arrow/issues/44555">#44555)</a>.</li>
  <li>The compute function <code class="language-plaintext highlighter-rouge">expm1</code> has been added to compute <code class="language-plaintext highlighter-rouge">exp(x) - 1</code> with better
accuracy when the input value is close to 0
(<a href="https://github.com/apache/arrow/issues/44903">#44903</a>).</li>
  <li>Hyperbolic trigonometric functions and their reciprocals have also been added.
(<a href="https://github.com/apache/arrow/issues/44952">#44952</a>).</li>
  <li>The new Decimal32 and Decimal64 types have been further supported by allowing
casting between numeric, string, and other decimal types
(<a href="https://github.com/apache/arrow/issues/43956">#43956</a>).</li>
</ul>

<h3 id="acero">Acero</h3>

<ul>
  <li>Added AVX2 support for decoding row tables in the Swiss join specialization of
hash joins, enabling up to 40% performance improvement for build-heavy
workloads. (<a href="https://github.com/apache/arrow/issues/43693">#43693</a>)</li>
</ul>

<h3 id="filesystems">Filesystems</h3>

<ul>
  <li>The S3 filesystem has gained support for server-side encryption with customer
provided keys, aka SSE-C.
(<a href="https://github.com/apache/arrow/issues/43535">#43535</a>)</li>
  <li>The S3 filesystem also gained an option to disable the SIGPIPE signals that
may be emitted on some network events.
(<a href="https://github.com/apache/arrow/issues/44695">#44695</a>)</li>
  <li>The Azure filesystem now supports SAS token authentication.
(<a href="https://github.com/apache/arrow/issues/44308">#44308</a>).</li>
</ul>

<h3 id="flight-rpc">Flight RPC</h3>

<ul>
  <li>The precision of a Timestamp (used for timeouts) is now nanoseconds on all
platforms; previously it was platform-dependent. This may be a breaking change
depending on your use case.
(<a href="https://github.com/apache/arrow/issues/44679">#44679</a>)</li>
  <li>The Python bindings now support various new fields that were added to
FlightEndpoint/FlightInfo (like <code class="language-plaintext highlighter-rouge">expiration_time</code>).
(<a href="https://github.com/apache/arrow/issues/36954">#36954</a>)</li>
  <li>The UCX backend has been deprecated and is scheduled for removal.
(<a href="https://github.com/apache/arrow/issues/45079">#45079</a>)</li>
</ul>

<h3 id="parquet">Parquet</h3>

<ul>
  <li>The initial footer read size can now be configured to reduce the number of
potential round-trips on hi-latency filesystems such as S3.
(<a href="https://github.com/apache/arrow/issues/45015">#45015</a>)</li>
  <li>The new <code class="language-plaintext highlighter-rouge">SizeStatistics</code> format feature has been implemented, though it is
disabled by default when writing.
(<a href="https://github.com/apache/arrow/issues/40592">#40592</a>)</li>
  <li>We’ve added a new method to the ParquetFileReader class,
<a href="https://arrow.apache.org/docs/cpp/api/formats.html#_CPPv4N7parquet17ParquetFileReader13GetReadRangesERKNSt6vectorIiEERKNSt6vectorIiEE7int64_t7int64_t">GetReadRanges</a>,
which can calculate the byte ranges necessary to read a given set of columns and
row groups. This may be useful to pre-buffer file data via caching mechanisms.
(<a href="https://github.com/apache/arrow/issues/45092">#45092</a>)</li>
  <li>We’ve added <code class="language-plaintext highlighter-rouge">arrow::Result</code>-returning variants for
<code class="language-plaintext highlighter-rouge">parquet::arrow::OpenFile()</code> and
<code class="language-plaintext highlighter-rouge">parquet::arrow::FileReader::GetRecordBatchReader()</code>.
(<a href="https://github.com/apache/arrow/issues/44784">#44784</a>,
<a href="https://github.com/apache/arrow/issues/44808">#44808</a>)</li>
</ul>

<h2 id="c-notes-1">C# Notes</h2>

<ul>
  <li>The <code class="language-plaintext highlighter-rouge">PrimitiveArrayBuilder</code> constructor has been made public to allow writing
custom builders. (<a href="https://github.com/apache/arrow/issues/23995">#23995</a>)</li>
  <li>Improved the performance of looking up schema fields by name.
(<a href="https://github.com/apache/arrow/issues/44575">#44575</a>)</li>
</ul>

<h2 id="java-go-and-rust-notes">Java, Go, and Rust Notes</h2>

<p>The Java, Go, and Rust Go projects have moved to separate repositories outside
the main Arrow <a href="https://github.com/apache/arrow">monorepo</a>.</p>

<ul>
  <li>For notes on the latest release of the <a href="https://github.com/apache/arrow-java">Java
implementation</a>, see the latest <a href="https://github.com/apache/arrow-java/releases">Arrow
Java changelog</a>.</li>
  <li>For notes on the latest release of the <a href="https://github.com/apache/arrow-rs">Rust
implementation</a> see the latest <a href="https://github.com/apache/arrow-rs/blob/main/CHANGELOG.md">Arrow Rust
changelog</a>.</li>
  <li>For notes on the latest release of the <a href="https://github.com/apache/arrow-go">Go
implementation</a>, see the latest <a href="https://github.com/apache/arrow-go/releases">Arrow Go
changelog</a>.</li>
</ul>

<h2 id="linux-packaging-notes">Linux Packaging Notes</h2>

<ul>
  <li>Debian: Fixed keyring format to support newer libapt (e.g., used by
Trixie). (<a href="https://github.com/apache/arrow/issues/45118">#45118</a>)</li>
</ul>

<h2 id="python-notes">Python Notes</h2>

<p>New features:</p>

<ul>
  <li>The upcoming pandas 3.0 <a href="https://pandas.pydata.org/pdeps/0014-string-dtype.html">string
dtype</a> is now
supported by PyArrow’s <code class="language-plaintext highlighter-rouge">to_pandas</code> routine. In the future, when using pandas &gt;=3.0,
the new pandas behavior will be enabled by default. You can opt into
the new behavior under pandas &gt;=2.3 by setting <code class="language-plaintext highlighter-rouge">pd.options.future.infer_string
= True</code>. This may be considered a breaking change.
(<a href="https://github.com/apache/arrow/issues/43683">#43683</a>)</li>
  <li>Support for 32-bit and 64-bit decimal types was added.
(<a href="https://github.com/apache/arrow/issues/44713">#44713</a>)</li>
  <li>Arrow PyCapsule stream objects are supported in <code class="language-plaintext highlighter-rouge">write_dataset</code>.
(<a href="https://github.com/apache/arrow/issues/43410">#43410</a>)</li>
  <li>New Flight features have been exposed.
(<a href="https://github.com/apache/arrow/issues/36954">#36954</a>)</li>
  <li>Bindings for <code class="language-plaintext highlighter-rouge">JsonExtensionType</code> and <code class="language-plaintext highlighter-rouge">JsonArray</code> were added.
(<a href="https://github.com/apache/arrow/issues/44066">#44066</a>)</li>
  <li>Hyperbolic trigonometry functions added to the Arrow C++ compute kernels are
also available in PyArrow.
(<a href="https://github.com/apache/arrow/issues/44952">#44952</a>)</li>
</ul>

<p>Other improvements:</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">strings_to_categorical</code> keyword in <code class="language-plaintext highlighter-rouge">to_pandas</code> can now be used for string
view type. (<a href="https://github.com/apache/arrow/issues/45175">#45175</a>)</li>
  <li><code class="language-plaintext highlighter-rouge">from_buffers</code> is updated to work with <code class="language-plaintext highlighter-rouge">StringView</code>.
(<a href="https://github.com/apache/arrow/issues/44651">#44651</a>)</li>
  <li>Version suffixes are also set for Arrow Python C++ (<code class="language-plaintext highlighter-rouge">libarrow_python*</code>)
libraries. (<a href="https://github.com/apache/arrow/issues/44614">#44614</a>)</li>
</ul>

<h2 id="ruby-and-c-glib-notes">Ruby and C GLib Notes</h2>

<h3 id="ruby">Ruby</h3>

<ul>
  <li>Added basic support for JRuby with an implementation based on Arrow Java
(<a href="https://github.com/apache/arrow/pull/44346">#44346</a>). The plan is to release
this as a gem once it covers a base set of features. See
<a href="https://github.com/apache/arrow/issues/45324">#45324</a> for more information.</li>
  <li>Added support for 32bit and 64bit decimal, binary view, and string view. See
<a href="https://github.com/apache/arrow/issues?q=is%3Aclosed%20milestone%3A19.0.0%20label%3A%22Component%3A%20GLib%22">issues
listed</a>
in the 19.0.0 milestone for more details.</li>
  <li>Fixed a bug that empty struct list can’t be built.
(<a href="https://github.com/apache/arrow/issues/44742">#44742</a>)</li>
  <li>Fixed a bug that <code class="language-plaintext highlighter-rouge">record_batch[:column].size</code> raises an exception.
(<a href="https://github.com/apache/arrow/issues/45119">#45119</a>)</li>
</ul>

<h3 id="c-glib">C GLib</h3>

<ul>
  <li>Added support for 32bit and 64bit decimal, binary view, and string view. See
<a href="https://github.com/apache/arrow/issues?q=is%3Aclosed%20milestone%3A19.0.0%20label%3A%22Component%3A%20GLib%22">issues listed in the 19.0.0
milestone</a>
for more details.</li>
</ul>]]></content><author><name>pmc</name></author><category term="release" /><summary type="html"><![CDATA[The Apache Arrow team is pleased to announce the 19.0.0 release. This release covers over 2 months of development work and includes 202 resolved issues on 330 distinct commits from 67 distinct contributors. See the Install Page to learn how to get the libraries for your platform. The release notes below are not exhaustive and only expose selected highlights of the release. Many other bugfixes and improvements have been made: we refer you to the complete changelog. Community Since the 18.1.0 release, Adam Reeve and Laurent Goujon have been invited to become committers. Gang Wu has been invited to join the Project Management Committee (PMC). Thanks for your contributions and participation in the project! Release Highlights A bug has been identified in the 19.0.0 versions of the C++ and Python libraries which prevents reading Parquet files written by Arrow Rust v53.0.0 or higher. The files written by Arrow Rust are correct and the bug was in the patch adding support for Parquet’s SizeStatistics feature to Arrow C++ and Python. See #45293 for more details including a potential workaround. As a result, we plan to create a 19.0.1 release to include a fix for this which should be available in next few weeks. Columnar Format We’ve added a new experimental specification for representing statistics on Arrow Arrays as Arrow Arrays. This is useful for preserving and exchanging statistics between systems such as when converting Parquet data to Arrow. See the statistics schema documentation for details. We’ve expanded the Arrow C Device Data Interface to include an experimental Async Device Stream Interface. While the existing Arrow C Device Data Interface is a pull-oriented API, the Async interface provides a push-oriented design for other workflows. See the documentation for more information. It currently has implementations in the C++ and Go libraries. Arrow Flight RPC Notes The precision of a Timestamp (used for timeouts) is now nanoseconds on all platforms; previously it was platform-dependent. This may be a breaking change depending on your use case. (#44679) The Python bindings now support various new fields that were added to FlightEndpoint/FlightInfo (like expiration_time). (#36954) C++ Notes Compute It is now possible to cast from a struct type to another struct type with additional columns, provided the additional columns are nullable (#44555). The compute function expm1 has been added to compute exp(x) - 1 with better accuracy when the input value is close to 0 (#44903). Hyperbolic trigonometric functions and their reciprocals have also been added. (#44952). The new Decimal32 and Decimal64 types have been further supported by allowing casting between numeric, string, and other decimal types (#43956). Acero Added AVX2 support for decoding row tables in the Swiss join specialization of hash joins, enabling up to 40% performance improvement for build-heavy workloads. (#43693) Filesystems The S3 filesystem has gained support for server-side encryption with customer provided keys, aka SSE-C. (#43535) The S3 filesystem also gained an option to disable the SIGPIPE signals that may be emitted on some network events. (#44695) The Azure filesystem now supports SAS token authentication. (#44308). Flight RPC The precision of a Timestamp (used for timeouts) is now nanoseconds on all platforms; previously it was platform-dependent. This may be a breaking change depending on your use case. (#44679) The Python bindings now support various new fields that were added to FlightEndpoint/FlightInfo (like expiration_time). (#36954) The UCX backend has been deprecated and is scheduled for removal. (#45079) Parquet The initial footer read size can now be configured to reduce the number of potential round-trips on hi-latency filesystems such as S3. (#45015) The new SizeStatistics format feature has been implemented, though it is disabled by default when writing. (#40592) We’ve added a new method to the ParquetFileReader class, GetReadRanges, which can calculate the byte ranges necessary to read a given set of columns and row groups. This may be useful to pre-buffer file data via caching mechanisms. (#45092) We’ve added arrow::Result-returning variants for parquet::arrow::OpenFile() and parquet::arrow::FileReader::GetRecordBatchReader(). (#44784, #44808) C# Notes The PrimitiveArrayBuilder constructor has been made public to allow writing custom builders. (#23995) Improved the performance of looking up schema fields by name. (#44575) Java, Go, and Rust Notes The Java, Go, and Rust Go projects have moved to separate repositories outside the main Arrow monorepo. For notes on the latest release of the Java implementation, see the latest Arrow Java changelog. For notes on the latest release of the Rust implementation see the latest Arrow Rust changelog. For notes on the latest release of the Go implementation, see the latest Arrow Go changelog. Linux Packaging Notes Debian: Fixed keyring format to support newer libapt (e.g., used by Trixie). (#45118) Python Notes New features: The upcoming pandas 3.0 string dtype is now supported by PyArrow’s to_pandas routine. In the future, when using pandas &gt;=3.0, the new pandas behavior will be enabled by default. You can opt into the new behavior under pandas &gt;=2.3 by setting pd.options.future.infer_string = True. This may be considered a breaking change. (#43683) Support for 32-bit and 64-bit decimal types was added. (#44713) Arrow PyCapsule stream objects are supported in write_dataset. (#43410) New Flight features have been exposed. (#36954) Bindings for JsonExtensionType and JsonArray were added. (#44066) Hyperbolic trigonometry functions added to the Arrow C++ compute kernels are also available in PyArrow. (#44952) Other improvements: strings_to_categorical keyword in to_pandas can now be used for string view type. (#45175) from_buffers is updated to work with StringView. (#44651) Version suffixes are also set for Arrow Python C++ (libarrow_python*) libraries. (#44614) Ruby and C GLib Notes Ruby Added basic support for JRuby with an implementation based on Arrow Java (#44346). The plan is to release this as a gem once it covers a base set of features. See #45324 for more information. Added support for 32bit and 64bit decimal, binary view, and string view. See issues listed in the 19.0.0 milestone for more details. Fixed a bug that empty struct list can’t be built. (#44742) Fixed a bug that record_batch[:column].size raises an exception. (#45119) C GLib Added support for 32bit and 64bit decimal, binary view, and string view. See issues listed in the 19.0.0 milestone for more details.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Apache Arrow Go 18.1.0 Release</title><link href="https://arrow.apache.org/blog/2025/01/13/arrow-go-18.1.0/" rel="alternate" type="text/html" title="Apache Arrow Go 18.1.0 Release" /><published>2025-01-13T00:00:00-05:00</published><updated>2025-01-13T00:00:00-05:00</updated><id>https://arrow.apache.org/blog/2025/01/13/arrow-go-18.1.0</id><content type="html" xml:base="https://arrow.apache.org/blog/2025/01/13/arrow-go-18.1.0/"><![CDATA[<!--

-->

<p>The Apache Arrow team is pleased to announce the <a href="https://github.com/apache/arrow-go/releases/tag/v18.1.0">v18.1.0</a> release of Apache Arrow Go. 
This minor release covers 32 commits from 6 distinct contributors.</p>

<h2 id="contributors">Contributors</h2>
<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">$</span><span class="w"> </span>git shortlog <span class="nt">-sn</span> v18.0.0..v18.1.0
<span class="go">    24	Matt Topol
     2	Sutou Kouhei
     2	Todd Treece
     1	Nick Ripley
     1	Raúl Cumplido
     1	Ruihao Chen
</span></code></pre></div></div>

<h2 id="changelog">Changelog</h2>

<h3 id="whats-changed">What’s Changed</h3>
<ul>
  <li>GH-19: Add Code Of Conduct by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/161">#161</a></li>
  <li>GH-22: Add issue template by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/163">#163</a></li>
  <li>GH-26: Add Codeowners by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/160">#160</a></li>
  <li>GH-41: [Array] Make String and Binary consistent by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/165">#165</a></li>
  <li>fix(arrow/compute): fix scenario where prealloc output is missed by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/167">#167</a></li>
  <li>fix: don’t clobber saved frame pointer in arm64 assembly functions by @nsrip-dd in <a href="https://github.com/apache/arrow-go/pull/170">#170</a></li>
  <li>fix(parquet): Reading UUID columns by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/173">#173</a></li>
  <li>fix(arrow/compute): compare kernels with UUID by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/174">#174</a></li>
  <li>GH-23: Add PR template by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/164">#164</a></li>
  <li>ci: add self-assignable issues by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/176">#176</a></li>
  <li>feat(arrow/compute): Implement kernel for “not” function by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/178">#178</a></li>
  <li>fix(arrow/cdata): handle export struct with no fields by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/175">#175</a></li>
  <li>feat(arrow/compute): make is_nan dispatchable by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/177">#177</a></li>
  <li>GH-20: Add CONTRIBUTING.md by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/162">#162</a></li>
  <li>chore: bump google.golang.org/grpc from 1.67.1 to 1.68.0 by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/190">#190</a></li>
  <li>feat(arrow/cdata): Add Implementation of Async C Data interface by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/169">#169</a></li>
  <li>fix(arrow/memory/internal/cgoalloc): Remove usage of reflect.SliceHeader by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/194">#194</a></li>
  <li>ci: update pkg-config to pkgconf on Homebrew by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/198">#198</a></li>
  <li>ci: use correct GitHub Actions permission “contents” instead of “content” by @raulcd in <a href="https://github.com/apache/arrow-go/pull/200">#200</a></li>
  <li>feat(arrow/ipc): add functions to generate payloads by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/196">#196</a></li>
  <li>fix(internal/json): add arrow_json_stdlib build tag by @toddtreece in <a href="https://github.com/apache/arrow-go/pull/199">#199</a></li>
  <li>feat(parquet/pqarrow): Add ForceLarge option by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/197">#197</a></li>
  <li>fix(parquet/metadata): fix default unsigned int statistics by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/210">#210</a></li>
  <li>chore: bump github.com/goccy/go-json to v0.10.4 by @toddtreece in <a href="https://github.com/apache/arrow-go/pull/218">#218</a></li>
  <li>feat(parquet): Move footerOffset into FileMetaData by @joechenrh in <a href="https://github.com/apache/arrow-go/pull/217">#217</a></li>
  <li>feat(arrow/ipc): implement lazy loading/zero-copy for IPC files by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/216">#216</a></li>
  <li>chore(arrow/compute/expr): upgrade substrait-go by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/234">#234</a></li>
  <li>chore: Update the project URL in apache/arrow-go by @kou in <a href="https://github.com/apache/arrow-go/pull/239">#239</a></li>
  <li>chore: Enable Java integration test by @kou in <a href="https://github.com/apache/arrow-go/pull/240">#240</a></li>
  <li>feat(parquet): Add support for Page Indexes by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/223">#223</a></li>
  <li>chore: bump version by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/241">#241</a></li>
</ul>

<h3 id="new-contributors">New Contributors</h3>
<ul>
  <li>@nsrip-dd made their first contribution in <a href="https://github.com/apache/arrow-go/pull/170">#170</a></li>
  <li>@toddtreece made their first contribution in <a href="https://github.com/apache/arrow-go/pull/199">#199</a></li>
  <li>@joechenrh made their first contribution in <a href="https://github.com/apache/arrow-go/pull/217">#217</a></li>
</ul>

<p><strong>Full Changelog</strong>: <a href="https://github.com/apache/arrow-go/compare/v18.0.0...v18.1.0">changelog</a></p>]]></content><author><name>pmc</name></author><category term="release" /><summary type="html"><![CDATA[The Apache Arrow team is pleased to announce the v18.1.0 release of Apache Arrow Go. This minor release covers 32 commits from 6 distinct contributors. Contributors $ git shortlog -sn v18.0.0..v18.1.0 24 Matt Topol 2 Sutou Kouhei 2 Todd Treece 1 Nick Ripley 1 Raúl Cumplido 1 Ruihao Chen Changelog What’s Changed GH-19: Add Code Of Conduct by @zeroshade in #161 GH-22: Add issue template by @zeroshade in #163 GH-26: Add Codeowners by @zeroshade in #160 GH-41: [Array] Make String and Binary consistent by @zeroshade in #165 fix(arrow/compute): fix scenario where prealloc output is missed by @zeroshade in #167 fix: don’t clobber saved frame pointer in arm64 assembly functions by @nsrip-dd in #170 fix(parquet): Reading UUID columns by @zeroshade in #173 fix(arrow/compute): compare kernels with UUID by @zeroshade in #174 GH-23: Add PR template by @zeroshade in #164 ci: add self-assignable issues by @zeroshade in #176 feat(arrow/compute): Implement kernel for “not” function by @zeroshade in #178 fix(arrow/cdata): handle export struct with no fields by @zeroshade in #175 feat(arrow/compute): make is_nan dispatchable by @zeroshade in #177 GH-20: Add CONTRIBUTING.md by @zeroshade in #162 chore: bump google.golang.org/grpc from 1.67.1 to 1.68.0 by @zeroshade in #190 feat(arrow/cdata): Add Implementation of Async C Data interface by @zeroshade in #169 fix(arrow/memory/internal/cgoalloc): Remove usage of reflect.SliceHeader by @zeroshade in #194 ci: update pkg-config to pkgconf on Homebrew by @zeroshade in #198 ci: use correct GitHub Actions permission “contents” instead of “content” by @raulcd in #200 feat(arrow/ipc): add functions to generate payloads by @zeroshade in #196 fix(internal/json): add arrow_json_stdlib build tag by @toddtreece in #199 feat(parquet/pqarrow): Add ForceLarge option by @zeroshade in #197 fix(parquet/metadata): fix default unsigned int statistics by @zeroshade in #210 chore: bump github.com/goccy/go-json to v0.10.4 by @toddtreece in #218 feat(parquet): Move footerOffset into FileMetaData by @joechenrh in #217 feat(arrow/ipc): implement lazy loading/zero-copy for IPC files by @zeroshade in #216 chore(arrow/compute/expr): upgrade substrait-go by @zeroshade in #234 chore: Update the project URL in apache/arrow-go by @kou in #239 chore: Enable Java integration test by @kou in #240 feat(parquet): Add support for Page Indexes by @zeroshade in #223 chore: bump version by @zeroshade in #241 New Contributors @nsrip-dd made their first contribution in #170 @toddtreece made their first contribution in #199 @joechenrh made their first contribution in #217 Full Changelog: changelog]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Apache Arrowフォーマットはどのようにクエリー結果の転送を高速にしているのか</title><link href="https://arrow.apache.org/blog/2025/01/10/arrow-result-transfer-japanese/" rel="alternate" type="text/html" title="Apache Arrowフォーマットはどのようにクエリー結果の転送を高速にしているのか" /><published>2025-01-10T00:00:00-05:00</published><updated>2025-01-10T00:00:00-05:00</updated><id>https://arrow.apache.org/blog/2025/01/10/arrow-result-transfer-japanese</id><content type="html" xml:base="https://arrow.apache.org/blog/2025/01/10/arrow-result-transfer-japanese/"><![CDATA[<!--

-->

<p><em>この記事はデータベースとクエリーエンジン間のデータ交換フォーマットとしてなぜArrowが使われているのかという謎を解くシリーズの最初の記事です。</em></p>

<p><img src="/img/arrow-result-transfer/part-1-banner.png" width="100%" class="img-responsive" alt="" aria-hidden="true" /></p>

<p>「どうしてこんなに時間がかかるの？」</p>

<p>これはデータを扱っている人がクエリー結果を待っている間によく考える質問です。たくさんの回答が考えられます。もしかしたら、データソースが適切にパーティショニングされていないかもしれません。もしかしたら、SaaSのデータウェアハウスのリソースが足りないのかもしれません。もしかしたら、クエリーオプティマイザーがSQL文を効率的な実行計画に落とし込めなかったのかもしれません。</p>

<p>しかし、驚くほど多くの場合、クエリ結果をクライアントに転送するために非効率なプロトコルを使っていることが答えになります。<a href="https://www.vldb.org/pvldb/vol10/p1022-muehleisen.pdf" target="_blank">2017年の論文</a>では、Mark RaasveldtとHannes Mühleisenは、特にクエリー結果が大きい場合、クエリー結果の転送がクエリー実行時間の大部分を占めることがよくあることを観測しました。しかし、ボトルネックはあなたが予想している場所ではありません。</p>

<p>送信元から受信先へのクエリー結果の転送は次の3つのステップになります。</p>

<ol>
  <li>送信元：結果を元のフォーマットから転送用のフォーマットにシリアライズ</li>
  <li>送信元：転送用のフォーマットでネットワーク経由でデータを送信<sup id="fnref:1"><a href="#fn:1" class="footnote" rel="footnote" role="doc-noteref">1</a></sup></li>
  <li>受信先：転送用のフォーマットのデータを扱いやすいフォーマットにデシリアライズ</li>
</ol>

<p>ネットワークが遅かった時代は、通常はデータ転送ステップ（ステップ2）がボトルネックでした。そのため、シリアライズステップ（ステップ1）とデシリアライズステップ（ステップ3）を高速にする動機はほとんどありませんでした。その代わり、転送するデータを小さくすることに重点がおかれていました。多くの場合は転送時間を短縮するためにデータを圧縮していました。広く使われているデータベース接続API（ODBCとJDBC）とデータベースクライアントプロトコル（MySQLのクライアント・サーバープロトコルやPostgreSQLのフロントエンド・バックエンドプロトコル）はこの時代に設計されました。しかし、ネットワークは高速になり、転送時間が短縮されたため、ボトルネックはシリアライズステップとデシリアライズステップに移ってきました<sup id="fnref:2"><a href="#fn:2" class="footnote" rel="footnote" role="doc-noteref">2</a></sup>。これは大きな結果を生成するクエリーで特に当てはまります。多くのデータエンジニアリングパイプライン・データ分析パイプラインではこのようなクエリーがよく実行されます。</p>

<p>今でも多くのクエリー結果はレガシーなAPI・プロトコルで転送されています。これらのAPI・プロトコルでは非効率な転送用のフォーマットを使わないといけないので、多くのシリアライズ・デシリアライズオーバーヘッドがあります。<a href="https://www.vldb.org/pvldb/vol14/p534-li.pdf" target="_blank">2021年の論文</a>では、Tianyu LiらはODBCとPostgreSQLプロトコルを使う例を示しました。この例では、総クエリー実行時間の99.996%がシリアライズ・デシリアライズに費やされていました。これは極端なケースですが、90%以上費やされている実例をたくさん見てきました。今では、データエンジニアリングクエリー・データ分析クエリーのために、シリアライズ・デシリアライズを高速にする転送用フォーマットを選択する強い動機があります。</p>

<p>そこでArrowですよ！</p>

<p>Apache Arrowオープンソースプロジェクトは<a href="https://arrow.apache.org/docs/format/Columnar.html" target="_blank">データフォーマット</a>を定義しています。このフォーマットはクエリー結果転送時のシリアライズ・デシリアライズを高速にするために設計されています。多くの場合は、シリアライズ・デシリアライズは不要になります。2016年に作られて以来、Arrowフォーマットおよびこのフォーマットを中心に構築された多言語対応ツールボックスは広く使われるようになりました。しかし、Arrowがどのようにシリアライズ・デシリアライズのオーバーヘッドを削減しているかの技術的な詳細はあまり理解されていません。この問題を解決するために、シリアライズ・デシリアライズのオーバーヘッドを削減することを可能にするArrowフォーマットの5つの特徴を説明します。</p>

<h3 id="1-arrowフォーマットは列指向">1. Arrowフォーマットは列指向</h3>

<p>列指向（カラムナー）データフォーマットは各カラムの値をメモリー上の連続した領域に保持します。これは行指向データフォーマットとは対象的です。行指向データフォーマットは各行の値をメモリー上の連続した領域に保持します。</p>

<figure style="text-align: center;">
  <img src="/img/arrow-result-transfer/part-1-figure-1-row-vs-column-layout.png" width="100%" class="img-responsive" alt="図1：5行3列のテーブルの物理メモリーレイアウトは行指向と列指向でどのように違うのか。" />
  <figcaption>図1：5行3列のテーブルの物理メモリーレイアウトは行指向と列指向でどのように違うのか。</figcaption>
</figure>

<p>高性能な分析データベース・データウェアハウス・クエリーエンジン・ストレージシステムは列指向アーキテクチャーを採用することが多いです。これは、よく使われる分析クエリーを高速に実行するためです。最新の列指向クエリーシステムは、Amazon Redshift・Apache DataFusion・ClickHouse・Databricks Photon Engine・DuckDB・Google BigQuery・Microsoft Azure Synapse Analytics・OpenText Analytics Database (Vertica)・Snowflake・Voltron Data Theseusなどです。</p>

<p>同様に、分析用クエリー結果の多くの出力先も列指向アーキテクチャーを採用しています。出力先は、たとえば、BIツール・データアプリケーションプラットフォーム・データフレームライブラリー・機械学習プラットフォームなどです。列指向のBIツールは、Amazon QuickSight・Domo・GoodData・Power BI・Qlik Sense・Spotfire・Tableauなどです。列指向のデータフレームライブラリーは、cuDF・pandas・Polarsなどです。</p>

<p>そのため、クエリー結果の送信元のフォーマットと受信先のフォーマットがどちらも列指向フォーマットであることがますます一般的になっています。列指向の送信元と列指向の受信先でもっとも効率的にデータを転送する方法は列指向の転送用フォーマットを使うことです。これにより、行と列を転置するという時間のかかる処理をせずに済みます。行指向の転送用フォーマットを使うと、データ転送元のシリアライズ処理で列を行に転置し、データ受信先のデシリアライズ処理で行を列に転置しないといけません。</p>

<p>Arrowは列指向のデータフォーマットです。Arrowフォーマットのデータの列指向のレイアウトは、広く使われている送信元システム・受信先システムでのデータのレイアウトと似ています。多くのケースでは似ているのではなく同一のレイアウトになっています。</p>

<h3 id="2-arrowフォーマットは自己記述的で型安全">2. Arrowフォーマットは自己記述的で型安全</h3>

<p>自己記述的なデータフォーマットではスキーマ（カラムの名前と型）とスキーマ以外のメタデータ（データの構造を説明するメタデータ）は、データの中に入っています。自己記述的なフォーマットはデータを安全かつ効率的に処理するために必要なすべての情報を受信したシステムに提供します。一方、自己記述的ではないフォーマットでは、受信したシステムはスキーマや構造を推測する（これは遅くエラーが発生しやすい処理）か、別途スキーマを取得しなければいけません。</p>

<p>いくつかの自己記述的なデータフォーマットの重要な特徴は型安全を強制することです。フォーマットが型安全を強制する場合、データの値は指定された型に準拠していることが保証されるため、受信したシステムはデータを処理するときに型エラーが発生する可能性を排除できます。一方、型安全を強制しないフォーマットでは、受信したシステムはデータ内の各値の有効性をチェックする（これは計算コストが高い処理）か、データを処理するときに型エラーを処理する必要があります。</p>

<p>自己記述的でなく型安全でもないCSVのようなデータを読むとき、データのスキャン・推測・チェックのすべての処理がデシリアライズの大きなオーバーヘッドになります。さらに悪いことに、このようなフォーマットはあいまいさ・デバッグの問題・メンテナンスの課題・セキュリティの脆弱性につながる可能性があります。</p>

<p>Arrowフォーマットは自己記述的で型安全を強制します。さらに、Arrowの型システムは広く使われているデータ送信元・受信先と似ています。多くのケースでは同じかそれらのスーパーセット（それらよりも表現力が高い）になっています。ここでのデータ送信元・データ受信先には、ほぼすべての列指向データシステムと多くの行指向データシステムが含まれています。たとえば、Apache Sparkやリレーショナル・データベースなどが含まれています。Arrowフォーマットを使うと、これらのシステムはネイティブな型と対応するArrowの型の間ですばやく安全にデータの値を変換できます。</p>

<h3 id="3-arrowフォーマットはゼロコピー可能">3. Arrowフォーマットはゼロコピー可能</h3>

<p>ゼロコピー操作とは、中間コピーをまったく作成せずにあるメディアから別のメディアにデータを転送する操作です。データフォーマットがゼロコピー操作をサポートしている場合、メモリー上の構造はディスク上あるいはネットワーク上の構造と同じになります。そのため、たとえば、メモリー上で使える構造のデータをネットワークから直接読み込めます。このとき、中間コピーや変換をする必要はありません。</p>

<p>Arrowフォーマットはゼロコピー操作をサポートしています。データの値の集まりを保持するために、Arrowは<a href="https://arrow.apache.org/docs/format/Columnar.html#serialization-and-interprocess-communication-ipc" target="_blank">レコードバッチ</a>と呼んでいる列指向の表形式のデータ構造を定義しています。Arrowのレコードバッチはメモリー上に保持することもできますし、ネットワーク経由で送信することもできますし、ディスクに保存することもできます。レコードバッチがどのメディアにあってもどのシステムで生成されてもバイナリー構造は変わりません。スキーマと他のメタデータを保存するために、ArrowはFlatBuffersを使っています。FlatBuffersはGoogleが作ったフォーマットです。FlatBuffersも、どのメディア上でも同じバイナリー構造になります。</p>

<p>これらの設計判断により、Arrowは転送用のフォーマットとしてだけでなく、メモリー上のフォーマットとしてもディスク上のフォーマットとしても使えます。これは、JSONやCSVといったテキストベースのフォーマットやProtocol BuffersやThriftといったシリアライズされたバイナリーフォーマットとは対照的です。これらのフォーマットは専用の構文を使ってデータをエンコードします。これらのフォーマットのデータをメモリー上で使える構造にロードするには、データをパースしてデコードする必要があります。これはParquetやORCといったバイナリーフォーマットとも対照的です。これらはディスク上でのデータサイズを削減するためにエンコードしたり圧縮したりします。これらのフォーマットのデータをメモリー上で使える構造にロードするためには、展開してデコードする必要があります<sup id="fnref:3"><a href="#fn:3" class="footnote" rel="footnote" role="doc-noteref">3</a></sup>。</p>

<p>データ送信元のシステムでは、メモリー上あるいはディスク上にArrowフォーマットのデータがあればシリアライズせずにArrowフォーマットでネットワーク越しにデータ転送できるということです。また、データ受信先のシステムでは、デシリアライズせずにネットワークからメモリー上にデータを読み込んだりディスク上にArrowファイルとして書き出したりできるということです。</p>

<p>Arrowフォーマットは非常に効率よく分析操作できるメモリー上のフォーマットとして設計されています。このため、多くの列指向データシステムは内部のメモリー上のフォーマットとしてArrowを採用しています。たとえば、Apache DataFusion・cuDF・Dremio・InfluxDB・Polars・Velox・Voltron Data Theseusなどが採用しています。これらのシステムがデータ送信元あるいはデータ受信先である場合、シリアライズ・デシリアライズのオーバーヘッドは完全になくなります。他の多くの列指向のデータシステムの場合、それらが使っているプロプライエタリなメモリー上のフォーマットはArrowと非常に似ています。それらのシステムでは、Arrowフォーマットとのシリアライズ・デシリアライズ処理は高速で効率的です。</p>

<h3 id="4-arrowフォーマットはストリーム可能">4. Arrowフォーマットはストリーム可能</h3>

<p>ストリーム可能なデータフォーマットはデータセット全体を待たずに1つのチャンクずつ順番に処理できます。データがストリーム可能なフォーマットで転送されているとき、受信先のシステムは最初のチャンクが到着したらすぐに処理を開始できます。これによりいくつかの方法でデータ転送を高速化できます。たとえば、データを処理している間に次のデータを受信できます。たとえば、受信先のシステムはメモリーをより効率的に使うことができます。たとえば、複数のストリームを並列に転送することができます。これにより、データ転送・データのデシリアライズ・データ処理を高速化できます。</p>

<p>たとえば、CSVはストリーム可能なデータフォーマットです。なぜなら、（もし含まれているなら）ファイルの先頭のヘッダーにカラム名があって、ファイル中のそれ以降の行は順番に処理できるからです。一方、ParquetとORCはストリーム可能ではないデータフォーマットです。なぜなら、データを処理するために必要なスキーマと他のメタデータがファイルの最後のフッターにあるからです。処理を始める前にファイル全体をダウンロードする（あるいはファイルの最後まで移動してフッターを別途ダウンロードする）必要があります<sup id="fnref:4"><a href="#fn:4" class="footnote" rel="footnote" role="doc-noteref">4</a></sup>。</p>

<p>Arrowはストリーム可能なデータフォーマットです。データセットは同じスキーマを持つレコードバッチの列としてArrowで表現できます。Arrowは<a href="https://arrow.apache.org/docs/format/Columnar.html#ipc-streaming-format" target="_blank">ストリーミングフォーマット</a>を定義しています。このフォーマットでは、まずスキーマがあり、そのあと、1つ以上のレコードバッチがあります。Arrowストリームを受信するシステムは受信した順にレコードバッチを処理できます。</p>

<figure style="text-align: center;">
  <img src="/img/arrow-result-transfer/part-1-figure-2-arrow-stream.png" width="100%" class="img-responsive" alt="図2：3列の表を転送するArrowストリーム。最初のレコードバッチは最初の3行の値だけを含み、2つめのレコードバッチは次の3行のデータが含まれている。実際のArrowレコードバッチには数千から数百万行が含まれていることもある。" />
  <figcaption>図2：3列の表を転送するArrowストリーム。最初のレコードバッチは最初の3行の値だけを含み、2つめのレコードバッチは次の3行のデータが含まれている。実際のArrowレコードバッチには数千から数百万行が含まれていることもある。</figcaption>
</figure>

<h3 id="5-arrowフォーマットは汎用的">5. Arrowフォーマットは汎用的</h3>

<p>Arrowは表形式のデータをメモリー上で扱うためのデファクトスタンダードなフォーマットとして登場しました。Arrowは言語に依存しないオープンな標準です。様々な言語向けにArrowデータを扱うためのライブラリーが用意されています。たとえば、C・C++・C#・Go・Java・JavaScript・Julia・MATLAB・Python・R・Ruby・Rust・Swift用の公式ライブラリーがあります。主流の言語で開発されているアプリケーションはArrowフォーマットでデータを送受信する機能をサポートできます。JDBCなどのいくつかデータベース接続APIでは特定の言語のランタイムを使わないといけませんが、Arrowフォーマットではそのような必要はありません。</p>

<p>Arrowの汎用性により、実際のデータシステムを高速化する際の基本的な問題に対処できます。その問題とは、性能向上はシステムのボトルネックに律速するということです。この問題は<a href="https://ja.wikipedia.org/wiki/%E3%82%A2%E3%83%A0%E3%83%80%E3%83%BC%E3%83%AB%E3%81%AE%E6%B3%95%E5%89%87" target="_blank">Amdahlの法則</a>として知られています。実際のデータパイプラインでは、クエリー結果が複数のステージを流れることはよくあり、各ステージでシリアライズ・デシリアライズのオーバーヘッドがあります。たとえば、もし、あなたのデータパイプラインに5つのステージがあり、そのうちの4つのステージでシリアライズ・デシリアライズオーバーヘッドを取り除くことができたとしても、あなたのシステムは速くならないでしょう。なぜなら、残った1ステージのシリアライズ・デシリアライズがパイプライン全体のボトルネックになるからです。</p>

<p>Arrowはどんな技術スタック上でも効率的に動くので、この問題の解決に役立ちます。たとえば、こんなデータフローがあったとしたらどうでしょう。NVIDIAのGPUを積んだワーカーを持つScalaベースの分散バックエンド→JettyベースのHTTPサーバー→Pyodideベースのブラウザーフロントエンドを持つNode.jsベースの機械学習フレームワークを使ってユーザーとやりとりするRailsベースの特徴量エンジニアリングアプリ。問題ありません。Arrowライブラリーはこれらのすべてのコンポーネント間のシリアライズ・デシリアライズオーバーヘッドを取り除けます。</p>

<h3 id="まとめ">まとめ</h3>

<p>より多くの商用・オープンソースのツールがArrowに対応するにつれ、シリアライズ・デシリアライズのないあるいは少ない高速なクエリー転送がますます一般的になっています。現在では、多くのデータベース・データプラットフォーム・クエリーエンジンがArrowフォーマットでクエリー結果を転送できます。たとえば、Databricks・Dremio・Google BigQuery・InfluxDB・Snowflake・Voltron Data Theseusといった商用プロダクトや、Apache DataFusion・Apache Doris・Apache Spark・ClickHouse・DuckDBといったオープンソースプロダクトがサポートしています。これにより大幅に高速化しています。</p>

<ul>
  <li>Apache Doris: <a href="https://doris.apache.org/blog/arrow-flight-sql-in-apache-doris-for-10x-faster-data-transfer" target="_blank">「20倍から数百倍」高速化</a></li>
  <li>Google BigQuery: <a href="https://medium.com/google-cloud/announcing-google-cloud-bigquery-version-1-17-0-1fc428512171" target="_blank">最大「31倍高速化」</a></li>
  <li>Dremio: <a href="https://www.dremio.com/press-releases/dremio-announces-support-for-apache-arrow-flight-high-performance-data-transfer/" target="_blank">「10倍以上高速化」</a></li>
  <li>DuckDB: <a href="https://duckdb.org/2023/08/04/adbc.html#benchmark-adbc-vs-odbc" target="_blank">「38倍」高速化</a></li>
  <li>Snowflake: <a href="https://www.snowflake.com/en/blog/fetching-query-results-from-snowflake-just-got-a-lot-faster-with-apache-arrow/" target="_blank">「最大10倍」高速化</a></li>
</ul>

<p>データ受信側では、データを扱っている人はArrowベースのツール・ライブラリー・インターフェイス・プロトコルを使うことでこの高速化を最大化できます。2025年には、より多くのプロジェクトとベンダーが<a href="https://arrow.apache.org/adbc/" target="_blank">ADBC</a>標準をサポートして、Arrowフォーマットでクエリー結果を受け取ることができるツールの数が増大するでしょう。</p>

<p>このシリーズの今後の記事を楽しみにしていてください。Arrowフォーマットと他のデータフォーマットを比較して、Arrowフォーマットで結果を取得するためにクライアントが使うことのできるプロトコルとAPIを説明します。</p>

<hr />

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1">
      <p>転送用のフォーマットは「ワイヤーフォーマット」や「シリアライズフォーマット」と呼ばれることもあります。 <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:2">
      <p>1990年代から現在に至るまで、ネットワーク性能の向上はCPU性能の向上を上回っています。たとえば、1990年代後半は、主流のデスクトップCPUはおおよそ1GFLOPSで処理でき、一般的なWANの接続速度は56Kb/sでした。現在、主流のデスクトップCPUはおおよそ100GFLOPSで処理でき、一般的なWANの接続速度は1Gb/sです。つまり、CPUの性能はおよそ100倍向上していて、ネットワーク速度はおよそ1万倍向上しています。 <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:3">
      <p>これはアーカイブ用のストレージのようなアプリケーションにおいてもParquetやORCよりArrowの方が速いということではありません。このシリーズの今後の記事では、ArrowフォーマットとParquet・ORC・それ以外のフォーマットをより技術的な詳細に踏み込んで比較します。そして、これらが相互に補完しあうものであることを説明します。 <a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:4">
      <p>これはCSVがParquet・ORCよりも高速に結果を転送できるということではありません。CSVとParquet・ORCの転送性能を比べるときは、通常、この特徴よりもここで説明している他の4つの特徴の影響の方が大きいです。 <a href="#fnref:4" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name>Ian Cook, David Li, Matt Topol, Sutou Kouhei [訳]</name></author><category term="application" /><summary type="html"><![CDATA[Arrowはシリアライズ・デシリアライズのオーバーヘッドを削減することでクエリー結果の転送を高速にします。これに関するArrowフォーマットの5つの特徴を説明します。]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-result-transfer/part-1-share-image.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-result-transfer/part-1-share-image.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>