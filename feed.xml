<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://arrow.apache.org/feed.xml" rel="self" type="application/atom+xml" /><link href="https://arrow.apache.org/" rel="alternate" type="text/html" /><updated>2025-11-09T18:20:55-05:00</updated><id>https://arrow.apache.org/feed.xml</id><title type="html">Apache Arrow</title><subtitle>Apache Arrow is the universal columnar format and multi-language toolbox for fast data interchange and in-memory analytics. It specifies a standardized language-independent column-oriented memory format for flat and nested data, organized for efficient analytic operations on modern hardware. It also provides computational libraries and zero-copy streaming messaging and interprocess communication. Languages currently supported include C, C++, C#, Go, Java, JavaScript, MATLAB, Python, R, Ruby, and Rust.</subtitle><entry><title type="html">Apache Arrow ADBC 21 (Libraries) Release</title><link href="https://arrow.apache.org/blog/2025/11/07/adbc-21-release/" rel="alternate" type="text/html" title="Apache Arrow ADBC 21 (Libraries) Release" /><published>2025-11-07T00:00:00-05:00</published><updated>2025-11-07T00:00:00-05:00</updated><id>https://arrow.apache.org/blog/2025/11/07/adbc-21-release</id><content type="html" xml:base="https://arrow.apache.org/blog/2025/11/07/adbc-21-release/"><![CDATA[<!--

-->
<p>The Apache Arrow team is pleased to announce the version 21 release of
the Apache Arrow ADBC libraries. This release includes <a href="https://github.com/apache/arrow-adbc/milestone/25"><strong>30
resolved issues</strong></a> from <a href="#contributors"><strong>23 distinct contributors</strong></a>.</p>
<p>This is a release of the <strong>libraries</strong>, which are at version 21.  The
<a href="https://arrow.apache.org/adbc/21/format/specification.html"><strong>API specification</strong></a> is versioned separately and is at
version 1.1.0.</p>
<p>The subcomponents are versioned independently:</p>
<ul>
<li>C/C++/GLib/Go/Python/Ruby: 1.9.0</li>
<li>C#: 0.21.0</li>
<li>Java: 0.21.0</li>
<li>R: 0.21.0</li>
<li>Rust: 0.21.0</li>
</ul>
<p>The release notes below are not exhaustive and only expose selected
highlights of the release. Many other bugfixes and improvements have
been made: we refer you to the <a href="https://github.com/apache/arrow-adbc/blob/apache-arrow-adbc-21/CHANGELOG.md">complete changelog</a>.</p>
<h2>Release Highlights</h2>
<p>Language bindings:</p>
<ul>
<li>The driver manager reports better errors when it fails to find a driver (<a href="https://github.com/apache/arrow-adbc/pull/3646">#3646</a>).</li>
<li>The Python driver manager now searches for manifests in the proper location when inside a Python virtual environment (<a href="https://github.com/apache/arrow-adbc/pull/3490">#3490</a>).</li>
<li>Added convenience methods to the driver manager on top of the standard DBAPI-2.0/PEP249 APIs (<a href="https://github.com/apache/arrow-adbc/pull/3539">#3539</a>).</li>
<li>The signature for <code>connect</code> has been simplified, so you can connect without having to repeat the driver name or specify explicit keyword arguments depending on the driver (<a href="https://github.com/apache/arrow-adbc/pull/3537">#3537</a>).</li>
<li>Support for Python 3.9 has been dropped (<a href="https://github.com/apache/arrow-adbc/pull/3573">#3573</a>, <a href="https://github.com/apache/arrow-adbc/pull/3663">#3663</a>).</li>
<li>Support for Python 3.14 (including the free-threading variant) has been added (<a href="https://github.com/apache/arrow-adbc/pull/3575">#3575</a>, <a href="https://github.com/apache/arrow-adbc/pull/3620">#3620</a>, <a href="https://github.com/apache/arrow-adbc/pull/3663">#3663</a>).</li>
<li>The R bindings now support <code>replace</code> and <code>create_append</code> ingest modes.</li>
</ul>
<p>Drivers:</p>
<ul>
<li>The Go BigQuery driver now supports service account impersonation (<a href="https://github.com/apache/arrow-adbc/pull/3488">#3488</a>) and setting a quota project (<a href="https://github.com/apache/arrow-adbc/pull/3622">#3622</a>).</li>
<li>The Go BigQuery driver now returns more detailed type metadata in result sets (<a href="https://github.com/apache/arrow-adbc/pull/3604">#3604</a>).</li>
<li>The C# BigQuery driver allows setting a location (<a href="https://github.com/apache/arrow-adbc/pull/3494">#3494</a>).</li>
<li>The C# Databricks driver adjusted default settings to make small queries faster (<a href="https://github.com/apache/arrow-adbc/pull/3489">#3489</a>).</li>
<li>Memory usage of the C# Databricks driver was improved (<a href="https://github.com/apache/arrow-adbc/pull/3652">#3652</a>, <a href="https://github.com/apache/arrow-adbc/pull/3656">#3656</a>).</li>
<li>All C# HiveServer2-based drivers (Hive, Impala, Spark, Databricks) throw Unauthorized exceptions when appropriate (<a href="https://github.com/apache/arrow-adbc/pull/3551">#3551</a>).</li>
<li>JNI bindings to the C++ driver manager are now released, making it possible to use non-Java drivers from a Java application (in a very limited fashion) (<a href="https://github.com/apache/arrow-adbc/pull/3429">#3429</a>).  Binary artifacts are available for amd64/arm64 Linux, arm64 macOS, and amd64 Windows.</li>
<li>The PostgreSQL driver can now return the schema of any bind parameters in a prepared query ([#3579[(https://github.com/apache/arrow-adbc/pull/3579)]]).</li>
<li>The PostgreSQL driver properly batches result sets with large string/binary values now (<a href="https://github.com/apache/arrow-adbc/pull/3616">#3616</a>).</li>
<li>The Snowflake driver now returns float64 for numeric columns when use_high_precision is false and scale is nonzero (<a href="https://github.com/apache/arrow-adbc/pull/3295">#3295</a>).  (Previously it incorrectly truncated to int64.)</li>
<li>Added an option to disable the &quot;vectorized&quot; scanner when ingesting data into Snowflake, which sometimes appeared to cause performance issues (<a href="https://github.com/apache/arrow-adbc/pull/3555">#3555</a>).</li>
</ul>
<p>Packaging:</p>
<ul>
<li>Added AlmaLinux 10 (<a href="https://github.com/apache/arrow-adbc/pull/3514">#3514</a>).</li>
<li>Added Debian Trixie (<a href="https://github.com/apache/arrow-adbc/pull/3513">#3513</a>).</li>
</ul>
<h2>Contributors</h2>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ git shortlog --perl-regexp --author='^((?!dependabot\[bot\]).*)$' -sn apache-arrow-adbc-20..apache-arrow-adbc-21
    28	David Li
     9	Bruce Irschick
     7	eric-wang-1990
     6	Curt Hagenlocher
     5	eitsupi
     4	msrathore-db
     3	Bryce Mecum
     3	Mandukhai Alimaa
     3	Sutou Kouhei
     3	davidhcoe
     2	Anna Lee
     2	Jason Lin
     2	Kevin Liu
     1	Dewey Dunnington
     1	Ian Cook
     1	Jacky Hu
     1	Jade Wang
     1	Kristin Cowalcijk
     1	Lucas Valente
     1	Matthijs Brobbel
     1	bruceNu1l
     1	praveentandra
     1	rnowacoski
</code></pre></div></div>
<h2>Roadmap</h2>
<p>We are starting work on async interfaces and other API enhancements, and
welcome comments or contributions from anyone interested.  See the initial
pull requests:</p>
<ul>
<li><a href="https://github.com/apache/arrow-adbc/pull/3607">https://github.com/apache/arrow-adbc/pull/3607</a></li>
<li><a href="https://github.com/apache/arrow-adbc/pull/3623">https://github.com/apache/arrow-adbc/pull/3623</a></li>
</ul>
<h2>Getting Involved</h2>
<p>We welcome questions and contributions from all interested.  Issues
can be filed on <a href="https://github.com/apache/arrow-adbc/issues">GitHub</a>, and questions can be directed to GitHub
or the <a href="/community/">Arrow mailing lists</a>.</p>]]></content><author><name>pmc</name></author><category term="release" /><summary type="html"><![CDATA[The Apache Arrow team is pleased to announce the version 21 release of the Apache Arrow ADBC libraries. This release includes 30 resolved issues from 23 distinct contributors. This is a release of the libraries, which are at version 21. The API specification is versioned separately and is at version 1.1.0. The subcomponents are versioned independently: C/C++/GLib/Go/Python/Ruby: 1.9.0 C#: 0.21.0 Java: 0.21.0 R: 0.21.0 Rust: 0.21.0 The release notes below are not exhaustive and only expose selected highlights of the release. Many other bugfixes and improvements have been made: we refer you to the complete changelog. Release Highlights Language bindings: The driver manager reports better errors when it fails to find a driver (#3646). The Python driver manager now searches for manifests in the proper location when inside a Python virtual environment (#3490). Added convenience methods to the driver manager on top of the standard DBAPI-2.0/PEP249 APIs (#3539). The signature for connect has been simplified, so you can connect without having to repeat the driver name or specify explicit keyword arguments depending on the driver (#3537). Support for Python 3.9 has been dropped (#3573, #3663). Support for Python 3.14 (including the free-threading variant) has been added (#3575, #3620, #3663). The R bindings now support replace and create_append ingest modes. Drivers: The Go BigQuery driver now supports service account impersonation (#3488) and setting a quota project (#3622). The Go BigQuery driver now returns more detailed type metadata in result sets (#3604). The C# BigQuery driver allows setting a location (#3494). The C# Databricks driver adjusted default settings to make small queries faster (#3489). Memory usage of the C# Databricks driver was improved (#3652, #3656). All C# HiveServer2-based drivers (Hive, Impala, Spark, Databricks) throw Unauthorized exceptions when appropriate (#3551). JNI bindings to the C++ driver manager are now released, making it possible to use non-Java drivers from a Java application (in a very limited fashion) (#3429). Binary artifacts are available for amd64/arm64 Linux, arm64 macOS, and amd64 Windows. The PostgreSQL driver can now return the schema of any bind parameters in a prepared query ([#3579[(https://github.com/apache/arrow-adbc/pull/3579)]]). The PostgreSQL driver properly batches result sets with large string/binary values now (#3616). The Snowflake driver now returns float64 for numeric columns when use_high_precision is false and scale is nonzero (#3295). (Previously it incorrectly truncated to int64.) Added an option to disable the &quot;vectorized&quot; scanner when ingesting data into Snowflake, which sometimes appeared to cause performance issues (#3555). Packaging: Added AlmaLinux 10 (#3514). Added Debian Trixie (#3513). Contributors $ git shortlog --perl-regexp --author='^((?!dependabot\[bot\]).*)$' -sn apache-arrow-adbc-20..apache-arrow-adbc-21 28 David Li 9 Bruce Irschick 7 eric-wang-1990 6 Curt Hagenlocher 5 eitsupi 4 msrathore-db 3 Bryce Mecum 3 Mandukhai Alimaa 3 Sutou Kouhei 3 davidhcoe 2 Anna Lee 2 Jason Lin 2 Kevin Liu 1 Dewey Dunnington 1 Ian Cook 1 Jacky Hu 1 Jade Wang 1 Kristin Cowalcijk 1 Lucas Valente 1 Matthijs Brobbel 1 bruceNu1l 1 praveentandra 1 rnowacoski Roadmap We are starting work on async interfaces and other API enhancements, and welcome comments or contributions from anyone interested. See the initial pull requests: https://github.com/apache/arrow-adbc/pull/3607 https://github.com/apache/arrow-adbc/pull/3623 Getting Involved We welcome questions and contributions from all interested. Issues can be filed on GitHub, and questions can be directed to GitHub or the Arrow mailing lists.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Apache Arrow Rust 57.0.0 Release</title><link href="https://arrow.apache.org/blog/2025/10/30/arrow-rs-57.0.0/" rel="alternate" type="text/html" title="Apache Arrow Rust 57.0.0 Release" /><published>2025-10-30T00:00:00-04:00</published><updated>2025-10-30T00:00:00-04:00</updated><id>https://arrow.apache.org/blog/2025/10/30/arrow-rs-57.0.0</id><content type="html" xml:base="https://arrow.apache.org/blog/2025/10/30/arrow-rs-57.0.0/"><![CDATA[<!--

-->
<p>The Apache Arrow team is pleased to announce that the v57.0.0 release of Apache Arrow
Rust is now available on crates.io (<a href="https://crates.io/crates/arrow">arrow</a> and <a href="https://crates.io/crates/parquet">parquet</a>) and as <a href="https://dist.apache.org/repos/dist/release/arrow/arrow-rs-57.0.0">source download</a>.</p>
<p>See the <a href="https://github.com/apache/arrow-rs/blob/57.0.0/CHANGELOG.md">57.0.0 changelog</a> for a full list of changes.</p>
<h2>New Features</h2>
<p>Note: Arrow Rust hosts the development of the <a href="https://crates.io/crates/parquet">parquet</a> crate, a high
performance Rust implementation of <a href="https://parquet.apache.org/">Apache Parquet</a>.</p>
<h3>Performance: 4x Faster Parquet Metadata Parsing üöÄ</h3>
<p>Ed Seidl (<a href="https://github.com/etseidl">@etseidl</a>) and J√∂rn Horstmann (<a href="https://github.com/jhorstmann">@jhorstmann</a>) contributed a rewritten
thrift metadata parser for Parquet files which is almost 4x faster than the
previous parser based on the <code>thrift</code> crate. This is especially exciting for low
latency use cases and reading Parquet files with large amounts of metadata (e.g.
many row groups or columns).
See the <a href="https://arrow.apache.org/blog/2025/10/23/rust-parquet-metadata/">blog post about the new Parquet metadata parser</a> for more details.</p>
<div style="display: flex; gap: 16px; justify-content: center; align-items: flex-start;">
  <img src="/img/rust-parquet-metadata/results.png" width="100%" class="img-responsive" alt="" aria-hidden="true">
</div>
<p><em>Figure 1:</em> Performance improvements of <a href="https://parquet.apache.org/">Apache Parquet</a> metadata parsing between version <code>56.2.0</code> and <code>57.0.0</code>.</p>
<h3>New <code>arrow-avro</code> Crate</h3>
<p>The <code>57.0.0</code> release introduces a new <a href="https://crates.io/crates/arrow-avro"><code>arrow-avro</code></a> crate contributed by <a href="https://github.com/jecsand838">@jecsand838</a>
and <a href="https://github.com/nathaniel-d-ef">@nathaniel-d-ef</a> that provides much more efficient conversion between
<a href="https://avro.apache.org/">Apache Avro</a> and Arrow <code>RecordBatch</code>es, as well as broader feature support.</p>
<p>Previously, Arrow‚Äëbased systems that read or wrote Avro data
typically used the general‚Äëpurpose <a href="https://crates.io/crates/apache-avro">apache-avro</a> crate. While mature and
feature‚Äëcomplete, its row-oriented API does not support features such as
projection pushdown or vectorized execution. The new <code>arrow-avro</code> crate supports
these features efficiently by converting Avro data directly into Arrow's
columnar format.</p>
<p>See the <a href="https://arrow.apache.org/blog/2025/10/23/introducing-arrow-avro/">blog post about adding arrow-avro</a> for more details.</p>
<div style="display: flex; gap: 16px; justify-content: center; align-items: flex-start; padding: 20px 15px;">
<img src="/img/introducing-arrow-avro/arrow-avro-architecture.svg"
        width="100%"
        alt="High-level `arrow-avro` architecture"
        style="background:#fff">
</div>
<p><em>Figure 2:</em> Architecture of the <code>arrow-avro</code> crate.</p>
<h3>Parquet Variant Support üß¨</h3>
<p>The Apache Parquet project recently added a <a href="https://github.com/apache/parquet-format/blob/master/VariantEncoding.md">new <code>Variant</code> type</a> for
representing semi-structured data. The <code>57.0.0</code> release includes support for reading and
writing both normal and shredded <code>Variant</code> values to and from Parquet files. It
also includes <a href="https://crates.io/crates/parquet-variant">parquet-variant</a>, a complete library for working with <code>Variant</code>
values, <a href="https://docs.rs/parquet/latest/parquet/variant/struct.VariantArray.html"><code>VariantArray</code></a> for working with arrays of <code>Variant</code> values in Apache
Arrow, computation kernels for converting to/from JSON and Arrow types,
extracting paths, and shredding values.</p>
<div class="language-rust highlighter-rouge"><div class="highlight"><pre class="highlight"><code data-lang="rust"> <span class="c1">// Use the VariantArrayBuilder to build a VariantArray</span>
<span class="k">let</span> <span class="k">mut</span> <span class="n">builder</span> <span class="o">=</span> <span class="nn">VariantArrayBuilder</span><span class="p">::</span><span class="nf">new</span><span class="p">(</span><span class="mi">3</span><span class="p">);</span>
<span class="n">builder</span><span class="nf">.new_object</span><span class="p">()</span><span class="nf">.with_field</span><span class="p">(</span><span class="s">"name"</span><span class="p">,</span> <span class="s">"Alice"</span><span class="p">)</span><span class="nf">.finish</span><span class="p">();</span> <span class="c1">// row 1: {"name": "Alice"}</span>
<span class="n">builder</span><span class="nf">.append_value</span><span class="p">(</span><span class="s">"such wow"</span><span class="p">);</span> <span class="c1">// row 2: "such wow" (a string)</span>
<span class="k">let</span> <span class="n">array</span> <span class="o">=</span> <span class="n">builder</span><span class="nf">.build</span><span class="p">();</span>

<span class="c1">// Since VariantArray is an ExtensionType, it needs to be converted</span>
<span class="c1">// to an ArrayRef and Field with the appropriate metadata</span>
<span class="c1">// before it can be written to a Parquet file</span>
<span class="k">let</span> <span class="n">field</span> <span class="o">=</span> <span class="n">array</span><span class="nf">.field</span><span class="p">(</span><span class="s">"data"</span><span class="p">);</span>
<span class="k">let</span> <span class="n">array</span> <span class="o">=</span> <span class="nn">ArrayRef</span><span class="p">::</span><span class="nf">from</span><span class="p">(</span><span class="n">array</span><span class="p">);</span>
<span class="c1">// create a RecordBatch with the VariantArray</span>
<span class="k">let</span> <span class="n">schema</span> <span class="o">=</span> <span class="nn">Schema</span><span class="p">::</span><span class="nf">new</span><span class="p">(</span><span class="nd">vec!</span><span class="p">[</span><span class="n">field</span><span class="p">]);</span>
<span class="k">let</span> <span class="n">batch</span> <span class="o">=</span> <span class="nn">RecordBatch</span><span class="p">::</span><span class="nf">try_new</span><span class="p">(</span><span class="nn">Arc</span><span class="p">::</span><span class="nf">new</span><span class="p">(</span><span class="n">schema</span><span class="p">),</span> <span class="nd">vec!</span><span class="p">[</span><span class="n">array</span><span class="p">])</span><span class="o">?</span><span class="p">;</span>

<span class="c1">// Now you can write the RecordBatch to the Parquet file, as normal</span>
<span class="k">let</span> <span class="n">file</span> <span class="o">=</span> <span class="nn">std</span><span class="p">::</span><span class="nn">fs</span><span class="p">::</span><span class="nn">File</span><span class="p">::</span><span class="nf">create</span><span class="p">(</span><span class="s">"variant.parquet"</span><span class="p">)</span><span class="o">?</span><span class="p">;</span>
<span class="k">let</span> <span class="k">mut</span> <span class="n">writer</span> <span class="o">=</span> <span class="nn">ArrowWriter</span><span class="p">::</span><span class="nf">try_new</span><span class="p">(</span><span class="n">file</span><span class="p">,</span> <span class="n">batch</span><span class="nf">.schema</span><span class="p">(),</span> <span class="nb">None</span><span class="p">)</span><span class="o">?</span><span class="p">;</span>
<span class="n">writer</span><span class="nf">.write</span><span class="p">(</span><span class="o">&amp;</span><span class="n">batch</span><span class="p">)</span><span class="o">?</span><span class="p">;</span>
<span class="n">writer</span><span class="nf">.close</span><span class="p">()</span><span class="o">?</span><span class="p">;</span>
</code></pre></div></div>
<p>This support is being integrated into query engines, such as
<a href="https://github.com/friendlymatthew">@friendlymatthew</a>'s <a href="https://github.com/datafusion-contrib/datafusion-variant"><code>datafusion-variant</code></a> crate to integrate into DataFusion
and <a href="https://github.com/delta-io/delta-rs/issues/3637">delta-rs</a>. While this support is still experimental, we believe the APIs
are mostly complete and do not expect major changes. Please consider trying
it out and providing feedback and improvements.</p>
<p>Thanks to the many contributors who made this possible, including:</p>
<ul>
<li>Ryan Johnson (<a href="https://github.com/scovich">@scovich</a>), Congxian Qiu (<a href="https://github.com/klion26">@klion26</a>), and Liam Bao (<a href="https://github.com/liamzwbao">@liamzwbao</a>) for completing the implementation</li>
<li>Li Jiaying (<a href="https://github.com/PinkCrow007">@PinkCrow007</a>), Aditya Bhatnagar (<a href="https://github.com/carpecodeum">@carpecodeum</a>), and Malthe Karbo (<a href="https://github.com/mkarbo">@mkarbo</a>) for
initiating the work</li>
<li>Everyone else who has contributed, including <a href="https://github.com/superserious-dev">@superserious-dev</a>, <a href="https://github.com/friendlymatthew">@friendlymatthew</a>, <a href="https://github.com/micoo227">@micoo227</a>, <a href="https://github.com/Weijun-H">@Weijun-H</a>,
<a href="https://github.com/harshmotw-db">@harshmotw-db</a>, <a href="https://github.com/odysa">@odysa</a>, <a href="https://github.com/viirya">@viirya</a>, <a href="https://github.com/adriangb">@adriangb</a>, <a href="https://github.com/kosiew">@kosiew</a>, <a href="https://github.com/codephage2020">@codephage2020</a>,
<a href="https://github.com/ding-young">@ding-young</a>, <a href="https://github.com/mbrobbel">@mbrobbel</a>, <a href="https://github.com/petern48">@petern48</a>, <a href="https://github.com/sdf-jkl">@sdf-jkl</a>, <a href="https://github.com/abacef">@abacef</a>, and <a href="https://github.com/mprammer">@mprammer</a>.</li>
</ul>
<p>See the ticket <a href="https://github.com/apache/arrow-rs/issues/6736">Variant type support in Parquet #6736</a> for more details</p>
<h3>Parquet Geometry Support üó∫Ô∏è</h3>
<p>The <code>57.0.0</code> release also includes support for reading and writing <a href="https://github.com/apache/parquet-format/blob/master/Geospatial.md">Parquet Geometry
types</a>, <code>GEOMETRY</code> and <code>GEOGRAPHY</code>, including <code>GeospatialStatistics</code>
contributed by Kyle Barron (<a href="https://github.com/kylebarron">@kylebarron</a>), Dewey Dunnington (<a href="https://github.com/paleolimbot">@paleolimbot</a>),
Kaushik Srinivasan (<a href="https://github.com/kaushiksrini">@kaushiksrini</a>), and Blake Orth (<a href="https://github.com/BlakeOrth">@BlakeOrth</a>).</p>
<p>Please see the <a href="https://github.com/apache/arrow-rs/issues/8373">Implement Geometry and Geography type support in Parquet</a> tracking ticket for more details.</p>
<h2>Thanks to Our Contributors</h2>
<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code data-lang="console"><span class="gp">$</span><span class="w"> </span>git shortlog <span class="nt">-sn</span> 56.0.0..57.0.0
<span class="go">    36  Matthijs Brobbel
    20  Andrew Lamb
    13  Ryan Johnson
    11  Ed Seidl
    10  Connor Sanders
     8  Alex Huang
     5  Emil Ernerfeldt
     5  Liam Bao
     5  Matthew Kim
     4  nathaniel-d-ef
     3  Raz Luvaton
     3  albertlockett
     3  dependabot[bot]
     3  mwish
     2  Ben Ye
     2  Congxian Qiu
     2  Dewey Dunnington
     2  Kyle Barron
     2  Lilian Maurel
     2  Mark Nash
     2  Nuno Faria
     2  Pepijn Van Eeckhoudt
     2  Tobias Schwarzinger
     2  lichuang
     1  Adam Gutglick
     1  Adam Reeve
     1  Alex Stephen
     1  Chen Chongchen
     1  Jack
     1  Jeffrey Vo
     1  J√∂rn Horstmann
     1  Kaushik Srinivasan
     1  Li Jiaying
     1  Lin Yihai
     1  Marco Neumann
     1  Piotr Findeisen
     1  Piotr Srebrny
     1  Samuele Resca
     1  Van De Bio
     1  Yan Tingwang
     1  ding-young
     1  kosiew
     1  Âº†Êûó‰ºü
</span></code></pre></div></div>]]></content><author><name>pmc</name></author><category term="release" /><summary type="html"><![CDATA[The Apache Arrow team is pleased to announce that the v57.0.0 release of Apache Arrow Rust is now available on crates.io (arrow and parquet) and as source download. See the 57.0.0 changelog for a full list of changes. New Features Note: Arrow Rust hosts the development of the parquet crate, a high performance Rust implementation of Apache Parquet. Performance: 4x Faster Parquet Metadata Parsing üöÄ Ed Seidl (@etseidl) and J√∂rn Horstmann (@jhorstmann) contributed a rewritten thrift metadata parser for Parquet files which is almost 4x faster than the previous parser based on the thrift crate. This is especially exciting for low latency use cases and reading Parquet files with large amounts of metadata (e.g. many row groups or columns). See the blog post about the new Parquet metadata parser for more details. Figure 1: Performance improvements of Apache Parquet metadata parsing between version 56.2.0 and 57.0.0. New arrow-avro Crate The 57.0.0 release introduces a new arrow-avro crate contributed by @jecsand838 and @nathaniel-d-ef that provides much more efficient conversion between Apache Avro and Arrow RecordBatches, as well as broader feature support. Previously, Arrow‚Äëbased systems that read or wrote Avro data typically used the general‚Äëpurpose apache-avro crate. While mature and feature‚Äëcomplete, its row-oriented API does not support features such as projection pushdown or vectorized execution. The new arrow-avro crate supports these features efficiently by converting Avro data directly into Arrow's columnar format. See the blog post about adding arrow-avro for more details. Figure 2: Architecture of the arrow-avro crate. Parquet Variant Support üß¨ The Apache Parquet project recently added a new Variant type for representing semi-structured data. The 57.0.0 release includes support for reading and writing both normal and shredded Variant values to and from Parquet files. It also includes parquet-variant, a complete library for working with Variant values, VariantArray for working with arrays of Variant values in Apache Arrow, computation kernels for converting to/from JSON and Arrow types, extracting paths, and shredding values. // Use the VariantArrayBuilder to build a VariantArray let mut builder = VariantArrayBuilder::new(3); builder.new_object().with_field("name", "Alice").finish(); // row 1: {"name": "Alice"} builder.append_value("such wow"); // row 2: "such wow" (a string) let array = builder.build(); // Since VariantArray is an ExtensionType, it needs to be converted // to an ArrayRef and Field with the appropriate metadata // before it can be written to a Parquet file let field = array.field("data"); let array = ArrayRef::from(array); // create a RecordBatch with the VariantArray let schema = Schema::new(vec![field]); let batch = RecordBatch::try_new(Arc::new(schema), vec![array])?; // Now you can write the RecordBatch to the Parquet file, as normal let file = std::fs::File::create("variant.parquet")?; let mut writer = ArrowWriter::try_new(file, batch.schema(), None)?; writer.write(&amp;batch)?; writer.close()?; This support is being integrated into query engines, such as @friendlymatthew's datafusion-variant crate to integrate into DataFusion and delta-rs. While this support is still experimental, we believe the APIs are mostly complete and do not expect major changes. Please consider trying it out and providing feedback and improvements. Thanks to the many contributors who made this possible, including: Ryan Johnson (@scovich), Congxian Qiu (@klion26), and Liam Bao (@liamzwbao) for completing the implementation Li Jiaying (@PinkCrow007), Aditya Bhatnagar (@carpecodeum), and Malthe Karbo (@mkarbo) for initiating the work Everyone else who has contributed, including @superserious-dev, @friendlymatthew, @micoo227, @Weijun-H, @harshmotw-db, @odysa, @viirya, @adriangb, @kosiew, @codephage2020, @ding-young, @mbrobbel, @petern48, @sdf-jkl, @abacef, and @mprammer. See the ticket Variant type support in Parquet #6736 for more details Parquet Geometry Support üó∫Ô∏è The 57.0.0 release also includes support for reading and writing Parquet Geometry types, GEOMETRY and GEOGRAPHY, including GeospatialStatistics contributed by Kyle Barron (@kylebarron), Dewey Dunnington (@paleolimbot), Kaushik Srinivasan (@kaushiksrini), and Blake Orth (@BlakeOrth). Please see the Implement Geometry and Geography type support in Parquet tracking ticket for more details. Thanks to Our Contributors $ git shortlog -sn 56.0.0..57.0.0 36 Matthijs Brobbel 20 Andrew Lamb 13 Ryan Johnson 11 Ed Seidl 10 Connor Sanders 8 Alex Huang 5 Emil Ernerfeldt 5 Liam Bao 5 Matthew Kim 4 nathaniel-d-ef 3 Raz Luvaton 3 albertlockett 3 dependabot[bot] 3 mwish 2 Ben Ye 2 Congxian Qiu 2 Dewey Dunnington 2 Kyle Barron 2 Lilian Maurel 2 Mark Nash 2 Nuno Faria 2 Pepijn Van Eeckhoudt 2 Tobias Schwarzinger 2 lichuang 1 Adam Gutglick 1 Adam Reeve 1 Alex Stephen 1 Chen Chongchen 1 Jack 1 Jeffrey Vo 1 J√∂rn Horstmann 1 Kaushik Srinivasan 1 Li Jiaying 1 Lin Yihai 1 Marco Neumann 1 Piotr Findeisen 1 Piotr Srebrny 1 Samuele Resca 1 Van De Bio 1 Yan Tingwang 1 ding-young 1 kosiew 1 Âº†Êûó‰ºü]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Apache Arrow 22.0.0 Release</title><link href="https://arrow.apache.org/blog/2025/10/24/22.0.0-release/" rel="alternate" type="text/html" title="Apache Arrow 22.0.0 Release" /><published>2025-10-24T00:00:00-04:00</published><updated>2025-10-24T00:00:00-04:00</updated><id>https://arrow.apache.org/blog/2025/10/24/22.0.0-release</id><content type="html" xml:base="https://arrow.apache.org/blog/2025/10/24/22.0.0-release/"><![CDATA[<!--

-->
<p>The Apache Arrow team is pleased to announce the 22.0.0 release. This release
covers over 3 months of development work and includes <a href="https://github.com/apache/arrow/milestone/70?closed=1"><strong>213 resolved
issues</strong></a> on <a href="/release/22.0.0.html#contributors"><strong>255 distinct commits</strong></a> from <a href="/release/22.0.0.html#contributors"><strong>60 distinct
contributors</strong></a>. See the <a href="https://arrow.apache.org/install/">Install Page</a> to
learn how to get the libraries for your platform.</p>
<p>The release notes below are not exhaustive and only expose selected highlights
of the release. Many other bugfixes and improvements have been made: we refer
you to the <a href="/release/22.0.0.html#changelog">complete changelog</a>.</p>
<h2>Community</h2>
<p>Since the 21.0.0 release, Kyle Barron has been invited to be committer.</p>
<p>Matthijs Brobbel, Adam Reeve and Rossi Sun have been joined the
Project Management Committee (PMC).</p>
<p>Thanks for your contributions and participation in the project!</p>
<p>The first Apache Arrow Summit was held on October 2nd 2025 in Paris, France
as part of <a href="https://pydata.org/paris2025">PyData Paris</a>.
Program details and agenda can be found here: <a href="https://www.meetup.com/pydata-paris/events/310646396/">https://www.meetup.com/pydata-paris/events/310646396/</a></p>
<p>There were around 35 attendees, of which ~20 were existing core developers or PMC members.
The Summit was overwhelmingly described as a success, with a friendly atmosphere between all participants.
Unfortunately, no Audio / Video recording system was available for this event.</p>
<h2>Arrow Flight RPC Notes</h2>
<p>Support for dictionary replacement and dictionary encoding has been added to the DoGet and DoExchange methods. (<a href="https://github.com/apache/arrow/issues/45056">GH-45056</a>, <a href="https://github.com/apache/arrow/issues/45055">GH-45055</a> and <a href="https://github.com/apache/arrow/issues/26727">GH-26727</a>).</p>
<p>As part of supporting dictionary replacement we have also exposed the <code>ipc::ReadStats</code> on the <code>FlightStreamReader</code> in order to facilitate debugging. (<a href="https://github.com/apache/arrow/issues/47422">GH-47422</a>)</p>
<h2>C++ Notes</h2>
<h3>Compute</h3>
<p>Timezone aware kernels can now handle timezone offset strings. (<a href="https://github.com/apache/arrow/issues/30036">GH-30036</a>)</p>
<p>Better decimal support has been added introducing a structure <code>MatchConstraint</code> for applying extra (and optional) matching constraint for kernel signature matching. (<a href="https://github.com/apache/arrow/issues/47287">GH-47287</a>, <a href="https://github.com/apache/arrow/issues/41336">GH-41336</a>)</p>
<p>The scatter function has been moved to Arrow core from Arrow Compute. (<a href="https://github.com/apache/arrow/issues/47375">GH-47375</a>)</p>
<h3>Filesystems</h3>
<p>The Request ID has been added when the AWS client raises an error. (<a href="https://github.com/apache/arrow/issues/47349">GH-47349</a>)</p>
<h3>Format</h3>
<p>Several improvements around Half Float (Float16) support. (<a href="https://github.com/apache/arrow/issues/46860">GH-46860</a>, <a href="https://github.com/apache/arrow/issues/46739">GH-46739</a>)</p>
<h3>Parquet</h3>
<p>Better Fuzzing support for Parquet and several related fixes. (<a href="https://github.com/apache/arrow/issues/47803">GH-47803</a>, <a href="https://github.com/apache/arrow/issues/47740">GH-47740</a>, <a href="https://github.com/apache/arrow/issues/47655">GH-47655</a>, <a href="https://github.com/apache/arrow/issues/47597">GH-47597</a>, <a href="https://github.com/apache/arrow/issues/47184">GH-47184</a>)</p>
<p>Rework around the RLE decoder in order to extract a RLE parser to drive further optimisations. (<a href="https://github.com/apache/arrow/issues/47112">GH-47112</a>)</p>
<p>Dynamic dispatch support has been added to Byte Stream Split. (<a href="https://github.com/apache/arrow/issues/46962">GH-46962</a>)</p>
<p>Now some statistics, i.e. null count. will not be discarded when the sort order of the column is unknown. (<a href="https://github.com/apache/arrow/issues/47449">GH-47449</a>)</p>
<p><code>is_min_value_exact</code> and <code>is_max_value_exact</code> now are exposed in Parquet Statistics if present when reading. (<a href="https://github.com/apache/arrow/issues/46905">GH-46905</a>)</p>
<p>We now reserve values correctly when reading <code>BYTE_ARRAY</code> and <code>FLBA</code>. (<a href="https://github.com/apache/arrow/issues/47012">GH-47012</a>)</p>
<h4>Encryption</h4>
<p>String based Parquet encrption methods have been deprecated. (<a href="https://github.com/apache/arrow/issues/47338">GH-47338</a>)</p>
<p>Memory usage required by decryption buffers when reading encrypted Parquet has been reduced. (<a href="https://github.com/apache/arrow/issues/46971">GH-46971</a>)</p>
<h4>Type support</h4>
<p>Improvements on the Parquet Variant type support. (<a href="https://github.com/apache/arrow/issues/47241">GH-47241</a>, <a href="https://github.com/apache/arrow/issues/47838">GH-47838</a>)</p>
<p>Better support for Decimal32 and Decimal64. (<a href="https://github.com/apache/arrow/issues/44345">GH-44345</a>)</p>
<h3>Gandiva</h3>
<p>Support for LLVM 21.1.0 has been added. (<a href="https://github.com/apache/arrow/issues/47469">GH-47469</a>)</p>
<h3>Miscellaneous C++ changes</h3>
<p>Add support for further Arrow Statistics. (<a href="https://github.com/apache/arrow/issues/47102">GH-47102</a>, <a href="https://github.com/apache/arrow/issues/47101">GH-47101</a>)</p>
<p>Support for shared memory comparison in <code>arrow::RecordBatch</code> has been added. (<a href="https://github.com/apache/arrow/pull/47149">GH-47149</a>)</p>
<p><code>arrow::Table::Equals</code> now allows an optional <code>arrow::EqualOptions</code> argument. (<a href="https://github.com/apache/arrow/issues/46937">GH-46937</a>)</p>
<p>Skyhook integration has been removed from the main repository and has been moved to its own repository. (<a href="https://github.com/apache/arrow/issues/47225">GH-47225</a>)</p>
<h2>Linux Packaging Notes</h2>
<p>Support for Debian forky has been added. (<a href="https://github.com/apache/arrow/issues/47312">GH-47312</a>)</p>
<h2>MATLAB Notes</h2>
<ul>
<li><code>NumNulls</code> property was added to <code>arrow.array.Array</code> and <code>arrow.arrray.ChunkedArray</code>. (<a href="https://github.com/apache/arrow/issues/47263">GH-47263</a>, <a href="https://github.com/apache/arrow/issues/38422">GH-38422</a>)</li>
</ul>
<h2>Python Notes</h2>
<p>Compatibility notes:</p>
<ul>
<li>Support for Python 3.9 has been dropped (<a href="https://github.com/apache/arrow/issues/47443">GH-47443</a>) and support for Python 3.14, regular and free-threaded has been added, (<a href="https://github.com/apache/arrow/issues/47438">GH-47438</a>).</li>
<li>Cython 3.1 is now required build-time dependency (<a href="https://github.com/apache/arrow/issues/47370">GH-47370</a>).</li>
<li><code>project.optional-dependencies</code> has been replaced with <code>dependency-groups</code> (<a href="https://github.com/apache/arrow/issues/47137">GH-47137</a>).</li>
</ul>
<p>New features:</p>
<ul>
<li>CSV writer option <code>quoting_header</code> is now exposed (<a href="https://github.com/apache/arrow/issues/47575">GH-47575</a>).</li>
</ul>
<p>Other improvements:</p>
<ul>
<li>Support for pandas <code>DataFrame.attrs</code> during conversion between a dataframe and a Parquet file has been added (<a href="https://github.com/apache/arrow/issues/45382">GH-45382</a>).</li>
<li>A utility function to create Arrow table instead of pandas dataframe has been added (<a href="https://github.com/apache/arrow/issues/47172">GH-47172</a>).</li>
<li>IPC and Flight options now have a nice repr/str methods (<a href="https://github.com/apache/arrow/issues/47358">GH-47358</a>).</li>
<li>Access to Request ID in AWS client error is now available from Python (<a href="https://github.com/apache/arrow/issues/47349">GH-47349</a>).</li>
<li>Public Type Enums are added (<a href="https://github.com/apache/arrow/issues/47123">GH-47123</a>).</li>
<li>Python Development documentation section has been restructured in order to make it easier for contributors to build and develop PyArrow (<a href="https://github.com/apache/arrow/issues/20125">GH-20125</a>.</li>
</ul>
<p>Relevant bug fixes:</p>
<ul>
<li>Schema is now hashable when metadata is set (<a href="https://github.com/apache/arrow/issues/47602">GH-47602</a>).</li>
<li><code>MapScalar.as_py(maps_as_pydicts=&quot;strict&quot;)</code> option now works for nested maps (<a href="https://github.com/apache/arrow/issues/47380">GH-47380</a>).</li>
<li><code>FileFragment.open()</code> no longer segfaults on file-like objects (<a href="https://github.com/apache/arrow/issues/47301">GH-47301</a>).</li>
<li><code>pa.compute.fill_null</code> regression on Windows due to a compiler bug has been fixed (<a href="https://github.com/apache/arrow/issues/47234">GH-47234</a>).</li>
<li>Integer dictionary bitwidth preservation no longer breaks multi-file read behaviour as <code>DatasetFactory.inspect</code> method now accepts <code>promote_options</code> and <code>fragments</code> parameters (<a href="https://github.com/apache/arrow/issues/46629">GH-46629</a>).</li>
<li><code>FileSystem.from_uri</code> is reverted to be a staticmethod again (<a href="https://github.com/apache/arrow/issues/47179">GH-47179</a>).</li>
</ul>
<h2>Java, JavaScript, Go, .NET, Swift and Rust Notes</h2>
<p>The Java, JavaScript, Go, .NET, Swift and Rust projects have moved to separate
repositories outside the main Arrow <a href="https://github.com/apache/arrow">monorepo</a>.</p>
<ul>
<li>For notes on the latest release of the <a href="https://github.com/apache/arrow-java">Java
implementation</a>, see the latest <a href="https://github.com/apache/arrow-java/releases">Arrow
Java changelog</a>.</li>
<li>For notes on the latest release of the <a href="https://github.com/apache/arrow-js">JavaScript
implementation</a>, see the latest <a href="https://github.com/apache/arrow-js/releases">Arrow
JavaScript changelog</a>.</li>
<li>For notes on the latest release of the <a href="https://github.com/apache/arrow-rs">Rust
implementation</a> see the latest <a href="https://github.com/apache/arrow-rs/blob/main/CHANGELOG.md">Arrow Rust
changelog</a>.</li>
<li>For notes on the latest release of the <a href="https://github.com/apache/arrow-go">Go
implementation</a>, see the latest <a href="https://github.com/apache/arrow-go/releases">Arrow Go
changelog</a>.</li>
<li>For notes on the latest release of the <a href="https://github.com/apache/arrow-dotnet">.NET
implementation</a>, see the latest <a href="https://github.com/apache/arrow-dotnet/releases">Arrow  .NET changelog</a>.</li>
<li>For notes on the latest release of the <a href="https://github.com/apache/arrow-swift">Swift implementation</a>, see the latest <a href="https://github.com/apache/arrow-swift/releases">Arrow Swift changelog</a>.</li>
</ul>]]></content><author><name>pmc</name></author><category term="release" /><summary type="html"><![CDATA[The Apache Arrow team is pleased to announce the 22.0.0 release. This release covers over 3 months of development work and includes 213 resolved issues on 255 distinct commits from 60 distinct contributors. See the Install Page to learn how to get the libraries for your platform. The release notes below are not exhaustive and only expose selected highlights of the release. Many other bugfixes and improvements have been made: we refer you to the complete changelog. Community Since the 21.0.0 release, Kyle Barron has been invited to be committer. Matthijs Brobbel, Adam Reeve and Rossi Sun have been joined the Project Management Committee (PMC). Thanks for your contributions and participation in the project! The first Apache Arrow Summit was held on October 2nd 2025 in Paris, France as part of PyData Paris. Program details and agenda can be found here: https://www.meetup.com/pydata-paris/events/310646396/ There were around 35 attendees, of which ~20 were existing core developers or PMC members. The Summit was overwhelmingly described as a success, with a friendly atmosphere between all participants. Unfortunately, no Audio / Video recording system was available for this event. Arrow Flight RPC Notes Support for dictionary replacement and dictionary encoding has been added to the DoGet and DoExchange methods. (GH-45056, GH-45055 and GH-26727). As part of supporting dictionary replacement we have also exposed the ipc::ReadStats on the FlightStreamReader in order to facilitate debugging. (GH-47422) C++ Notes Compute Timezone aware kernels can now handle timezone offset strings. (GH-30036) Better decimal support has been added introducing a structure MatchConstraint for applying extra (and optional) matching constraint for kernel signature matching. (GH-47287, GH-41336) The scatter function has been moved to Arrow core from Arrow Compute. (GH-47375) Filesystems The Request ID has been added when the AWS client raises an error. (GH-47349) Format Several improvements around Half Float (Float16) support. (GH-46860, GH-46739) Parquet Better Fuzzing support for Parquet and several related fixes. (GH-47803, GH-47740, GH-47655, GH-47597, GH-47184) Rework around the RLE decoder in order to extract a RLE parser to drive further optimisations. (GH-47112) Dynamic dispatch support has been added to Byte Stream Split. (GH-46962) Now some statistics, i.e. null count. will not be discarded when the sort order of the column is unknown. (GH-47449) is_min_value_exact and is_max_value_exact now are exposed in Parquet Statistics if present when reading. (GH-46905) We now reserve values correctly when reading BYTE_ARRAY and FLBA. (GH-47012) Encryption String based Parquet encrption methods have been deprecated. (GH-47338) Memory usage required by decryption buffers when reading encrypted Parquet has been reduced. (GH-46971) Type support Improvements on the Parquet Variant type support. (GH-47241, GH-47838) Better support for Decimal32 and Decimal64. (GH-44345) Gandiva Support for LLVM 21.1.0 has been added. (GH-47469) Miscellaneous C++ changes Add support for further Arrow Statistics. (GH-47102, GH-47101) Support for shared memory comparison in arrow::RecordBatch has been added. (GH-47149) arrow::Table::Equals now allows an optional arrow::EqualOptions argument. (GH-46937) Skyhook integration has been removed from the main repository and has been moved to its own repository. (GH-47225) Linux Packaging Notes Support for Debian forky has been added. (GH-47312) MATLAB Notes NumNulls property was added to arrow.array.Array and arrow.arrray.ChunkedArray. (GH-47263, GH-38422) Python Notes Compatibility notes: Support for Python 3.9 has been dropped (GH-47443) and support for Python 3.14, regular and free-threaded has been added, (GH-47438). Cython 3.1 is now required build-time dependency (GH-47370). project.optional-dependencies has been replaced with dependency-groups (GH-47137). New features: CSV writer option quoting_header is now exposed (GH-47575). Other improvements: Support for pandas DataFrame.attrs during conversion between a dataframe and a Parquet file has been added (GH-45382). A utility function to create Arrow table instead of pandas dataframe has been added (GH-47172). IPC and Flight options now have a nice repr/str methods (GH-47358). Access to Request ID in AWS client error is now available from Python (GH-47349). Public Type Enums are added (GH-47123). Python Development documentation section has been restructured in order to make it easier for contributors to build and develop PyArrow (GH-20125. Relevant bug fixes: Schema is now hashable when metadata is set (GH-47602). MapScalar.as_py(maps_as_pydicts=&quot;strict&quot;) option now works for nested maps (GH-47380). FileFragment.open() no longer segfaults on file-like objects (GH-47301). pa.compute.fill_null regression on Windows due to a compiler bug has been fixed (GH-47234). Integer dictionary bitwidth preservation no longer breaks multi-file read behaviour as DatasetFactory.inspect method now accepts promote_options and fragments parameters (GH-46629). FileSystem.from_uri is reverted to be a staticmethod again (GH-47179). Java, JavaScript, Go, .NET, Swift and Rust Notes The Java, JavaScript, Go, .NET, Swift and Rust projects have moved to separate repositories outside the main Arrow monorepo. For notes on the latest release of the Java implementation, see the latest Arrow Java changelog. For notes on the latest release of the JavaScript implementation, see the latest Arrow JavaScript changelog. For notes on the latest release of the Rust implementation see the latest Arrow Rust changelog. For notes on the latest release of the Go implementation, see the latest Arrow Go changelog. For notes on the latest release of the .NET implementation, see the latest Arrow .NET changelog. For notes on the latest release of the Swift implementation, see the latest Arrow Swift changelog.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Announcing arrow-avro in Arrow Rust</title><link href="https://arrow.apache.org/blog/2025/10/23/introducing-arrow-avro/" rel="alternate" type="text/html" title="Announcing arrow-avro in Arrow Rust" /><published>2025-10-23T00:00:00-04:00</published><updated>2025-10-23T00:00:00-04:00</updated><id>https://arrow.apache.org/blog/2025/10/23/introducing-arrow-avro</id><content type="html" xml:base="https://arrow.apache.org/blog/2025/10/23/introducing-arrow-avro/"><![CDATA[<!--

-->
<p><a href="https://crates.io/crates/arrow-avro"><code>arrow-avro</code></a>, a newly rewritten Rust crate that reads and writes <a href="https://avro.apache.org/">Apache Avro</a> data directly as Arrow <code>RecordBatch</code>es, is now available. It supports <a href="https://avro.apache.org/docs/1.11.1/specification/#object-container-files">Avro Object Container Files</a> (OCF), <a href="https://avro.apache.org/docs/1.11.1/specification/#single-object-encoding">Single‚ÄëObject Encoding</a> (SOE), the <a href="https://docs.confluent.io/platform/current/schema-registry/fundamentals/serdes-develop/index.html#wire-format">Confluent Schema Registry wire format</a>, and the <a href="https://www.apicur.io/registry/docs/apicurio-registry/1.3.3.Final/getting-started/assembly-using-kafka-client-serdes.html#registry-serdes-types-avro-registry">Apicurio Registry wire format</a>, with projection/evolution, tunable batch sizing, and optional <code>StringViewArray</code> support for faster strings. Its vectorized design reduces copies and cache misses, making both batch and streaming pipelines simpler and faster.</p>
<h2>Motivation</h2>
<p>Apache Avro‚Äôs row‚Äëoriented design is effective for encoding one record at a time, while Apache Arrow‚Äôs columnar layout is optimized for vectorized analytics. A major challenge lies in converting between these formats without reintroducing row‚Äëwise overhead. Decoding Avro a row at a time and then building Arrow arrays incurs extra allocations and cache‚Äëunfriendly access (the very costs Arrow is designed to avoid). In the real world, this overhead commonly shows up in analytical hot paths. For instance, <a href="https://github.com/apache/datafusion/tree/main/datafusion/datasource-avro">DataFusion‚Äôs Avro data source</a> currently ships with its own row‚Äëcentric Avro‚Äëto‚ÄëArrow layer. This implementation has led to an open issue for <a href="https://github.com/apache/datafusion/issues/14097">using an upstream arrow-avro reader</a> to simplify the code and speed up scans. Additionally, DataFusion has another open issue for <a href="https://github.com/apache/datafusion/issues/7679#issuecomment-3412302891">supporting Avro format writes</a> that is predicated on the development of an upstream <code>arrow-avro</code> writer.</p>
<h3>Why not use the existing <code>apache-avro</code> crate?</h3>
<p>Rust already has a mature, general‚Äëpurpose Avro crate, <a href="https://crates.io/crates/apache-avro">apache-avro</a>. It reads and writes Avro records as Avro value types and provides Object Container File readers and writers. What it does not do is decode directly into Arrow arrays, so any Arrow integration must materialize rows and then build columns.</p>
<p>What‚Äôs needed is a complementary approach that decodes column‚Äëby‚Äëcolumn straight into Arrow builders and emits <code>RecordBatch</code>es. This would enable projection pushdown while keeping execution vectorized end to end. For projects such as <a href="https://datafusion.apache.org/">Apache DataFusion</a>, access to a mature, upstream Arrow‚Äënative reader and writer would help simplify the code path and reduce duplication.</p>
<p>Modern pipelines heighten this need because <a href="https://www.confluent.io/blog/avro-kafka-data/">Avro is also used on the wire</a>, not just in files. Kafka ecosystems commonly use Confluent‚Äôs Schema Registry framing, and many services adopt the Avro Single‚ÄëObject Encoding format. An approach that enables decoding straight into Arrow batches (rather than through per‚Äërow values) would let downstream compute remain vectorized at streaming rates.</p>
<h3>Why this matters</h3>
<p>Apache Avro is a first‚Äëclass format across stream processors and cloud services:</p>
<ul>
<li>Confluent Schema Registry supports <a href="https://docs.confluent.io/platform/current/schema-registry/fundamentals/serdes-develop/serdes-avro.html">Avro across multiple languages and tooling</a>.</li>
<li>Apache Flink exposes an <a href="https://nightlies.apache.org/flink/flink-docs-release-1.19/docs/connectors/table/formats/avro-confluent/"><code>avro-confluent</code> format for Kafka</a>.</li>
<li>AWS Lambda <a href="https://aws.amazon.com/about-aws/whats-new/2025/06/aws-lambda-native-support-avro-protobuf-kafka-events/">(June 2025) added native handling for Avro‚Äëformatted Kafka events</a> with Glue and Confluent Schema Registry integrations.</li>
<li>Azure Event Hubs provides a <a href="https://learn.microsoft.com/en-us/azure/event-hubs/schema-registry-overview">Schema Registry with Avro support</a> for Kafka‚Äëcompatible clients.</li>
</ul>
<p>In short: Arrow users encounter Avro both on disk (OCF) and on the wire (SOE). An Arrow‚Äëfirst, vectorized reader/writer for OCF, SOE, and Confluent framing removes a pervasive bottleneck and keeps pipelines columnar end‚Äëto‚Äëend.</p>
<h2>Introducing <code>arrow-avro</code></h2>
<p><a href="https://github.com/apache/arrow-rs/tree/main/arrow-avro"><code>arrow-avro</code></a> is a high-performance Rust crate that converts between Avro and Arrow with a column‚Äëfirst, batch‚Äëoriented design. On the read side, it decodes Avro Object Container Files (OCF), Single‚ÄëObject Encoding (SOE), and the Confluent Schema Registry wire format directly into Arrow <code>RecordBatch</code>es. Meanwhile, the write path provides formats for encoding to OCF and SOE as well.</p>
<p>The crate exposes two primary read APIs: a high-level <code>Reader</code> for OCF inputs and a low-level <code>Decoder</code> for streaming SOE frames. For SOE and Confluent/Apicurio frames, a <code>SchemaStore</code> is provided that resolves fingerprints or schema IDs to full Avro writer schemas, enabling schema evolution while keeping the decode path vectorized.</p>
<p>On the write side, <code>AvroWriter</code> produces OCF (including container‚Äëlevel compression), while <code>AvroStreamWriter</code> produces framed Avro messages for Single‚ÄëObject or Confluent/Apicurio encodings, as configured via the <code>WriterBuilder::with_fingerprint_strategy(...)</code> knob.</p>
<p>Configuration is intentionally minimal but practical. For instance, the <code>ReaderBuilder</code> exposes knobs covering both batch file ingestion and streaming systems without forcing format‚Äëspecific code paths.</p>
<h3>How this mirrors Parquet in Arrow‚Äërs</h3>
<p>If you have used Parquet with Arrow‚Äërs, you already know the pattern. The <code>parquet</code> crate exposes a <a href="https://docs.rs/parquet/latest/parquet/arrow/index.html">parquet::arrow module</a> that reads and writes Arrow <code>RecordBatch</code>es directly. Most users reach for <code>ParquetRecordBatchReaderBuilder</code> when reading and <code>ArrowWriter</code> when writing. You choose columns up front, set a batch size, and the reader gives you Arrow batches that flow straight into vectorized operators. This is the widely adopted &quot;format crate + Arrow‚Äënative bridge&quot; approach in Rust.</p>
<p><code>arrow‚Äëavro</code> brings that same bridge to Avro. You get a single <code>ReaderBuilder</code> that can produce a <code>Reader</code> for OCF, or a streaming <code>Decoder</code> for on‚Äëthe‚Äëwire frames. Both return Arrow <code>RecordBatch</code>es, which means engines can keep projection and filtering close to the reader and avoid building rows only to reassemble them back into columns later. For evolving streams, a small <code>SchemaStore</code> resolves fingerprints or ids before decoding, so the batches that come out are already shaped for vectorized execution.</p>
<p>The reason this pattern matters is straightforward. Arrow‚Äôs columnar format is designed for vectorized work and good cache locality. When a format reader produces Arrow batches directly, copies and branchy per‚Äërow work are minimized, keeping downstream operators fast. That is the same story that made <code>parquet::arrow</code> popular in Rust, and it is what <code>arrow‚Äëavro</code> now enables for Avro.</p>
<h2>Architecture &amp; Technical Overview</h2>
<div style="display: flex; gap: 16px; justify-content: center; align-items: flex-start; padding: 20px 15px;">
<img src="/img/introducing-arrow-avro/arrow-avro-architecture.svg"
        width="100%"
        alt="High-level `arrow-avro` architecture"
        style="background:#fff">
</div>
<p>At a high level, <a href="https://arrow.apache.org/rust/arrow_avro/index.html">arrow-avro</a> splits cleanly into read and write paths built around Arrow <code>RecordBatch</code>es. The read side turns Avro (OCF files or framed byte streams) into batched Arrow arrays, while the write side takes Arrow batches and produces OCF files or streaming frames. When using an <code>AvroStreamWriter</code>, the framing (SOE or Confluent) is part of the stream output based on the configured fingerprint strategy; thus no separate framing work is required. The public API and module layout are intentionally small, so most applications only touch a builder, a reader/decoder, and (optionally) a schema store for schema evolution.</p>
<p>On the <a href="https://arrow.apache.org/rust/arrow_avro/reader/index.html">read</a> path, everything starts with the <a href="https://arrow.apache.org/rust/arrow_avro/reader/struct.ReaderBuilder.html">ReaderBuilder</a>. A single builder can create a <a href="https://arrow.apache.org/rust/arrow_avro/reader/struct.Reader.html">Reader</a> for Object Container Files (OCF) or a streaming <a href="https://arrow.apache.org/rust/arrow_avro/reader/struct.Decoder.html">Decoder</a> for SOE/Confluent/Apicurio frames. The <code>Reader</code> pulls OCF blocks and yields Arrow <code>RecordBatch</code>es while the <code>Decoder</code> is push‚Äëbased, i.e., bytes are fed in as they arrive and then drained as completed batches once <code>flush</code> is called. Both use the same schema‚Äëdriven decoding logic (per‚Äëcolumn decoders with projection/union/nullability handling), so file and streaming inputs produce batches using fewer per‚Äërow allocations and minimal branching/redundancy. Additionally, the streaming <code>Decoder</code> maintains a cache of per‚Äëschema record decoders keyed by fingerprint to avoid re‚Äëplanning when a stream interleaves schema versions. This keeps steady‚Äëstate decode fast even as schemas evolve.</p>
<p>When reading an OCF, the <code>Reader</code> parses a header and then iterates over blocks of encoded data. The header contains a metadata map with the embedded Avro schema and optional compression (i.e., <code>deflate</code>, <code>snappy</code>, <code>zstd</code>, <code>bzip2</code>, <code>xz</code>), plus a 16‚Äëbyte sync marker used to delimit blocks. Each subsequent OCF block then carries a row count and the encoded payload. The parsed OCF header and block structures are also encoded with variable‚Äëlength integers that use zig‚Äëzag encoding for signed values. <code>arrow-avro</code> implements a small <code>vlq</code> (variable‚Äëlength quantity) module, which is used during both header parsing and block iteration. Efficient <code>vlq</code> decode is part of why the <code>Reader</code> and <code>Decoder</code> can stay vectorized and avoid unnecessary per‚Äërow overhead.</p>
<p>On the <a href="https://arrow.apache.org/rust/arrow_avro/writer/index.html">write</a> path, the <a href="https://arrow.apache.org/rust/arrow_avro/writer/struct.WriterBuilder.html">WriterBuilder</a> produces either an <a href="https://arrow.apache.org/rust/arrow_avro/writer/type.AvroWriter.html">AvroWriter</a> (OCF) or an <a href="https://arrow.apache.org/rust/arrow_avro/writer/type.AvroStreamWriter.html">AvroStreamWriter</a> (SOE/Message). The <code>with_compression(...)</code> knob is used for OCF block compression while <code>with_fingerprint_strategy(...)</code> selects the streaming frame, i.e., Rabin for SOE, a 32‚Äëbit schema ID for Confluent, or a 64-bit schema ID for Apicurio. The <code>AvroStreamWriter</code> also adds the appropriate prefix automatically while encoding, thus eliminating the need for potentially expensive post‚Äëprocessing steps to wrap output Avro SOEs.</p>
<p>Schema handling is centralized in the <a href="https://arrow.apache.org/rust/arrow_avro/schema/index.html">schema</a> module. <a href="https://arrow.apache.org/rust/arrow_avro/schema/struct.AvroSchema.html">AvroSchema</a> wraps a valid Avro Schema JSON string, supports computing a <code>Fingerprint</code>, and can be loaded into a <a href="https://arrow.apache.org/rust/arrow_avro/schema/struct.SchemaStore.html">SchemaStore</a> as a writer schema. At runtime, the <code>Reader</code>/<code>Decoder</code> can use a <code>SchemaStore</code> to resolve fingerprints before decoding, enabling <a href="https://avro.apache.org/docs/1.11.1/specification/#schema-resolution">schema resolution</a>. The <code>FingerprintAlgorithm</code> captures how fingerprints are derived (i.e., CRC‚Äë64‚ÄëAVRO Rabin, MD5, SHA‚Äë256, or a registry ID), and <code>FingerprintStrategy</code> configures how the <code>Writer</code> prefixes each record while encoding SOE streams. This schema module is the glue that enables SOE and Confluent/Apicurio support without coupling to a specific registry client.</p>
<p>At the heart of <code>arrow-avro</code> is a type‚Äëmapping <code>Codec</code> that the library uses to construct both encoders and decoders. The <code>Codec</code> captures, for every Avro field, how it maps to Arrow and how it should be encoded or decoded. The <code>Reader</code> logic builds a <code>Codec</code> per <em>(writer, reader)</em> schema pair, which the decoder later uses to vectorize parsing of Avro values directly into the correct Arrow builders. The <code>Writer</code> logic uses the same <code>Codec</code> mappings to drive pre-computed record encoding plans which enable fast serialization of Arrow arrays to the correct Avro physical representation (i.e., decimals as bytes vs fixed, enum symbol handling, union branch tagging, etc.). Because the <code>Codec</code> informs union and nullable decisions in both the encoder and decoder, the common Avro pattern <code>[&quot;null&quot;, T]</code> seamlessly maps to and from an Arrow optional field, while Avro unions map to Arrow unions using an 8‚Äëbit type‚Äëid with minimal overhead. Meanwhile, enabling <code>strict_mode</code> applies tighter Avro resolution rules in the <code>Codec</code> to help surface ambiguous unions early.</p>
<p>Finally, by keeping container and stream framing (OCF vs. SOE) separate from encoding and decoding, the crate composes naturally with the rest of Arrow‚Äërs: you read or write Arrow <code>RecordBatch</code>es, pick OCF or SOE streams as needed, and wire up fingerprints only when you're on a streaming path. This results in a compact API surface that covers both batch files and high‚Äëthroughput streams without sacrificing columnar, vectorized execution.</p>
<h2>Examples</h2>
<h3>Decoding a Confluent-framed Kafka Stream</h3>
<div class="language-rust highlighter-rouge"><div class="highlight"><pre class="highlight"><code data-lang="rust"><span class="k">use</span> <span class="nn">arrow_avro</span><span class="p">::</span><span class="nn">reader</span><span class="p">::</span><span class="n">ReaderBuilder</span><span class="p">;</span>
<span class="k">use</span> <span class="nn">arrow_avro</span><span class="p">::</span><span class="nn">schema</span><span class="p">::{</span>
    <span class="n">SchemaStore</span><span class="p">,</span> <span class="n">AvroSchema</span><span class="p">,</span> <span class="n">Fingerprint</span><span class="p">,</span> <span class="n">FingerprintAlgorithm</span><span class="p">,</span> <span class="n">CONFLUENT_MAGIC</span>
<span class="p">};</span>

<span class="k">fn</span> <span class="nf">main</span><span class="p">()</span> <span class="k">-&gt;</span> <span class="nb">Result</span><span class="o">&lt;</span><span class="p">(),</span> <span class="nb">Box</span><span class="o">&lt;</span><span class="k">dyn</span> <span class="nn">std</span><span class="p">::</span><span class="nn">error</span><span class="p">::</span><span class="n">Error</span><span class="o">&gt;&gt;</span> <span class="p">{</span>
    <span class="c1">// Register writer schema under Confluent id=1.</span>
    <span class="k">let</span> <span class="k">mut</span> <span class="n">store</span> <span class="o">=</span> <span class="nn">SchemaStore</span><span class="p">::</span><span class="nf">new_with_type</span><span class="p">(</span><span class="nn">FingerprintAlgorithm</span><span class="p">::</span><span class="n">Id</span><span class="p">);</span>
    <span class="n">store</span><span class="nf">.set</span><span class="p">(</span>
        <span class="nn">Fingerprint</span><span class="p">::</span><span class="nf">Id</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span>
        <span class="nn">AvroSchema</span><span class="p">::</span><span class="nf">new</span><span class="p">(</span><span class="s">r#"{"type":"record","name":"T","fields":[{"name":"x","type":"long"}]}"#</span><span class="nf">.into</span><span class="p">()),</span>
    <span class="p">)</span><span class="o">?</span><span class="p">;</span>

    <span class="c1">// Define reader schema to enable projection/schema evolution.</span>
    <span class="k">let</span> <span class="n">reader_schema</span> <span class="o">=</span> <span class="nn">AvroSchema</span><span class="p">::</span><span class="nf">new</span><span class="p">(</span><span class="s">r#"{"type":"record","name":"T","fields":[{"name":"x","type":"long"}]}"#</span><span class="nf">.into</span><span class="p">());</span>

    <span class="c1">// Build Decoder using reader and writer schemas</span>
    <span class="k">let</span> <span class="k">mut</span> <span class="n">decoder</span> <span class="o">=</span> <span class="nn">ReaderBuilder</span><span class="p">::</span><span class="nf">new</span><span class="p">()</span>
        <span class="nf">.with_reader_schema</span><span class="p">(</span><span class="n">reader_schema</span><span class="p">)</span>
        <span class="nf">.with_writer_schema_store</span><span class="p">(</span><span class="n">store</span><span class="p">)</span>
        <span class="nf">.build_decoder</span><span class="p">()</span><span class="o">?</span><span class="p">;</span>

    <span class="c1">// Simulate one frame: magic 0x00 + 4‚Äëbyte big‚Äëendian schema ID + Avro body (x=1 encoded as zig‚Äëzag/VLQ).</span>
    <span class="k">let</span> <span class="k">mut</span> <span class="n">frame</span> <span class="o">=</span> <span class="nn">Vec</span><span class="p">::</span><span class="nf">from</span><span class="p">(</span><span class="n">CONFLUENT_MAGIC</span><span class="p">);</span> <span class="n">frame</span><span class="nf">.extend_from_slice</span><span class="p">(</span><span class="o">&amp;</span><span class="mi">1u32</span><span class="nf">.to_be_bytes</span><span class="p">());</span> <span class="n">frame</span><span class="nf">.extend_from_slice</span><span class="p">(</span><span class="o">&amp;</span><span class="p">[</span><span class="mi">2</span><span class="p">]);</span>

    <span class="c1">// Consume from decoder</span>
    <span class="k">let</span> <span class="n">_consumed</span> <span class="o">=</span> <span class="n">decoder</span><span class="nf">.decode</span><span class="p">(</span><span class="o">&amp;</span><span class="n">frame</span><span class="p">)</span><span class="o">?</span><span class="p">;</span>
    <span class="k">while</span> <span class="k">let</span> <span class="nf">Some</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span> <span class="o">=</span> <span class="n">decoder</span><span class="nf">.flush</span><span class="p">()</span><span class="o">?</span> <span class="p">{</span>
        <span class="nd">println!</span><span class="p">(</span><span class="s">"rows={}, cols={}"</span><span class="p">,</span> <span class="n">batch</span><span class="nf">.num_rows</span><span class="p">(),</span> <span class="n">batch</span><span class="nf">.num_columns</span><span class="p">());</span>
    <span class="p">}</span>
    <span class="nf">Ok</span><span class="p">(())</span>
<span class="p">}</span>
</code></pre></div></div>
<p>The <code>SchemaStore</code> maps the incoming schema ID to the correct Avro writer schema so the decoder can perform projection/evolution against the reader schema. Confluent's wire format prefixes each message with a magic byte <code>0x00</code> followed by a big‚Äëendian 4‚Äëbyte schema ID. After decoding Avro messages, the <code>Decoder::flush()</code> method yields Arrow <code>RecordBatch</code>es suitable for vectorized processing.</p>
<p>A more advanced example can be found <a href="https://github.com/apache/arrow-rs/blob/main/arrow-avro/examples/decode_kafka_stream.rs">here</a>.</p>
<h3>Writing a Snappy Compressed Avro OCF file</h3>
<div class="language-rust highlighter-rouge"><div class="highlight"><pre class="highlight"><code data-lang="rust"><span class="k">use</span> <span class="nn">arrow_array</span><span class="p">::{</span><span class="n">Int64Array</span><span class="p">,</span> <span class="n">RecordBatch</span><span class="p">};</span>
<span class="k">use</span> <span class="nn">arrow_schema</span><span class="p">::{</span><span class="n">Schema</span><span class="p">,</span> <span class="n">Field</span><span class="p">,</span> <span class="n">DataType</span><span class="p">};</span>
<span class="k">use</span> <span class="nn">arrow_avro</span><span class="p">::</span><span class="nn">writer</span><span class="p">::{</span><span class="n">Writer</span><span class="p">,</span> <span class="n">WriterBuilder</span><span class="p">};</span>
<span class="k">use</span> <span class="nn">arrow_avro</span><span class="p">::</span><span class="nn">writer</span><span class="p">::</span><span class="nn">format</span><span class="p">::</span><span class="n">AvroOcfFormat</span><span class="p">;</span>
<span class="k">use</span> <span class="nn">arrow_avro</span><span class="p">::</span><span class="nn">compression</span><span class="p">::</span><span class="n">CompressionCodec</span><span class="p">;</span>
<span class="k">use</span> <span class="nn">std</span><span class="p">::{</span><span class="nn">sync</span><span class="p">::</span><span class="nb">Arc</span><span class="p">,</span> <span class="nn">fs</span><span class="p">::</span><span class="n">File</span><span class="p">,</span> <span class="nn">io</span><span class="p">::</span><span class="n">BufWriter</span><span class="p">};</span>

<span class="k">fn</span> <span class="nf">main</span><span class="p">()</span> <span class="k">-&gt;</span> <span class="nb">Result</span><span class="o">&lt;</span><span class="p">(),</span> <span class="nb">Box</span><span class="o">&lt;</span><span class="k">dyn</span> <span class="nn">std</span><span class="p">::</span><span class="nn">error</span><span class="p">::</span><span class="n">Error</span><span class="o">&gt;&gt;</span> <span class="p">{</span>
  <span class="k">let</span> <span class="n">schema</span> <span class="o">=</span> <span class="nn">Schema</span><span class="p">::</span><span class="nf">new</span><span class="p">(</span><span class="nd">vec!</span><span class="p">[</span><span class="nn">Field</span><span class="p">::</span><span class="nf">new</span><span class="p">(</span><span class="s">"id"</span><span class="p">,</span> <span class="nn">DataType</span><span class="p">::</span><span class="n">Int64</span><span class="p">,</span> <span class="kc">false</span><span class="p">)]);</span>
  <span class="k">let</span> <span class="n">batch</span> <span class="o">=</span> <span class="nn">RecordBatch</span><span class="p">::</span><span class="nf">try_new</span><span class="p">(</span>
    <span class="nn">Arc</span><span class="p">::</span><span class="nf">new</span><span class="p">(</span><span class="n">schema</span><span class="nf">.clone</span><span class="p">()),</span>
    <span class="nd">vec!</span><span class="p">[</span><span class="nn">Arc</span><span class="p">::</span><span class="nf">new</span><span class="p">(</span><span class="nn">Int64Array</span><span class="p">::</span><span class="nf">from</span><span class="p">(</span><span class="nd">vec!</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">]))],</span>
  <span class="p">)</span><span class="o">?</span><span class="p">;</span>
  <span class="k">let</span> <span class="n">file</span> <span class="o">=</span> <span class="nn">File</span><span class="p">::</span><span class="nf">create</span><span class="p">(</span><span class="s">"target/example.avro"</span><span class="p">)</span><span class="o">?</span><span class="p">;</span>

  <span class="c1">// Choose OCF block compression (e.g., None, Deflate, Snappy, Zstd)</span>
  <span class="k">let</span> <span class="k">mut</span> <span class="n">writer</span><span class="p">:</span> <span class="n">Writer</span><span class="o">&lt;</span><span class="n">_</span><span class="p">,</span> <span class="n">AvroOcfFormat</span><span class="o">&gt;</span> <span class="o">=</span> <span class="nn">WriterBuilder</span><span class="p">::</span><span class="nf">new</span><span class="p">(</span><span class="n">schema</span><span class="p">)</span>
      <span class="nf">.with_compression</span><span class="p">(</span><span class="nf">Some</span><span class="p">(</span><span class="nn">CompressionCodec</span><span class="p">::</span><span class="n">Snappy</span><span class="p">))</span>
      <span class="nf">.build</span><span class="p">(</span><span class="nn">BufWriter</span><span class="p">::</span><span class="nf">new</span><span class="p">(</span><span class="n">file</span><span class="p">))</span><span class="o">?</span><span class="p">;</span>
  <span class="n">writer</span><span class="nf">.write</span><span class="p">(</span><span class="o">&amp;</span><span class="n">batch</span><span class="p">)</span><span class="o">?</span><span class="p">;</span>
  <span class="n">writer</span><span class="nf">.finish</span><span class="p">()</span><span class="o">?</span><span class="p">;</span>
  <span class="nf">Ok</span><span class="p">(())</span>
<span class="p">}</span>
</code></pre></div></div>
<p>The example above configures an Avro OCF <code>Writer</code>. It constructs a <code>Writer&lt;_, AvroOcfFormat&gt;</code> using <code>WriterBuilder::new(schema)</code> and wraps a <code>File</code> in a <code>BufWriter</code> for efficient I/O. The call to <code>.with_compression(Some(CompressionCodec::Snappy))</code> enables block‚Äëlevel snappy compression. Finally, <code>writer.write(&amp;batch)?</code> serializes the batch as an Avro encoded block, and <code>writer.finish()?</code> flushes and finalizes the outputted file.</p>
<h2>Alternatives &amp; Benchmarks</h2>
<p>There are fundamentally two different approaches for bringing Avro into Arrow:</p>
<ol>
<li>Row‚Äëcentric approach, typical of general Avro libraries such as <code>apache-avro</code>, deserializes one record at a time into native Rust values (i.e., <code>Value</code> or Serde types) and then builds Arrow arrays from those values.</li>
<li>Vectorized approach, what <code>arrow-avro</code> provides, decodes directly into Arrow builders/arrays and emits <code>RecordBatch</code>es, avoiding most per‚Äërow overhead.</li>
</ol>
<p>This section compares the performance of both approaches using these <a href="https://github.com/jecsand838/arrow-rs/tree/blog-benches/arrow-avro/benches">Criterion benchmarks</a>.</p>
<h3>Read performance (1M)</h3>
<div style="display: flex; gap: 16px; justify-content: center; align-items: flex-start; padding: 5px 5px;">
<img src="/img/introducing-arrow-avro/read_violin_1m.svg"
        width="100%"
        alt="1M Row Read Violin Plot"
        style="background:#fff">
</div>
<h3>Read performance (10K)</h3>
<div style="display: flex; gap: 16px; justify-content: center; align-items: flex-start; padding: 5px 5px;">
<img src="/img/introducing-arrow-avro/read_violin_10k.svg"
        width="100%"
        alt="10K Row Read Violin Plot"
        style="background:#fff">
</div>
<h3>Write performance (1M)</h3>
<div style="display: flex; gap: 16px; justify-content: center; align-items: flex-start; padding: 5px 5px;">
<img src="/img/introducing-arrow-avro/write_violin_1m.svg"
        width="100%"
        alt="1M Row Write Violin Plot"
        style="background:#fff">
</div>
<h3>Write performance (10K)</h3>
<div style="display: flex; gap: 16px; justify-content: center; align-items: flex-start; padding: 5px 5px;">
<img src="/img/introducing-arrow-avro/write_violin_10k.svg"
        width="100%"
        alt="10K Row Write Violin Plot"
        style="background:#fff">
</div>
<p>Across benchmarks, the violin plots show lower medians and tighter spreads for <code>arrow-avro</code> on both read and write paths. The gap widens when per‚Äërow work dominates (i.e., 10K‚Äërow scenarios). At 1M rows, the distributions remain favorable to <code>arrow-avro</code>, reflecting better cache locality and fewer copies once decoding goes straight to Arrow arrays. The general behavior is consistent with <code>apache-avro</code>'s record‚Äëby‚Äërecord iteration and <code>arrow-avro</code>'s batch‚Äëoriented design.</p>
<p>The table below lists the cases we report in the figures:</p>
<ul>
<li>10K vs 1M rows for multiple data shapes.</li>
<li><strong>Read cases:</strong>
<ul>
<li><code>f8</code>: <em>Full schema, 8K batch size.</em>
Decode all four columns with batch_size = 8192.</li>
<li><code>f1</code>: <em>Full schema, 1K batch size.</em>
Decode all four columns with batch_size = 1024.</li>
<li><code>p8</code>: <em>Projected <code>{id,name}</code>, 8K batch size (pushdown).</em>
Decode only <code>id</code> and <code>name</code> with batch_size = 8192`.
<em>How projection is applied:</em>
<ul>
<li><code>arrow-avro/p8</code>: projection via reader schema (<code>ReaderBuilder::with_reader_schema(...)</code>) so decoding is column‚Äëpushed down in the Arrow‚Äëfirst reader.</li>
<li><code>apache-avro/p8</code>: projection via Avro reader schema (<code>AvroReader::with_schema(...)</code>) so the Avro library decodes only the projected fields.</li>
</ul>
</li>
<li><code>np</code>: <em>Projected <code>{id,name}</code>, no pushdown, 8K batch size.</em>
Both readers decode the full record (all four columns), materialize all arrays, then project down to <code>{id,name}</code> after decode. This models systems that can't push projection into the file/codec reader.</li>
</ul>
</li>
<li><strong>Write cases:</strong>
<ul>
<li><code>c</code> (cold): <em>Schema conversion each iteration.</em></li>
<li><code>h</code> (hot): <em>Avro JSON &quot;hot&quot; path.</em></li>
</ul>
</li>
<li>The resulting Apache‚ÄëAvro vs Arrow‚ÄëAvro medians with the computed speedup.</li>
</ul>
<h3>Benchmark Median Time Results (Apple Silicon Mac)</h3>
<table>
<thead>
<tr>
<th>Case</th>
<th align="right">apache-avro median</th>
<th align="right">arrow-avro median</th>
<th align="right">speedup</th>
</tr>
</thead>
<tbody>
<tr>
<td>R/f8/10K</td>
<td align="right">2.60 ms</td>
<td align="right">0.24 ms</td>
<td align="right">10.83x</td>
</tr>
<tr>
<td>R/p8/10K</td>
<td align="right">7.91 ms</td>
<td align="right">0.24 ms</td>
<td align="right">32.95x</td>
</tr>
<tr>
<td>R/f1/10K</td>
<td align="right">2.65 ms</td>
<td align="right">0.25 ms</td>
<td align="right">10.60x</td>
</tr>
<tr>
<td>R/np/10K</td>
<td align="right">2.62 ms</td>
<td align="right">0.25 ms</td>
<td align="right">10.48x</td>
</tr>
<tr>
<td>R/f8/1M</td>
<td align="right">267.21 ms</td>
<td align="right">27.91 ms</td>
<td align="right">9.57x</td>
</tr>
<tr>
<td>R/p8/1M</td>
<td align="right">791.79 ms</td>
<td align="right">26.28 ms</td>
<td align="right">30.13x</td>
</tr>
<tr>
<td>R/f1/1M</td>
<td align="right">262.93 ms</td>
<td align="right">28.25 ms</td>
<td align="right">9.31x</td>
</tr>
<tr>
<td>R/np/1M</td>
<td align="right">268.79 ms</td>
<td align="right">27.69 ms</td>
<td align="right">9.71x</td>
</tr>
<tr>
<td>W/c/10K</td>
<td align="right">4.78 ms</td>
<td align="right">0.27 ms</td>
<td align="right">17.70x</td>
</tr>
<tr>
<td>W/h/10K</td>
<td align="right">0.82 ms</td>
<td align="right">0.28 ms</td>
<td align="right">2.93x</td>
</tr>
<tr>
<td>W/c/1M</td>
<td align="right">485.58 ms</td>
<td align="right">36.97 ms</td>
<td align="right">13.13x</td>
</tr>
<tr>
<td>W/h/1M</td>
<td align="right">83.58 ms</td>
<td align="right">36.75 ms</td>
<td align="right">2.27x</td>
</tr>
</tbody>
</table>
<h2>Closing</h2>
<p><code>arrow-avro</code> brings a purpose‚Äëbuilt, vectorized bridge connecting Arrow-rs and Avro that covers Object Container Files (OCF), Single‚ÄëObject Encoding (SOE), and the Confluent/Apicurio Schema Registry wire formats. This means you can now keep your ingestion paths columnar for both batch files and streaming systems. The reader and writer APIs shown above are now available for you to use with the v57.0.0 release of <code>arrow-rs</code>.</p>
<p>This work is part of the ongoing Arrow‚Äërs effort to implement first-class Avro support in Rust. We'd love your feedback on real‚Äëworld use-cases, workloads, and integrations. We also welcome contributions, whether that's issues, benchmarks, or PRs. To follow along or help, open an <a href="https://github.com/apache/arrow-rs/issues">issue on GitHub</a> and/or track <a href="https://github.com/apache/arrow-rs/issues/4886">Add Avro Support</a> in <code>apache/arrow-rs</code>.</p>
<h3>Acknowledgments</h3>
<p>Special thanks to:</p>
<ul>
<li><a href="https://github.com/tustvold">tustvold</a> for laying an incredible zero-copy foundation.</li>
<li><a href="https://github.com/nathaniel-d-ef">nathaniel-d-ef</a> and <a href="https://github.com/elastiflow">ElastiFlow</a> for their numerous and invaluable project-wide contributions.</li>
<li><a href="https://github.com/veronica-m-ef">veronica-m-ef</a> for making Impala‚Äërelated contributions to the <code>Reader</code>.</li>
<li><a href="https://github.com/Supermetal-Inc">Supermetal</a> for contributions related to Apicurio Registry and Run-End Encoding type support.</li>
<li><a href="https://github.com/kumarlokesh">kumarlokesh</a> for contributing <code>Utf8View</code> support.</li>
<li><a href="https://github.com/alamb">alamb</a>, <a href="https://github.com/scovich">scovich</a>, <a href="https://github.com/mbrobbel">mbrobbel</a>, and <a href="https://github.com/klion26">klion26</a> for their thoughtful reviews, detailed feedback, and support throughout the development of <code>arrow-avro</code>.</li>
</ul>
<p>If you have any questions about this blog post, please feel free to contact the author, <a href="mailto:jecs838@gmail.com">Connor Sanders</a>.</p>]]></content><author><name>jecsand838</name></author><category term="application" /><summary type="html"><![CDATA[A new native Rust vectorized reader/writer for Avro to Arrow, with OCF, Single‚ÄëObject, and Confluent wire format support.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">3x-9x Faster Apache Parquet Footer Metadata Using a Custom Thrift Parser in Rust</title><link href="https://arrow.apache.org/blog/2025/10/23/rust-parquet-metadata/" rel="alternate" type="text/html" title="3x-9x Faster Apache Parquet Footer Metadata Using a Custom Thrift Parser in Rust" /><published>2025-10-23T00:00:00-04:00</published><updated>2025-10-23T00:00:00-04:00</updated><id>https://arrow.apache.org/blog/2025/10/23/rust-parquet-metadata</id><content type="html" xml:base="https://arrow.apache.org/blog/2025/10/23/rust-parquet-metadata/"><![CDATA[<!--

-->
<p><em>Editor‚Äôs Note: While <a href="https://arrow.apache.org/">Apache Arrow</a> and <a href="https://parquet.apache.org/">Apache Parquet</a> are separate projects,
the Arrow <a href="https://github.com/apache/arrow-rs">arrow-rs</a> repository hosts the development of the <a href="https://crates.io/crates/parquet">parquet</a> Rust
crate, a widely used and high-performance Parquet implementation.</em></p>
<h2>Summary</h2>
<p>Version <a href="https://crates.io/crates/parquet/57.0.0">57.0.0</a> of the <a href="https://crates.io/crates/parquet">parquet</a> Rust crate decodes metadata more than three times
faster than previous versions thanks to a new custom <a href="https://thrift.apache.org/">Apache Thrift</a> parser. The new
parser is both faster in all cases and enables further performance improvements not
possible with generated parsers, such as skipping unnecessary fields and selective parsing.</p>
<!-- Image source: https://docs.google.com/presentation/d/1WjX4t7YVj2kY14SqCpenGqNl_swjdHvPg86UeBT3IcY -->
<div style="display: flex; gap: 16px; justify-content: center; align-items: flex-start;">
  <img src="/img/rust-parquet-metadata/results.png" width="100%" class="img-responsive" alt="" aria-hidden="true">
</div>
<p><em>Figure 1:</em> Performance comparison of <a href="https://parquet.apache.org/">Apache Parquet</a> metadata parsing using a generated
Thrift parser (versions <code>56.2.0</code> and earlier) and the new
<a href="https://github.com/apache/arrow-rs/issues/5854">custom Thrift parser</a> in <a href="https://github.com/apache/arrow-rs">arrow-rs</a> version <a href="https://crates.io/crates/parquet/57.0.0">57.0.0</a>. No
changes are needed to the Parquet format itself.
See the <a href="https://github.com/alamb/parquet_footer_parsing">benchmark page</a> for more details.</p>
<!-- Image source: https://docs.google.com/presentation/d/1WjX4t7YVj2kY14SqCpenGqNl_swjdHvPg86UeBT3IcY -->
<div style="display: flex; gap: 16px; justify-content: center; align-items: flex-start;">
  <img src="/img/rust-parquet-metadata/scaling.png" width="100%" class="img-responsive" alt="Scaling behavior of custom Thrift parser" aria-hidden="true">
</div>
<p><em>Figure 2:</em> Speedup of the [custom Thrift decoder] for string and floating-point data types,
for <code>100</code>, <code>1000</code>, <code>10,000</code>, and <code>100,000</code> columns. The new parser is faster in all cases,
and the speedup is similar regardless of the number of columns. See the <a href="https://github.com/alamb/parquet_footer_parsing">benchmark page</a> for more details.</p>
<h2>Introduction: Parquet and the Importance of Metadata Parsing</h2>
<p><a href="https://parquet.apache.org/">Apache Parquet</a> is a popular columnar storage format
designed to be efficient for both storage and query processing. Parquet
files consist of a series of data pages, and a footer, as shown in Figure 3. The footer
contains metadata about the file, including schema, statistics, and other
information needed to decode the data pages.</p>
<!-- Image source: https://docs.google.com/presentation/d/1WjX4t7YVj2kY14SqCpenGqNl_swjdHvPg86UeBT3IcY -->
<div style="display: flex; gap: 16px; justify-content: center; align-items: flex-start;">
  <img src="/img/rust-parquet-metadata/parquet.png" width="100%" class="img-responsive" alt="Physical File Structure of Parquet" aria-hidden="true">
</div>
<p><em>Figure 3:</em> Structure of a Parquet file showing the header, data pages, and footer metadata.</p>
<p>Getting information stored in the footer is typically the first step in reading
a Parquet file, as it is required to interpret the data pages. <em>Parsing</em> the
footer is often performance critical:</p>
<ul>
<li>When reading from fast local storage, such as modern NVMe SSDs, footer parsing
must be completed to know what data pages to read, placing it directly on the critical
I/O path.</li>
<li>Footer parsing scales linearly with the number of columns and row groups in a
Parquet file and thus can be a bottleneck for tables with many columns or files
with many row groups.</li>
<li>Even in systems that cache the parsed footer in memory (see <a href="https://datafusion.apache.org/blog/2025/08/15/external-parquet-indexes/">Using
External Indexes, Metadata Stores, Catalogs and Caches to Accelerate Queries
on Apache Parquet</a>), the footer must still be parsed on cache miss.</li>
</ul>
<!-- Image source: https://docs.google.com/presentation/d/1WjX4t7YVj2kY14SqCpenGqNl_swjdHvPg86UeBT3IcY -->
<div style="display: flex; gap: 16px; justify-content: center; align-items: flex-start;">
  <img src="/img/rust-parquet-metadata/flow.png" width="100%" class="img-responsive" alt="Typical Parquet processing flow" aria-hidden="true">
</div>
<p><em>Figure 4:</em> Typical processing flow for Parquet files for stateless and stateful
systems. Stateless engines read the footer on every query, so the time taken to
parse the footer directly adds to query latency. Stateful systems cache some or
all of the parsed footer in advance of queries.</p>
<p>The speed of parsing metadata has grown even more important as Parquet spreads
throughout the data ecosystem and is used for more latency-sensitive workloads such
as observability, interactive analytics, and single-point
lookups for Retrieval-Augmented Generation (RAG) applications feeding LLMs.
As overall query times decrease, the proportion spent on footer parsing increases.</p>
<h2>Background: Apache Thrift</h2>
<p>Parquet stores metadata using <a href="https://thrift.apache.org/">Apache Thrift</a>, a framework for
network data types and service interfaces. It includes a <a href="https://thrift.apache.org/docs/idl">data definition
language</a> similar to <a href="https://developers.google.com/protocol-buffers">Protocol Buffers</a>. Thrift definition files describe data
types in a language-neutral way, and systems typically use code generators to
automatically create code for a specific programming language to read and write
those data types.</p>
<p>The <a href="https://github.com/apache/parquet-format/blob/master/src/main/thrift/parquet.thrift">parquet.thrift</a> file defines the format of the metadata
serialized at the end of each Parquet file in the <a href="https://github.com/apache/thrift/blob/master/doc/specs/thrift-compact-protocol.md">Thrift Compact
protocol</a>, as shown below in Figure 5. The binary encoding is &quot;variable-length&quot;,
meaning that the length of each element depends on its content, not
just its type. Smaller-valued primitive types are encoded in fewer bytes than
larger values, and strings and lists are stored inline, prefixed with their
length.</p>
<p>This encoding is space-efficient but, due to being variable-length, does not
support random access: it is not possible to locate a particular field without
scanning all previous fields. Other formats such as <a href="https://google.github.io/flatbuffers/">FlatBuffers</a> provide
random-access parsing and have been <a href="https://lists.apache.org/thread/j9qv5vyg0r4jk6tbm6sqthltly4oztd3">proposed as alternatives</a> given their
theoretical performance advantages. However, changing the Parquet format is a
significant undertaking, requires buy-in from the community and ecosystem,
and would likely take years to be adopted.</p>
<!-- Image source: https://docs.google.com/presentation/d/1WjX4t7YVj2kY14SqCpenGqNl_swjdHvPg86UeBT3IcY -->
<div style="display: flex; gap: 16px; justify-content: center; align-items: flex-start;">
  <img src="/img/rust-parquet-metadata/thrift-compact-encoding.png" width="100%" class="img-responsive" alt="Thrift Compact Encoding Illustration" aria-hidden="true">
</div>
<p><em>Figure 5:</em> Parquet metadata is serialized using the <a href="https://github.com/apache/thrift/blob/master/doc/specs/thrift-compact-protocol.md">Thrift Compact protocol</a>.
Each field is stored using a variable number of bytes that depends on its value.
Primitive types use a variable-length encoding and strings and lists are
prefixed with their lengths.</p>
<p>Despite Thrift's very real disadvantage due to lack of random access, software
optimizations are much easier to deploy than format changes. <a href="https://xiangpeng.systems/">Xiangpeng Hao</a>'s
previous analysis theorized significant (2x‚Äì4x) potential performance
improvements simply by optimizing the implementation of Parquet footer parsing
(see <a href="https://www.influxdata.com/blog/how-good-parquet-wide-tables/">How Good is Parquet for Wide Tables (Machine Learning
Workloads) Really?</a> for more details).</p>
<h2>Processing Thrift Using Generated Parsers</h2>
<p><em>Parsing</em> Parquet metadata is the process of decoding the Thrift-encoded bytes
into in-memory structures that can be used for computation. Most Parquet
implementations use one of the existing <a href="https://thrift.apache.org/lib/">Thrift compilers</a> to generate a parser
that converts Thrift binary data into generated code structures, and then copy
relevant portions of those generated structures into API-level structures.
For example, the <a href="https://github.com/apache/arrow/blob/e1f727cbb447d2385949a54d8f4be2fdc6cefe29/cpp/src/parquet">C/C++ Parquet implementation</a> includes a <a href="https://github.com/apache/arrow/blob/e1f727cbb447d2385949a54d8f4be2fdc6cefe29/cpp/build-support/update-thrift.sh#L23">two</a>-<a href="https://github.com/apache/arrow/blob/e1f727cbb447d2385949a54d8f4be2fdc6cefe29/cpp/src/parquet/thrift_internal.h#L56">step</a> process,
as does <a href="https://github.com/apache/parquet-java/blob/0fea3e1e22fffb0a25193e3efb9a5d090899458a/parquet-format-structures/pom.xml#L69-L88">parquet-java</a>. <a href="https://github.com/duckdb/duckdb/blob/8f512187537c65d36ce6d6f562b75a37e8d4ee54/third_party/parquet/parquet_types.h#L1-L6">DuckDB</a> also contains a Thrift compiler‚Äìgenerated
parser.</p>
<p>In versions <code>56.2.0</code> and earlier, the Apache Arrow Rust implementation used the
same pattern. The <a href="https://docs.rs/parquet/56.2.0/parquet/format/index.html">format</a> module contains a parser generated by the <a href="https://crates.io/crates/thrift">thrift
crate</a> and the <a href="https://github.com/apache/parquet-format/blob/master/src/main/thrift/parquet.thrift">parquet.thrift</a> definition. Parsing metadata involves:</p>
<ol>
<li>Invoke the generated parser on the Thrift binary data, producing
generated in-memory structures (e.g., <a href="https://docs.rs/parquet/56.2.0/parquet/format/struct.FileMetaData.html"><code>struct FileMetaData</code></a>), then</li>
<li>Copy the relevant fields into a more user-friendly representation,
<a href="https://docs.rs/parquet/56.2.0/parquet/file/metadata/struct.ParquetMetaData.html"><code>ParquetMetadata</code></a>.</li>
</ol>
<!-- Image source: https://docs.google.com/presentation/d/1WjX4t7YVj2kY14SqCpenGqNl_swjdHvPg86UeBT3IcY -->
<div style="display: flex; gap: 16px; justify-content: center; align-items: flex-start;">
  <img src="/img/rust-parquet-metadata/original-pipeline.png" width="100%" class="img-responsive" alt="Original Parquet Parsing Pipeline" aria-hidden="true">
</div>
<p><em>Figure 6:</em> Two-step process to read Parquet metadata: A parser created with the
<code>thrift</code> crate and <code>parquet.thrift</code> parses the metadata bytes
into generated in-memory structures. These structures are then converted into
API objects.</p>
<p>The parsers generated by standard Thrift compilers typically parse <em>all</em> fields
in a single pass over the Thrift-encoded bytes, copying data into in-memory,
heap-allocated structures (e.g., Rust <a href="https://doc.rust-lang.org/std/vec/struct.Vec.html"><code>Vec</code></a>, or C++ <a href="https://en.cppreference.com/w/cpp/container/vector.html"><code>std::vector</code></a>) as shown
in Figure 7 below.</p>
<p>Parsing all fields is straightforward and a good default
choice given Thrift's original design goal of encoding network messages.
Network messages typically don't contain extra information irrelevant for receivers;
however, Parquet metadata often <em>does</em> contain information
that is not needed for a particular query. In such cases, parsing the entire
metadata into in-memory structures is wasteful.</p>
<p>For example, a query on a file with 1,000 columns that reads
only 10 columns and has a single column predicate
(e.g., <code>time &gt; now() - '1 minute'</code>) only needs</p>
<ol>
<li><a href="https://github.com/apache/parquet-format/blob/9fd57b59e0ce1a82a69237dcf8977d3e72a2965d/src/main/thrift/parquet.thrift#L912"><code>Statistics</code></a> (or <a href="https://github.com/apache/parquet-format/blob/9fd57b59e0ce1a82a69237dcf8977d3e72a2965d/src/main/thrift/parquet.thrift#L1163"><code>ColumnIndex</code></a>) for the <code>time</code> column</li>
<li><a href="https://github.com/apache/parquet-format/blob/9fd57b59e0ce1a82a69237dcf8977d3e72a2965d/src/main/thrift/parquet.thrift#L958"><code>ColumnChunk</code></a> information for the 10 selected columns</li>
</ol>
<p>The default strategy to parse (allocating and copying) all statistics and all
<code>ColumnChunks</code> results in creating 999 more statistics and 990 more <code>ColumnChunks</code>
than necessary. As discussed above, given the
variable encoding used for the metadata, all metadata bytes must still be
fetched and scanned; however, CPUs are (very) fast at scanning data, and
skipping <em>parsing</em> of unneeded fields speeds up overall metadata performance
significantly.</p>
<!-- Image source: https://docs.google.com/presentation/d/1WjX4t7YVj2kY14SqCpenGqNl_swjdHvPg86UeBT3IcY -->
<div style="display: flex; gap: 16px; justify-content: center; align-items: flex-start;">
  <img src="/img/rust-parquet-metadata/thrift-parsing-allocations.png" width="100%" class="img-responsive" alt="Thrift Parsing Allocations" aria-hidden="true">
</div>
<p><em>Figure 7:</em> Generated Thrift parsers typically parse encoded bytes into
structures requiring many small heap allocations, which are expensive.</p>
<h2>New Design: Custom Thrift Parser</h2>
<p>As is typical of generated code, opportunities for specializing
the behavior of generated Thrift parsers is limited:</p>
<ol>
<li>It is not easy to modify (it is re-generated from the
Thrift definitions when they change and carries the warning
<code>/* DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING */</code>).</li>
<li>It typically maps one-to-one with Thrift definitions, limiting
additional optimizations such as zero-copy parsing, field
skipping, and amortized memory allocation strategies.</li>
<li>Its API is very stable (hard to change), which is important for easy maintenance when a large number
of projects are built using the <a href="https://crates.io/crates/thrift">thrift crate</a>. For example, the
<a href="https://crates.io/crates/thrift/0.17.0">last release of the Rust <code>thrift</code> crate</a> was almost three years ago at
the time of this writing.</li>
</ol>
<p>These limitations are a consequence of the Thrift project's design goals: general purpose
code that is easy to embed in a wide variety of other projects, rather than
any fundamental limitation of the Thrift format.
Given our goal of fast Parquet metadata parsing, we needed
a custom, easier to optimize parser, to convert Thrift binary directly into the needed
structures (Figure 8). Since arrow-rs already did some postprocessing on the generated code
and included a custom implementation of the compact protocol api, this change
to a completely custom parser was a natural next step.</p>
<!-- Image source: https://docs.google.com/presentation/d/1WjX4t7YVj2kY14SqCpenGqNl_swjdHvPg86UeBT3IcY -->
<div style="display: flex; gap: 16px; justify-content: center; align-items: flex-start;">
  <img src="/img/rust-parquet-metadata/new-pipeline.png" width="100%" class="img-responsive" alt="New Parquet Parsing Pipeline" aria-hidden="true">
</div>
<p><em>Figure 8:</em> One-step Parquet metadata parsing using a custom Thrift parser. The
Thrift binary is parsed directly into the desired in-memory representation with
highly optimized code.</p>
<p>Our new custom parser is optimized for the specific subset of Thrift used by
Parquet and contains various performance optimizations, such as careful
memory allocation. The largest initial speedup came from removing
intermediate structures and directly creating the needed in-memory representation.
We also carefully hand-optimized several performance-critical code paths (see <a href="https://github.com/apache/arrow-rs/pull/8574">#8574</a>,
<a href="https://github.com/apache/arrow-rs/pull/8587">#8587</a>, and <a href="https://github.com/apache/arrow-rs/pull/8599">#8599</a>).</p>
<h3>Maintainability</h3>
<p>The largest concern with a custom parser is that it is more difficult
to maintain than generated parsers because the custom parser must be updated to
reflect any changes to <a href="https://github.com/apache/parquet-format/blob/master/src/main/thrift/parquet.thrift">parquet.thrift</a>. This is a growing concern given the
resurgent interest in Parquet and the recent addition of new features such as
<a href="https://github.com/apache/parquet-format/blob/master/Geospatial.md">Geospatial</a> and <a href="https://github.com/apache/parquet-format/blob/master/VariantEncoding.md">Variant</a> types.</p>
<p>Thankfully, after discussions with the community, <a href="https://github.com/jhorstmann">J√∂rn Horstmann</a> developed
a <a href="https://github.com/jhorstmann/compact-thrift">Rust macro based approach</a> for generating code with annotated Rust structs
that closely resemble the Thrift definitions while permitting additional hand
optimization where necessary. This approach is similar to the <a href="https://serde.rs/">serde</a> crate
where generic implementations can be generated with <code>#[derive]</code> annotations and
specialized serialization is written by hand where needed. <a href="https://github.com/etseidl">Ed Seidl</a> then
rewrote the metadata parsing code in the <a href="https://crates.io/crates/parquet">parquet</a> crate using these macros.
Please see the <a href="https://github.com/apache/arrow-rs/pull/8530">final PR</a> for details of the level of effort involved.</p>
<p>For example, here is the original Thrift definition of the <a href="https://github.com/apache/parquet-format/blob/9fd57b59e0ce1a82a69237dcf8977d3e72a2965d/src/main/thrift/parquet.thrift#L1254C1-L1314C2"><code>FileMetaData</code></a> structure (comments omitted for brevity):</p>
<div class="language-thrift highlighter-rouge"><div class="highlight"><pre class="highlight"><code data-lang="thrift">struct FileMetaData {
  1: required i32 version
  2: required list&lt;SchemaElement&gt; schema;
  3: required i64 num_rows
  4: required list&lt;RowGroup&gt; row_groups
  5: optional list&lt;KeyValue&gt; key_value_metadata
  6: optional string created_by
  7: optional list&lt;ColumnOrder&gt; column_orders;
  8: optional EncryptionAlgorithm encryption_algorithm
  9: optional binary footer_signing_key_metadata
}
</code></pre></div></div>
<p>And here (<a href="https://github.com/apache/arrow-rs/blob/02fa779a9cb122c5218293be3afb980832701683/parquet/src/file/metadata/thrift_gen.rs#L146-L158">source</a>) is the corresponding Rust structure using the Thrift macros (before Ed wrote a custom version in <a href="https://github.com/apache/arrow-rs/pull/8574">#8574</a>):</p>
<div class="language-rust highlighter-rouge"><div class="highlight"><pre class="highlight"><code data-lang="rust"><span class="nd">thrift_struct!</span><span class="p">(</span>
<span class="k">struct</span> <span class="n">FileMetaData</span><span class="o">&lt;</span><span class="nv">'a</span><span class="o">&gt;</span> <span class="p">{</span>
<span class="mi">1</span><span class="p">:</span> <span class="n">required</span> <span class="nb">i32</span> <span class="n">version</span>
<span class="mi">2</span><span class="p">:</span> <span class="n">required</span> <span class="n">list</span><span class="o">&lt;</span><span class="nv">'a</span><span class="o">&gt;&lt;</span><span class="n">SchemaElement</span><span class="o">&gt;</span> <span class="n">schema</span><span class="p">;</span>
<span class="mi">3</span><span class="p">:</span> <span class="n">required</span> <span class="nb">i64</span> <span class="n">num_rows</span>
<span class="mi">4</span><span class="p">:</span> <span class="n">required</span> <span class="n">list</span><span class="o">&lt;</span><span class="nv">'a</span><span class="o">&gt;&lt;</span><span class="n">RowGroup</span><span class="o">&gt;</span> <span class="n">row_groups</span>
<span class="mi">5</span><span class="p">:</span> <span class="n">optional</span> <span class="n">list</span><span class="o">&lt;</span><span class="n">KeyValue</span><span class="o">&gt;</span> <span class="n">key_value_metadata</span>
<span class="mi">6</span><span class="p">:</span> <span class="n">optional</span> <span class="n">string</span><span class="o">&lt;</span><span class="nv">'a</span><span class="o">&gt;</span> <span class="n">created_by</span>
<span class="mi">7</span><span class="p">:</span> <span class="n">optional</span> <span class="n">list</span><span class="o">&lt;</span><span class="n">ColumnOrder</span><span class="o">&gt;</span> <span class="n">column_orders</span><span class="p">;</span>
<span class="mi">8</span><span class="p">:</span> <span class="n">optional</span> <span class="n">EncryptionAlgorithm</span> <span class="n">encryption_algorithm</span>
<span class="mi">9</span><span class="p">:</span> <span class="n">optional</span> <span class="n">binary</span><span class="o">&lt;</span><span class="nv">'a</span><span class="o">&gt;</span> <span class="n">footer_signing_key_metadata</span>
<span class="p">}</span>
<span class="p">);</span>
</code></pre></div></div>
<p>This system makes it easy to see the correspondence between the Thrift
definition and the Rust structure, and it is straightforward to support newly added
features such as <code>GeospatialStatistics</code>. The carefully hand-
optimized parsers for the most performance-critical structures, such as
<code>RowGroupMetaData</code> and <code>ColumnChunkMetaData</code>, are harder‚Äîthough still
straightforward‚Äîto update (see <a href="https://github.com/apache/arrow-rs/pull/8587">#8587</a>). However, those structures are also less
likely to change frequently.</p>
<h3>Future Improvements</h3>
<p>With the custom parser in place, we are working on additional improvements:</p>
<ul>
<li>Implementing special &quot;skip&quot; indexes to skip directly to the parts of the metadata
that are needed for a particular query, such as the row group offsets.</li>
<li>Selectively decoding only the statistics for columns that are needed for a particular query.</li>
<li>Potentially contributing the macros back to the thrift crate.</li>
</ul>
<h3>Conclusion</h3>
<p>We believe metadata parsing in many open source Parquet
readers is slow primarily because they use parsers automatically generated by Thrift
compilers, which are not optimized for Parquet metadata parsing. By writing a
custom parser, we significantly sped up metadata parsing in the
<a href="https://crates.io/crates/parquet">parquet</a> Rust crate, which is widely used in the <a href="https://arrow.apache.org/">Apache Arrow</a> ecosystem.</p>
<p>While this is not the first open source custom Thrift parser for Parquet
metadata (<a href="https://github.com/rapidsai/cudf/blob/branch-25.12/cpp/src/io/parquet/compact_protocol_reader.hpp">CUDF has had one</a> for many years), we hope that our results will
encourage additional Parquet implementations to consider similar optimizations.
The approach and optimizations we describe in this post are likely applicable to
Parquet implementations in other languages, such as C++ and Java.</p>
<p>Previously, efforts like this were only possible at well-financed commercial
enterprises. On behalf of the arrow-rs and Parquet contributors, we are excited
to share this technology with the community in the upcoming <a href="https://crates.io/crates/parquet/57.0.0">57.0.0</a> release and
invite you to <a href="https://github.com/apache/arrow-rs/blob/main/CONTRIBUTING.md">come join us</a> and help make it even better!</p>]]></content><author><name>alamb</name></author><category term="release" /><summary type="html"><![CDATA[Editor‚Äôs Note: While Apache Arrow and Apache Parquet are separate projects, the Arrow arrow-rs repository hosts the development of the parquet Rust crate, a widely used and high-performance Parquet implementation. Summary Version 57.0.0 of the parquet Rust crate decodes metadata more than three times faster than previous versions thanks to a new custom Apache Thrift parser. The new parser is both faster in all cases and enables further performance improvements not possible with generated parsers, such as skipping unnecessary fields and selective parsing. Figure 1: Performance comparison of Apache Parquet metadata parsing using a generated Thrift parser (versions 56.2.0 and earlier) and the new custom Thrift parser in arrow-rs version 57.0.0. No changes are needed to the Parquet format itself. See the benchmark page for more details. Figure 2: Speedup of the [custom Thrift decoder] for string and floating-point data types, for 100, 1000, 10,000, and 100,000 columns. The new parser is faster in all cases, and the speedup is similar regardless of the number of columns. See the benchmark page for more details. Introduction: Parquet and the Importance of Metadata Parsing Apache Parquet is a popular columnar storage format designed to be efficient for both storage and query processing. Parquet files consist of a series of data pages, and a footer, as shown in Figure 3. The footer contains metadata about the file, including schema, statistics, and other information needed to decode the data pages. Figure 3: Structure of a Parquet file showing the header, data pages, and footer metadata. Getting information stored in the footer is typically the first step in reading a Parquet file, as it is required to interpret the data pages. Parsing the footer is often performance critical: When reading from fast local storage, such as modern NVMe SSDs, footer parsing must be completed to know what data pages to read, placing it directly on the critical I/O path. Footer parsing scales linearly with the number of columns and row groups in a Parquet file and thus can be a bottleneck for tables with many columns or files with many row groups. Even in systems that cache the parsed footer in memory (see Using External Indexes, Metadata Stores, Catalogs and Caches to Accelerate Queries on Apache Parquet), the footer must still be parsed on cache miss. Figure 4: Typical processing flow for Parquet files for stateless and stateful systems. Stateless engines read the footer on every query, so the time taken to parse the footer directly adds to query latency. Stateful systems cache some or all of the parsed footer in advance of queries. The speed of parsing metadata has grown even more important as Parquet spreads throughout the data ecosystem and is used for more latency-sensitive workloads such as observability, interactive analytics, and single-point lookups for Retrieval-Augmented Generation (RAG) applications feeding LLMs. As overall query times decrease, the proportion spent on footer parsing increases. Background: Apache Thrift Parquet stores metadata using Apache Thrift, a framework for network data types and service interfaces. It includes a data definition language similar to Protocol Buffers. Thrift definition files describe data types in a language-neutral way, and systems typically use code generators to automatically create code for a specific programming language to read and write those data types. The parquet.thrift file defines the format of the metadata serialized at the end of each Parquet file in the Thrift Compact protocol, as shown below in Figure 5. The binary encoding is &quot;variable-length&quot;, meaning that the length of each element depends on its content, not just its type. Smaller-valued primitive types are encoded in fewer bytes than larger values, and strings and lists are stored inline, prefixed with their length. This encoding is space-efficient but, due to being variable-length, does not support random access: it is not possible to locate a particular field without scanning all previous fields. Other formats such as FlatBuffers provide random-access parsing and have been proposed as alternatives given their theoretical performance advantages. However, changing the Parquet format is a significant undertaking, requires buy-in from the community and ecosystem, and would likely take years to be adopted. Figure 5: Parquet metadata is serialized using the Thrift Compact protocol. Each field is stored using a variable number of bytes that depends on its value. Primitive types use a variable-length encoding and strings and lists are prefixed with their lengths. Despite Thrift's very real disadvantage due to lack of random access, software optimizations are much easier to deploy than format changes. Xiangpeng Hao's previous analysis theorized significant (2x‚Äì4x) potential performance improvements simply by optimizing the implementation of Parquet footer parsing (see How Good is Parquet for Wide Tables (Machine Learning Workloads) Really? for more details). Processing Thrift Using Generated Parsers Parsing Parquet metadata is the process of decoding the Thrift-encoded bytes into in-memory structures that can be used for computation. Most Parquet implementations use one of the existing Thrift compilers to generate a parser that converts Thrift binary data into generated code structures, and then copy relevant portions of those generated structures into API-level structures. For example, the C/C++ Parquet implementation includes a two-step process, as does parquet-java. DuckDB also contains a Thrift compiler‚Äìgenerated parser. In versions 56.2.0 and earlier, the Apache Arrow Rust implementation used the same pattern. The format module contains a parser generated by the thrift crate and the parquet.thrift definition. Parsing metadata involves: Invoke the generated parser on the Thrift binary data, producing generated in-memory structures (e.g., struct FileMetaData), then Copy the relevant fields into a more user-friendly representation, ParquetMetadata. Figure 6: Two-step process to read Parquet metadata: A parser created with the thrift crate and parquet.thrift parses the metadata bytes into generated in-memory structures. These structures are then converted into API objects. The parsers generated by standard Thrift compilers typically parse all fields in a single pass over the Thrift-encoded bytes, copying data into in-memory, heap-allocated structures (e.g., Rust Vec, or C++ std::vector) as shown in Figure 7 below. Parsing all fields is straightforward and a good default choice given Thrift's original design goal of encoding network messages. Network messages typically don't contain extra information irrelevant for receivers; however, Parquet metadata often does contain information that is not needed for a particular query. In such cases, parsing the entire metadata into in-memory structures is wasteful. For example, a query on a file with 1,000 columns that reads only 10 columns and has a single column predicate (e.g., time &gt; now() - '1 minute') only needs Statistics (or ColumnIndex) for the time column ColumnChunk information for the 10 selected columns The default strategy to parse (allocating and copying) all statistics and all ColumnChunks results in creating 999 more statistics and 990 more ColumnChunks than necessary. As discussed above, given the variable encoding used for the metadata, all metadata bytes must still be fetched and scanned; however, CPUs are (very) fast at scanning data, and skipping parsing of unneeded fields speeds up overall metadata performance significantly. Figure 7: Generated Thrift parsers typically parse encoded bytes into structures requiring many small heap allocations, which are expensive. New Design: Custom Thrift Parser As is typical of generated code, opportunities for specializing the behavior of generated Thrift parsers is limited: It is not easy to modify (it is re-generated from the Thrift definitions when they change and carries the warning /* DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING */). It typically maps one-to-one with Thrift definitions, limiting additional optimizations such as zero-copy parsing, field skipping, and amortized memory allocation strategies. Its API is very stable (hard to change), which is important for easy maintenance when a large number of projects are built using the thrift crate. For example, the last release of the Rust thrift crate was almost three years ago at the time of this writing. These limitations are a consequence of the Thrift project's design goals: general purpose code that is easy to embed in a wide variety of other projects, rather than any fundamental limitation of the Thrift format. Given our goal of fast Parquet metadata parsing, we needed a custom, easier to optimize parser, to convert Thrift binary directly into the needed structures (Figure 8). Since arrow-rs already did some postprocessing on the generated code and included a custom implementation of the compact protocol api, this change to a completely custom parser was a natural next step. Figure 8: One-step Parquet metadata parsing using a custom Thrift parser. The Thrift binary is parsed directly into the desired in-memory representation with highly optimized code. Our new custom parser is optimized for the specific subset of Thrift used by Parquet and contains various performance optimizations, such as careful memory allocation. The largest initial speedup came from removing intermediate structures and directly creating the needed in-memory representation. We also carefully hand-optimized several performance-critical code paths (see #8574, #8587, and #8599). Maintainability The largest concern with a custom parser is that it is more difficult to maintain than generated parsers because the custom parser must be updated to reflect any changes to parquet.thrift. This is a growing concern given the resurgent interest in Parquet and the recent addition of new features such as Geospatial and Variant types. Thankfully, after discussions with the community, J√∂rn Horstmann developed a Rust macro based approach for generating code with annotated Rust structs that closely resemble the Thrift definitions while permitting additional hand optimization where necessary. This approach is similar to the serde crate where generic implementations can be generated with #[derive] annotations and specialized serialization is written by hand where needed. Ed Seidl then rewrote the metadata parsing code in the parquet crate using these macros. Please see the final PR for details of the level of effort involved. For example, here is the original Thrift definition of the FileMetaData structure (comments omitted for brevity): struct FileMetaData { 1: required i32 version 2: required list&lt;SchemaElement&gt; schema; 3: required i64 num_rows 4: required list&lt;RowGroup&gt; row_groups 5: optional list&lt;KeyValue&gt; key_value_metadata 6: optional string created_by 7: optional list&lt;ColumnOrder&gt; column_orders; 8: optional EncryptionAlgorithm encryption_algorithm 9: optional binary footer_signing_key_metadata } And here (source) is the corresponding Rust structure using the Thrift macros (before Ed wrote a custom version in #8574): thrift_struct!( struct FileMetaData&lt;'a&gt; { 1: required i32 version 2: required list&lt;'a&gt;&lt;SchemaElement&gt; schema; 3: required i64 num_rows 4: required list&lt;'a&gt;&lt;RowGroup&gt; row_groups 5: optional list&lt;KeyValue&gt; key_value_metadata 6: optional string&lt;'a&gt; created_by 7: optional list&lt;ColumnOrder&gt; column_orders; 8: optional EncryptionAlgorithm encryption_algorithm 9: optional binary&lt;'a&gt; footer_signing_key_metadata } ); This system makes it easy to see the correspondence between the Thrift definition and the Rust structure, and it is straightforward to support newly added features such as GeospatialStatistics. The carefully hand- optimized parsers for the most performance-critical structures, such as RowGroupMetaData and ColumnChunkMetaData, are harder‚Äîthough still straightforward‚Äîto update (see #8587). However, those structures are also less likely to change frequently. Future Improvements With the custom parser in place, we are working on additional improvements: Implementing special &quot;skip&quot; indexes to skip directly to the parts of the metadata that are needed for a particular query, such as the row group offsets. Selectively decoding only the statistics for columns that are needed for a particular query. Potentially contributing the macros back to the thrift crate. Conclusion We believe metadata parsing in many open source Parquet readers is slow primarily because they use parsers automatically generated by Thrift compilers, which are not optimized for Parquet metadata parsing. By writing a custom parser, we significantly sped up metadata parsing in the parquet Rust crate, which is widely used in the Apache Arrow ecosystem. While this is not the first open source custom Thrift parser for Parquet metadata (CUDF has had one for many years), we hope that our results will encourage additional Parquet implementations to consider similar optimizations. The approach and optimizations we describe in this post are likely applicable to Parquet implementations in other languages, such as C++ and Java. Previously, efforts like this were only possible at well-financed commercial enterprises. On behalf of the arrow-rs and Parquet contributors, we are excited to share this technology with the community in the upcoming 57.0.0 release and invite you to come join us and help make it even better!]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Apache Arrow ADBC 20 (Libraries) Release</title><link href="https://arrow.apache.org/blog/2025/09/12/adbc-20-release/" rel="alternate" type="text/html" title="Apache Arrow ADBC 20 (Libraries) Release" /><published>2025-09-12T00:00:00-04:00</published><updated>2025-09-12T00:00:00-04:00</updated><id>https://arrow.apache.org/blog/2025/09/12/adbc-20-release</id><content type="html" xml:base="https://arrow.apache.org/blog/2025/09/12/adbc-20-release/"><![CDATA[<!--

-->
<p>The Apache Arrow team is pleased to announce the version 20 release of
the Apache Arrow ADBC libraries. This release includes <a href="https://github.com/apache/arrow-adbc/milestone/24"><strong>44
resolved issues</strong></a> from <a href="#contributors"><strong>29 distinct contributors</strong></a>.</p>
<p>This is a release of the <strong>libraries</strong>, which are at version 20.  The
<a href="https://arrow.apache.org/adbc/20/format/specification.html"><strong>API specification</strong></a> is versioned separately and is at
version 1.1.0.</p>
<p>The subcomponents are versioned independently:</p>
<ul>
<li>C/C++/GLib/Go/Python/Ruby: 1.8.0</li>
<li>C#: 0.20.0</li>
<li>Java: 0.20.0</li>
<li>R: 0.20.0</li>
<li>Rust: 0.20.0</li>
</ul>
<p>The release notes below are not exhaustive and only expose selected
highlights of the release. Many other bugfixes and improvements have
been made: we refer you to the <a href="https://github.com/apache/arrow-adbc/blob/apache-arrow-adbc-20/CHANGELOG.md">complete changelog</a>.</p>
<h2>Release Highlights</h2>
<p>Driver managers now support loading driver manifests.  To learn more about
this feature, please see the <a href="https://arrow.apache.org/adbc/current/format/driver_manifests.html#driver-manifests">documentation</a>.</p>
<p>The Rust crates were reorganized.  <strong>This is a breaking change.</strong>  Now,
FFI-related code is part of <code>adbc_ffi</code> and the driver manager is part of
<code>adbc_driver_manager</code>.  Previously these were features of a single crate
<code>adbc_core</code>, which now only contains API definitions
(<a href="https://github.com/apache/arrow-adbc/pull/3381">#3381</a>,
<a href="https://github.com/apache/arrow-adbc/pull/3197">#3197</a>).  Also, some enums
are no longer marked as <code>#[non_exhaustive]</code>
(<a href="https://github.com/apache/arrow-adbc/pull/3245">#3245</a>).</p>
<p>The Java JNI bindings support a few more features
(<a href="https://github.com/apache/arrow-adbc/pull/3373">#3373</a>,
<a href="https://github.com/apache/arrow-adbc/pull/3372">#3372</a>,
<a href="https://github.com/apache/arrow-adbc/pull/3370">#3370</a>,
<a href="https://github.com/apache/arrow-adbc/pull/3348">#3348</a>).</p>
<p>The BigQuery driver properly uses microsecond timestamps
(<a href="https://github.com/apache/arrow-adbc/pull/3364">#3364</a>), has an improved
error message if your user account lacks the proper permissions
(<a href="https://github.com/apache/arrow-adbc/pull/3297">#3297</a>), properly handles
nested data (<a href="https://github.com/apache/arrow-adbc/pull/3240">#3240</a>), and
supports service account impersonation
(<a href="https://github.com/apache/arrow-adbc/pull/3174">#3174</a>).  The C#
Databricks/HiveServer2 Thrift-protocol drivers continues to expand their
featureset, such as support for cancelling statements, token exchange, and
better tracing (<a href="https://github.com/apache/arrow-adbc/pull/3304">#3304</a>,
<a href="https://github.com/apache/arrow-adbc/pull/3302">#3302</a>,
<a href="https://github.com/apache/arrow-adbc/pull/3301">#3301</a>,
<a href="https://github.com/apache/arrow-adbc/pull/3224">#3224</a>,
<a href="https://github.com/apache/arrow-adbc/pull/3218">#3218</a>,
<a href="https://github.com/apache/arrow-adbc/pull/3192">#3192</a>,
<a href="https://github.com/apache/arrow-adbc/pull/3177">#3177</a>,
<a href="https://github.com/apache/arrow-adbc/pull/3137">#3137</a>,
<a href="https://github.com/apache/arrow-adbc/pull/3127">#3127</a>).  The PostgreSQL
driver will properly bind <code>arrow.json</code> extension arrays as JSON parameters
(<a href="https://github.com/apache/arrow-adbc/pull/3333">#3333</a>).  The Snowflake
driver supports more authentication methods
(<a href="https://github.com/apache/arrow-adbc/pull/3366">#3366</a>).  The SQLite driver
can bind parameters by name instead of position
(<a href="https://github.com/apache/arrow-adbc/pull/3362">#3362</a>).</p>
<p>The C# library has been upgraded to .NET 8
(<a href="https://github.com/apache/arrow-adbc/pull/3120">#3120</a>).</p>
<p>GLib has more bindings to ADBC functions
(<a href="https://github.com/apache/arrow-adbc/pull/3118">#3118</a>).</p>
<p>The Go library has some experimental helpers to simplify getting driver
metadata (<a href="https://github.com/apache/arrow-adbc/pull/3239">#3239</a>) and
ingesting Arrow data
(<a href="https://github.com/apache/arrow-adbc/pull/3150">#3150</a>).  The <code>database/sql</code>
adapter handles <code>time.Time</code> values for bind parameters now
(<a href="https://github.com/apache/arrow-adbc/pull/3109">#3109</a>).  Drivers will
forward SQLSTATE and other error metadata across the FFI boundary
(<a href="https://github.com/apache/arrow-adbc/pull/2801">#2801</a>).</p>
<h2>Contributors</h2>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ git shortlog --perl-regexp --author='^((?!dependabot\[bot\]).*)$' -sn apache-arrow-adbc-19..apache-arrow-adbc-20
    28	David Li
    14	Todd Meng
    13	Bryce Mecum
    13	eitsupi
    12	Jacky Hu
    12	Matt Topol
     8	Bruce Irschick
     7	Matthijs Brobbel
     6	davidhcoe
     5	eric-wang-1990
     4	Alex Guo
     3	Daijiro Fukuda
     3	Felipe Oliveira Carvalho
     3	Sutou Kouhei
     2	Curt Hagenlocher
     2	Jade Wang
     2	Mandukhai Alimaa
     2	amangoyal
     1	Arseny Tsypushkin
     1	Dewey Dunnington
     1	Even Rouault
     1	Ian Cook
     1	Jordan E
     1	Lucas Valente
     1	Mila Page
     1	Ryan Syed
     1	Sudhir Reddy Emmadi
     1	Xuliang (Harry) Sun
     1	Yu Ishikawa
</code></pre></div></div>
<h2>Roadmap</h2>
<p>A Go-based driver for Databricks is in the works from a contributor.</p>
<h2>Getting Involved</h2>
<p>We welcome questions and contributions from all interested.  Issues
can be filed on <a href="https://github.com/apache/arrow-adbc/issues">GitHub</a>, and questions can be directed to GitHub
or the <a href="/community/">Arrow mailing lists</a>.</p>]]></content><author><name>pmc</name></author><category term="release" /><summary type="html"><![CDATA[The Apache Arrow team is pleased to announce the version 20 release of the Apache Arrow ADBC libraries. This release includes 44 resolved issues from 29 distinct contributors. This is a release of the libraries, which are at version 20. The API specification is versioned separately and is at version 1.1.0. The subcomponents are versioned independently: C/C++/GLib/Go/Python/Ruby: 1.8.0 C#: 0.20.0 Java: 0.20.0 R: 0.20.0 Rust: 0.20.0 The release notes below are not exhaustive and only expose selected highlights of the release. Many other bugfixes and improvements have been made: we refer you to the complete changelog. Release Highlights Driver managers now support loading driver manifests. To learn more about this feature, please see the documentation. The Rust crates were reorganized. This is a breaking change. Now, FFI-related code is part of adbc_ffi and the driver manager is part of adbc_driver_manager. Previously these were features of a single crate adbc_core, which now only contains API definitions (#3381, #3197). Also, some enums are no longer marked as #[non_exhaustive] (#3245). The Java JNI bindings support a few more features (#3373, #3372, #3370, #3348). The BigQuery driver properly uses microsecond timestamps (#3364), has an improved error message if your user account lacks the proper permissions (#3297), properly handles nested data (#3240), and supports service account impersonation (#3174). The C# Databricks/HiveServer2 Thrift-protocol drivers continues to expand their featureset, such as support for cancelling statements, token exchange, and better tracing (#3304, #3302, #3301, #3224, #3218, #3192, #3177, #3137, #3127). The PostgreSQL driver will properly bind arrow.json extension arrays as JSON parameters (#3333). The Snowflake driver supports more authentication methods (#3366). The SQLite driver can bind parameters by name instead of position (#3362). The C# library has been upgraded to .NET 8 (#3120). GLib has more bindings to ADBC functions (#3118). The Go library has some experimental helpers to simplify getting driver metadata (#3239) and ingesting Arrow data (#3150). The database/sql adapter handles time.Time values for bind parameters now (#3109). Drivers will forward SQLSTATE and other error metadata across the FFI boundary (#2801). Contributors $ git shortlog --perl-regexp --author='^((?!dependabot\[bot\]).*)$' -sn apache-arrow-adbc-19..apache-arrow-adbc-20 28 David Li 14 Todd Meng 13 Bryce Mecum 13 eitsupi 12 Jacky Hu 12 Matt Topol 8 Bruce Irschick 7 Matthijs Brobbel 6 davidhcoe 5 eric-wang-1990 4 Alex Guo 3 Daijiro Fukuda 3 Felipe Oliveira Carvalho 3 Sutou Kouhei 2 Curt Hagenlocher 2 Jade Wang 2 Mandukhai Alimaa 2 amangoyal 1 Arseny Tsypushkin 1 Dewey Dunnington 1 Even Rouault 1 Ian Cook 1 Jordan E 1 Lucas Valente 1 Mila Page 1 Ryan Syed 1 Sudhir Reddy Emmadi 1 Xuliang (Harry) Sun 1 Yu Ishikawa Roadmap A Go-based driver for Databricks is in the works from a contributor. Getting Involved We welcome questions and contributions from all interested. Issues can be filed on GitHub, and questions can be directed to GitHub or the Arrow mailing lists.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Apache Arrow Go 18.4.1 Release</title><link href="https://arrow.apache.org/blog/2025/09/04/arrow-go-18.4.1/" rel="alternate" type="text/html" title="Apache Arrow Go 18.4.1 Release" /><published>2025-09-04T00:00:00-04:00</published><updated>2025-09-04T00:00:00-04:00</updated><id>https://arrow.apache.org/blog/2025/09/04/arrow-go-18.4.1</id><content type="html" xml:base="https://arrow.apache.org/blog/2025/09/04/arrow-go-18.4.1/"><![CDATA[<!--

-->
<p>The Apache Arrow team is pleased to announce the v18.4.1 release of Apache Arrow Go.
This patch release covers 15 commits from 7 distinct contributors.</p>
<h2>Contributors</h2>
<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code data-lang="console"><span class="gp">$</span><span class="w"> </span>git shortlog <span class="nt">-sn</span> v18.4.0..v18.4.1
<span class="go">     7	Matt Topol
     4	Mandukhai Alimaa
     1	Chromo-residuum-opec
     1	Ryan Schneider
     1	Travis Patterson
     1	daniel-adam-tfs
     1	secfree
</span></code></pre></div></div>
<h2>Highlights</h2>
<ul>
<li>The <code>Record</code> interface type has been renamed to <code>RecordBatch</code> to align with
other Arrow implementations and to avoid confusion. The old <code>Record</code> type is
aliased to the new <code>RecordBatch</code> type so existing code works but users may wish
to update references now. This work was contributed by a first-time contributor,
@Mandukhai-Alimaa. See <a href="https://github.com/apache/arrow-go/pull/466">#466</a>,
<a href="https://github.com/apache/arrow-go/pull/473">#473</a>,
<a href="https://github.com/apache/arrow-go/pull/478">#478</a>, and
<a href="https://github.com/apache/arrow-go/pull/486">#486</a>.</li>
</ul>
<h3>Important Note</h3>
<ul>
<li>A side effect of the above was an unintentional breaking change by introducing a new
method to the <code>RecordReader</code> interface. This shouldn't affect the majority of consumers,
but is a breaking change for any who implemented their own concrete <code>RecordReader</code> type.
The solution is simply to add a <code>RecordBatch()</code> method to the type when upgrading to v18.4.1</li>
</ul>
<h2>Changelog</h2>
<h3>What's Changed</h3>
<ul>
<li>fix(arrow/compute/exprs): Handle large types in expr handling by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/440">https://github.com/apache/arrow-go/pull/440</a></li>
<li>fix(arrow/compute/exprs): fix literalToDatum for precision types by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/447">https://github.com/apache/arrow-go/pull/447</a></li>
<li>fix(arrow/array): Fix RecordFromJSON perf by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/449">https://github.com/apache/arrow-go/pull/449</a></li>
<li>fix(arrow/array): update timestamp json format by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/450">https://github.com/apache/arrow-go/pull/450</a></li>
<li>refactor: switch golang.org/x/exp to standard library packages by @ufUNnxagpM in <a href="https://github.com/apache/arrow-go/pull/453">https://github.com/apache/arrow-go/pull/453</a></li>
<li>fix(parquet/pqarrow): supress io.EOF in RecordReader.Err() by @ryanschneider in <a href="https://github.com/apache/arrow-go/pull/452">https://github.com/apache/arrow-go/pull/452</a></li>
<li>fix(array): add nil checks in Data.Release() for childData by @secfree in <a href="https://github.com/apache/arrow-go/pull/456">https://github.com/apache/arrow-go/pull/456</a></li>
<li>fix(arrow/compute): Fix scalar comparison batches by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/465">https://github.com/apache/arrow-go/pull/465</a></li>
<li>refactor(arrow): rename Record to RecordBatch and add deprecated alias by @Mandukhai-Alimaa in <a href="https://github.com/apache/arrow-go/pull/466">https://github.com/apache/arrow-go/pull/466</a></li>
<li>refactor(arrow): migrate leaf packages to use RecordBatch by @Mandukhai-Alimaa in <a href="https://github.com/apache/arrow-go/pull/473">https://github.com/apache/arrow-go/pull/473</a></li>
<li>refactor(arrow): third increment of the Record -&gt; RecordBatch migration by @Mandukhai-Alimaa in <a href="https://github.com/apache/arrow-go/pull/478">https://github.com/apache/arrow-go/pull/478</a></li>
<li>ci(parquet/pqarrow): integration tests for reading shredded variants by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/455">https://github.com/apache/arrow-go/pull/455</a></li>
<li>Implement RLE dictionary decoder using generics by @daniel-adam-tfs in <a href="https://github.com/apache/arrow-go/pull/477">https://github.com/apache/arrow-go/pull/477</a></li>
<li>fix(parquet/internal/encoding): Fix typed dictionary encoding by @MasslessParticle in <a href="https://github.com/apache/arrow-go/pull/479">https://github.com/apache/arrow-go/pull/479</a></li>
<li>refactor(arrow): fourth increment of the Record -&gt; RecordBatch migration by @Mandukhai-Alimaa in <a href="https://github.com/apache/arrow-go/pull/486">https://github.com/apache/arrow-go/pull/486</a></li>
<li>chore: bump version number by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/487">https://github.com/apache/arrow-go/pull/487</a></li>
</ul>
<h3>New Contributors</h3>
<ul>
<li>@ufUNnxagpM made their first contribution in <a href="https://github.com/apache/arrow-go/pull/453">https://github.com/apache/arrow-go/pull/453</a></li>
<li>@ryanschneider made their first contribution in <a href="https://github.com/apache/arrow-go/pull/452">https://github.com/apache/arrow-go/pull/452</a></li>
<li>@secfree made their first contribution in <a href="https://github.com/apache/arrow-go/pull/456">https://github.com/apache/arrow-go/pull/456</a></li>
<li>@Mandukhai-Alimaa made their first contribution in <a href="https://github.com/apache/arrow-go/pull/466">https://github.com/apache/arrow-go/pull/466</a></li>
<li>@daniel-adam-tfs made their first contribution in <a href="https://github.com/apache/arrow-go/pull/477">https://github.com/apache/arrow-go/pull/477</a></li>
</ul>
<p><strong>Full Changelog</strong>: <a href="https://github.com/apache/arrow-go/compare/v18.4.0...v18.4.1">https://github.com/apache/arrow-go/compare/v18.4.0...v18.4.1</a></p>]]></content><author><name>pmc</name></author><category term="release" /><summary type="html"><![CDATA[The Apache Arrow team is pleased to announce the v18.4.1 release of Apache Arrow Go. This patch release covers 15 commits from 7 distinct contributors. Contributors $ git shortlog -sn v18.4.0..v18.4.1 7 Matt Topol 4 Mandukhai Alimaa 1 Chromo-residuum-opec 1 Ryan Schneider 1 Travis Patterson 1 daniel-adam-tfs 1 secfree Highlights The Record interface type has been renamed to RecordBatch to align with other Arrow implementations and to avoid confusion. The old Record type is aliased to the new RecordBatch type so existing code works but users may wish to update references now. This work was contributed by a first-time contributor, @Mandukhai-Alimaa. See #466, #473, #478, and #486. Important Note A side effect of the above was an unintentional breaking change by introducing a new method to the RecordReader interface. This shouldn't affect the majority of consumers, but is a breaking change for any who implemented their own concrete RecordReader type. The solution is simply to add a RecordBatch() method to the type when upgrading to v18.4.1 Changelog What's Changed fix(arrow/compute/exprs): Handle large types in expr handling by @zeroshade in https://github.com/apache/arrow-go/pull/440 fix(arrow/compute/exprs): fix literalToDatum for precision types by @zeroshade in https://github.com/apache/arrow-go/pull/447 fix(arrow/array): Fix RecordFromJSON perf by @zeroshade in https://github.com/apache/arrow-go/pull/449 fix(arrow/array): update timestamp json format by @zeroshade in https://github.com/apache/arrow-go/pull/450 refactor: switch golang.org/x/exp to standard library packages by @ufUNnxagpM in https://github.com/apache/arrow-go/pull/453 fix(parquet/pqarrow): supress io.EOF in RecordReader.Err() by @ryanschneider in https://github.com/apache/arrow-go/pull/452 fix(array): add nil checks in Data.Release() for childData by @secfree in https://github.com/apache/arrow-go/pull/456 fix(arrow/compute): Fix scalar comparison batches by @zeroshade in https://github.com/apache/arrow-go/pull/465 refactor(arrow): rename Record to RecordBatch and add deprecated alias by @Mandukhai-Alimaa in https://github.com/apache/arrow-go/pull/466 refactor(arrow): migrate leaf packages to use RecordBatch by @Mandukhai-Alimaa in https://github.com/apache/arrow-go/pull/473 refactor(arrow): third increment of the Record -&gt; RecordBatch migration by @Mandukhai-Alimaa in https://github.com/apache/arrow-go/pull/478 ci(parquet/pqarrow): integration tests for reading shredded variants by @zeroshade in https://github.com/apache/arrow-go/pull/455 Implement RLE dictionary decoder using generics by @daniel-adam-tfs in https://github.com/apache/arrow-go/pull/477 fix(parquet/internal/encoding): Fix typed dictionary encoding by @MasslessParticle in https://github.com/apache/arrow-go/pull/479 refactor(arrow): fourth increment of the Record -&gt; RecordBatch migration by @Mandukhai-Alimaa in https://github.com/apache/arrow-go/pull/486 chore: bump version number by @zeroshade in https://github.com/apache/arrow-go/pull/487 New Contributors @ufUNnxagpM made their first contribution in https://github.com/apache/arrow-go/pull/453 @ryanschneider made their first contribution in https://github.com/apache/arrow-go/pull/452 @secfree made their first contribution in https://github.com/apache/arrow-go/pull/456 @Mandukhai-Alimaa made their first contribution in https://github.com/apache/arrow-go/pull/466 @daniel-adam-tfs made their first contribution in https://github.com/apache/arrow-go/pull/477 Full Changelog: https://github.com/apache/arrow-go/compare/v18.4.0...v18.4.1]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Apache Arrow Go 18.4.0 Release</title><link href="https://arrow.apache.org/blog/2025/07/21/arrow-go-18.4.0/" rel="alternate" type="text/html" title="Apache Arrow Go 18.4.0 Release" /><published>2025-07-21T00:00:00-04:00</published><updated>2025-07-21T00:00:00-04:00</updated><id>https://arrow.apache.org/blog/2025/07/21/arrow-go-18.4.0</id><content type="html" xml:base="https://arrow.apache.org/blog/2025/07/21/arrow-go-18.4.0/"><![CDATA[<!--

-->
<p>The Apache Arrow team is pleased to announce the v18.4.0 release of Apache Arrow Go.
This minor release covers 25 commits from 11 distinct contributors.</p>
<h2>Contributors</h2>
<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code data-lang="console"><span class="gp">$</span><span class="w"> </span>git shortlog <span class="nt">-sn</span> v18.3.1..v18.4.0
<span class="go">    16	Matt Topol
     1	Alvaro Viebrantz
     1	Arnold Wakim
     1	Daniil Mileev
     1	Kristofer Gaudel
     1	Marcin Bojanczyk
     1	Ra√∫l Cumplido
     1	Saurabh Singh
     1	Sutou Kouhei
     1	Victor Perez
     1	Willem Jan
</span></code></pre></div></div>
<h2>Changelog</h2>
<h3>What's Changed</h3>
<ul>
<li>feat(arrow/cdata): Add ReleaseCArrowArrayStream function by @karsov in <a href="https://github.com/apache/arrow-go/pull/373">https://github.com/apache/arrow-go/pull/373</a></li>
<li>fix: TestDeltaByteArray implementation and fix by @MetalBlueberry in <a href="https://github.com/apache/arrow-go/pull/369">https://github.com/apache/arrow-go/pull/369</a></li>
<li>chore: move .github/ISSUE_TEMPLATE/config.yaml to config.yml as currently does not work by @raulcd in <a href="https://github.com/apache/arrow-go/pull/383">https://github.com/apache/arrow-go/pull/383</a></li>
<li>fix: list_columns.parquet testing by @MetalBlueberry in <a href="https://github.com/apache/arrow-go/pull/378">https://github.com/apache/arrow-go/pull/378</a></li>
<li>feat: Extend arrow csv writter by @MetalBlueberry in <a href="https://github.com/apache/arrow-go/pull/375">https://github.com/apache/arrow-go/pull/375</a></li>
<li>feat(parquet/pqarrow): parallelize SeekToRow by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/380">https://github.com/apache/arrow-go/pull/380</a></li>
<li>chore: Use apache/arrow-js for JS in integration test by @kou in <a href="https://github.com/apache/arrow-go/pull/389">https://github.com/apache/arrow-go/pull/389</a></li>
<li>feat(parquet): add variant encoder/decoder by @sfc-gh-mbojanczyk in <a href="https://github.com/apache/arrow-go/pull/344">https://github.com/apache/arrow-go/pull/344</a></li>
<li>chore: remove extra binary by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/391">https://github.com/apache/arrow-go/pull/391</a></li>
<li>CI: add benchmark workflow and script by @singh1203 in <a href="https://github.com/apache/arrow-go/pull/250">https://github.com/apache/arrow-go/pull/250</a></li>
<li>feat(arrow/extensions): Add Variant extension type, array, and builder by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/395">https://github.com/apache/arrow-go/pull/395</a></li>
<li>fix(parquet/pqarrow): Fix propagation of field-ids for Lists by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/397">https://github.com/apache/arrow-go/pull/397</a></li>
<li>feat(arrow/_examples): enhance library examples by @kris-gaudel in <a href="https://github.com/apache/arrow-go/pull/394">https://github.com/apache/arrow-go/pull/394</a></li>
<li>chore: bump Windows GitHub hosted runner to windows-2022 by @raulcd in <a href="https://github.com/apache/arrow-go/pull/407">https://github.com/apache/arrow-go/pull/407</a></li>
<li>ci(benchmark): Fix benchmark runs by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/409">https://github.com/apache/arrow-go/pull/409</a></li>
<li>ci: fix flaky test by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/411">https://github.com/apache/arrow-go/pull/411</a></li>
<li>ci: make additional checks to prevent flaky EOF by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/414">https://github.com/apache/arrow-go/pull/414</a></li>
<li>refactor: update linter and run it by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/419">https://github.com/apache/arrow-go/pull/419</a></li>
<li>feat: expose Payload.WritePayload to allow serializing into IPC format by @alvarowolfx in <a href="https://github.com/apache/arrow-go/pull/421">https://github.com/apache/arrow-go/pull/421</a></li>
<li>feat(parquet/variant): Parse JSON into variant by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/426">https://github.com/apache/arrow-go/pull/426</a></li>
<li>refactor(parquet/internal/encoding): Refactor parquet logic to use generics by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/428">https://github.com/apache/arrow-go/pull/428</a></li>
<li>feat(arrrow/compute/expr): support substrait timestamp and decimal properly by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/418">https://github.com/apache/arrow-go/pull/418</a></li>
<li>feat(parquet/examples): enhance library examples by @milden6 in <a href="https://github.com/apache/arrow-go/pull/429">https://github.com/apache/arrow-go/pull/429</a></li>
<li>feat(arrow/compute): support some float16 casts by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/430">https://github.com/apache/arrow-go/pull/430</a></li>
<li>feat(parquet/pqarrow): Correctly handle Variant types in schema by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/433">https://github.com/apache/arrow-go/pull/433</a></li>
<li>fix(arrow/avro-reader): bunch of types that didn't work by @Willem-J-an in <a href="https://github.com/apache/arrow-go/pull/416">https://github.com/apache/arrow-go/pull/416</a></li>
<li>feat(parquet/pqarrow): read/write variant by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/434">https://github.com/apache/arrow-go/pull/434</a></li>
<li>build(deps): update to substrait-go v4.3.0 by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/438">https://github.com/apache/arrow-go/pull/438</a></li>
<li>fix(arrow/flight/flightsql): drain channel in flightSqlServer.DoGet by @arnoldwakim in <a href="https://github.com/apache/arrow-go/pull/437">https://github.com/apache/arrow-go/pull/437</a></li>
<li>chore(arrow): Update PkgVersion by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/439">https://github.com/apache/arrow-go/pull/439</a></li>
</ul>
<h3>New Contributors</h3>
<ul>
<li>@karsov made their first contribution in <a href="https://github.com/apache/arrow-go/pull/373">https://github.com/apache/arrow-go/pull/373</a></li>
<li>@MetalBlueberry made their first contribution in <a href="https://github.com/apache/arrow-go/pull/369">https://github.com/apache/arrow-go/pull/369</a></li>
<li>@sfc-gh-mbojanczyk made their first contribution in <a href="https://github.com/apache/arrow-go/pull/344">https://github.com/apache/arrow-go/pull/344</a></li>
<li>@kris-gaudel made their first contribution in <a href="https://github.com/apache/arrow-go/pull/394">https://github.com/apache/arrow-go/pull/394</a></li>
<li>@alvarowolfx made their first contribution in <a href="https://github.com/apache/arrow-go/pull/421">https://github.com/apache/arrow-go/pull/421</a></li>
<li>@milden6 made their first contribution in <a href="https://github.com/apache/arrow-go/pull/429">https://github.com/apache/arrow-go/pull/429</a></li>
<li>@Willem-J-an made their first contribution in <a href="https://github.com/apache/arrow-go/pull/416">https://github.com/apache/arrow-go/pull/416</a></li>
<li>@arnoldwakim made their first contribution in <a href="https://github.com/apache/arrow-go/pull/437">https://github.com/apache/arrow-go/pull/437</a></li>
</ul>
<p><strong>Full Changelog</strong>: <a href="https://github.com/apache/arrow-go/compare/v18.3.0...v18.4.0">https://github.com/apache/arrow-go/compare/v18.3.0...v18.4.0</a></p>]]></content><author><name>pmc</name></author><category term="release" /><summary type="html"><![CDATA[The Apache Arrow team is pleased to announce the v18.4.0 release of Apache Arrow Go. This minor release covers 25 commits from 11 distinct contributors. Contributors $ git shortlog -sn v18.3.1..v18.4.0 16 Matt Topol 1 Alvaro Viebrantz 1 Arnold Wakim 1 Daniil Mileev 1 Kristofer Gaudel 1 Marcin Bojanczyk 1 Ra√∫l Cumplido 1 Saurabh Singh 1 Sutou Kouhei 1 Victor Perez 1 Willem Jan Changelog What's Changed feat(arrow/cdata): Add ReleaseCArrowArrayStream function by @karsov in https://github.com/apache/arrow-go/pull/373 fix: TestDeltaByteArray implementation and fix by @MetalBlueberry in https://github.com/apache/arrow-go/pull/369 chore: move .github/ISSUE_TEMPLATE/config.yaml to config.yml as currently does not work by @raulcd in https://github.com/apache/arrow-go/pull/383 fix: list_columns.parquet testing by @MetalBlueberry in https://github.com/apache/arrow-go/pull/378 feat: Extend arrow csv writter by @MetalBlueberry in https://github.com/apache/arrow-go/pull/375 feat(parquet/pqarrow): parallelize SeekToRow by @zeroshade in https://github.com/apache/arrow-go/pull/380 chore: Use apache/arrow-js for JS in integration test by @kou in https://github.com/apache/arrow-go/pull/389 feat(parquet): add variant encoder/decoder by @sfc-gh-mbojanczyk in https://github.com/apache/arrow-go/pull/344 chore: remove extra binary by @zeroshade in https://github.com/apache/arrow-go/pull/391 CI: add benchmark workflow and script by @singh1203 in https://github.com/apache/arrow-go/pull/250 feat(arrow/extensions): Add Variant extension type, array, and builder by @zeroshade in https://github.com/apache/arrow-go/pull/395 fix(parquet/pqarrow): Fix propagation of field-ids for Lists by @zeroshade in https://github.com/apache/arrow-go/pull/397 feat(arrow/_examples): enhance library examples by @kris-gaudel in https://github.com/apache/arrow-go/pull/394 chore: bump Windows GitHub hosted runner to windows-2022 by @raulcd in https://github.com/apache/arrow-go/pull/407 ci(benchmark): Fix benchmark runs by @zeroshade in https://github.com/apache/arrow-go/pull/409 ci: fix flaky test by @zeroshade in https://github.com/apache/arrow-go/pull/411 ci: make additional checks to prevent flaky EOF by @zeroshade in https://github.com/apache/arrow-go/pull/414 refactor: update linter and run it by @zeroshade in https://github.com/apache/arrow-go/pull/419 feat: expose Payload.WritePayload to allow serializing into IPC format by @alvarowolfx in https://github.com/apache/arrow-go/pull/421 feat(parquet/variant): Parse JSON into variant by @zeroshade in https://github.com/apache/arrow-go/pull/426 refactor(parquet/internal/encoding): Refactor parquet logic to use generics by @zeroshade in https://github.com/apache/arrow-go/pull/428 feat(arrrow/compute/expr): support substrait timestamp and decimal properly by @zeroshade in https://github.com/apache/arrow-go/pull/418 feat(parquet/examples): enhance library examples by @milden6 in https://github.com/apache/arrow-go/pull/429 feat(arrow/compute): support some float16 casts by @zeroshade in https://github.com/apache/arrow-go/pull/430 feat(parquet/pqarrow): Correctly handle Variant types in schema by @zeroshade in https://github.com/apache/arrow-go/pull/433 fix(arrow/avro-reader): bunch of types that didn't work by @Willem-J-an in https://github.com/apache/arrow-go/pull/416 feat(parquet/pqarrow): read/write variant by @zeroshade in https://github.com/apache/arrow-go/pull/434 build(deps): update to substrait-go v4.3.0 by @zeroshade in https://github.com/apache/arrow-go/pull/438 fix(arrow/flight/flightsql): drain channel in flightSqlServer.DoGet by @arnoldwakim in https://github.com/apache/arrow-go/pull/437 chore(arrow): Update PkgVersion by @zeroshade in https://github.com/apache/arrow-go/pull/439 New Contributors @karsov made their first contribution in https://github.com/apache/arrow-go/pull/373 @MetalBlueberry made their first contribution in https://github.com/apache/arrow-go/pull/369 @sfc-gh-mbojanczyk made their first contribution in https://github.com/apache/arrow-go/pull/344 @kris-gaudel made their first contribution in https://github.com/apache/arrow-go/pull/394 @alvarowolfx made their first contribution in https://github.com/apache/arrow-go/pull/421 @milden6 made their first contribution in https://github.com/apache/arrow-go/pull/429 @Willem-J-an made their first contribution in https://github.com/apache/arrow-go/pull/416 @arnoldwakim made their first contribution in https://github.com/apache/arrow-go/pull/437 Full Changelog: https://github.com/apache/arrow-go/compare/v18.3.0...v18.4.0]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Recent Improvements to Hash Join in Arrow C++</title><link href="https://arrow.apache.org/blog/2025/07/18/recent-improvements-to-hash-join/" rel="alternate" type="text/html" title="Recent Improvements to Hash Join in Arrow C++" /><published>2025-07-18T00:00:00-04:00</published><updated>2025-07-18T00:00:00-04:00</updated><id>https://arrow.apache.org/blog/2025/07/18/recent-improvements-to-hash-join</id><content type="html" xml:base="https://arrow.apache.org/blog/2025/07/18/recent-improvements-to-hash-join/"><![CDATA[<!--

-->
<p><em>Editor‚Äôs Note: Apache Arrow is an expansive project, ranging from the Arrow columnar format itself, to its numerous specifications, and a long list of implementations. Arrow is also an expansive project in terms of its community of contributors. In this blog post, we‚Äôd like to highlight recent work by Apache Arrow Committer Rossi Sun on improving the performance and stability of Arrow‚Äôs embeddable query execution engine: Acero.</em></p>
<h1>Introduction</h1>
<p>Hash join is a fundamental operation in analytical processing engines ‚Äî it matches rows from two tables based on key values using a hash table for fast lookup. In the C++ implementation of Apache Arrow, the hash join is implemented in the C++ engine Acero, which powers query execution in bindings like PyArrow and the R Arrow package. Even if you haven't used Acero directly, your code may already be benefiting from it under the hood.</p>
<p>For example, this simple PyArrow example uses Acero:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code data-lang="python"><span class="kn">import</span> <span class="n">pyarrow</span> <span class="k">as</span> <span class="n">pa</span>

<span class="n">t1</span> <span class="o">=</span> <span class="n">pa</span><span class="p">.</span><span class="nf">table</span><span class="p">({</span><span class="sh">'</span><span class="s">id</span><span class="sh">'</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
               <span class="sh">'</span><span class="s">year</span><span class="sh">'</span><span class="p">:</span> <span class="p">[</span><span class="mi">2020</span><span class="p">,</span> <span class="mi">2022</span><span class="p">,</span> <span class="mi">2019</span><span class="p">]})</span>
<span class="n">t2</span> <span class="o">=</span> <span class="n">pa</span><span class="p">.</span><span class="nf">table</span><span class="p">({</span><span class="sh">'</span><span class="s">id</span><span class="sh">'</span><span class="p">:</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span>
               <span class="sh">'</span><span class="s">n_legs</span><span class="sh">'</span><span class="p">:</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">100</span><span class="p">],</span>
               <span class="sh">'</span><span class="s">animal</span><span class="sh">'</span><span class="p">:</span> <span class="p">[</span><span class="sh">"</span><span class="s">Brittle stars</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Centipede</span><span class="sh">"</span><span class="p">]})</span>

<span class="n">t1</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">t2</span><span class="p">,</span> <span class="sh">'</span><span class="s">id</span><span class="sh">'</span><span class="p">).</span><span class="nf">combine_chunks</span><span class="p">().</span><span class="nf">sort_by</span><span class="p">(</span><span class="sh">'</span><span class="s">year</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div>
<p>Acero was originally created in 2019 to demonstrate that the ever-growing library of compute kernels in Arrow C++ could be linked together into realistic workflows and also to take advantage of the emerging Datasets API to give these workflows access to data. Rather than aiming to compete with full query engines like DuckDB, Acero focuses on enabling flexible, composable, and embeddable query execution ‚Äî serving as a building block for tools and systems that need fast, modular analytics capabilities ‚Äî including those built atop Arrow C++, or integrating via bindings like PyArrow, Substrait, or ADBC.</p>
<p>Across several recent Arrow C++ releases, we've made substantial improvements to the hash join implementation to address common user pain points. These changes improve stability, memory efficiency, and parallel performance, with a focus on making joins more usable and scalable out of the box. If you've had trouble using Arrow‚Äôs hash join in the past, now is a great time to try again.</p>
<h1>Scaling Safely: Improvements to Stability</h1>
<p>In earlier versions of Arrow C++, the hash join implementation used internal data structures that weren‚Äôt designed for very large datasets and lacked safeguards in some of the underlying memory operations. These limitations rarely surfaced in small to medium workloads but became problematic at scale, manifesting as crashes or subtle correctness issues.</p>
<p>At the core of Arrow‚Äôs join implementation is a compact, row-oriented structure known as the ‚Äúrow table‚Äù. While Arrow‚Äôs data model is columnar, its hash join implementation operates in a row-wise fashion ‚Äî similar to modern engines like DuckDB and Meta‚Äôs Velox. This layout minimizes CPU cache misses during hash table lookups by collocating keys, payloads, and null bits in memory so they can be accessed together.</p>
<p>In previous versions, the row table used 32-bit offsets to reference packed rows. This capped each table‚Äôs size to 4GB and introduced risks of overflow when working with large datasets or wide rows. Several reported issues ‚Äî <a href="https://github.com/apache/arrow/issues/34474">GH-34474</a>, <a href="https://github.com/apache/arrow/issues/41813">GH-41813</a>, and <a href="https://github.com/apache/arrow/issues/43202">GH-43202</a> ‚Äî highlighted the limitations of this design. In response, PR <a href="https://github.com/apache/arrow/pull/43389">GH-43389</a> widened the internal offset type to 64-bit, reworking key parts of the row table infrastructure to support larger data sizes more safely and scalably.</p>
<p>Besides the offset limitation, earlier versions of Arrow C++ also included overflow-prone logic in the buffer indexing paths used throughout the hash join implementation. Many internal calculations assumed that 32-bit integers were sufficient for addressing memory ‚Äî a fragile assumption when working with large datasets or wide rows. These issues appeared not only in conventional C++ indexing code but also in Arrow‚Äôs SIMD-accelerated paths ‚Äî Arrow includes heavy SIMD specializations, used to speed up operations like hash table probing and row comparison. Together, these assumptions led to subtle overflows and incorrect behavior, as documented in issues like <a href="https://github.com/apache/arrow/issues/44513">GH-44513</a>, <a href="https://github.com/apache/arrow/issues/45334">GH-45334</a>, and <a href="https://github.com/apache/arrow/issues/45506">GH-45506</a>.</p>
<p>Two representative examples:</p>
<ul>
<li>Row-wise buffer access in C++</li>
</ul>
<p>The aforementioned row table stores fixed-length data in tightly packed buffers. Accessing a particular row (and optionally a column within it) typically involves <a href="https://github.com/apache/arrow/blob/12f62653c825fbf305bfde61c112d2aa69203c62/cpp/src/arrow/acero/swiss_join_internal.h#L120">pointer arithmetic</a>:</p>
<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code data-lang="cpp"><span class="k">const</span> <span class="kt">uint8_t</span><span class="o">*</span> <span class="n">row_ptr</span> <span class="o">=</span> <span class="n">row_ptr_base</span> <span class="o">+</span> <span class="n">row_length</span> <span class="o">*</span> <span class="n">row_id</span><span class="p">;</span>
</code></pre></div></div>
<p>When both <code>row_length</code> and <code>row_id</code> are large 32-bit integers, their product can overflow.</p>
<p>Similarly, accessing null masks involves <a href="https://github.com/apache/arrow/blob/12f62653c825fbf305bfde61c112d2aa69203c62/cpp/src/arrow/acero/swiss_join_internal.h#L150">null-bit indexing arithmetic</a>:</p>
<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code data-lang="cpp"><span class="kt">int64_t</span> <span class="n">bit_id</span> <span class="o">=</span> <span class="n">row_id</span> <span class="o">*</span> <span class="n">null_mask_num_bytes</span> <span class="o">*</span> <span class="mi">8</span> <span class="o">+</span> <span class="n">pos_after_encoding</span><span class="p">;</span>
</code></pre></div></div>
<p>The intermediate multiplication is performed using 32-bit arithmetic and can overflow even though the final result is stored in a 64-bit variable.</p>
<ul>
<li>SIMD gathers with 32-bit offsets</li>
</ul>
<p>One essential SIMD instruction is the AVX2 intrinsic <code>__m256i _mm256_i32gather_epi32(int const * base, __m256i vindex, const int scale);</code>, which performs a parallel memory gather of eight 32-bit integers based on eight 32-bit signed offsets. It was extensively used in Arrow for hash table operations, for example, <a href="https://github.com/apache/arrow/blob/0a00e25f2f6fb927fb555b69038d0be9b9d9f265/cpp/src/arrow/compute/key_map_internal_avx2.cc#L404">fetching 8 group IDs</a> (hash table slots) in parallel during hash table probing:</p>
<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code data-lang="cpp"><span class="n">__m256i</span> <span class="n">group_id</span> <span class="o">=</span> <span class="n">_mm256_i32gather_epi32</span><span class="p">(</span><span class="n">elements</span><span class="p">,</span> <span class="n">pos</span><span class="p">,</span> <span class="mi">1</span><span class="p">);</span>
</code></pre></div></div>
<p>and <a href="https://github.com/apache/arrow/blob/69e8a78c018da88b60f9eb2b3b45703f81f3c93d/cpp/src/arrow/compute/row/compare_internal_avx2.cc#L284">loading 8 corresponding key values</a> from the right-side input in parallel for comparison:</p>
<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code data-lang="cpp"><span class="n">__m256i</span> <span class="n">right</span> <span class="o">=</span> <span class="n">_mm256_i32gather_epi32</span><span class="p">((</span><span class="k">const</span> <span class="kt">int</span><span class="o">*</span><span class="p">)</span><span class="n">right_base</span><span class="p">,</span> <span class="n">offset_right</span><span class="p">,</span> <span class="mi">1</span><span class="p">);</span>
</code></pre></div></div>
<p>If any of the computed offsets exceed <code>2^31 - 1</code>, they wrap into the negative range, which can lead to invalid memory access (i.e., a crash) or, more subtly, fetch data from a valid but incorrect location ‚Äî producing silently wrong results (trust me, you don‚Äôt want to debug that).</p>
<p>To mitigate these risks, PR <a href="https://github.com/apache/arrow/pull/45108">GH-45108</a>, <a href="https://github.com/apache/arrow/pull/45336">GH-45336</a>, and <a href="https://github.com/apache/arrow/pull/45515">GH-45515</a> promoted critical arithmetic to 64-bit and reworked SIMD logic to use safer indexing. Buffer access logic was also encapsulated in safer abstractions to avoid repeated manual casting or unchecked offset math. These examples are not unique to Arrow ‚Äî they reflect common pitfalls in building data-intensive systems, where unchecked assumptions about integer sizes can silently compromise correctness.</p>
<p>Together, these changes make Arrow‚Äôs hash join implementation significantly more robust and better equipped for modern data workloads. These foundations not only resolve known issues but also reduce the risk of similar bugs in future development.</p>
<h1>Leaner Memory Usage</h1>
<p>While refining overflow-prone parts of the hash join implementation, I ended up examining most of the code path for potential pitfalls. When doing this kind of work, one sits down quietly and interrogates every line ‚Äî asking not just whether an intermediate value might overflow, but whether it even needs to exist at all. And during that process, I came across something unrelated to overflow ‚Äî but even more impactful.</p>
<p>In a textbook hash join algorithm, once the right-side table (the build-side) is fully accumulated, a hash table is constructed to support probing the left-side table (the probe-side) for matches. To parallelize this build step, Arrow C++‚Äôs implementation partitions the build-side into <code>N</code> partitions ‚Äî typically matching the number of available CPU cores ‚Äî and builds a separate hash table for each partition in parallel. These are then merged into a final, unified hash table used during the probe phase.</p>
<p>The issue? The memory footprint. The total size of the partitioned hash tables is roughly equal to that of the final hash table, but they were being held in memory even after merging. Once the final hash table was built, these temporary structures had no further use ‚Äî yet they persisted through the entire join operation. There were no crashes, no warnings, no visible red flags ‚Äî just silent overhead.</p>
<p>Once spotted, the fix was straightforward: restructure the join process to release these buffers immediately after the merge. The change was implemented in PR <a href="https://github.com/apache/arrow/issues/45552">GH-45552</a>. The memory profiles below illustrate its impact.</p>
<div style="display: flex; gap: 16px; justify-content: center; align-items: flex-start;">
  <img src="/img/recent-improvements-to-hash-join/memory-profile-baseline.png" width="50%" class="img-responsive" alt="Memory profile before" aria-hidden="true">
  <img src="/img/recent-improvements-to-hash-join/memory-profile-opt.png" width="50%" class="img-responsive" alt="Memory profile after" aria-hidden="true">
</div>
<p>At <code>A</code>, memory usage rises steadily as the join builds partitioned hash tables in parallel. <code>B</code> marks the merge point, where these partitions are combined into a final, unified hash table. <code>C</code> represents the start of the probe phase, where the left-side table is scanned and matched against the final hash table. Memory begins to rise again as join results are materialized. <code>D</code> is the peak of the join operation, just before memory begins to drop as processing completes. The ‚Äúleap of faith‚Äù occurs at the star on the right profile, where the partitioned hash tables are released immediately after merging. This early release frees up substantial memory and makes room for downstream processing ‚Äî reducing the overall peak memory observed at <code>D</code>.</p>
<p>This improvement already benefits real-world scenarios ‚Äî for example, the <a href="https://duckdblabs.github.io/db-benchmark/">DuckDB Labs DB Benchmark</a>. Some benchmark queries that previously failed with out-of-memory (OOM) errors can now complete successfully ‚Äî as shown in the comparison below.</p>
<div style="display: flex; gap: 16px; justify-content: center; align-items: flex-start;">
  <img src="/img/recent-improvements-to-hash-join/duckdb-bench-oom-baseline.png" width="50%" class="img-responsive" alt="DuckDB benchmark OOM before" aria-hidden="true">
  <img src="/img/recent-improvements-to-hash-join/duckdb-bench-oom-opt.png" width="50%" class="img-responsive" alt="DuckDB benchmark OOM after" aria-hidden="true">
</div>
<p>As one reviewer noted in the PR, this was a ‚Äúlow-hanging fruit.‚Äù And sometimes, meaningful performance gains don‚Äôt come from tuning hot loops or digging through flame graphs ‚Äî they come from noticing something that doesn‚Äôt feel right and asking: why are we still keeping this around?</p>
<h1>Faster Execution Through Better Parallelism</h1>
<p>Not every improvement comes from poring over flame graphs ‚Äî but some definitely do. Performance is, after all, the most talked-about aspect of any query engine. So, how about a nice cup of flame graph?</p>
<img src="/img/recent-improvements-to-hash-join/a-nice-cup-of-flame-graph.png" width="100%" class="img-responsive" alt="A nice cup of flame graph" aria-hidden="true">
<p>It‚Äôs hard not to notice the long, flat bar dominating the middle ‚Äî especially with the rather alarming word ‚ÄúLock‚Äù in it. That‚Äôs our red flag.</p>
<p>We‚Äôve mentioned that in the build phase, we build partitioned hash tables in parallel. In earlier versions of Arrow C++, this parallelism was implemented on a batch basis ‚Äî each thread processed a build-side batch concurrently. Since each batch contained arbitrary data that could fall into any partition, threads had to synchronize when accessing shared partitions. This was managed through <a href="https://github.com/apache/arrow/blob/196cde38c112d32a944afe978b6da9c7ce935ef7/cpp/src/arrow/acero/partition_util.h#L93">locks on partitions</a>. Although we introduced some randomness in the locking order to reduce contention, it remained high ‚Äî clearly visible in the flame graph.</p>
<p>To mitigate this contention, we restructured the build phase in PR <a href="https://github.com/apache/arrow/issues/45612">GH-45612</a>. Instead of having all threads partition and insert at once ‚Äî each thread touching every hash table ‚Äî we split the work into two distinct stages. In the first partition stage, <code>M</code> threads take their assigned batches and only partition them, recording which rows belong to which partition. No insertion happens yet ‚Äî just classification. Then comes the second, newly separated build stage. Here, <code>N</code> threads take over, and each thread is responsible for building just one of the <code>N</code> partitioned hash tables. Every thread scans all the relevant partitions across all batches but inserts only the rows belonging to its assigned partition. This restructuring eliminates the need for locking between threads during insertion ‚Äî each thread now has exclusive access to its partitioned hash table. By decoupling the work this way, we turned a highly contentious operation into a clean, embarrassingly parallel one. As a result, we saw performance improve by up to 10x in dedicated build benchmarks. The <a href="https://github.com/apache/arrow/blob/196cde38c112d32a944afe978b6da9c7ce935ef7/cpp/src/arrow/acero/hash_join_benchmark.cc#L302">example</a> below is from a more typical, general-purpose workload ‚Äî not especially build-heavy ‚Äî but it still shows a solid 2x speedup. In the chart, the leap of faith ‚Äî marked by the purple icons üü£‚¨áÔ∏è ‚Äî represents results with this improvement applied, while the gray and black ones show earlier runs before the change.</p>
<img src="/img/recent-improvements-to-hash-join/internal-benchmark.png" width="100%" class="img-responsive" alt="Internal benchmark" aria-hidden="true">
<p>Also in real-world scenarios like the <a href="https://duckdblabs.github.io/db-benchmark/">DuckDB Labs DB Benchmark</a>, we‚Äôve observed similar gains. The comparison below shows around a 2x improvement in query performance after this change was applied.</p>
<div style="display: flex; gap: 16px; justify-content: center; align-items: flex-start;">
  <img src="/img/recent-improvements-to-hash-join/duckdb-bench-perf-baseline.png" width="50%" class="img-responsive" alt="DuckDB benchmark perf before" aria-hidden="true">
  <img src="/img/recent-improvements-to-hash-join/duckdb-bench-perf-opt.png" width="50%" class="img-responsive" alt="DuckDB benchmark perf after" aria-hidden="true">
</div>
<p>Additional improvements include <a href="https://github.com/apache/arrow/pull/43832">GH-43832</a>, which extends AVX2 acceleration to more probing code paths, and <a href="https://github.com/apache/arrow/pull/45918">GH-45918</a>, which introduces parallelism to a previously sequential task phase. These target more specialized scenarios and edge cases.</p>
<h2>Closing</h2>
<p>These improvements reflect ongoing investment in Arrow C++‚Äôs execution engine and a commitment to delivering fast, robust building blocks for analytic workloads. They are available in recent Arrow C++ releases and exposed through higher-level bindings like PyArrow and the Arrow R package ‚Äî starting from version 18.0.0, with the most significant improvements landing in 20.0.0. If joins were a blocker for you before ‚Äî due to memory, scale, or correctness ‚Äî recent changes may offer a very different experience.</p>
<p>The Arrow C++ engine is not just alive ‚Äî it‚Äôs improving in meaningful, user-visible ways. We‚Äôre also actively monitoring for further issues and open to expanding the design based on user feedback and real-world needs. If you‚Äôve tried joins in the past and run into performance or stability issues, we encourage you to give them another try and file an <a href="https://github.com/apache/arrow/issues">issue on GitHub</a> if you run into any issues.</p>
<p>If you have any questions about this blog post, please feel free to contact the author, <a href="mailto:zanmato1984@gmail.com">Rossi Sun</a>.</p>]]></content><author><name>zanmato</name></author><category term="application" /><summary type="html"><![CDATA[A deep dive into recent improvements to Apache Arrow‚Äôs hash join implementation ‚Äî enhancing stability, memory efficiency, and parallel performance for modern analytic workloads.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Apache Arrow 21.0.0 Release</title><link href="https://arrow.apache.org/blog/2025/07/17/21.0.0-release/" rel="alternate" type="text/html" title="Apache Arrow 21.0.0 Release" /><published>2025-07-17T00:00:00-04:00</published><updated>2025-07-17T00:00:00-04:00</updated><id>https://arrow.apache.org/blog/2025/07/17/21.0.0-release</id><content type="html" xml:base="https://arrow.apache.org/blog/2025/07/17/21.0.0-release/"><![CDATA[<!--

-->
<p>The Apache Arrow team is pleased to announce the 21.0.0 release. This release
covers over 2 months of development work and includes <a href="https://github.com/apache/arrow/milestone/69?closed=1"><strong>339 resolved
issues</strong></a> on <a href="/release/21.0.0.html#contributors"><strong>400 distinct commits</strong></a> from <a href="/release/21.0.0.html#contributors"><strong>82 distinct
contributors</strong></a>. See the <a href="https://arrow.apache.org/install/">Install Page</a> to
learn how to get the libraries for your platform.</p>
<p>The release notes below are not exhaustive and only expose selected highlights
of the release. Many other bugfixes and improvements have been made: we refer
you to the <a href="/release/21.0.0.html#changelog">complete changelog</a>.</p>
<h2>Community</h2>
<p>Since the 20.0.0 release, Alenka Frim has been invited to join the Project
Management Committee (PMC).</p>
<p>Thanks for your contributions and participation in the project!</p>
<p>The <a href="https://sessionize.com/arrow-summit-2025/">Call for Speakers</a> for the
Apache Arrow Summit 2025 is now open! The Summit will take place on October 2nd,
2025 in Paris, France as part of <a href="https://pydata.org/paris2025">PyData Paris</a>.
The call will be open until July 26, 2025. Please see the <a href="https://sessionize.com/arrow-summit-2025/">Call for
Speakers</a> link to submit a talk or
the <a href="https://lists.apache.org/thread/f0vcbtpzg6rntzbvmjstyd27bd9qfhl0">developer mailing
list</a> for more
information.</p>
<h2>Arrow Flight RPC Notes</h2>
<p>In C++ and Python, a new IPC reader option was added to force data buffers to be
aligned based on the data type, making it easier to work with systems that
expected alignment (<a href="https://github.com/apache/arrow/issues/32276">GH-32276</a>).
While this is not a Flight-specific option, it tended to occur with Flight due
to implementation details. Also, C++ and Python are now consistent with other
Flight implementations in allowing the schema of a <code>FlightInfo</code> to be omitted
(<a href="https://github.com/apache/arrow/issues/37677">GH-37677</a>).</p>
<p>We have accepted a donation of an ODBC driver for Flight SQL from Dremio
(<a href="https://github.com/apache/arrow/issues/46522">GH-46522</a>). Note that the driver
is not usable in its current state and contributors are working on implementing
the rest of the driver.</p>
<h2>C++ Notes</h2>
<h3>Compute</h3>
<p>The Cast function is now able to reorder fields when casting from one struct
type to another; the fields are matched by name, not by index
(<a href="https://github.com/apache/arrow/issues/45028">GH-45028</a>).</p>
<p>Many compute kernels have been moved into a separate, optional, shared library
(<a href="https://github.com/apache/arrow/issues/25025">GH-25025</a>). This improves
modularity for dependency management in the application and reduces the Arrow
C++ distribution size when the compute functionality is not being used. Note
that some compute functions, such as the Cast function, will still be built for
internal use in various Arrow components.</p>
<p>Better half-float support has been added to some compute functions: <code>is_nan</code>,
<code>is_inf</code>, <code>is_finite</code>, <code>negate</code>, <code>negate_checked</code>, <code>sign</code>
(<a href="https://github.com/apache/arrow/issues/45083">GH-45083</a>); <code>if_else</code>,
<code>case_when</code>, <code>coalesce</code>, <code>choose</code>, <code>replace_with_mask</code>, <code>fill_null_forward</code>,
<code>fill_null_backward</code> (<a href="https://github.com/apache/arrow/issues/37027">GH-37027</a>);
<code>run_end_encode</code>, <code>run_end_decode</code>
(<a href="https://github.com/apache/arrow/issues/46285">GH-46285</a>).</p>
<p>Better decimal32 and decimal64 support has been added to some compute functions:
<code>run_end_encode</code>, <code>run_end_decode</code>
(<a href="https://github.com/apache/arrow/issues/46285">GH-46285</a>).</p>
<p>A new function <code>utf8_zero_fill</code> acts like Python's <code>str.zfill</code> method by
providing a left-padding function that preserves the optional leading plus/minus
sign (<a href="https://github.com/apache/arrow/issues/46683">GH-46683</a>).</p>
<p>Decimal sum aggregation now produces a decimal result with an increased
precision in order to reduce the risk of overflowing the result type
(<a href="https://github.com/apache/arrow/issues/35166">GH-35166</a>).</p>
<h3>CSV</h3>
<p>Reading Duration columns is now supported
(<a href="https://github.com/apache/arrow/issues/40278">GH-40278</a>).</p>
<h3>Dataset</h3>
<p>It is now possible to preserve order when writing a dataset multi-threaded. The
feature is disabled by default
(<a href="https://github.com/apache/arrow/issues/26818">GH-26818</a>).</p>
<h3>Filesystems</h3>
<p>The S3 filesystem can optionally be built into a separate DLL
(<a href="https://github.com/apache/arrow/issues/40343">GH-40343</a>).</p>
<h3>Parquet</h3>
<h4>Encryption</h4>
<p>A new <code>SecureString</code> class must now be used to communicate sensitive data (such
as secret keys) with Parquet encryption APIs. This class automatically wipes its
contents from memory when destroyed, unlike regular <code>std::string</code>
(<a href="https://github.com/apache/arrow/issues/31603">GH-31603</a>).</p>
<h4>Type support</h4>
<p>The new VARIANT logical type is supported at a low level, and an extension type
<code>parquet.variant</code> is added to reflect such columns when reading them to Arrow
(<a href="https://github.com/apache/arrow/issues/45937">GH-45937</a>).</p>
<p>The UUID logical type is automatically converted to/from the <code>arrow.uuid</code>
canonical extension type when reading or writing Parquet data, respectively.</p>
<p>The GEOMETRY and GEOGRAPHY logical types are supported
(<a href="https://github.com/apache/arrow/issues/45522">GH-45522</a>). They are
automatically converted to/from the corresponding GeoArrow extension type, if it
has been registered by GeoArrow. Geospatial column statistics are also
supported.</p>
<p>It is now possible to read BYTE_ARRAY columns directly as LargeBinary or
BinaryView, without any intermediate conversion from Binary. Similarly, those
types can be written directly to Parquet
(<a href="https://github.com/apache/arrow/issues/43041">GH-43041</a>). This allows
bypassing the 2 GiB data per chunk limitation of the Binary type, and can also
improve performance. This also applies to String types when a Parquet column has
the STRING logical type.</p>
<p>Similarly, LIST columns can now be read directly as LargeList rather than List.
This allows bypassing the 2^31 values per chunk limitation of regular List types
(<a href="https://github.com/apache/arrow/issues/46676">GH-46676</a>).</p>
<h4>Other Parquet improvements</h4>
<p>A new feature named Content-Defined Chunking improves deduplication of Parquet
files with mostly identical contents, by choosing data page boundaries based on
actual contents rather than a number of values. For that, it uses a rolling hash
function, and the min and max chunk size can be chosen. The feature is disabled
by default and can be enabled on a per-file basis in the Parquet
<code>WriterProperties</code> (<a href="https://github.com/apache/arrow/issues/45750">GH-45750</a>).</p>
<p>The <code>EncodedStatistics</code> of a column chunk are publicly exposed in
<code>ColumnChunkMetaData</code> and can be read faster than if decoded as <code>Statistics</code>
(<a href="https://github.com/apache/arrow/issues/46462">GH-46462</a>).</p>
<p>SIMD optimizations for the BYTE_STREAM_SPLIT have been improved
(<a href="https://github.com/apache/arrow/issues/46788">GH-46788</a>).</p>
<p>Reading FIXED_LEN_BYTE_ARRAY data has been made significantly faster (up to 3x
faster on some benchmarks). This benefits logical types such as FLOAT16
(<a href="https://github.com/apache/arrow/issues/43891">GH-43891</a>).</p>
<h3>Miscellaneous C++ changes</h3>
<p>The <code>ARROW_USE_PRECOMPILED_HEADERS</code> build option was removed, as
<code>CMAKE_UNITY_BUILD</code> usually provides more benefits while requiring less
maintenance.</p>
<p>New data creation helpers <code>ArrayFromJSONString</code>, <code>ChunkedArrayFromJSONString</code>,
<code>DictArrayFromJSONString</code>, <code>ScalarFromJSONString</code> and <code>DictScalarFromJSONString</code>
are now exposed publicly. While not as high-performing as <code>BufferBuilder</code> and
the concrete <code>ArrayBuilder</code> subclasses, they allow easy creation of test or
example data, for example:</p>
<div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code data-lang="c++">  <span class="n">ARROW_ASSIGN_OR_RAISE</span><span class="p">(</span>
      <span class="k">auto</span> <span class="n">string_array</span><span class="p">,</span>
      <span class="n">arrow</span><span class="o">::</span><span class="n">ArrayFromJSONString</span><span class="p">(</span><span class="n">arrow</span><span class="o">::</span><span class="n">utf8</span><span class="p">(),</span> <span class="s">R"(["Hello", "World", null])"</span><span class="p">));</span>
  <span class="n">ARROW_ASSIGN_OR_RAISE</span><span class="p">(</span>
      <span class="k">auto</span> <span class="n">list_array</span><span class="p">,</span>
      <span class="n">arrow</span><span class="o">::</span><span class="n">ArrayFromJSONString</span><span class="p">(</span><span class="n">arrow</span><span class="o">::</span><span class="n">list</span><span class="p">(</span><span class="n">arrow</span><span class="o">::</span><span class="n">int32</span><span class="p">()),</span>
                                 <span class="s">"[[1, null, 2], [], [3]]"</span><span class="p">));</span>
</code></pre></div></div>
<p>Some APIs were changed to accept <code>std::string_view</code> instead of <code>const std::string&amp;</code>. Most uses of those APIs should not be affected
(<a href="https://github.com/apache/arrow/issues/46551">GH-46551</a>).</p>
<p>A new pretty-print option allows limiting element size when printing string or
binary data (<a href="https://github.com/apache/arrow/issues/46403">GH-46403</a>).</p>
<p>It is now possible to export <code>Tensor</code> data using
<a href="https://dmlc.github.io/dlpack/latest/">DLPack</a>
(<a href="https://github.com/apache/arrow/issues/39294">GH-39294</a>).</p>
<p>Half-float arrays can be properly diff'ed and pretty-printed
(<a href="https://github.com/apache/arrow/issues/36753">GH-36753</a>).</p>
<p>Some header files in <code>arrow/util</code> that were not supposed to be exposed are
now made internal (<a href="https://github.com/apache/arrow/issues/46459">GH-46459</a>).</p>
<h2>C# Notes</h2>
<p>The C# Arrow implementation is being extracted from the <a href="https://github.com/apache/arrow">Arrow
monorepo</a> into a standalone repository to allow
it to have its own release cadence. See <a href="https://lists.apache.org/thread/0vj7hlzbzrv0lcrm92tgtfdh9gsj4dqb">the mailing
list</a> for more
information. This is the final release of the Arrow monorepo that will include
the the C# implementation.</p>
<h2>Java, JavaScript, Go, and Rust Notes</h2>
<p>The Java, JavaScript, Go, and Rust Go projects have moved to separate
repositories outside the main Arrow <a href="https://github.com/apache/arrow">monorepo</a>.</p>
<ul>
<li>For notes on the latest release of the <a href="https://github.com/apache/arrow-java">Java
implementation</a>, see the latest <a href="https://github.com/apache/arrow-java/releases">Arrow
Java changelog</a>.</li>
<li>For notes on the latest release of the <a href="https://github.com/apache/arrow-js">JavaScript
implementation</a>, see the latest <a href="https://github.com/apache/arrow-js/releases">Arrow
JavaScript changelog</a>.</li>
<li>For notes on the latest release of the <a href="https://github.com/apache/arrow-rs">Rust
implementation</a> see the latest <a href="https://github.com/apache/arrow-rs/blob/main/CHANGELOG.md">Arrow Rust
changelog</a>.</li>
<li>For notes on the latest release of the <a href="https://github.com/apache/arrow-go">Go
implementation</a>, see the latest <a href="https://github.com/apache/arrow-go/releases">Arrow Go
changelog</a>.</li>
</ul>
<h2>Linux Packaging Notes</h2>
<p>We added support for AlmaLinux 10. You can use AlmaLinux 10 packages on Red Hat
Enterprise Linux 10 like distributions too.</p>
<p>We dropped support for CentOS Stream 8 because it reached EOL on 2024-05-31.</p>
<h2>MATLAB Notes</h2>
<h3>New Features</h3>
<p>Added support for creating an <code>arrow.tabular.Table</code> from a list of
<code>arrow.tabular.RecordBatch</code> instances
(<a href="https://github.com/apache/arrow/issues/46877">GH-46877</a>)</p>
<h3>Packaging</h3>
<p>The MLTBX available in apache/arrow's GitHub Releases area was built against
MATLAB R2025a.</p>
<h2>Python Notes</h2>
<p>Compatibility notes:</p>
<ul>
<li>Deprecated <code>PyExtensionType</code> has been removed
(<a href="https://github.com/apache/arrow/issues/46198">GH-46198</a>).</li>
<li>Deprecated <code>use_legacy_format</code>has been removed in favour of setting
<code>IpcWriteOptions</code> (<a href="https://github.com/apache/arrow/issues/46130">GH-46130</a>).</li>
<li>Due to SciPy 1.15's stricter sparse code changes are made to
<code>pa.SparseCXXMatrix</code> constructors and <code>pa.SparseCXXMatrix.to_scipy</code> methods
with migrating from <code>scipy.spmatrix</code> to <code>scipy.sparray</code>
(<a href="https://github.com/apache/arrow/issues/45229">GH-45229</a>).</li>
</ul>
<p>New features:</p>
<ul>
<li>PyArrow does not require NumPy anymore to generate <code>float16</code> scalars and
arrays (<a href="https://github.com/apache/arrow/issues/46611">GH-46611</a>).</li>
<li><code>pc.utf8_zero_fill</code> is now available in the compute module imitating
Python‚Äôs `str.zfill`` (<a href="https://github.com/apache/arrow/issues/46683">GH-46683</a>).</li>
<li><code>pa.arange</code> utility function is now available which creates an array of
evenly spaced values within a given interval
(<a href="https://github.com/apache/arrow/issues/46771">GH-46771</a>).</li>
<li>Scalar subclasses are now implementing Python protocols
(<a href="https://github.com/apache/arrow/issues/45653">GH-45653</a>).</li>
<li>It is possible now to specify footer metadata when opening IPC file for
writing using metadata keyword in <code>pa.ipc.new_file()</code>
(<a href="https://github.com/apache/arrow/issues/46222">GH-46222</a>).</li>
<li>DLPack is now implemented (export) on the Tensor class in C++ and available in
Python (<a href="https://github.com/apache/arrow/issues/39294">GH-39294</a>).</li>
</ul>
<p>Other improvements:</p>
<ul>
<li>Couple of improvements have been included in the Filesystems module:
Filesystem operations are more convenient to users by supporting explicit
<code>fsspec+{protocol}</code> and <code>hf://</code> filesystem URIs
(<a href="https://github.com/apache/arrow/issues/44900">GH-44900</a>),
<code>ConfigureManagedIdentityCredential</code> and <code>ConfigureClientSecretCredential</code>
have been exposed to <code>AzureFileSystem</code>
(<a href="https://github.com/apache/arrow/issues/46833">GH-46833</a>), <code>allow_delayed_open</code>
(<a href="https://github.com/apache/arrow/issues/45957">GH-45957</a>) and
<code>tls_ca_file_path</code> (<a href="https://github.com/apache/arrow/issues/40754">GH-40754</a>)
have been exposed to <code>S3FileSystem</code>.</li>
<li>Parquet module improvements include: mapping of logical types to Arrow
extension types by default
(<a href="https://github.com/apache/arrow/issues/44500">GH-44500</a>), UUID extension type
conversion support when writing or reading to/from Parquet
(<a href="https://github.com/apache/arrow/issues/43807">GH-43807</a>) and support for uniform
encryption when writing parquet files by exposing
<code>EncryptionConfiguration.uniform_encryption</code>
(<a href="https://github.com/apache/arrow/issues/38914">GH-38914</a>).</li>
<li><code>filter_expression</code> is exposed in <code>Table.join</code> and <code>Dataset.join</code> to
support filtering rows when performing hash joins with Acero
(<a href="https://github.com/apache/arrow/issues/46572">GH-46572</a>).</li>
<li><code>dim_names</code> argument can now be passed to <code>from_numpy_ndarray</code> constructor
(<a href="https://github.com/apache/arrow/issues/45531">GH-45531</a>).</li>
</ul>
<p>Relevant bug fixes:</p>
<ul>
<li><code>pyarrow.Table.to_struct_array</code> failure when the table is empty has been
fixed (<a href="https://github.com/apache/arrow/issues/46355">GH-46355</a>).</li>
<li>Filtering all rows with <code>RecordBatch.filter</code> using an expression now returns
empty table with same schema instead of erroring
(<a href="https://github.com/apache/arrow/issues/44366">GH-44366</a>).</li>
</ul>
<h2>Ruby and C GLib Notes</h2>
<p>A number of changes were made in the 21.0.0 release which affect both Ruby and C GLib:</p>
<ul>
<li>Added support for fixed shape tensor extension data type.</li>
<li>Added support for UUID extension data type.</li>
<li>Added support for fixed size list data type.</li>
<li>Added support for <a href="https://arrow.apache.org/docs/format/CDataInterface.html">the Arrow C data
interface</a> for
chunked array.</li>
<li>Added support for distinct count in array statistics.</li>
</ul>
<h3>Ruby</h3>
<p>There were no update only for Ruby.</p>
<h3>C GLib</h3>
<p>You must call <code>garrow_compute_initialize()</code> explicitly before you use
computation related features.</p>]]></content><author><name>pmc</name></author><category term="release" /><summary type="html"><![CDATA[The Apache Arrow team is pleased to announce the 21.0.0 release. This release covers over 2 months of development work and includes 339 resolved issues on 400 distinct commits from 82 distinct contributors. See the Install Page to learn how to get the libraries for your platform. The release notes below are not exhaustive and only expose selected highlights of the release. Many other bugfixes and improvements have been made: we refer you to the complete changelog. Community Since the 20.0.0 release, Alenka Frim has been invited to join the Project Management Committee (PMC). Thanks for your contributions and participation in the project! The Call for Speakers for the Apache Arrow Summit 2025 is now open! The Summit will take place on October 2nd, 2025 in Paris, France as part of PyData Paris. The call will be open until July 26, 2025. Please see the Call for Speakers link to submit a talk or the developer mailing list for more information. Arrow Flight RPC Notes In C++ and Python, a new IPC reader option was added to force data buffers to be aligned based on the data type, making it easier to work with systems that expected alignment (GH-32276). While this is not a Flight-specific option, it tended to occur with Flight due to implementation details. Also, C++ and Python are now consistent with other Flight implementations in allowing the schema of a FlightInfo to be omitted (GH-37677). We have accepted a donation of an ODBC driver for Flight SQL from Dremio (GH-46522). Note that the driver is not usable in its current state and contributors are working on implementing the rest of the driver. C++ Notes Compute The Cast function is now able to reorder fields when casting from one struct type to another; the fields are matched by name, not by index (GH-45028). Many compute kernels have been moved into a separate, optional, shared library (GH-25025). This improves modularity for dependency management in the application and reduces the Arrow C++ distribution size when the compute functionality is not being used. Note that some compute functions, such as the Cast function, will still be built for internal use in various Arrow components. Better half-float support has been added to some compute functions: is_nan, is_inf, is_finite, negate, negate_checked, sign (GH-45083); if_else, case_when, coalesce, choose, replace_with_mask, fill_null_forward, fill_null_backward (GH-37027); run_end_encode, run_end_decode (GH-46285). Better decimal32 and decimal64 support has been added to some compute functions: run_end_encode, run_end_decode (GH-46285). A new function utf8_zero_fill acts like Python's str.zfill method by providing a left-padding function that preserves the optional leading plus/minus sign (GH-46683). Decimal sum aggregation now produces a decimal result with an increased precision in order to reduce the risk of overflowing the result type (GH-35166). CSV Reading Duration columns is now supported (GH-40278). Dataset It is now possible to preserve order when writing a dataset multi-threaded. The feature is disabled by default (GH-26818). Filesystems The S3 filesystem can optionally be built into a separate DLL (GH-40343). Parquet Encryption A new SecureString class must now be used to communicate sensitive data (such as secret keys) with Parquet encryption APIs. This class automatically wipes its contents from memory when destroyed, unlike regular std::string (GH-31603). Type support The new VARIANT logical type is supported at a low level, and an extension type parquet.variant is added to reflect such columns when reading them to Arrow (GH-45937). The UUID logical type is automatically converted to/from the arrow.uuid canonical extension type when reading or writing Parquet data, respectively. The GEOMETRY and GEOGRAPHY logical types are supported (GH-45522). They are automatically converted to/from the corresponding GeoArrow extension type, if it has been registered by GeoArrow. Geospatial column statistics are also supported. It is now possible to read BYTE_ARRAY columns directly as LargeBinary or BinaryView, without any intermediate conversion from Binary. Similarly, those types can be written directly to Parquet (GH-43041). This allows bypassing the 2 GiB data per chunk limitation of the Binary type, and can also improve performance. This also applies to String types when a Parquet column has the STRING logical type. Similarly, LIST columns can now be read directly as LargeList rather than List. This allows bypassing the 2^31 values per chunk limitation of regular List types (GH-46676). Other Parquet improvements A new feature named Content-Defined Chunking improves deduplication of Parquet files with mostly identical contents, by choosing data page boundaries based on actual contents rather than a number of values. For that, it uses a rolling hash function, and the min and max chunk size can be chosen. The feature is disabled by default and can be enabled on a per-file basis in the Parquet WriterProperties (GH-45750). The EncodedStatistics of a column chunk are publicly exposed in ColumnChunkMetaData and can be read faster than if decoded as Statistics (GH-46462). SIMD optimizations for the BYTE_STREAM_SPLIT have been improved (GH-46788). Reading FIXED_LEN_BYTE_ARRAY data has been made significantly faster (up to 3x faster on some benchmarks). This benefits logical types such as FLOAT16 (GH-43891). Miscellaneous C++ changes The ARROW_USE_PRECOMPILED_HEADERS build option was removed, as CMAKE_UNITY_BUILD usually provides more benefits while requiring less maintenance. New data creation helpers ArrayFromJSONString, ChunkedArrayFromJSONString, DictArrayFromJSONString, ScalarFromJSONString and DictScalarFromJSONString are now exposed publicly. While not as high-performing as BufferBuilder and the concrete ArrayBuilder subclasses, they allow easy creation of test or example data, for example: ARROW_ASSIGN_OR_RAISE( auto string_array, arrow::ArrayFromJSONString(arrow::utf8(), R"(["Hello", "World", null])")); ARROW_ASSIGN_OR_RAISE( auto list_array, arrow::ArrayFromJSONString(arrow::list(arrow::int32()), "[[1, null, 2], [], [3]]")); Some APIs were changed to accept std::string_view instead of const std::string&amp;. Most uses of those APIs should not be affected (GH-46551). A new pretty-print option allows limiting element size when printing string or binary data (GH-46403). It is now possible to export Tensor data using DLPack (GH-39294). Half-float arrays can be properly diff'ed and pretty-printed (GH-36753). Some header files in arrow/util that were not supposed to be exposed are now made internal (GH-46459). C# Notes The C# Arrow implementation is being extracted from the Arrow monorepo into a standalone repository to allow it to have its own release cadence. See the mailing list for more information. This is the final release of the Arrow monorepo that will include the the C# implementation. Java, JavaScript, Go, and Rust Notes The Java, JavaScript, Go, and Rust Go projects have moved to separate repositories outside the main Arrow monorepo. For notes on the latest release of the Java implementation, see the latest Arrow Java changelog. For notes on the latest release of the JavaScript implementation, see the latest Arrow JavaScript changelog. For notes on the latest release of the Rust implementation see the latest Arrow Rust changelog. For notes on the latest release of the Go implementation, see the latest Arrow Go changelog. Linux Packaging Notes We added support for AlmaLinux 10. You can use AlmaLinux 10 packages on Red Hat Enterprise Linux 10 like distributions too. We dropped support for CentOS Stream 8 because it reached EOL on 2024-05-31. MATLAB Notes New Features Added support for creating an arrow.tabular.Table from a list of arrow.tabular.RecordBatch instances (GH-46877) Packaging The MLTBX available in apache/arrow's GitHub Releases area was built against MATLAB R2025a. Python Notes Compatibility notes: Deprecated PyExtensionType has been removed (GH-46198). Deprecated use_legacy_formathas been removed in favour of setting IpcWriteOptions (GH-46130). Due to SciPy 1.15's stricter sparse code changes are made to pa.SparseCXXMatrix constructors and pa.SparseCXXMatrix.to_scipy methods with migrating from scipy.spmatrix to scipy.sparray (GH-45229). New features: PyArrow does not require NumPy anymore to generate float16 scalars and arrays (GH-46611). pc.utf8_zero_fill is now available in the compute module imitating Python‚Äôs `str.zfill`` (GH-46683). pa.arange utility function is now available which creates an array of evenly spaced values within a given interval (GH-46771). Scalar subclasses are now implementing Python protocols (GH-45653). It is possible now to specify footer metadata when opening IPC file for writing using metadata keyword in pa.ipc.new_file() (GH-46222). DLPack is now implemented (export) on the Tensor class in C++ and available in Python (GH-39294). Other improvements: Couple of improvements have been included in the Filesystems module: Filesystem operations are more convenient to users by supporting explicit fsspec+{protocol} and hf:// filesystem URIs (GH-44900), ConfigureManagedIdentityCredential and ConfigureClientSecretCredential have been exposed to AzureFileSystem (GH-46833), allow_delayed_open (GH-45957) and tls_ca_file_path (GH-40754) have been exposed to S3FileSystem. Parquet module improvements include: mapping of logical types to Arrow extension types by default (GH-44500), UUID extension type conversion support when writing or reading to/from Parquet (GH-43807) and support for uniform encryption when writing parquet files by exposing EncryptionConfiguration.uniform_encryption (GH-38914). filter_expression is exposed in Table.join and Dataset.join to support filtering rows when performing hash joins with Acero (GH-46572). dim_names argument can now be passed to from_numpy_ndarray constructor (GH-45531). Relevant bug fixes: pyarrow.Table.to_struct_array failure when the table is empty has been fixed (GH-46355). Filtering all rows with RecordBatch.filter using an expression now returns empty table with same schema instead of erroring (GH-44366). Ruby and C GLib Notes A number of changes were made in the 21.0.0 release which affect both Ruby and C GLib: Added support for fixed shape tensor extension data type. Added support for UUID extension data type. Added support for fixed size list data type. Added support for the Arrow C data interface for chunked array. Added support for distinct count in array statistics. Ruby There were no update only for Ruby. C GLib You must call garrow_compute_initialize() explicitly before you use computation related features.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>