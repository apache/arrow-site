<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://arrow.apache.org/feed.xml" rel="self" type="application/atom+xml" /><link href="https://arrow.apache.org/" rel="alternate" type="text/html" /><updated>2025-07-18T16:53:15-04:00</updated><id>https://arrow.apache.org/feed.xml</id><title type="html">Apache Arrow</title><subtitle>Apache Arrow is the universal columnar format and multi-language toolbox for fast data interchange and in-memory analytics. It specifies a standardized language-independent column-oriented memory format for flat and nested data, organized for efficient analytic operations on modern hardware. It also provides computational libraries and zero-copy streaming messaging and interprocess communication. Languages currently supported include C, C++, C#, Go, Java, JavaScript, MATLAB, Python, R, Ruby, and Rust.</subtitle><entry><title type="html">Recent Improvements to Hash Join in Arrow C++</title><link href="https://arrow.apache.org/blog/2025/07/18/recent-improvements-to-hash-join/" rel="alternate" type="text/html" title="Recent Improvements to Hash Join in Arrow C++" /><published>2025-07-18T00:00:00-04:00</published><updated>2025-07-18T00:00:00-04:00</updated><id>https://arrow.apache.org/blog/2025/07/18/recent-improvements-to-hash-join</id><content type="html" xml:base="https://arrow.apache.org/blog/2025/07/18/recent-improvements-to-hash-join/"><![CDATA[<!--

-->

<p><em>Editor‚Äôs Note: Apache Arrow is an expansive project, ranging from the Arrow columnar format itself, to its numerous specifications, and a long list of implementations. Arrow is also an expansive project in terms of its community of contributors. In this blog post, we‚Äôd like to highlight recent work by Apache Arrow Committer Rossi Sun on improving the performance and stability of Arrow‚Äôs embeddable query execution engine: Acero.</em></p>

<h1 id="introduction">Introduction</h1>

<p>Hash join is a fundamental operation in analytical processing engines ‚Äî it matches rows from two tables based on key values using a hash table for fast lookup. In the C++ implementation of Apache Arrow, the hash join is implemented in the C++ engine Acero, which powers query execution in bindings like PyArrow and the R Arrow package. Even if you haven‚Äôt used Acero directly, your code may already be benefiting from it under the hood.</p>

<p>For example, this simple PyArrow example uses Acero:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">pyarrow</span> <span class="k">as</span> <span class="n">pa</span>

<span class="n">t1</span> <span class="o">=</span> <span class="n">pa</span><span class="p">.</span><span class="nf">table</span><span class="p">({</span><span class="sh">'</span><span class="s">id</span><span class="sh">'</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
               <span class="sh">'</span><span class="s">year</span><span class="sh">'</span><span class="p">:</span> <span class="p">[</span><span class="mi">2020</span><span class="p">,</span> <span class="mi">2022</span><span class="p">,</span> <span class="mi">2019</span><span class="p">]})</span>
<span class="n">t2</span> <span class="o">=</span> <span class="n">pa</span><span class="p">.</span><span class="nf">table</span><span class="p">({</span><span class="sh">'</span><span class="s">id</span><span class="sh">'</span><span class="p">:</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span>
               <span class="sh">'</span><span class="s">n_legs</span><span class="sh">'</span><span class="p">:</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">100</span><span class="p">],</span>
               <span class="sh">'</span><span class="s">animal</span><span class="sh">'</span><span class="p">:</span> <span class="p">[</span><span class="sh">"</span><span class="s">Brittle stars</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Centipede</span><span class="sh">"</span><span class="p">]})</span>

<span class="n">t1</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">t2</span><span class="p">,</span> <span class="sh">'</span><span class="s">id</span><span class="sh">'</span><span class="p">).</span><span class="nf">combine_chunks</span><span class="p">().</span><span class="nf">sort_by</span><span class="p">(</span><span class="sh">'</span><span class="s">year</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div>

<p>Acero was originally created in 2019 to demonstrate that the ever-growing library of compute kernels in Arrow C++ could be linked together into realistic workflows and also to take advantage of the emerging Datasets API to give these workflows access to data. Rather than aiming to compete with full query engines like DuckDB, Acero focuses on enabling flexible, composable, and embeddable query execution ‚Äî serving as a building block for tools and systems that need fast, modular analytics capabilities ‚Äî including those built atop Arrow C++, or integrating via bindings like PyArrow, Substrait, or ADBC.</p>

<p>Across several recent Arrow C++ releases, we‚Äôve made substantial improvements to the hash join implementation to address common user pain points. These changes improve stability, memory efficiency, and parallel performance, with a focus on making joins more usable and scalable out of the box. If you‚Äôve had trouble using Arrow‚Äôs hash join in the past, now is a great time to try again.</p>

<h1 id="scaling-safely-improvements-to-stability">Scaling Safely: Improvements to Stability</h1>

<p>In earlier versions of Arrow C++, the hash join implementation used internal data structures that weren‚Äôt designed for very large datasets and lacked safeguards in some of the underlying memory operations. These limitations rarely surfaced in small to medium workloads but became problematic at scale, manifesting as crashes or subtle correctness issues.</p>

<p>At the core of Arrow‚Äôs join implementation is a compact, row-oriented structure known as the ‚Äúrow table‚Äù. While Arrow‚Äôs data model is columnar, its hash join implementation operates in a row-wise fashion ‚Äî similar to modern engines like DuckDB and Meta‚Äôs Velox. This layout minimizes CPU cache misses during hash table lookups by collocating keys, payloads, and null bits in memory so they can be accessed together.</p>

<p>In previous versions, the row table used 32-bit offsets to reference packed rows. This capped each table‚Äôs size to 4GB and introduced risks of overflow when working with large datasets or wide rows. Several reported issues ‚Äî <a href="https://github.com/apache/arrow/issues/34474">GH-34474</a>, <a href="https://github.com/apache/arrow/issues/41813">GH-41813</a>, and <a href="https://github.com/apache/arrow/issues/43202">GH-43202</a> ‚Äî highlighted the limitations of this design. In response, PR <a href="https://github.com/apache/arrow/pull/43389">GH-43389</a> widened the internal offset type to 64-bit, reworking key parts of the row table infrastructure to support larger data sizes more safely and scalably.</p>

<p>Besides the offset limitation, earlier versions of Arrow C++ also included overflow-prone logic in the buffer indexing paths used throughout the hash join implementation. Many internal calculations assumed that 32-bit integers were sufficient for addressing memory ‚Äî a fragile assumption when working with large datasets or wide rows. These issues appeared not only in conventional C++ indexing code but also in Arrow‚Äôs SIMD-accelerated paths ‚Äî Arrow includes heavy SIMD specializations, used to speed up operations like hash table probing and row comparison. Together, these assumptions led to subtle overflows and incorrect behavior, as documented in issues like <a href="https://github.com/apache/arrow/issues/44513">GH-44513</a>, <a href="https://github.com/apache/arrow/issues/45334">GH-45334</a>, and <a href="https://github.com/apache/arrow/issues/45506">GH-45506</a>.</p>

<p>Two representative examples:</p>

<ul>
  <li>Row-wise buffer access in C++</li>
</ul>

<p>The aforementioned row table stores fixed-length data in tightly packed buffers. Accessing a particular row (and optionally a column within it) typically involves <a href="https://github.com/apache/arrow/blob/12f62653c825fbf305bfde61c112d2aa69203c62/cpp/src/arrow/acero/swiss_join_internal.h#L120">pointer arithmetic</a>:</p>
<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">const</span> <span class="kt">uint8_t</span><span class="o">*</span> <span class="n">row_ptr</span> <span class="o">=</span> <span class="n">row_ptr_base</span> <span class="o">+</span> <span class="n">row_length</span> <span class="o">*</span> <span class="n">row_id</span><span class="p">;</span>
</code></pre></div></div>
<p>When both <code class="language-plaintext highlighter-rouge">row_length</code> and <code class="language-plaintext highlighter-rouge">row_id</code> are large 32-bit integers, their product can overflow.</p>

<p>Similarly, accessing null masks involves <a href="https://github.com/apache/arrow/blob/12f62653c825fbf305bfde61c112d2aa69203c62/cpp/src/arrow/acero/swiss_join_internal.h#L150">null-bit indexing arithmetic</a>:</p>
<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">int64_t</span> <span class="n">bit_id</span> <span class="o">=</span> <span class="n">row_id</span> <span class="o">*</span> <span class="n">null_mask_num_bytes</span> <span class="o">*</span> <span class="mi">8</span> <span class="o">+</span> <span class="n">pos_after_encoding</span><span class="p">;</span>
</code></pre></div></div>
<p>The intermediate multiplication is performed using 32-bit arithmetic and can overflow even though the final result is stored in a 64-bit variable.</p>

<ul>
  <li>SIMD gathers with 32-bit offsets</li>
</ul>

<p>One essential SIMD instruction is the AVX2 intrinsic <code class="language-plaintext highlighter-rouge">__m256i _mm256_i32gather_epi32(int const * base, __m256i vindex, const int scale);</code>, which performs a parallel memory gather of eight 32-bit integers based on eight 32-bit signed offsets. It was extensively used in Arrow for hash table operations, for example, <a href="https://github.com/apache/arrow/blob/0a00e25f2f6fb927fb555b69038d0be9b9d9f265/cpp/src/arrow/compute/key_map_internal_avx2.cc#L404">fetching 8 group IDs</a> (hash table slots) in parallel during hash table probing:</p>
<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">__m256i</span> <span class="n">group_id</span> <span class="o">=</span> <span class="n">_mm256_i32gather_epi32</span><span class="p">(</span><span class="n">elements</span><span class="p">,</span> <span class="n">pos</span><span class="p">,</span> <span class="mi">1</span><span class="p">);</span>
</code></pre></div></div>
<p>and <a href="https://github.com/apache/arrow/blob/69e8a78c018da88b60f9eb2b3b45703f81f3c93d/cpp/src/arrow/compute/row/compare_internal_avx2.cc#L284">loading 8 corresponding key values</a> from the right-side input in parallel for comparison:</p>
<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">__m256i</span> <span class="n">right</span> <span class="o">=</span> <span class="n">_mm256_i32gather_epi32</span><span class="p">((</span><span class="k">const</span> <span class="kt">int</span><span class="o">*</span><span class="p">)</span><span class="n">right_base</span><span class="p">,</span> <span class="n">offset_right</span><span class="p">,</span> <span class="mi">1</span><span class="p">);</span>
</code></pre></div></div>
<p>If any of the computed offsets exceed <code class="language-plaintext highlighter-rouge">2^31 - 1</code>, they wrap into the negative range, which can lead to invalid memory access (i.e., a crash) or, more subtly, fetch data from a valid but incorrect location ‚Äî producing silently wrong results (trust me, you don‚Äôt want to debug that).</p>

<p>To mitigate these risks, PR <a href="https://github.com/apache/arrow/pull/45108">GH-45108</a>, <a href="https://github.com/apache/arrow/pull/45336">GH-45336</a>, and <a href="https://github.com/apache/arrow/pull/45515">GH-45515</a> promoted critical arithmetic to 64-bit and reworked SIMD logic to use safer indexing. Buffer access logic was also encapsulated in safer abstractions to avoid repeated manual casting or unchecked offset math. These examples are not unique to Arrow ‚Äî they reflect common pitfalls in building data-intensive systems, where unchecked assumptions about integer sizes can silently compromise correctness.</p>

<p>Together, these changes make Arrow‚Äôs hash join implementation significantly more robust and better equipped for modern data workloads. These foundations not only resolve known issues but also reduce the risk of similar bugs in future development.</p>

<h1 id="leaner-memory-usage">Leaner Memory Usage</h1>

<p>While refining overflow-prone parts of the hash join implementation, I ended up examining most of the code path for potential pitfalls. When doing this kind of work, one sits down quietly and interrogates every line ‚Äî asking not just whether an intermediate value might overflow, but whether it even needs to exist at all. And during that process, I came across something unrelated to overflow ‚Äî but even more impactful.</p>

<p>In a textbook hash join algorithm, once the right-side table (the build-side) is fully accumulated, a hash table is constructed to support probing the left-side table (the probe-side) for matches. To parallelize this build step, Arrow C++‚Äôs implementation partitions the build-side into <code class="language-plaintext highlighter-rouge">N</code> partitions ‚Äî typically matching the number of available CPU cores ‚Äî and builds a separate hash table for each partition in parallel. These are then merged into a final, unified hash table used during the probe phase.</p>

<p>The issue? The memory footprint. The total size of the partitioned hash tables is roughly equal to that of the final hash table, but they were being held in memory even after merging. Once the final hash table was built, these temporary structures had no further use ‚Äî yet they persisted through the entire join operation. There were no crashes, no warnings, no visible red flags ‚Äî just silent overhead.</p>

<p>Once spotted, the fix was straightforward: restructure the join process to release these buffers immediately after the merge. The change was implemented in PR <a href="https://github.com/apache/arrow/issues/45552">GH-45552</a>. The memory profiles below illustrate its impact.</p>

<div style="display: flex; gap: 16px; justify-content: center; align-items: flex-start;">
  <img src="/img/recent-improvements-to-hash-join/memory-profile-baseline.png" width="50%" class="img-responsive" alt="Memory profile before" aria-hidden="true" />
  <img src="/img/recent-improvements-to-hash-join/memory-profile-opt.png" width="50%" class="img-responsive" alt="Memory profile after" aria-hidden="true" />
</div>

<p>At <code class="language-plaintext highlighter-rouge">A</code>, memory usage rises steadily as the join builds partitioned hash tables in parallel. <code class="language-plaintext highlighter-rouge">B</code> marks the merge point, where these partitions are combined into a final, unified hash table. <code class="language-plaintext highlighter-rouge">C</code> represents the start of the probe phase, where the left-side table is scanned and matched against the final hash table. Memory begins to rise again as join results are materialized. <code class="language-plaintext highlighter-rouge">D</code> is the peak of the join operation, just before memory begins to drop as processing completes. The ‚Äúleap of faith‚Äù occurs at the star on the right profile, where the partitioned hash tables are released immediately after merging. This early release frees up substantial memory and makes room for downstream processing ‚Äî reducing the overall peak memory observed at <code class="language-plaintext highlighter-rouge">D</code>.</p>

<p>This improvement already benefits real-world scenarios ‚Äî for example, the <a href="https://duckdblabs.github.io/db-benchmark/">DuckDB Labs DB Benchmark</a>. Some benchmark queries that previously failed with out-of-memory (OOM) errors can now complete successfully ‚Äî as shown in the comparison below.</p>

<div style="display: flex; gap: 16px; justify-content: center; align-items: flex-start;">
  <img src="/img/recent-improvements-to-hash-join/duckdb-bench-oom-baseline.png" width="50%" class="img-responsive" alt="DuckDB benchmark OOM before" aria-hidden="true" />
  <img src="/img/recent-improvements-to-hash-join/duckdb-bench-oom-opt.png" width="50%" class="img-responsive" alt="DuckDB benchmark OOM after" aria-hidden="true" />
</div>

<p>As one reviewer noted in the PR, this was a ‚Äúlow-hanging fruit.‚Äù And sometimes, meaningful performance gains don‚Äôt come from tuning hot loops or digging through flame graphs ‚Äî they come from noticing something that doesn‚Äôt feel right and asking: why are we still keeping this around?</p>

<h1 id="faster-execution-through-better-parallelism">Faster Execution Through Better Parallelism</h1>

<p>Not every improvement comes from poring over flame graphs ‚Äî but some definitely do. Performance is, after all, the most talked-about aspect of any query engine. So, how about a nice cup of flame graph?</p>

<p><img src="/img/recent-improvements-to-hash-join/a-nice-cup-of-flame-graph.png" width="100%" class="img-responsive" alt="A nice cup of flame graph" aria-hidden="true" /></p>

<p>It‚Äôs hard not to notice the long, flat bar dominating the middle ‚Äî especially with the rather alarming word ‚ÄúLock‚Äù in it. That‚Äôs our red flag.</p>

<p>We‚Äôve mentioned that in the build phase, we build partitioned hash tables in parallel. In earlier versions of Arrow C++, this parallelism was implemented on a batch basis ‚Äî each thread processed a build-side batch concurrently. Since each batch contained arbitrary data that could fall into any partition, threads had to synchronize when accessing shared partitions. This was managed through <a href="https://github.com/apache/arrow/blob/196cde38c112d32a944afe978b6da9c7ce935ef7/cpp/src/arrow/acero/partition_util.h#L93">locks on partitions</a>. Although we introduced some randomness in the locking order to reduce contention, it remained high ‚Äî clearly visible in the flame graph.</p>

<p>To mitigate this contention, we restructured the build phase in PR <a href="https://github.com/apache/arrow/issues/45612">GH-45612</a>. Instead of having all threads partition and insert at once ‚Äî each thread touching every hash table ‚Äî we split the work into two distinct stages. In the first partition stage, <code class="language-plaintext highlighter-rouge">M</code> threads take their assigned batches and only partition them, recording which rows belong to which partition. No insertion happens yet ‚Äî just classification. Then comes the second, newly separated build stage. Here, <code class="language-plaintext highlighter-rouge">N</code> threads take over, and each thread is responsible for building just one of the <code class="language-plaintext highlighter-rouge">N</code> partitioned hash tables. Every thread scans all the relevant partitions across all batches but inserts only the rows belonging to its assigned partition. This restructuring eliminates the need for locking between threads during insertion ‚Äî each thread now has exclusive access to its partitioned hash table. By decoupling the work this way, we turned a highly contentious operation into a clean, embarrassingly parallel one. As a result, we saw performance improve by up to 10x in dedicated build benchmarks. The <a href="https://github.com/apache/arrow/blob/196cde38c112d32a944afe978b6da9c7ce935ef7/cpp/src/arrow/acero/hash_join_benchmark.cc#L302">example</a> below is from a more typical, general-purpose workload ‚Äî not especially build-heavy ‚Äî but it still shows a solid 2x speedup. In the chart, the leap of faith ‚Äî marked by the purple icons üü£‚¨áÔ∏è ‚Äî represents results with this improvement applied, while the gray and black ones show earlier runs before the change.</p>

<p><img src="/img/recent-improvements-to-hash-join/internal-benchmark.png" width="100%" class="img-responsive" alt="Internal benchmark" aria-hidden="true" /></p>

<p>Also in real-world scenarios like the <a href="https://duckdblabs.github.io/db-benchmark/">DuckDB Labs DB Benchmark</a>, we‚Äôve observed similar gains. The comparison below shows around a 2x improvement in query performance after this change was applied.</p>

<div style="display: flex; gap: 16px; justify-content: center; align-items: flex-start;">
  <img src="/img/recent-improvements-to-hash-join/duckdb-bench-perf-baseline.png" width="50%" class="img-responsive" alt="DuckDB benchmark perf before" aria-hidden="true" />
  <img src="/img/recent-improvements-to-hash-join/duckdb-bench-perf-opt.png" width="50%" class="img-responsive" alt="DuckDB benchmark perf after" aria-hidden="true" />
</div>

<p>Additional improvements include <a href="https://github.com/apache/arrow/pull/43832">GH-43832</a>, which extends AVX2 acceleration to more probing code paths, and <a href="https://github.com/apache/arrow/pull/45918">GH-45918</a>, which introduces parallelism to a previously sequential task phase. These target more specialized scenarios and edge cases.</p>

<h2 id="closing">Closing</h2>

<p>These improvements reflect ongoing investment in Arrow C++‚Äôs execution engine and a commitment to delivering fast, robust building blocks for analytic workloads. They are available in recent Arrow C++ releases and exposed through higher-level bindings like PyArrow and the Arrow R package ‚Äî starting from version 18.0.0, with the most significant improvements landing in 20.0.0. If joins were a blocker for you before ‚Äî due to memory, scale, or correctness ‚Äî recent changes may offer a very different experience.</p>

<p>The Arrow C++ engine is not just alive ‚Äî it‚Äôs improving in meaningful, user-visible ways. We‚Äôre also actively monitoring for further issues and open to expanding the design based on user feedback and real-world needs. If you‚Äôve tried joins in the past and run into performance or stability issues, we encourage you to give them another try and file an <a href="https://github.com/apache/arrow/issues">issue on GitHub</a> if you run into any issues.</p>

<p>If you have any questions about this blog post, please feel free to contact the author, <a href="mailto:zanmato1984@gmail.com">Rossi Sun</a>.</p>]]></content><author><name>zanmato</name></author><category term="application" /><summary type="html"><![CDATA[A deep dive into recent improvements to Apache Arrow‚Äôs hash join implementation ‚Äî enhancing stability, memory efficiency, and parallel performance for modern analytic workloads.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Apache Arrow ADBC 19 (Libraries) Release</title><link href="https://arrow.apache.org/blog/2025/07/08/adbc-19-release/" rel="alternate" type="text/html" title="Apache Arrow ADBC 19 (Libraries) Release" /><published>2025-07-08T00:00:00-04:00</published><updated>2025-07-08T00:00:00-04:00</updated><id>https://arrow.apache.org/blog/2025/07/08/adbc-19-release</id><content type="html" xml:base="https://arrow.apache.org/blog/2025/07/08/adbc-19-release/"><![CDATA[<!--

-->

<p>The Apache Arrow team is pleased to announce the version 19 release of
the Apache Arrow ADBC libraries. This release includes <a href="https://github.com/apache/arrow-adbc/milestone/23"><strong>60
resolved issues</strong></a> from <a href="#contributors"><strong>27 distinct contributors</strong></a>.</p>

<p>This is a release of the <strong>libraries</strong>, which are at version 19.  The
<a href="https://arrow.apache.org/adbc/19/format/specification.html"><strong>API specification</strong></a> is versioned separately and is at
version 1.1.0.</p>

<p>The subcomponents are versioned independently:</p>

<ul>
  <li>C/C++/GLib/Go/Python/Ruby: 1.7.0</li>
  <li>C#: 0.19.0</li>
  <li>Java: 0.19.0</li>
  <li>R: 0.19.0</li>
  <li>Rust: 0.19.0</li>
</ul>

<p>The release notes below are not exhaustive and only expose selected
highlights of the release. Many other bugfixes and improvements have
been made: we refer you to the <a href="https://github.com/apache/arrow-adbc/blob/apache-arrow-adbc-19/CHANGELOG.md">complete changelog</a>.</p>

<h2 id="release-highlights">Release Highlights</h2>

<ul>
  <li>Apache Hive/Impala/Spark, Databricks: these drivers have received a plethora of improvements,
optimizations, and bug fixes.</li>
  <li>DataFusion: the arrow crate version requirement is now independent from that
of the <code class="language-plaintext highlighter-rouge">adbc_core</code> crate, to make it easier to use older versions of the
dependency when not using the DataFusion driver
(<a href="https://github.com/apache/arrow-adbc/pull/3017">#3017</a>).</li>
  <li>Driver Manager: drivers can now be loaded by searching configuration
directories (or on Windows, the registry) for ‚Äòmanifest‚Äô files describing
where the driver is located
(<a href="https://github.com/apache/arrow-adbc/pull/2918">#2918</a>,
<a href="https://github.com/apache/arrow-adbc/pull/3018">#3018</a>,
<a href="https://github.com/apache/arrow-adbc/pull/3021">#3021</a>,
<a href="https://github.com/apache/arrow-adbc/pull/3036">#3036</a>,
<a href="https://github.com/apache/arrow-adbc/pull/3041">#3041</a>).  Add freethreaded
wheels for the driver manager in Python 3.13.  These are still experimental;
please <a href="https://github.com/apache/arrow-adbc/issues">file a bug report</a> with
any feedback (<a href="https://github.com/apache/arrow-adbc/pull/3063">#3063</a>).
Make it easier to use the DB-API layer in Python without depending on
PyArrow, to make it easier for users of polars and other libraries
(<a href="https://github.com/apache/arrow-adbc/pull/2839">#2839</a>).</li>
  <li>Flight SQL (Go): the last release unintentionally renamed the entrypoint
symbol.  Both the old and ‚Äònew‚Äô names are now present
(<a href="https://github.com/apache/arrow-adbc/pull/3056">#3056</a>).
Use custom certificates (when present) for OAuth
(<a href="https://github.com/apache/arrow-adbc/pull/2829">#2829</a>).</li>
  <li>PostgreSQL: ingest zoned timestamps as <code class="language-plaintext highlighter-rouge">TIMESTAMP WITH TIME ZONE</code>
(<a href="https://github.com/apache/arrow-adbc/pull/2904">#2904</a>) and support
reading the <code class="language-plaintext highlighter-rouge">int2vector</code> type
(<a href="https://github.com/apache/arrow-adbc/pull/2919">#2919</a>).</li>
  <li>Snowflake: fix issues with COPY concurrency options
(<a href="https://github.com/apache/arrow-adbc/pull/2805">#2805</a>), logging spam
(<a href="https://github.com/apache/arrow-adbc/pull/2807">#2807</a>), and boolean
result columns (<a href="https://github.com/apache/arrow-adbc/pull/2854">#2854</a>).
Add an option to return timestamps in microseconds to avoid overflow with
extreme values (<a href="https://github.com/apache/arrow-adbc/pull/2917">#2917</a>).</li>
  <li>Rust: make a breaking change from <code class="language-plaintext highlighter-rouge">&amp;mut self</code> to <code class="language-plaintext highlighter-rouge">&amp;mut</code> in one API to enable
fearless concurrency
(<a href="https://github.com/apache/arrow-adbc/pull/2788">#2788</a>).</li>
  <li>Add experimental support for integrating with
<a href="https://opentelemetry.io/">OpenTelemetry</a>, starting with the Snowflake
driver.  (<a href="https://github.com/apache/arrow-adbc/pull/2729">#2729</a>,
<a href="https://github.com/apache/arrow-adbc/pull/2825">#2825</a>,
<a href="https://github.com/apache/arrow-adbc/pull/2847">#2847</a>,
<a href="https://github.com/apache/arrow-adbc/pull/2951">#2951</a>).</li>
  <li>Improve the build experience when using Meson
(<a href="https://github.com/apache/arrow-adbc/pull/2848">#2848</a>,
<a href="https://github.com/apache/arrow-adbc/pull/2849">#2849</a>).
Make it easier to statically link drivers
(<a href="https://github.com/apache/arrow-adbc/pull/2738">#2738</a>).</li>
</ul>

<h2 id="contributors">Contributors</h2>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ git shortlog --perl-regexp --author='^((?!dependabot\[bot\]).*)$' -sn apache-arrow-adbc-18..apache-arrow-adbc-19
    32	David Li
    20	Daijiro Fukuda
    16	Todd Meng
    10	eric-wang-1990
     7	eitsupi
     6	Matt Topol
     6	davidhcoe
     5	Bruce Irschick
     5	Sutou Kouhei
     3	Dewey Dunnington
     3	Jacky Hu
     2	Alex Guo
     2	Bryce Mecum
     2	Jade Wang
     2	James Thompson
     2	William Ayd
     2	qifanzhang-ms
     1	Arseny Tsypushkin
     1	Felipe Oliveira Carvalho
     1	Hiroyuki Sato
     1	H√©lder Greg√≥rio
     1	Jan-Hendrik Zab
     1	Jarro van Ginkel
     1	Jolan Rensen
     1	Sergei Grebnov
     1	Sudhir Reddy Emmadi
     1	amangoyal
</code></pre></div></div>

<h2 id="roadmap">Roadmap</h2>

<p>We plan to continue expanding support for features like OpenTelemetry that
have been introduced experimentally.</p>

<p>There is some discussion on a potential second revision of ADBC to include
more missing functionality and asynchronous API support.  For more, see the
<a href="https://github.com/apache/arrow-adbc/milestone/8">milestone</a>.  We would
welcome suggestions on APIs that could be added or extended.  Some of the
contributors are planning to begin work on a proposal in the eventual future.</p>

<h2 id="getting-involved">Getting Involved</h2>

<p>We welcome questions and contributions from all interested.  Issues
can be filed on <a href="https://github.com/apache/arrow-adbc/issues">GitHub</a>, and questions can be directed to GitHub
or the <a href="/community/">Arrow mailing lists</a>.</p>]]></content><author><name>pmc</name></author><category term="release" /><summary type="html"><![CDATA[The Apache Arrow team is pleased to announce the version 19 release of the Apache Arrow ADBC libraries. This release includes 60 resolved issues from 27 distinct contributors. This is a release of the libraries, which are at version 19. The API specification is versioned separately and is at version 1.1.0. The subcomponents are versioned independently: C/C++/GLib/Go/Python/Ruby: 1.7.0 C#: 0.19.0 Java: 0.19.0 R: 0.19.0 Rust: 0.19.0 The release notes below are not exhaustive and only expose selected highlights of the release. Many other bugfixes and improvements have been made: we refer you to the complete changelog. Release Highlights Apache Hive/Impala/Spark, Databricks: these drivers have received a plethora of improvements, optimizations, and bug fixes. DataFusion: the arrow crate version requirement is now independent from that of the adbc_core crate, to make it easier to use older versions of the dependency when not using the DataFusion driver (#3017). Driver Manager: drivers can now be loaded by searching configuration directories (or on Windows, the registry) for ‚Äòmanifest‚Äô files describing where the driver is located (#2918, #3018, #3021, #3036, #3041). Add freethreaded wheels for the driver manager in Python 3.13. These are still experimental; please file a bug report with any feedback (#3063). Make it easier to use the DB-API layer in Python without depending on PyArrow, to make it easier for users of polars and other libraries (#2839). Flight SQL (Go): the last release unintentionally renamed the entrypoint symbol. Both the old and ‚Äònew‚Äô names are now present (#3056). Use custom certificates (when present) for OAuth (#2829). PostgreSQL: ingest zoned timestamps as TIMESTAMP WITH TIME ZONE (#2904) and support reading the int2vector type (#2919). Snowflake: fix issues with COPY concurrency options (#2805), logging spam (#2807), and boolean result columns (#2854). Add an option to return timestamps in microseconds to avoid overflow with extreme values (#2917). Rust: make a breaking change from &amp;mut self to &amp;mut in one API to enable fearless concurrency (#2788). Add experimental support for integrating with OpenTelemetry, starting with the Snowflake driver. (#2729, #2825, #2847, #2951). Improve the build experience when using Meson (#2848, #2849). Make it easier to statically link drivers (#2738). Contributors $ git shortlog --perl-regexp --author='^((?!dependabot\[bot\]).*)$' -sn apache-arrow-adbc-18..apache-arrow-adbc-19 32 David Li 20 Daijiro Fukuda 16 Todd Meng 10 eric-wang-1990 7 eitsupi 6 Matt Topol 6 davidhcoe 5 Bruce Irschick 5 Sutou Kouhei 3 Dewey Dunnington 3 Jacky Hu 2 Alex Guo 2 Bryce Mecum 2 Jade Wang 2 James Thompson 2 William Ayd 2 qifanzhang-ms 1 Arseny Tsypushkin 1 Felipe Oliveira Carvalho 1 Hiroyuki Sato 1 H√©lder Greg√≥rio 1 Jan-Hendrik Zab 1 Jarro van Ginkel 1 Jolan Rensen 1 Sergei Grebnov 1 Sudhir Reddy Emmadi 1 amangoyal Roadmap We plan to continue expanding support for features like OpenTelemetry that have been introduced experimentally. There is some discussion on a potential second revision of ADBC to include more missing functionality and asynchronous API support. For more, see the milestone. We would welcome suggestions on APIs that could be added or extended. Some of the contributors are planning to begin work on a proposal in the eventual future. Getting Involved We welcome questions and contributions from all interested. Issues can be filed on GitHub, and questions can be directed to GitHub or the Arrow mailing lists.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Apache Arrow nanoarrow 0.7.0 Release</title><link href="https://arrow.apache.org/blog/2025/07/02/nanoarrow-0.7.0-release/" rel="alternate" type="text/html" title="Apache Arrow nanoarrow 0.7.0 Release" /><published>2025-07-02T00:00:00-04:00</published><updated>2025-07-02T00:00:00-04:00</updated><id>https://arrow.apache.org/blog/2025/07/02/nanoarrow-0.7.0-release</id><content type="html" xml:base="https://arrow.apache.org/blog/2025/07/02/nanoarrow-0.7.0-release/"><![CDATA[<!--

-->

<p>The Apache Arrow team is pleased to announce the 0.7.0 release of
Apache Arrow nanoarrow. This release covers 117 resolved issues from
12 contributors.</p>

<h2 id="release-highlights">Release Highlights</h2>

<ul>
  <li>Migrate Python bindings to Meson Python</li>
  <li>Better support for shared linkage</li>
  <li>ZSTD Decompression support in IPC reader</li>
  <li>Decimal32, Decimal64, ListView and LargeListView support</li>
  <li>Support for vcpkg</li>
</ul>

<p>See the
<a href="https://github.com/apache/arrow-nanoarrow/blob/apache-arrow-nanoarrow-0.7.0-rc1/CHANGELOG.md">Changelog</a>
for a detailed list of contributions to this release.</p>

<h2 id="features">Features</h2>

<h3 id="meson-python">Meson Python</h3>

<p>The Python bindings now use <a href="https://mesonbuild.com/meson-python/">Meson Python</a> as
the build backend. The main benefit is that adding C or C++ library dependencies
like ZSTD is much simpler than with setuptools which was needed to add the new
decompression support to the Python bindings.</p>

<p>Thanks to <a href="https://github.com/WillAyd">@WillAyd</a> for this contribution and continued
maintenance of the Python build infrastructure!</p>

<h3 id="shared-linkage">Shared Linkage</h3>

<p>The nanoarrow C library is generally designed to be statically linked into an
application or library; however, there were some applications that did want
shared linkage and on Windows some extra work was needed to ensure this worked
as intended. Version 0.7.0 includes the appropriate DLL import/export attributes
and adds dedicated <code class="language-plaintext highlighter-rouge">nanoarrow_shared</code> and <code class="language-plaintext highlighter-rouge">nanoarrow_static</code> targets to the CMake
configuration to explicitly choose a strategy (linking to <code class="language-plaintext highlighter-rouge">nanoarrow</code> will continue
to use the CMake default as it did in previous versions).</p>

<p>Thanks to <a href="https://github.com/m-kuhn">@m-kuhn</a> for authoring the initial vcpkg
configuration that brought this to our attention!</p>

<h3 id="zstd-decompression-support">ZSTD Decompression Support</h3>

<p>The Arrow IPC reader included in the nanoarrow C library supports most features
of the Arrow IPC format; however, decompression support was missing which made
the library and its bindings unusable for some common use cases. In 0.7.0,
decompression support was added to the C library and R and Python bindings.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">library</span><span class="p">(</span><span class="n">nanoarrow</span><span class="p">)</span><span class="w">

</span><span class="n">url</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="s2">"https://github.com/geoarrow/geoarrow-data/releases/download/v0.2.0/ns-water_water-point.arrows"</span><span class="w">
</span><span class="n">read_nanoarrow</span><span class="p">(</span><span class="n">url</span><span class="p">)</span><span class="w"> </span><span class="o">|&gt;</span><span class="w">
  </span><span class="n">tibble</span><span class="o">::</span><span class="n">as_tibble</span><span class="p">()</span><span class="w">
</span><span class="c1">#&gt; # A tibble: 44,690 √ó 8</span><span class="w">
</span><span class="c1">#&gt;    OBJECTID FEAT_CODE ZVALUE PT_CLASS NAMEID_1 NAME_1 HID             geometry$x</span><span class="w">
</span><span class="c1">#&gt;       &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt;    &lt;int&gt; &lt;chr&gt;    &lt;chr&gt;  &lt;chr&gt;                &lt;dbl&gt;</span><span class="w">
</span><span class="c1">#&gt;  1     1055 WARK60      -0.5        4 &lt;NA&gt;     &lt;NA&gt;   252C345D59374D‚Ä¶    258976.</span><span class="w">
</span><span class="c1">#&gt;  2     1023 WARK60       0.6        4 &lt;NA&gt;     &lt;NA&gt;   1DAB1D800FB84E‚Ä¶    258341.</span><span class="w">
</span><span class="c1">#&gt;  3     1021 WARK60       0.5        4 &lt;NA&gt;     &lt;NA&gt;   838438F1BBE745‚Ä¶    258338.</span><span class="w">
</span><span class="c1">#&gt;  4      985 WARK60       0          4 &lt;NA&gt;     &lt;NA&gt;   0A4BE2AB03D845‚Ä¶    258527.</span><span class="w">
</span><span class="c1">#&gt;  5      994 WARK60       1.9        4 &lt;NA&gt;     &lt;NA&gt;   6ACD71128B6B49‚Ä¶    258499.</span><span class="w">
</span><span class="c1">#&gt;  6      995 WARK60       1.4        4 &lt;NA&gt;     &lt;NA&gt;   B10B26FA32FB44‚Ä¶    258502.</span><span class="w">
</span><span class="c1">#&gt;  7      997 WARK60       1.1        4 &lt;NA&gt;     &lt;NA&gt;   28E47E22D71549‚Ä¶    258498.</span><span class="w">
</span><span class="c1">#&gt;  8      993 WARK60       1.9        4 &lt;NA&gt;     &lt;NA&gt;   FC9A29123BEF4A‚Ä¶    258499.</span><span class="w">
</span><span class="c1">#&gt;  9     1003 WARK60       0.7        4 &lt;NA&gt;     &lt;NA&gt;   3C7CA3CD0E8840‚Ä¶    258528.</span><span class="w">
</span><span class="c1">#&gt; 10     1001 WARK60       0.7        4 &lt;NA&gt;     &lt;NA&gt;   A6F508B066DC4A‚Ä¶    258511.</span><span class="w">
</span><span class="c1">#&gt; # ‚Ñπ 44,680 more rows</span><span class="w">
</span><span class="c1">#&gt; # ‚Ñπ 2 more variables: geometry$y &lt;dbl&gt;, $z &lt;dbl&gt;</span><span class="w">
</span></code></pre></div></div>

<p>Users of the C library will need to configure CMake with <code class="language-plaintext highlighter-rouge">-DNANOARROW_IPC_WITH_ZSTD=ON</code>
and <code class="language-plaintext highlighter-rouge">-DNANOARROW_IPC=ON</code> to use CMake-resolved ZSTD; however, client libraries
can also use an existing ZSTD or LZ4 implementation using callbacks.</p>

<h3 id="new-type-support">New Type Support</h3>

<p>While the nanoarrow C library is a minimal library, we do strive to support the full
specification and several new types were not supported by the C library. Version 0.7.0
includes support in the C library for Decimal32, Decimal64, ListView, and LargeListView
and improved support for support for decimal types in the nanoarrow R bindings.</p>

<p>Thanks to <a href="https://github.com/zeroshade">@zeroshade</a> for contributing Decimal32/Decimal64
support and <a href="https://github.com/WillAyd">@WillAyd</a> for contributing</p>

<h3 id="nanoarrow-on-vcpkg">nanoarrow on vcpkg</h3>

<p>The nanoarrow C library can now be installed using
<a href="https://github.com/microsoft/vcpkg">vcpkg</a>!</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git clone https://github.com/microsoft/vcpkg.git
<span class="nb">cd </span>vcpkg <span class="o">&amp;&amp;</span> ./bootstrap-vcpkg.sh
./vcpkg <span class="nb">install </span>nanoarrow
</code></pre></div></div>

<p>CMake projects can then use <code class="language-plaintext highlighter-rouge">find_package(nanoarrow)</code> when using the vcpkg
toolchain (i.e., <code class="language-plaintext highlighter-rouge">-DCMAKE_TOOLCHAIN_FILE=path/to/vcpkg/scripts/buildsystems/vcpkg.cmake</code>).
This also allows other vcpkg ports to use nanoarrow as a dependency in addition
to a convenience for projects already using vcpkg.</p>

<p>Thanks to <a href="https://github.com/m-kuhn">@m-kuhn</a> for contributing the nanoarrow port to
vcpkg!</p>

<h2 id="contributors">Contributors</h2>

<p>This release consists of contributions from 12 contributors in addition
to the invaluable advice and support of the Apache Arrow community.</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">$</span><span class="w"> </span>git shortlog <span class="nt">-sn</span> apache-arrow-nanoarrow-0.7.0.dev..apache-arrow-nanoarrow-0.7.0-rc1
<span class="go">    53  Dewey Dunnington
    27  William Ayd
     3  Michael Chirico
     2  Sutou Kouhei
     1  Bryce Mecum
     1  David Li
     1  Gang Wu
     1  Ilya Verbin
     1  Jacob Wujciak-Jens
     1  Matt Topol
     1  Matthias Kuhn
     1  eitsupi
</span></code></pre></div></div>]]></content><author><name>pmc</name></author><category term="release" /><summary type="html"><![CDATA[The Apache Arrow team is pleased to announce the 0.7.0 release of Apache Arrow nanoarrow. This release covers 117 resolved issues from 12 contributors. Release Highlights Migrate Python bindings to Meson Python Better support for shared linkage ZSTD Decompression support in IPC reader Decimal32, Decimal64, ListView and LargeListView support Support for vcpkg See the Changelog for a detailed list of contributions to this release. Features Meson Python The Python bindings now use Meson Python as the build backend. The main benefit is that adding C or C++ library dependencies like ZSTD is much simpler than with setuptools which was needed to add the new decompression support to the Python bindings. Thanks to @WillAyd for this contribution and continued maintenance of the Python build infrastructure! Shared Linkage The nanoarrow C library is generally designed to be statically linked into an application or library; however, there were some applications that did want shared linkage and on Windows some extra work was needed to ensure this worked as intended. Version 0.7.0 includes the appropriate DLL import/export attributes and adds dedicated nanoarrow_shared and nanoarrow_static targets to the CMake configuration to explicitly choose a strategy (linking to nanoarrow will continue to use the CMake default as it did in previous versions). Thanks to @m-kuhn for authoring the initial vcpkg configuration that brought this to our attention! ZSTD Decompression Support The Arrow IPC reader included in the nanoarrow C library supports most features of the Arrow IPC format; however, decompression support was missing which made the library and its bindings unusable for some common use cases. In 0.7.0, decompression support was added to the C library and R and Python bindings. library(nanoarrow) url &lt;- "https://github.com/geoarrow/geoarrow-data/releases/download/v0.2.0/ns-water_water-point.arrows" read_nanoarrow(url) |&gt; tibble::as_tibble() #&gt; # A tibble: 44,690 √ó 8 #&gt; OBJECTID FEAT_CODE ZVALUE PT_CLASS NAMEID_1 NAME_1 HID geometry$x #&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 1055 WARK60 -0.5 4 &lt;NA&gt; &lt;NA&gt; 252C345D59374D‚Ä¶ 258976. #&gt; 2 1023 WARK60 0.6 4 &lt;NA&gt; &lt;NA&gt; 1DAB1D800FB84E‚Ä¶ 258341. #&gt; 3 1021 WARK60 0.5 4 &lt;NA&gt; &lt;NA&gt; 838438F1BBE745‚Ä¶ 258338. #&gt; 4 985 WARK60 0 4 &lt;NA&gt; &lt;NA&gt; 0A4BE2AB03D845‚Ä¶ 258527. #&gt; 5 994 WARK60 1.9 4 &lt;NA&gt; &lt;NA&gt; 6ACD71128B6B49‚Ä¶ 258499. #&gt; 6 995 WARK60 1.4 4 &lt;NA&gt; &lt;NA&gt; B10B26FA32FB44‚Ä¶ 258502. #&gt; 7 997 WARK60 1.1 4 &lt;NA&gt; &lt;NA&gt; 28E47E22D71549‚Ä¶ 258498. #&gt; 8 993 WARK60 1.9 4 &lt;NA&gt; &lt;NA&gt; FC9A29123BEF4A‚Ä¶ 258499. #&gt; 9 1003 WARK60 0.7 4 &lt;NA&gt; &lt;NA&gt; 3C7CA3CD0E8840‚Ä¶ 258528. #&gt; 10 1001 WARK60 0.7 4 &lt;NA&gt; &lt;NA&gt; A6F508B066DC4A‚Ä¶ 258511. #&gt; # ‚Ñπ 44,680 more rows #&gt; # ‚Ñπ 2 more variables: geometry$y &lt;dbl&gt;, $z &lt;dbl&gt; Users of the C library will need to configure CMake with -DNANOARROW_IPC_WITH_ZSTD=ON and -DNANOARROW_IPC=ON to use CMake-resolved ZSTD; however, client libraries can also use an existing ZSTD or LZ4 implementation using callbacks. New Type Support While the nanoarrow C library is a minimal library, we do strive to support the full specification and several new types were not supported by the C library. Version 0.7.0 includes support in the C library for Decimal32, Decimal64, ListView, and LargeListView and improved support for support for decimal types in the nanoarrow R bindings. Thanks to @zeroshade for contributing Decimal32/Decimal64 support and @WillAyd for contributing nanoarrow on vcpkg The nanoarrow C library can now be installed using vcpkg! git clone https://github.com/microsoft/vcpkg.git cd vcpkg &amp;&amp; ./bootstrap-vcpkg.sh ./vcpkg install nanoarrow CMake projects can then use find_package(nanoarrow) when using the vcpkg toolchain (i.e., -DCMAKE_TOOLCHAIN_FILE=path/to/vcpkg/scripts/buildsystems/vcpkg.cmake). This also allows other vcpkg ports to use nanoarrow as a dependency in addition to a convenience for projects already using vcpkg. Thanks to @m-kuhn for contributing the nanoarrow port to vcpkg! Contributors This release consists of contributions from 12 contributors in addition to the invaluable advice and support of the Apache Arrow community. $ git shortlog -sn apache-arrow-nanoarrow-0.7.0.dev..apache-arrow-nanoarrow-0.7.0-rc1 53 Dewey Dunnington 27 William Ayd 3 Michael Chirico 2 Sutou Kouhei 1 Bryce Mecum 1 David Li 1 Gang Wu 1 Ilya Verbin 1 Jacob Wujciak-Jens 1 Matt Topol 1 Matthias Kuhn 1 eitsupi]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Apache Arrow Java 18.3.0 Release</title><link href="https://arrow.apache.org/blog/2025/05/13/arrow-java-18.3.0/" rel="alternate" type="text/html" title="Apache Arrow Java 18.3.0 Release" /><published>2025-05-13T00:00:00-04:00</published><updated>2025-05-13T00:00:00-04:00</updated><id>https://arrow.apache.org/blog/2025/05/13/arrow-java-18.3.0</id><content type="html" xml:base="https://arrow.apache.org/blog/2025/05/13/arrow-java-18.3.0/"><![CDATA[<!--

-->

<p>The Apache Arrow team is pleased to announce the <a href="https://github.com/apache/arrow-java/releases/tag/v18.3.0">v18.3.0</a> release of Apache Arrow Java.
This is a minor release since the last release <a href="https://github.com/apache/arrow-java/releases/tag/v18.2.0">v18.2.0</a>.</p>

<h2 id="changelog">Changelog</h2>

<h3 id="new-features-and-enhancements">New Features and Enhancements</h3>
<ul>
  <li>MINOR: ZstdCompressionCodec should use decompressedSize to get error name by @libenchao in <a href="https://github.com/apache/arrow-java/pull/619">#619</a></li>
  <li>MINOR: Add explicit exception when no more buffer can be read when loading buffers by @viirya in <a href="https://github.com/apache/arrow-java/pull/649">#649</a></li>
  <li>GH-81: [Flight] Expose gRPC in Flight client builder by @lidavidm in <a href="https://github.com/apache/arrow-java/pull/660">#660</a></li>
  <li>GH-615: Produce Avro core data types out of Arrow VSR by @martin-traverse in <a href="https://github.com/apache/arrow-java/pull/638">#638</a></li>
  <li>GH-494: [Flight] Allow configuring connect timeout in JDBC by @lidavidm in <a href="https://github.com/apache/arrow-java/pull/495">#495</a></li>
  <li>GH-87: [Vector] Add ExtensionWriter by @xxlaykxx in <a href="https://github.com/apache/arrow-java/pull/697">#697</a></li>
  <li>GH-698: Improve and fix Avro read consumers by @martin-traverse in <a href="https://github.com/apache/arrow-java/pull/718">#718</a></li>
  <li>GH-737: [FlightSQL] Allow returning column remarks in FlightSQL‚Äôs CommandGetTables by @mateuszrzeszutek in <a href="https://github.com/apache/arrow-java/pull/727">#727</a></li>
  <li>GH-661: [Flight] JDBC: Cache failed locations by @lidavidm in <a href="https://github.com/apache/arrow-java/pull/662">#662</a></li>
</ul>

<h3 id="bug-fixes">Bug Fixes</h3>
<ul>
  <li>GH-601: [Gandiva] Synchronize some methods on the Projector by @lriggs in <a href="https://github.com/apache/arrow-java/pull/602">#602</a></li>
  <li>GH-625: Map MinorType getNewFieldWriter returns UnionMapWriter by @wsuppiger in <a href="https://github.com/apache/arrow-java/pull/627">#627</a></li>
  <li>GH-653: Nullify fieldReader when invalidating parent object by @lriggs in <a href="https://github.com/apache/arrow-java/pull/654">#654</a></li>
  <li>GH-655: Failure in UnionReader.read after DecimalVector promotion to UnionVector by @lriggs in <a href="https://github.com/apache/arrow-java/pull/656">#656</a></li>
  <li>GH-692: Preserve nullability information while transfering DecimalVector and Decimal256Vector by @bodduv in <a href="https://github.com/apache/arrow-java/pull/693">#693</a></li>
  <li>GH-704: Fix initialization of offset buffer when exporting VarChar vectors through C Data Interface by @Kontinuation in <a href="https://github.com/apache/arrow-java/pull/705">#705</a></li>
  <li>GH-709: Correct length calculation of value buffers of variable-sized arrays by @pepijnve in <a href="https://github.com/apache/arrow-java/pull/707">#707</a></li>
  <li>GH-721: Allow using 1GB+ data buffers in variable width vectors by @gszadovszky in <a href="https://github.com/apache/arrow-java/pull/722">#722</a></li>
  <li>GH-463: Improve TZ support for JDBC driver by @aiguofer in <a href="https://github.com/apache/arrow-java/pull/464">#464</a></li>
  <li>GH-729: [JDBC] Fix BinaryConsumer consuming null value by @hnwyllmm in <a href="https://github.com/apache/arrow-java/pull/730">#730</a></li>
</ul>

<p><strong>Full Changelog</strong>: <a href="https://github.com/apache/arrow-java/commits/v18.3.0">changelog</a></p>]]></content><author><name>pmc</name></author><category term="release" /><summary type="html"><![CDATA[The Apache Arrow team is pleased to announce the v18.3.0 release of Apache Arrow Java. This is a minor release since the last release v18.2.0. Changelog New Features and Enhancements MINOR: ZstdCompressionCodec should use decompressedSize to get error name by @libenchao in #619 MINOR: Add explicit exception when no more buffer can be read when loading buffers by @viirya in #649 GH-81: [Flight] Expose gRPC in Flight client builder by @lidavidm in #660 GH-615: Produce Avro core data types out of Arrow VSR by @martin-traverse in #638 GH-494: [Flight] Allow configuring connect timeout in JDBC by @lidavidm in #495 GH-87: [Vector] Add ExtensionWriter by @xxlaykxx in #697 GH-698: Improve and fix Avro read consumers by @martin-traverse in #718 GH-737: [FlightSQL] Allow returning column remarks in FlightSQL‚Äôs CommandGetTables by @mateuszrzeszutek in #727 GH-661: [Flight] JDBC: Cache failed locations by @lidavidm in #662 Bug Fixes GH-601: [Gandiva] Synchronize some methods on the Projector by @lriggs in #602 GH-625: Map MinorType getNewFieldWriter returns UnionMapWriter by @wsuppiger in #627 GH-653: Nullify fieldReader when invalidating parent object by @lriggs in #654 GH-655: Failure in UnionReader.read after DecimalVector promotion to UnionVector by @lriggs in #656 GH-692: Preserve nullability information while transfering DecimalVector and Decimal256Vector by @bodduv in #693 GH-704: Fix initialization of offset buffer when exporting VarChar vectors through C Data Interface by @Kontinuation in #705 GH-709: Correct length calculation of value buffers of variable-sized arrays by @pepijnve in #707 GH-721: Allow using 1GB+ data buffers in variable width vectors by @gszadovszky in #722 GH-463: Improve TZ support for JDBC driver by @aiguofer in #464 GH-729: [JDBC] Fix BinaryConsumer consuming null value by @hnwyllmm in #730 Full Changelog: changelog]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Apache Arrow Go 18.3.0 Release</title><link href="https://arrow.apache.org/blog/2025/05/09/arrow-go-18.3.0/" rel="alternate" type="text/html" title="Apache Arrow Go 18.3.0 Release" /><published>2025-05-09T00:00:00-04:00</published><updated>2025-05-09T00:00:00-04:00</updated><id>https://arrow.apache.org/blog/2025/05/09/arrow-go-18.3.0</id><content type="html" xml:base="https://arrow.apache.org/blog/2025/05/09/arrow-go-18.3.0/"><![CDATA[<!--

-->

<p>The Apache Arrow team is pleased to announce the v18.3.0 release of Apache Arrow Go. 
This minor release covers 21 commits from 8 distinct contributors.</p>

<h2 id="contributors">Contributors</h2>
<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">$</span><span class="w"> </span>git shortlog <span class="nt">-sn</span> v18.2.0..v18.3.0
<span class="go">    13	Matt Topol
     2	Chris Pahl
     1	Ashish Negi
     1	David Li
     1	Jeroen Demeyer
     1	Mateusz Rzeszutek
     1	Ra√∫l Cumplido
     1	Saurabh Singh
</span></code></pre></div></div>

<h2 id="highlights">Highlights</h2>

<ul>
  <li>Fix alignment of atomic refcount handling for ARM <a href="https://github.com/apache/arrow-go/pull/323">#323</a></li>
</ul>

<h3 id="arrow">Arrow</h3>

<ul>
  <li>Functions to convert RecordReader to Go iter.Seq and vice versa <a href="https://github.com/apache/arrow-go/pull/314">#314</a></li>
  <li>New ‚Äúis_in‚Äù function for Arrow compute package</li>
  <li>Allow returning column remarks for FlightSQL CommandGetTables <a href="https://github.com/apache/arrow-go/pull/361">#361</a></li>
</ul>

<h3 id="parquet">Parquet</h3>

<ul>
  <li>Added new <code class="language-plaintext highlighter-rouge">SeekToRow</code> function for pqarrow.RecordReader <a href="https://github.com/apache/arrow-go/pull/321">#321</a></li>
  <li>Bloom filters can now be read and written, then utilized for skipping <a href="https://github.com/apache/arrow-go/pull/341">#341</a> <a href="https://github.com/apache/arrow-go/pull/336">#336</a></li>
  <li>Fix a panic when <code class="language-plaintext highlighter-rouge">WriteDataPage</code> fails <a href="https://github.com/apache/arrow-go/pull/366">#366</a></li>
</ul>

<h2 id="changelog">Changelog</h2>

<h3 id="whats-changed">What‚Äôs Changed</h3>
<ul>
  <li>feat(arrow/array): convert RecordReader and iterators by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/314">#314</a></li>
  <li>refactor(arrow/array): replace some codegen with generics by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/315">#315</a></li>
  <li>feat(parquet/pqarrow): Add SeekToRow for RecordReader by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/321">#321</a></li>
  <li>fix: go‚Äôs atomic operations require 64bit alignment in structs on ARM by @sahib in <a href="https://github.com/apache/arrow-go/pull/323">#323</a></li>
  <li>feat(arrow/compute): implement ‚Äúis_in‚Äù function by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/319">#319</a></li>
  <li>fix(parquet/pqarrow): fix propagation of FieldIds for nested fields by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/324">#324</a></li>
  <li>Fix: Handle null values in PlainFixedLenByteArrayEncoder gracefully by @singh1203 in <a href="https://github.com/apache/arrow-go/pull/320">#320</a></li>
  <li>fix(parquet/pqarrow): fix definition levels with non-nullable lists by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/325">#325</a></li>
  <li>chore: fix macOS Go 1.24 CI by @lidavidm in <a href="https://github.com/apache/arrow-go/pull/334">#334</a></li>
  <li>feat(parquet/metadata): bloom filter implementation by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/336">#336</a></li>
  <li>feat(parquet): Write/Read bloom filters from files by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/341">#341</a></li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>fix: move from atomic.(Add</td>
          <td>Load</td>
          <td>Store) to atomic.Int64{} by @sahib in <a href="https://github.com/apache/arrow-go/pull/326">#326</a></td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>fix(parquet/file): restore goroutine safety for reader by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/343">#343</a></li>
  <li>chore: Enable GitHub discussions on arrow-go repository by @raulcd in <a href="https://github.com/apache/arrow-go/pull/353">#353</a></li>
  <li>Compress: add MarshalText and UnmarshalText by @jdemeyer in <a href="https://github.com/apache/arrow-go/pull/357">#357</a></li>
  <li>fix(arrow/array): optional struct array with required field by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/359">#359</a></li>
  <li>feat(parquet/schema): initial variant logical type by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/352">#352</a></li>
  <li>chore(arrow): remove most lock copies by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/362">#362</a></li>
  <li>Fix panic when WriteDataPage fails by @ashishnegi in <a href="https://github.com/apache/arrow-go/pull/366">#366</a></li>
  <li>GH-46087: [FlightSQL] Allow returning column remarks in FlightSQL‚Äôs CommandGetTables by @mateuszrzeszutek in <a href="https://github.com/apache/arrow-go/pull/361">#361</a></li>
</ul>

<h3 id="new-contributors">New Contributors</h3>
<ul>
  <li>@sahib made their first contribution in <a href="https://github.com/apache/arrow-go/pull/323">#323</a></li>
  <li>@jdemeyer made their first contribution in <a href="https://github.com/apache/arrow-go/pull/357">#357</a></li>
  <li>@ashishnegi made their first contribution in <a href="https://github.com/apache/arrow-go/pull/366">#366</a></li>
  <li>@mateuszrzeszutek made their first contribution in <a href="https://github.com/apache/arrow-go/pull/361">#361</a></li>
</ul>

<p><strong>Full Changelog</strong>: https://github.com/apache/arrow-go/compare/v18.2.0‚Ä¶v18.3.0</p>]]></content><author><name>pmc</name></author><category term="release" /><summary type="html"><![CDATA[The Apache Arrow team is pleased to announce the v18.3.0 release of Apache Arrow Go. This minor release covers 21 commits from 8 distinct contributors. Contributors $ git shortlog -sn v18.2.0..v18.3.0 13 Matt Topol 2 Chris Pahl 1 Ashish Negi 1 David Li 1 Jeroen Demeyer 1 Mateusz Rzeszutek 1 Ra√∫l Cumplido 1 Saurabh Singh Highlights Fix alignment of atomic refcount handling for ARM #323 Arrow Functions to convert RecordReader to Go iter.Seq and vice versa #314 New ‚Äúis_in‚Äù function for Arrow compute package Allow returning column remarks for FlightSQL CommandGetTables #361 Parquet Added new SeekToRow function for pqarrow.RecordReader #321 Bloom filters can now be read and written, then utilized for skipping #341 #336 Fix a panic when WriteDataPage fails #366 Changelog What‚Äôs Changed feat(arrow/array): convert RecordReader and iterators by @zeroshade in #314 refactor(arrow/array): replace some codegen with generics by @zeroshade in #315 feat(parquet/pqarrow): Add SeekToRow for RecordReader by @zeroshade in #321 fix: go‚Äôs atomic operations require 64bit alignment in structs on ARM by @sahib in #323 feat(arrow/compute): implement ‚Äúis_in‚Äù function by @zeroshade in #319 fix(parquet/pqarrow): fix propagation of FieldIds for nested fields by @zeroshade in #324 Fix: Handle null values in PlainFixedLenByteArrayEncoder gracefully by @singh1203 in #320 fix(parquet/pqarrow): fix definition levels with non-nullable lists by @zeroshade in #325 chore: fix macOS Go 1.24 CI by @lidavidm in #334 feat(parquet/metadata): bloom filter implementation by @zeroshade in #336 feat(parquet): Write/Read bloom filters from files by @zeroshade in #341 fix: move from atomic.(Add Load Store) to atomic.Int64{} by @sahib in #326 fix(parquet/file): restore goroutine safety for reader by @zeroshade in #343 chore: Enable GitHub discussions on arrow-go repository by @raulcd in #353 Compress: add MarshalText and UnmarshalText by @jdemeyer in #357 fix(arrow/array): optional struct array with required field by @zeroshade in #359 feat(parquet/schema): initial variant logical type by @zeroshade in #352 chore(arrow): remove most lock copies by @zeroshade in #362 Fix panic when WriteDataPage fails by @ashishnegi in #366 GH-46087: [FlightSQL] Allow returning column remarks in FlightSQL‚Äôs CommandGetTables by @mateuszrzeszutek in #361 New Contributors @sahib made their first contribution in #323 @jdemeyer made their first contribution in #357 @ashishnegi made their first contribution in #366 @mateuszrzeszutek made their first contribution in #361 Full Changelog: https://github.com/apache/arrow-go/compare/v18.2.0‚Ä¶v18.3.0]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Apache Arrow ADBC 18 (Libraries) Release</title><link href="https://arrow.apache.org/blog/2025/05/06/adbc-18-release/" rel="alternate" type="text/html" title="Apache Arrow ADBC 18 (Libraries) Release" /><published>2025-05-06T00:00:00-04:00</published><updated>2025-05-06T00:00:00-04:00</updated><id>https://arrow.apache.org/blog/2025/05/06/adbc-18-release</id><content type="html" xml:base="https://arrow.apache.org/blog/2025/05/06/adbc-18-release/"><![CDATA[<!--

-->

<p>The Apache Arrow team is pleased to announce the version 18 release of
the Apache Arrow ADBC libraries. This release includes <a href="https://github.com/apache/arrow-adbc/milestone/22"><strong>28
resolved issues</strong></a> from <a href="#contributors"><strong>22 distinct contributors</strong></a>.</p>

<p>This is a release of the <strong>libraries</strong>, which are at version 18.  The
<a href="https://arrow.apache.org/adbc/18/format/specification.html"><strong>API specification</strong></a> is versioned separately and is at
version 1.1.0.</p>

<p>The subcomponents are versioned independently:</p>

<ul>
  <li>C/C++/GLib/Go/Python/Ruby: 1.6.0</li>
  <li>C#: 0.18.0</li>
  <li>Java: 0.18.0</li>
  <li>R: 0.18.0</li>
  <li>Rust: 0.18.0</li>
</ul>

<p>The release notes below are not exhaustive and only expose selected
highlights of the release. Many other bugfixes and improvements have
been made: we refer you to the <a href="https://github.com/apache/arrow-adbc/blob/apache-arrow-adbc-18/CHANGELOG.md">complete changelog</a>.</p>

<h2 id="release-highlights">Release Highlights</h2>

<p>Using Meson to build the project has been improved (#2735, #2746).</p>

<p>The C# bindings and its drivers have seen a lot of activity in this release.  A Databricks Spark driver is now available (#2672, #2737, #2743, #2692), with support for features like CloudFetch (#2634, #2678, #2691).  The general Spark driver now has better retry behavior for 503 responses (#2664), supports LZ4 compression applied outside of the Arrow IPC format (#2669), and supports OAuth (#2579), among other improvements.  The ‚ÄúApache‚Äù driver for various Thrift-based systems now supports Apache Hive in addition to Apache Spark and Apache Impala (#2540), among other improvements.  The BigQuery driver adds more authentication and other configuration settings (#2655, #2566, #2541, #2698).</p>

<p>The Flight SQL driver supports OAuth (#2651).</p>

<p>The Java bindings experimentally support a JNI wrapper around drivers exposing the ADBC C API (#2401).  These are not currently distributed via Maven and must be built by hand.</p>

<p>The Go bindings now support union types in the <code class="language-plaintext highlighter-rouge">database/sql</code> wrapper (#2637).  The Golang-based BigQuery driver returns more metadata about tables (#2697).</p>

<p>The PostgreSQL driver now avoids spurious commit/rollback commands (#2685).  It also handles improper usage more gracefully (#2653).</p>

<p>The Python bindings now make it easier to pass options in various places (#2589, #2700).  Also, the DB-API layer can be minimally used without PyArrow installed, making it easier for users of libraries like polars that don‚Äôt need or want a second Arrow implementation (#2609).</p>

<p>The Rust bindings now avoid locking the driver on every operation, allowing concurrent usage (#2736).</p>

<h2 id="contributors">Contributors</h2>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ git shortlog --perl-regexp --author='^((?!dependabot\[bot\]).*)$' -sn apache-arrow-adbc-17..apache-arrow-adbc-18
    20	David Li
     6	William Ayd
     5	Curt Hagenlocher
     5	davidhcoe
     4	Alex Guo
     4	Felipe Oliveira Carvalho
     4	Jade Wang
     4	Matthijs Brobbel
     4	Sutou Kouhei
     4	eric-wang-1990
     3	Bruce Irschick
     2	Milos Gligoric
     2	Sudhir Reddy Emmadi
     2	Todd Meng
     1	Bryce Mecum
     1	Dewey Dunnington
     1	Filip Wojciechowski
     1	Hiroaki Yutani
     1	H√©lder Greg√≥rio
     1	Marin Nozhchev
     1	amangoyal
     1	qifanzhang-ms
</code></pre></div></div>

<h2 id="roadmap">Roadmap</h2>

<p>There is some discussion on a potential second revision of ADBC to include more missing functionality and asynchronous API support.  For more, see the <a href="https://github.com/apache/arrow-adbc/milestone/8">milestone</a>.  We would welcome suggestions on APIs that could be added or extended.  Some of the contributors are planning to begin work on a proposal in the near future.</p>

<h2 id="getting-involved">Getting Involved</h2>

<p>We welcome questions and contributions from all interested.  Issues
can be filed on <a href="https://github.com/apache/arrow-adbc/issues">GitHub</a>, and questions can be directed to GitHub
or the <a href="/community/">Arrow mailing lists</a>.</p>]]></content><author><name>pmc</name></author><category term="release" /><summary type="html"><![CDATA[The Apache Arrow team is pleased to announce the version 18 release of the Apache Arrow ADBC libraries. This release includes 28 resolved issues from 22 distinct contributors. This is a release of the libraries, which are at version 18. The API specification is versioned separately and is at version 1.1.0. The subcomponents are versioned independently: C/C++/GLib/Go/Python/Ruby: 1.6.0 C#: 0.18.0 Java: 0.18.0 R: 0.18.0 Rust: 0.18.0 The release notes below are not exhaustive and only expose selected highlights of the release. Many other bugfixes and improvements have been made: we refer you to the complete changelog. Release Highlights Using Meson to build the project has been improved (#2735, #2746). The C# bindings and its drivers have seen a lot of activity in this release. A Databricks Spark driver is now available (#2672, #2737, #2743, #2692), with support for features like CloudFetch (#2634, #2678, #2691). The general Spark driver now has better retry behavior for 503 responses (#2664), supports LZ4 compression applied outside of the Arrow IPC format (#2669), and supports OAuth (#2579), among other improvements. The ‚ÄúApache‚Äù driver for various Thrift-based systems now supports Apache Hive in addition to Apache Spark and Apache Impala (#2540), among other improvements. The BigQuery driver adds more authentication and other configuration settings (#2655, #2566, #2541, #2698). The Flight SQL driver supports OAuth (#2651). The Java bindings experimentally support a JNI wrapper around drivers exposing the ADBC C API (#2401). These are not currently distributed via Maven and must be built by hand. The Go bindings now support union types in the database/sql wrapper (#2637). The Golang-based BigQuery driver returns more metadata about tables (#2697). The PostgreSQL driver now avoids spurious commit/rollback commands (#2685). It also handles improper usage more gracefully (#2653). The Python bindings now make it easier to pass options in various places (#2589, #2700). Also, the DB-API layer can be minimally used without PyArrow installed, making it easier for users of libraries like polars that don‚Äôt need or want a second Arrow implementation (#2609). The Rust bindings now avoid locking the driver on every operation, allowing concurrent usage (#2736). Contributors $ git shortlog --perl-regexp --author='^((?!dependabot\[bot\]).*)$' -sn apache-arrow-adbc-17..apache-arrow-adbc-18 20 David Li 6 William Ayd 5 Curt Hagenlocher 5 davidhcoe 4 Alex Guo 4 Felipe Oliveira Carvalho 4 Jade Wang 4 Matthijs Brobbel 4 Sutou Kouhei 4 eric-wang-1990 3 Bruce Irschick 2 Milos Gligoric 2 Sudhir Reddy Emmadi 2 Todd Meng 1 Bryce Mecum 1 Dewey Dunnington 1 Filip Wojciechowski 1 Hiroaki Yutani 1 H√©lder Greg√≥rio 1 Marin Nozhchev 1 amangoyal 1 qifanzhang-ms Roadmap There is some discussion on a potential second revision of ADBC to include more missing functionality and asynchronous API support. For more, see the milestone. We would welcome suggestions on APIs that could be added or extended. Some of the contributors are planning to begin work on a proposal in the near future. Getting Involved We welcome questions and contributions from all interested. Issues can be filed on GitHub, and questions can be directed to GitHub or the Arrow mailing lists.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Apache Arrow 20.0.0 Release</title><link href="https://arrow.apache.org/blog/2025/04/27/20.0.0-release/" rel="alternate" type="text/html" title="Apache Arrow 20.0.0 Release" /><published>2025-04-27T00:00:00-04:00</published><updated>2025-04-27T00:00:00-04:00</updated><id>https://arrow.apache.org/blog/2025/04/27/20.0.0-release</id><content type="html" xml:base="https://arrow.apache.org/blog/2025/04/27/20.0.0-release/"><![CDATA[<!--

-->

<p>The Apache Arrow team is pleased to announce the 20.0.0 release. This release
covers over 2 months of development work and includes <a href="https://github.com/apache/arrow/milestone/65?closed=1"><strong>259 resolved
issues</strong></a> on <a href="/release/20.0.0.html#contributors"><strong>327 distinct commits</strong></a> from <a href="/release/20.0.0.html#contributors"><strong>63 distinct
contributors</strong></a>. See the <a href="https://arrow.apache.org/install/">Install Page</a> to
learn how to get the libraries for your platform.</p>

<p>The release notes below are not exhaustive and only expose selected highlights
of the release. Many other bugfixes and improvements have been made: we refer
you to the <a href="/release/20.0.0.html#changelog">complete changelog</a>.</p>

<h2 id="community">Community</h2>

<p>Since the 19.0.0 release, Ed Seidl, Jean-Baptiste Onofr√© and Matthijs Brobbel
have been invited to become committers. Bryce Mecum, Ian Cook, Jacob Wujciak-Jens 
and Rok Mihevc have been invited to join the Project Management Committee (PMC).</p>

<p>Thanks for your contributions and participation in the project!</p>

<h2 id="c-notes">C++ Notes</h2>

<h3 id="compute">Compute</h3>

<p>We‚Äôve added several new compute functions: inverse_permutation/scatter (<a href="https://github.com/apache/arrow/issues/44393">GH-44393</a>), pivot_wider/hash_pivot_wider (<a href="https://github.com/apache/arrow/issues/45269">GH-45269</a>), rank_normal (<a href="https://github.com/apache/arrow/issues/45572">GH-45572</a>), skew/kurtosis (<a href="https://github.com/apache/arrow/issues/45676">GH-45676</a>), and winsorize (<a href="https://github.com/apache/arrow/issues/45755">GH-45755</a>).</p>

<h3 id="acero">Acero</h3>

<p>We‚Äôve significantly improved the Hash Join in terms of overflow-safety (<a href="https://github.com/apache/arrow/issues/44513">GH-44513</a>, <a href="https://github.com/apache/arrow/issues/45334">GH-45334</a>, <a href="https://github.com/apache/arrow/issues/45506">GH-45506</a>), memory consumption (peak memory usage reduced by half: <a href="https://github.com/apache/arrow/issues/45551">GH-45551</a>), and performance (up to several dozen times faster: <a href="https://github.com/apache/arrow/issues/45611">GH-45611</a>, <a href="https://github.com/apache/arrow/issues/45917">GH-45917</a>).</p>

<h3 id="flight-rpc">Flight RPC</h3>

<ul>
  <li>The experimental Flight over UCX feature has been removed.
(<a href="https://github.com/apache/arrow/issues/43296">#43296</a>)</li>
</ul>

<h2 id="c-notes-1">C# Notes</h2>

<ul>
  <li>Added support for the <code class="language-plaintext highlighter-rouge">Ordered</code> and <code class="language-plaintext highlighter-rouge">AppMetaData</code> fields to FlightInfo (<a href="https://github.com/apache/arrow/pull/45753">#45753</a>)</li>
  <li>FlightClient can now be integrated with <a href="https://learn.microsoft.com/en-us/aspnet/core/grpc/clientfactory?view=aspnetcore-9.0">Grpc.Net.ClientFactory</a> (<a href="https://github.com/apache/arrow/issues/45451">#45451</a>)</li>
</ul>

<h2 id="linux-packaging-notes">Linux Packaging Notes</h2>

<p>https://apache.jfrog.io/ is still available but https://packages.apache.org/ is preferred because the latter uses the apache.org domain.</p>

<h2 id="python-notes">Python Notes</h2>

<p>Compatibility notes:</p>
<ul>
  <li>Minimum supported Cython has been raised to 3 and higher <a href="https://github.com/apache/arrow/issues/45237">GH-45237</a> .</li>
  <li>A subset of deprecated APIs have been removed
<a href="https://github.com/apache/arrow/issues/45680">GH-45680</a>: 
 <code class="language-plaintext highlighter-rouge">PARQUET_2_0</code> <a href="https://github.com/apache/arrow/issues/45848">GH-45848</a>,
 <code class="language-plaintext highlighter-rouge">use_legacy_dataset</code> <a href="https://github.com/apache/arrow/issues/44790">GH-44790</a>,
 serialize/deserialize PyArrow C++ code <a href="https://github.com/apache/arrow/issues/43587">GH-43587</a> .</li>
</ul>

<p>New features:</p>
<ul>
  <li>Large variable width types are supported in NumPy conversion
<a href="https://github.com/apache/arrow/issues/35289">GH-35289</a>.</li>
  <li>Biased/unbiased option are available in skew and kurtosis compute functions
<a href="https://github.com/apache/arrow/issues/45733">GH-45733</a>.</li>
  <li>Support for SAS token in the <code class="language-plaintext highlighter-rouge">AzureFileSystem</code> has been added
<a href="https://github.com/apache/arrow/issues/45705">GH-45705</a>.</li>
  <li>Interchange of  <code class="language-plaintext highlighter-rouge">decimal32</code>, <code class="language-plaintext highlighter-rouge">decimal64</code> and <code class="language-plaintext highlighter-rouge">decimal256</code> data type objects between
Pandas and PyArrow is now supported <a href="https://github.com/apache/arrow/issues/45582">GH-45582</a>,
<a href="https://github.com/apache/arrow/issues/45570">GH-45570</a>.</li>
  <li><code class="language-plaintext highlighter-rouge">pyarrow.ArrayStatistics</code>and  <code class="language-plaintext highlighter-rouge">pyarrow.Array.statistics()</code> are added
<a href="https://github.com/apache/arrow/issues/45457">GH-45457</a>.</li>
  <li>Bindings for JSON streaming reader are added
<a href="https://github.com/apache/arrow/issues/14932">GH-14932</a>.</li>
  <li>Bindings for <code class="language-plaintext highlighter-rouge">MemoryPool::total_bytes_allocated</code> and <code class="language-plaintext highlighter-rouge">MemoryPool::num_allocations</code> are added.
Also allocator-specific statistics can now be printed to stderr <a href="https://github.com/apache/arrow/issues/45358">GH-45358</a>.</li>
  <li>A new <code class="language-plaintext highlighter-rouge">maps_as_pydicts</code>parameter is introduced to <code class="language-plaintext highlighter-rouge">to_pylist</code>, <code class="language-plaintext highlighter-rouge">to_pydict</code> and <code class="language-plaintext highlighter-rouge">as_py</code>
methods enabling deserialization into Python dictionary instead of list of tuples
<a href="https://github.com/apache/arrow/issues/39010">GH-39010</a>.</li>
</ul>

<p>Other improvements:</p>
<ul>
  <li>Source (sdist) and binary distribution (wheels) are now uploaded to GitHub Releases
  <a href="https://github.com/apache/arrow/issues/45920">GH-45920</a>.</li>
  <li>Cython code has been cleaned up as we now require at least Cython 3.0
<a href="https://github.com/apache/arrow/issues/45433">GH-45433</a>.</li>
  <li>Building of free-threaded wheels on Windows is enabled
<a href="https://github.com/apache/arrow/issues/44421">GH-44421</a> .
*Wheels for Alpine Linux are now provided <a href="https://github.com/apache/arrow/issues/18036">GH-18036</a> .</li>
</ul>

<p>Relevant bug fixes:</p>
<ul>
  <li>Pandas conversion roundtrip with bytes column names error is fixed
<a href="https://github.com/apache/arrow/issues/44188">GH-44188</a>.</li>
  <li>Exceptions are raised instead of showing segfaults when users try to instantiate internal Parquet
metadata classes <a href="https://github.com/apache/arrow/issues/36628">GH-36628</a>.</li>
</ul>

<h2 id="r-notes">R Notes</h2>

<ul>
  <li>Binary Arrays now inherit from <code class="language-plaintext highlighter-rouge">blob::blob</code> in addition to <code class="language-plaintext highlighter-rouge">arrow_binary</code> when
<a href="https://arrow.apache.org/docs/r/articles/data_types.html#translations-from-arrow-to-r">converted to R
objects</a>.
This change is the first step in eventually deprecating the <code class="language-plaintext highlighter-rouge">arrow_binary</code>
class in favor of the <code class="language-plaintext highlighter-rouge">blob</code> class in the
<a href="https://cran.r-project.org/package=blob"><code class="language-plaintext highlighter-rouge">blob</code></a> package (See
<a href="https://github.com/apache/arrow/issues/45709">GH-45709</a>).</li>
</ul>

<h2 id="ruby-and-c-glib-notes">Ruby and C GLib Notes</h2>

<h3 id="improvements">Improvements</h3>

<ul>
  <li><code class="language-plaintext highlighter-rouge">garrow_array_validate()</code> / <code class="language-plaintext highlighter-rouge">Arrow::Array#validate</code>: Added.
    <ul>
      <li><a href="https://github.com/apache/arrow/issues/45328">GH-45328</a></li>
    </ul>
  </li>
  <li><code class="language-plaintext highlighter-rouge">garrow_array_validate_full()</code> / <code class="language-plaintext highlighter-rouge">Arrow::Array#validate_full</code>: Added.
    <ul>
      <li><a href="https://github.com/apache/arrow/issues/45342">GH-45342</a></li>
    </ul>
  </li>
  <li><code class="language-plaintext highlighter-rouge">garrow_record_batch_validate()</code> / <code class="language-plaintext highlighter-rouge">Arrow::RecordBatch#validate</code>: Added.
    <ul>
      <li><a href="https://github.com/apache/arrow/issues/45353">GH-45353</a></li>
    </ul>
  </li>
  <li><code class="language-plaintext highlighter-rouge">garrow_record_batch_validate_full()</code> /
<code class="language-plaintext highlighter-rouge">Arrow::RecordBatch#validate_full</code>: Added.
    <ul>
      <li><a href="https://github.com/apache/arrow/issues/45386">GH-45386</a></li>
    </ul>
  </li>
  <li><code class="language-plaintext highlighter-rouge">garrow_table_validate()</code> / <code class="language-plaintext highlighter-rouge">Arrow::Table#validate</code>: Added.
    <ul>
      <li><a href="https://github.com/apache/arrow/issues/45414">GH-45414</a></li>
    </ul>
  </li>
  <li><code class="language-plaintext highlighter-rouge">garrow_table_validate_full()</code> / <code class="language-plaintext highlighter-rouge">Arrow::Table#validate_full</code>: Added.
    <ul>
      <li><a href="https://github.com/apache/arrow/issues/45468">GH-45468</a></li>
    </ul>
  </li>
  <li><code class="language-plaintext highlighter-rouge">GArrowArrayStatistics()</code> / <code class="language-plaintext highlighter-rouge">Arrow::ArrayStatistics</code>: Added.
    <ul>
      <li><a href="https://github.com/apache/arrow/issues/45490">GH-45490</a></li>
    </ul>
  </li>
  <li>Changed to require Meson 0.61.2 or later.</li>
  <li><code class="language-plaintext highlighter-rouge">GArrowBinaryViewArray()</code> / <code class="language-plaintext highlighter-rouge">Arrow::BinaryViewArray</code>: Added.
    <ul>
      <li><a href="https://github.com/apache/arrow/issues/45650">GH-45650</a></li>
    </ul>
  </li>
  <li><code class="language-plaintext highlighter-rouge">GArrowStringViewArray()</code> / <code class="language-plaintext highlighter-rouge">Arrow::StringViewArray</code>: Added.
    <ul>
      <li><a href="https://github.com/apache/arrow/issues/45711">GH-45711</a></li>
    </ul>
  </li>
  <li>Added support for
<a href="https://rubygems.org/gems/rubygems-requirements-system">rubygems-requirements-system</a>
    <ul>
      <li><a href="https://github.com/apache/arrow/issues/45976">GH-45976</a></li>
    </ul>
  </li>
</ul>

<h3 id="incompatible-changes">Incompatible changes</h3>

<ul>
  <li><code class="language-plaintext highlighter-rouge">gparquet_arrow_file_writer_new_row_group()</code> /
<code class="language-plaintext highlighter-rouge">Parquet:ArrowFileWriter#new_row_group</code>: Removed <code class="language-plaintext highlighter-rouge">chunk_size</code> argument.
    <ul>
      <li><a href="https://github.com/apache/arrow/issues/45088">GH-45088</a></li>
    </ul>
  </li>
  <li><code class="language-plaintext highlighter-rouge">garrow_record_batch_new()</code> / <code class="language-plaintext highlighter-rouge">Arrow::RecordBatch#initialize</code>:
Stopped validating automatically. If you want to validate a created
record batch, call <code class="language-plaintext highlighter-rouge">garrow_record_batch_validate()</code> /
<code class="language-plaintext highlighter-rouge">Arrow::RecordBatch#validate</code> explicitly.
    <ul>
      <li><a href="https://github.com/apache/arrow/issues/45353">GH-45353</a></li>
    </ul>
  </li>
  <li><code class="language-plaintext highlighter-rouge">garrow_table_new()</code> / <code class="language-plaintext highlighter-rouge">Arrow::Table#initialize</code>: Stopped validating
automatically. If you want to validate a created table, call
<code class="language-plaintext highlighter-rouge">garrow_table_validate()</code> / <code class="language-plaintext highlighter-rouge">Arrow::Table#validate</code> explicitly.
    <ul>
      <li><a href="https://github.com/apache/arrow/issues/45414">GH-45414</a></li>
    </ul>
  </li>
</ul>

<h2 id="java-go-and-rust-notes">Java, Go, and Rust Notes</h2>

<p>The Java, Go, and Rust Go projects have moved to separate repositories outside
the main Arrow <a href="https://github.com/apache/arrow">monorepo</a>.</p>

<ul>
  <li>For notes on the latest release of the <a href="https://github.com/apache/arrow-java">Java
implementation</a>, see the latest <a href="https://github.com/apache/arrow-java/releases">Arrow
Java changelog</a>.</li>
  <li>For notes on the latest release of the <a href="https://github.com/apache/arrow-rs">Rust
implementation</a> see the latest <a href="https://github.com/apache/arrow-rs/blob/main/CHANGELOG.md">Arrow Rust
changelog</a>.</li>
  <li>For notes on the latest release of the <a href="https://github.com/apache/arrow-go">Go
implementation</a>, see the latest <a href="https://github.com/apache/arrow-go/releases">Arrow Go
changelog</a>.</li>
</ul>]]></content><author><name>pmc</name></author><category term="release" /><summary type="html"><![CDATA[The Apache Arrow team is pleased to announce the 20.0.0 release. This release covers over 2 months of development work and includes 259 resolved issues on 327 distinct commits from 63 distinct contributors. See the Install Page to learn how to get the libraries for your platform. The release notes below are not exhaustive and only expose selected highlights of the release. Many other bugfixes and improvements have been made: we refer you to the complete changelog. Community Since the 19.0.0 release, Ed Seidl, Jean-Baptiste Onofr√© and Matthijs Brobbel have been invited to become committers. Bryce Mecum, Ian Cook, Jacob Wujciak-Jens and Rok Mihevc have been invited to join the Project Management Committee (PMC). Thanks for your contributions and participation in the project! C++ Notes Compute We‚Äôve added several new compute functions: inverse_permutation/scatter (GH-44393), pivot_wider/hash_pivot_wider (GH-45269), rank_normal (GH-45572), skew/kurtosis (GH-45676), and winsorize (GH-45755). Acero We‚Äôve significantly improved the Hash Join in terms of overflow-safety (GH-44513, GH-45334, GH-45506), memory consumption (peak memory usage reduced by half: GH-45551), and performance (up to several dozen times faster: GH-45611, GH-45917). Flight RPC The experimental Flight over UCX feature has been removed. (#43296) C# Notes Added support for the Ordered and AppMetaData fields to FlightInfo (#45753) FlightClient can now be integrated with Grpc.Net.ClientFactory (#45451) Linux Packaging Notes https://apache.jfrog.io/ is still available but https://packages.apache.org/ is preferred because the latter uses the apache.org domain. Python Notes Compatibility notes: Minimum supported Cython has been raised to 3 and higher GH-45237 . A subset of deprecated APIs have been removed GH-45680: PARQUET_2_0 GH-45848, use_legacy_dataset GH-44790, serialize/deserialize PyArrow C++ code GH-43587 . New features: Large variable width types are supported in NumPy conversion GH-35289. Biased/unbiased option are available in skew and kurtosis compute functions GH-45733. Support for SAS token in the AzureFileSystem has been added GH-45705. Interchange of decimal32, decimal64 and decimal256 data type objects between Pandas and PyArrow is now supported GH-45582, GH-45570. pyarrow.ArrayStatisticsand pyarrow.Array.statistics() are added GH-45457. Bindings for JSON streaming reader are added GH-14932. Bindings for MemoryPool::total_bytes_allocated and MemoryPool::num_allocations are added. Also allocator-specific statistics can now be printed to stderr GH-45358. A new maps_as_pydictsparameter is introduced to to_pylist, to_pydict and as_py methods enabling deserialization into Python dictionary instead of list of tuples GH-39010. Other improvements: Source (sdist) and binary distribution (wheels) are now uploaded to GitHub Releases GH-45920. Cython code has been cleaned up as we now require at least Cython 3.0 GH-45433. Building of free-threaded wheels on Windows is enabled GH-44421 . *Wheels for Alpine Linux are now provided GH-18036 . Relevant bug fixes: Pandas conversion roundtrip with bytes column names error is fixed GH-44188. Exceptions are raised instead of showing segfaults when users try to instantiate internal Parquet metadata classes GH-36628. R Notes Binary Arrays now inherit from blob::blob in addition to arrow_binary when converted to R objects. This change is the first step in eventually deprecating the arrow_binary class in favor of the blob class in the blob package (See GH-45709). Ruby and C GLib Notes Improvements garrow_array_validate() / Arrow::Array#validate: Added. GH-45328 garrow_array_validate_full() / Arrow::Array#validate_full: Added. GH-45342 garrow_record_batch_validate() / Arrow::RecordBatch#validate: Added. GH-45353 garrow_record_batch_validate_full() / Arrow::RecordBatch#validate_full: Added. GH-45386 garrow_table_validate() / Arrow::Table#validate: Added. GH-45414 garrow_table_validate_full() / Arrow::Table#validate_full: Added. GH-45468 GArrowArrayStatistics() / Arrow::ArrayStatistics: Added. GH-45490 Changed to require Meson 0.61.2 or later. GArrowBinaryViewArray() / Arrow::BinaryViewArray: Added. GH-45650 GArrowStringViewArray() / Arrow::StringViewArray: Added. GH-45711 Added support for rubygems-requirements-system GH-45976 Incompatible changes gparquet_arrow_file_writer_new_row_group() / Parquet:ArrowFileWriter#new_row_group: Removed chunk_size argument. GH-45088 garrow_record_batch_new() / Arrow::RecordBatch#initialize: Stopped validating automatically. If you want to validate a created record batch, call garrow_record_batch_validate() / Arrow::RecordBatch#validate explicitly. GH-45353 garrow_table_new() / Arrow::Table#initialize: Stopped validating automatically. If you want to validate a created table, call garrow_table_validate() / Arrow::Table#validate explicitly. GH-45414 Java, Go, and Rust Notes The Java, Go, and Rust Go projects have moved to separate repositories outside the main Arrow monorepo. For notes on the latest release of the Java implementation, see the latest Arrow Java changelog. For notes on the latest release of the Rust implementation see the latest Arrow Rust changelog. For notes on the latest release of the Go implementation, see the latest Arrow Go changelog.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Apache Arrow Go 18.2.0 Release</title><link href="https://arrow.apache.org/blog/2025/03/16/arrow-go-18.2.0/" rel="alternate" type="text/html" title="Apache Arrow Go 18.2.0 Release" /><published>2025-03-16T00:00:00-04:00</published><updated>2025-03-16T00:00:00-04:00</updated><id>https://arrow.apache.org/blog/2025/03/16/arrow-go-18.2.0</id><content type="html" xml:base="https://arrow.apache.org/blog/2025/03/16/arrow-go-18.2.0/"><![CDATA[<!--

-->

<p>The Apache Arrow team is pleased to announce the <a href="https://github.com/apache/arrow-go/releases/tag/v18.2.0">v18.2.0</a> release of Apache Arrow Go. 
This minor release covers 21 commits from 7 distinct contributors.</p>

<h2 id="highlights">Highlights</h2>

<h3 id="arrow">Arrow</h3>

<ul>
  <li>Fixed bitmap ops on 32-bit platforms <a href="https://github.com/apache/arrow-go/pull/277">#277</a></li>
  <li>Allocations by arrow/memory will always be aligned even from the Mallocator <a href="https://github.com/apache/arrow-go/pull/289">#289</a></li>
  <li>Sped up overflow checks for small integers in compute library <a href="https://github.com/apache/arrow-go/pull/303">#303</a></li>
</ul>

<h3 id="parquet">Parquet</h3>

<ul>
  <li>The <code class="language-plaintext highlighter-rouge">parquet_reader</code> CLI now has an option to dump the column and index offsets <a href="https://github.com/apache/arrow-go/pull/281">#281</a></li>
  <li>Column readers now have a <code class="language-plaintext highlighter-rouge">SeekToRow</code> method that will leverage column/index offsets if they exist <a href="https://github.com/apache/arrow-go/pull/283">#283</a></li>
</ul>

<h2 id="full-changelog">Full Changelog</h2>

<h3 id="whats-changed">What‚Äôs Changed</h3>
<ul>
  <li>fix(release): fix wrong upload path by @kou in <a href="https://github.com/apache/arrow-go/pull/243">#243</a></li>
  <li>fix(release): fix svn add command and add script to generate release notes by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/249">#249</a></li>
  <li>fix(arrow/cdata): move headers into parent Go package by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/251">#251</a></li>
  <li>chore: Add NOTICE.txt file by @singh1203 in <a href="https://github.com/apache/arrow-go/pull/257">#257</a></li>
  <li>chore: Update self-hosted arm runner to ubuntu-24.04-arm by @singh1203 in <a href="https://github.com/apache/arrow-go/pull/268">#268</a></li>
  <li>chore: Drop executable file bit of source files by @jas4711 in <a href="https://github.com/apache/arrow-go/pull/274">#274</a></li>
  <li>ci: update actions/cache by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/276">#276</a></li>
  <li>fix(arrow/bitutil): fix bitmap ops on 32-bit platforms by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/277">#277</a></li>
  <li>feat(internal/encoding): add Discard method to decoders by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/280">#280</a></li>
  <li>feat(parquet/cmd/parquet_reader): Add command to dump the column and offset indices by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/281">#281</a></li>
  <li>fix(internal/utils): fix clobbering BP by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/288">#288</a></li>
  <li>docs(license): update LICENSE.txt by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/290">#290</a></li>
  <li>feat(parquet/file): Add SeekToRow for Column readers by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/283">#283</a></li>
  <li>fix(parquet/pqarrow): propagate field id metadata for lists/maps by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/293">#293</a></li>
  <li>feat: Add arrayApproxEqualString to handle null characters in string. by @singh1203 in <a href="https://github.com/apache/arrow-go/pull/291">#291</a></li>
  <li>feat(parquet): update comments for <code class="language-plaintext highlighter-rouge">BufferedStreamEnabled</code> by @joechenrh in <a href="https://github.com/apache/arrow-go/pull/295">#295</a></li>
  <li>fix(arrow/memory): Align allocations always by @lidavidm in <a href="https://github.com/apache/arrow-go/pull/289">#289</a></li>
  <li>feat(parquet): add byte buffer when disable buffered stream by @joechenrh in <a href="https://github.com/apache/arrow-go/pull/302">#302</a></li>
  <li>perf(overflow): Speed up overflow checks for small integers by @cbandy in <a href="https://github.com/apache/arrow-go/pull/303">#303</a></li>
  <li>chore: bump version by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/305">#305</a></li>
  <li>fix(pqarrow): respect list element nullability during conversion by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/311">#311</a></li>
  <li>chore(testing): Update testing submodules by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/312">#312</a></li>
</ul>

<h3 id="new-contributors">New Contributors</h3>
<ul>
  <li>@singh1203 made their first contribution in <a href="https://github.com/apache/arrow-go/pull/257">#257</a></li>
  <li>@jas4711 made their first contribution in <a href="https://github.com/apache/arrow-go/pull/274">#274</a></li>
  <li>@lidavidm made their first contribution in <a href="https://github.com/apache/arrow-go/pull/289">#289</a></li>
  <li>@cbandy made their first contribution in <a href="https://github.com/apache/arrow-go/pull/303">#303</a></li>
</ul>

<p><strong>Full Changelog</strong>: https://github.com/apache/arrow-go/compare/v18.1.0‚Ä¶v18.2.0</p>

<h2 id="contributors">Contributors</h2>
<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">$</span><span class="w"> </span>git shortlog <span class="nt">-sn</span> v18.1.0..v18.2.0
<span class="go">    13	Matt Topol
     3	Saurabh Singh
     2	Ruihao Chen
     1	Chris Bandy
     1	David Li
     1	Simon Josefsson
     1	Sutou Kouhei
</span></code></pre></div></div>]]></content><author><name>pmc</name></author><category term="release" /><summary type="html"><![CDATA[The Apache Arrow team is pleased to announce the v18.2.0 release of Apache Arrow Go. This minor release covers 21 commits from 7 distinct contributors. Highlights Arrow Fixed bitmap ops on 32-bit platforms #277 Allocations by arrow/memory will always be aligned even from the Mallocator #289 Sped up overflow checks for small integers in compute library #303 Parquet The parquet_reader CLI now has an option to dump the column and index offsets #281 Column readers now have a SeekToRow method that will leverage column/index offsets if they exist #283 Full Changelog What‚Äôs Changed fix(release): fix wrong upload path by @kou in #243 fix(release): fix svn add command and add script to generate release notes by @zeroshade in #249 fix(arrow/cdata): move headers into parent Go package by @zeroshade in #251 chore: Add NOTICE.txt file by @singh1203 in #257 chore: Update self-hosted arm runner to ubuntu-24.04-arm by @singh1203 in #268 chore: Drop executable file bit of source files by @jas4711 in #274 ci: update actions/cache by @zeroshade in #276 fix(arrow/bitutil): fix bitmap ops on 32-bit platforms by @zeroshade in #277 feat(internal/encoding): add Discard method to decoders by @zeroshade in #280 feat(parquet/cmd/parquet_reader): Add command to dump the column and offset indices by @zeroshade in #281 fix(internal/utils): fix clobbering BP by @zeroshade in #288 docs(license): update LICENSE.txt by @zeroshade in #290 feat(parquet/file): Add SeekToRow for Column readers by @zeroshade in #283 fix(parquet/pqarrow): propagate field id metadata for lists/maps by @zeroshade in #293 feat: Add arrayApproxEqualString to handle null characters in string. by @singh1203 in #291 feat(parquet): update comments for BufferedStreamEnabled by @joechenrh in #295 fix(arrow/memory): Align allocations always by @lidavidm in #289 feat(parquet): add byte buffer when disable buffered stream by @joechenrh in #302 perf(overflow): Speed up overflow checks for small integers by @cbandy in #303 chore: bump version by @zeroshade in #305 fix(pqarrow): respect list element nullability during conversion by @zeroshade in #311 chore(testing): Update testing submodules by @zeroshade in #312 New Contributors @singh1203 made their first contribution in #257 @jas4711 made their first contribution in #274 @lidavidm made their first contribution in #289 @cbandy made their first contribution in #303 Full Changelog: https://github.com/apache/arrow-go/compare/v18.1.0‚Ä¶v18.2.0 Contributors $ git shortlog -sn v18.1.0..v18.2.0 13 Matt Topol 3 Saurabh Singh 2 Ruihao Chen 1 Chris Bandy 1 David Li 1 Simon Josefsson 1 Sutou Kouhei]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Fast Streaming Inserts in DuckDB with ADBC</title><link href="https://arrow.apache.org/blog/2025/03/10/fast-streaming-inserts-in-duckdb-with-adbc/" rel="alternate" type="text/html" title="Fast Streaming Inserts in DuckDB with ADBC" /><published>2025-03-10T00:00:00-04:00</published><updated>2025-03-10T00:00:00-04:00</updated><id>https://arrow.apache.org/blog/2025/03/10/fast-streaming-inserts-in-duckdb-with-adbc</id><content type="html" xml:base="https://arrow.apache.org/blog/2025/03/10/fast-streaming-inserts-in-duckdb-with-adbc/"><![CDATA[<!--

-->
<style>
.a-header {
  color: #984EA3;
  font-weight: bold;
}
.a-data {
  color: #377EB8;
  font-weight: bold;
}
.a-length {
  color: #FF7F00;
  font-weight: bold;
}
.a-padding {
  color: #E41A1C;
  font-weight: bold;
}
</style>

<p><img src="/img/adbc-duckdb/adbc-duckdb.png" width="100%" class="img-responsive" alt="" aria-hidden="true" /></p>
<h1 id="tldr">TL;DR</h1>

<p>DuckDB is rapidly becoming an essential part of data practitioners‚Äô toolbox, finding use cases in data engineering, machine learning, and local analytics. In many cases DuckDB has been used to query and process data that has already been saved to storage (file-based or external database) by another process. Arrow Database Connectivity APIs enable high-throughput data processing using DuckDB as the engine.</p>

<h1 id="how-it-started">How it started</h1>

<p>The company I work for is the leading digital out-of-home marketing platform, including a programmatic ad tech stack. For several years, my technical operations team was making use of logs emitted by the real-time programmatic auction system in the <a href="http://avro.apache.org/">Apache Avro</a> format. Over time we‚Äôve built an entire operations and analytics back end using this data. Avro files are row-based which is less than ideal for analytics at scale, in fact it‚Äôs downright painful. So much so that I developed and contributed an Avro reader feature to the <a href="https://github.com/apache/arrow-go">Apache Arrow  Go</a> library to be able to convert Avro files to parquet. This data pipeline is now humming along transforming hundreds of GB/day from Avro to Parquet.</p>

<p>Since ‚Äúany problem in computer science can be solved with another layer of indirection‚Äù, the original system has grown layers (like an onion) and started to emit other logs, this time in <a href="https://parquet.apache.org/">Apache Parquet</a> format‚Ä¶</p>
<figure style="text-align: center;">
  <img src="/img/adbc-duckdb/muchrejoicing.webp" width="80%" class="img-responsive" alt="Figure 1: And there was much rejoicing" />
  <figcaption>Figure 1: A pseudo-medieval tapestry displaying intrepid data practitioners rejoicing due to a columnar data storage format.</figcaption>
</figure>
<p>As we learned in Shrek, onions are like ogres: they‚Äôre green, they have layers and they make you cry, so this rejoicing was rather short-lived, as the mechanism chosen to emit the parquet files was rather inefficient:</p>

<ul>
  <li>the new onion-layer (ahem‚Ä¶system component) sends Protobuf encoded messages to Kafka topics</li>
  <li>a Kafka Connect cluster with the S3 sink connector consumes topics and saves the parquet files to object storage</li>
</ul>

<p>Due to the firehose of data, the cluster size over time grew to &gt; 25 nodes and was producing thousands of small Parquet files (13 MB or smaller) an hour. This led to ever-increasing query latency, in some cases breaking our tools due to query timeouts (aka <a href="https://www.dremio.com/blog/compaction-in-apache-iceberg-fine-tuning-your-iceberg-tables-data-files/#h-the-small-files-problem">the small files problem</a>). Not to mention that running aggregations on the raw data in our data warehouse wasn‚Äôt fast or cheap.</p>

<h1 id="duckdb-to-the-rescue-i-think">DuckDB to the rescue‚Ä¶ I think</h1>

<p>I‚Äôd used DuckDB to process and analyse Parquet data so I knew it could do that very quickly. Then I came across this post on LinkedIn (<a href="https://www.linkedin.com/posts/shubham-dhal-349626ba_real-time-analytics-with-kafka-and-duckdb-activity-7258424841538555904-xfU6">Real-Time Analytics using Kafka and DuckDB</a>), where someone has built a system for near-realtime analytics in Go using DuckDB.</p>

<p>The slides listed DuckDB‚Äôs limitations:<br />
<img src="/img/adbc-duckdb/duckdb.png" width="100%" class="img-responsive" alt="DuckDB limitations: Single Pod, *Data should fit in memory, *Low Query Concurrency, *Low Ingest Rate - *Solvable with some efforts" aria-hidden="true" /> 
The poster‚Äôs solution batches data at the application layer managing to scale up ingestion 100x to ~20k inserts/second, noting that they thought that using the DuckDB Appender API could possibly increase this 10x. So, potentially ~200k inserts/second. Yayyyyy‚Ä¶</p>

<figure style="text-align: center;">
  <img src="/img/adbc-duckdb/Yay.gif" width="40%" class="img-responsive" alt="Figure 2: Yay" />
</figure>

<p>Then I noticed the data schema in the slides was flat and had only 4 fields (vs. <a href="https://github.com/InteractiveAdvertisingBureau/openrtb2.x/blob/main/2.6.md#31---object-model-">OpenRTB</a> schema with deeply nested Lists and Structs); and then looked at our monitoring dashboards whereupon I realized that at peak our system was emitting &gt;250k events/second. [cue sad trombone]</p>

<p>Undeterred (and not particularly enamored with the idea of setting up/running/maintaining a Spark cluster), I suspected that Apache Arrow‚Äôs columnar memory representation might still make DuckDB viable since it has an Arrow API; getting Parquet files would be as easy as running <code class="language-plaintext highlighter-rouge">COPY...TO (format parquet)</code>.</p>

<p>Using a pattern found in a Github issue, I wrote a POC using <a href="http://github.com/marcboeker/go-duckdb">github.com/marcboeker/go-duckdb</a> to connect to a DB, retrieve an Arrow, create an Arrow Reader, register a view on the reader, then run an INSERT statement from the view.</p>

<p>This felt a bit like a rabbit pulling itself out of a hat, but no matter, it managed between ~74k and ~110k rows/sec on my laptop.</p>

<p>To make sure this was really the right solution, I also tried out DuckDB‚Äôs Appender API (at time of writing the official recommendation for fast inserts) and managed‚Ä¶ ~63k rows/sec on my laptop. OK, but‚Ä¶ meh.</p>

<h1 id="a-new-hope">A new hope</h1>

<p>In a discussion on the Gopher Slack, Matthew Topol aka <a href="https://github.com/zeroshade">zeroshade</a> suggested using <a href="http://arrow.apache.org/adbc">ADBC</a> with its much simpler API. Who is Matt Topol you ask? Just the guy who <em>literally</em> wrote the book on Apache Arrow, that‚Äôs who (<a href="https://www.packtpub.com/en-us/product/in-memory-analytics-with-apache-arrow-9781835461228"><strong><em>In-Memory Analytics with Apache Arrow: Accelerate data analytics for efficient processing of flat and hierarchical data structures 2nd Edition</em></strong></a>). It‚Äôs an excellent resource and guide for working with Arrow. <br />
BTW, should you prefer an acronym to remember the name of the book, it‚Äôs <strong><em>IMAAA:ADAFEPOFAHDS2E</em></strong>.<br />
<img src="/img/adbc-duckdb/imaaapfedaobfhsd2e.png" width="100%" class="img-responsive" alt="Episode IX: In-Memory Analytics with Apache Arrow: Perform fast and efficient data analytics on both flat and hierarchical structured data 2nd Edition aka IMAAA:PFEDAOBFHSD2E by Matt Topol" aria-hidden="true" /><br />
But I digress. Matt is also a member of the Apache Arrow PMC, a major contributor to the Go implementation of Apache Iceberg and generally a nice, helpful guy.</p>

<h1 id="adbc">ADBC</h1>
<p>ADBC is:</p>
<ul>
  <li>
    <p>A set of <a href="https://arrow.apache.org/adbc/current/format/specification.html">abstract APIs</a> in different languages (C/C++, Go, and Java, with more on the way) for working with databases and Arrow data.</p>

    <p>For example, result sets of queries in ADBC are all returned as streams of Arrow data, not row-by-row.</p>
  </li>
  <li>
    <p>A set of implementations of that API in different languages (C/C++, C#/.NET, Go, Java, Python, and Ruby) that target different databases (e.g. PostgreSQL, SQLite, DuckDB, any database supporting Flight SQL).</p>
  </li>
</ul>

<p>Going back to the drawing board, I created <a href="https://github.com/loicalleyne/quacfka">Quacfka</a>, a Go library built using ADBC and split out my system into 3 worker pools, connected by channels:</p>

<ul>
  <li>Kafka clients consuming topic messages and writing the bytes to a message channel</li>
  <li>Processing routines using the <a href="https://github.com/loicalleyne/bufarrow">Bufarrow</a> library to deserialize Protobuf data and append it to Arrow arrays, writing Arrow Records to a record channel</li>
  <li>DuckDB inserters binding the Arrow Records to ADBC statements and executing insertions</li>
</ul>

<p>I first ran these in series to determine how fast each could run:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight">
<code>2025/01/23 23:39:27 <span class="a-header">kafka read start with 8 readers</span>
2025/01/23 23:39:41 <span class="a-header">read 15728642 kafka records in 14.385530 secs @</span><span class="a-padding">1093365.498477 messages/sec</span>
2025/01/23 23:39:41 <span class="a-length">deserialize []byte to proto, convert to arrow records with 32 goroutines start</span>
2025/01/23 23:40:04 <span class="a-length">deserialize to arrow done - 15728642 records in 22.283532 secs @</span><span class="a-padding"> 705841.509812 messages/sec</span>
2025/01/23 23:40:04 <span class="a-data">ADBC IngestCreateAppend start with 32 connections</span> 
2025/01/23 23:40:25 <span class="a-data">duck ADBC insert 15728642 records in 21.145649535 secs @</span><span class="a-padding"> 743824.007783 rows/sec</span></code></pre></div></div>
<p><img src="/img/adbc-duckdb/holdmybeer.png" width="100%" class="img-responsive" alt="20k rows/sec? Hold my beer" aria-hidden="true" /></p>

<p>With this architecture decided, I then started running the workers concurrently, instrumenting the system, profiling my code to identify performance issues and tweaking the settings to maximize throughput. It seemed to me that there was enough performance headroom to allow for in-flight aggregations.</p>

<p>One issue: Despite DuckDB‚Äôs excellent <a href="https://duckdb.org/2022/10/28/lightweight-compression.html">lightweight compression</a>, inserts from this source were making the file size increase at a rate of <strong><em>~8GB/minute</em></strong>. Putting inserts on hold to export the Parquet files and release the storage would reduce the overall throughput to an unacceptable level. I decided to implement a rotation of database files based on a file size threshold.</p>

<p>DuckDB being able to query Hive partitioned parquet on disk or in object storage, the analytics part could be decoupled from the data ingestion pipeline by running a separate querying server pointing at wherever the parquet files would end up.</p>

<p>Iterating, I created several APIs to try to make in-flight aggregations efficient enough to keep the overall throughput above my 250k rows/second target.</p>

<p>The first two either ran into issues of data locality or weren‚Äôt optimized enough:</p>

<ul>
  <li><strong>CustomArrows</strong> : functions to run on each Arrow Record to create a new Record to insert along with the original</li>
  <li><strong>DuckRunner</strong> : run a series of queries on the database file before rotation</li>
</ul>

<p>Reasoning that if unnesting deeply nested data in Arrow Record arrays was causing data locality issues:</p>

<ul>
  <li><strong>Normalizer</strong>: a Bufarrow API used in the in the deserialization function to normalize the message data and append it to another Arrow Record, inserted into a separate table</li>
</ul>

<p>This approach allowed throughput to go back to levels almost as high as without Normalizer - flat data is much faster to process and insert.</p>

<h1 id="oh-were-halfway-therelivin-on-a-prayer">Oh, we‚Äôre halfway there‚Ä¶livin‚Äô on a prayer</h1>

<p>Next, I tried opening concurrent connections to multiple databases. <strong>BAM!</strong> <strong><em>Segfault</em></strong>. DuckDB concurrency model isn‚Äôt <a href="https://duckdb.org/docs/stable/connect/concurrency.html#handling-concurrency">designed</a> that way. From within a process only a single database (in-memory or file) can be opened, then other database files can be <a href="https://duckdb.org/docs/stable/sql/statements/attach.html">attached</a> to the central db‚Äôs catalog.</p>

<p>Having already decided to rotate DB files, I decided to make a separate program (<a href="https://github.com/loicalleyne/quacfka-runner">Runner</a>) to process the database files as they were rotated, running aggregations on normalized data and table dumps to parquet. This meant setting up an RPC connection between the two and figuring out a backpressure mechanism to avoid <code class="language-plaintext highlighter-rouge">disk full</code> events.</p>

<p>However having the two running simultaneously was causing memory pressure issues, not to mention massively slowing down the throughput. Upgrading the VM to one with more vCPUs and memory only helped a little, there was clearly some resource contention going on.</p>

<p>Since Go 1.5, the default <code class="language-plaintext highlighter-rouge">GOMAXPROCS</code> value is the number of CPU cores available. What if this was reduced to ‚Äúsandbox‚Äù the ingestion process, along with setting the DuckDB thread count in the Runner? This actually worked so well, it increased the overall throughput. <a href="https://github.com/loicalleyne/quacfka-runner">Runner</a> runs the <code class="language-plaintext highlighter-rouge">COPY...TO...parquet</code> queries, walks the parquet output folder, uploads files to object storage and deletes the uploaded files. Balancing the DuckDB file rotation size threshold in <a href="https://github.com/loicalleyne/quacfka-service">Quafka-Service</a> allows Runner to keep up and avoid a backlog of DB files on disk.</p>

<h1 id="results">Results</h1>

<figure style="text-align: center;">
  <img src="/img/adbc-duckdb/btop.png" width="100%" class="img-responsive" alt="Figure 1: btop utility showing CPU and memory usage of quacfka-service and runner" />
  <figcaption>Figure 2: btop utility showing CPU and memory usage of quacfka-service and runner.</figcaption>
</figure>
<p>Note: both runs with <code class="language-plaintext highlighter-rouge">GOMAXPROCS</code> set to 24 (the number of DuckDB insertion routines)</p>

<p>Ingesting the raw data (14 fields with one deeply nested LIST.STRUCT.LIST field) + normalized data:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight">
<code>
num_cpu:                <span class="a-header">60</span>  
runtime_os:             <span class="a-header">linux</span>  
kafka_clients:          <span class="a-header">5</span>  
kafka_queue_cap:        <span class="a-header">983040</span>  
processor_routines:     <span class="a-header">32</span>  
arrow_queue_cap:        <span class="a-header">4</span>  
duckdb_threshold_mb:    <span class="a-header">4200</span>  
duckdb_connections:     <span class="a-header">24</span>  
normalizer_fields:      <span class="a-header">10</span>  
start_time:             <span class="a-header">2025-02-24T21:06:23Z</span>  
end_time:               <span class="a-header">2025-02-24T21:11:23Z</span>  
records:                <span class="a-header">123_686_901.00</span>  
norm_records:           <span class="a-header">122_212_452.00</span>  
data_transferred:       <span class="a-header">146.53 GB</span>  
duration:               <span class="a-header">4m59.585s</span>  
records_per_second:     <span class="a-padding">398_271.90</span>  
total_rows_per_second:  <span class="a-padding">806_210.41</span>  
transfer_rate:          <span class="a-header">500.86 MB/second</span>  
duckdb_files:           <span class="a-header">9</span>  
duckdb_files_MB:        <span class="a-header">38429</span>
file_avg_duration:      <span class="a-header">33.579s</span></code></pre></div></div>

<p>How many rows/second could we get if we only inserted the flat, normalized data? (Note: original records are still processed, just not inserted):</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight">
<code>
num_cpu:                <span class="a-header">60</span>
runtime_os:             <span class="a-header">linux</span>
kafka_clients:          <span class="a-header">10</span>
kafka_queue_cap:        <span class="a-header">1228800</span>  
processor_routines:     <span class="a-header">32</span>  
arrow_queue_cap:        <span class="a-header">4</span>  
duckdb_threshold_mb:    <span class="a-header">4200</span>  
duckdb_connections:     <span class="a-header">24</span>  
normalizer_fields:      <span class="a-header">10</span>  
start_time:             <span class="a-header">2025-02-25T19:04:33Z</span>  
end_time:               <span class="a-header">2025-02-25T19:09:36Z</span>  
records:                <span class="a-header">231_852_772.00</span>  
norm_records:           <span class="a-header">363_247_327.00</span>  
data_transferred:       <span class="a-header">285.76 GB</span>  
duration:               <span class="a-header">5m3.059s</span>  
records_per_second:     <span class="a-header">0.00</span>  
total_rows_per_second:  <span class="a-padding">1_198_601.39</span> 
transfer_rate:          <span class="a-header">965.54 MB/second</span> 
duckdb_files:           <span class="a-header">5</span>  
duckdb_files_MB:        <span class="a-header">20056</span>  
file_avg_duration:      <span class="a-header">58.975s</span></code></pre></div></div>

<p><img src="/img/adbc-duckdb/onemillionrows.png" width="100%" class="img-responsive" alt="One million rows/second" aria-hidden="true" /></p>

<p>Once deployed, the number of parquet files fell from ~3000 small files per hour to &lt; 20 files per hour. Goodbye small files!</p>

<p><img src="/img/adbc-duckdb/kip_yes.gif" width="25%" class="img-responsive" alt="Yesss" aria-hidden="true" /></p>

<h1 id="challengeslearnings">Challenges/Learnings</h1>

<ul>
  <li>DuckDB insertions are the bottleneck; network speed, Protobuf deserialization, <strong>building Arrow Records are not</strong>.</li>
  <li>For fastest insertion into DuckDB, Arrow Record Batches should contain at least 122880 rows (to align with DuckDB storage row group size).</li>
  <li>DuckDB won‚Äôt let you open more than one database at once within the same process (results in a segfault). DuckDB is designed to run only once in a process, with a central database‚Äôs catalog having the ability to add connections to other databases.
    <ul>
      <li>Workarounds:
        <ul>
          <li>Use separate processes for writing and reading multiple database files.</li>
          <li>Open a single DuckDB database and use <a href="https://duckdb.org/docs/stable/sql/statements/attach.html">ATTACH</a> to attach other DB files.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Flat data is much, much faster to insert than nested data.</li>
</ul>

<p><img src="/img/adbc-duckdb/whatdoesitallmean.gif" width="100%" class="img-responsive" alt="Whoopdy doo, what does it all mean Basil?" aria-hidden="true" /></p>

<p>ADBC provides DuckDB with a truly high-throughput data ingestion API, unlocking a slew of use cases for using DuckDB with streaming data, making this an ever more useful tool for data practitioners.</p>]]></content><author><name>loicalleyne</name></author><category term="application" /><summary type="html"><![CDATA[ADBC enables high throughput insertion into DuckDB]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/adbc-duckdb/adbc-duckdb.png" /><media:content medium="image" url="https://arrow.apache.org/img/adbc-duckdb/adbc-duckdb.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Apache Arrow ADBC 17 (Libraries) Release</title><link href="https://arrow.apache.org/blog/2025/03/07/adbc-17-release/" rel="alternate" type="text/html" title="Apache Arrow ADBC 17 (Libraries) Release" /><published>2025-03-07T00:00:00-05:00</published><updated>2025-03-07T00:00:00-05:00</updated><id>https://arrow.apache.org/blog/2025/03/07/adbc-17-release</id><content type="html" xml:base="https://arrow.apache.org/blog/2025/03/07/adbc-17-release/"><![CDATA[<!--

-->

<p>The Apache Arrow team is pleased to announce the version 17 release of
the Apache Arrow ADBC libraries. This release includes <a href="https://github.com/apache/arrow-adbc/milestone/21"><strong>18
resolved issues</strong></a> from <a href="#contributors"><strong>13 distinct contributors</strong></a>.</p>

<p>This is a release of the <strong>libraries</strong>, which are at version 17.  The
<a href="https://arrow.apache.org/adbc/17/format/specification.html"><strong>API specification</strong></a> is versioned separately and is at
version 1.1.0.</p>

<p>The subcomponents are versioned independently:</p>

<ul>
  <li>C/C++/GLib/Go/Python/Ruby: 1.5.0</li>
  <li>C#: 0.17.0</li>
  <li>Java: 0.17.0</li>
  <li>R: 0.17.0</li>
  <li>Rust: 0.17.0</li>
</ul>

<p>The release notes below are not exhaustive and only expose selected
highlights of the release. Many other bugfixes and improvements have
been made: we refer you to the <a href="https://github.com/apache/arrow-adbc/blob/apache-arrow-adbc-17/CHANGELOG.md">complete changelog</a>.</p>

<h2 id="release-highlights">Release Highlights</h2>

<p>The CMake config can now build against system dependencies instead of forcing vendored dependencies (<a href="https://github.com/apache/arrow-adbc/pull/2546">#2546</a>).  Also, CMake files are now installed even for the drivers written in Go (<a href="https://github.com/apache/arrow-adbc/issues/2506">#2506</a>).  Packages for Ubuntu 24.04 LTS are now available (<a href="https://github.com/apache/arrow-adbc/pull/2482">#2482</a>).</p>

<p>The performance of <code class="language-plaintext highlighter-rouge">AdbcDataReader</code> and <code class="language-plaintext highlighter-rouge">ValueAt</code> has been improved in C# (<a href="https://github.com/apache/arrow-adbc/pull/2534">#2534</a>).  The C# BigQuery driver will now use a default project ID if one is not specified (<a href="https://github.com/apache/arrow-adbc/pull/2471">#2471</a>).</p>

<p>The Flight SQL and Snowflake drivers allow passing low-level options in Go (gRPC dial options in <a href="https://github.com/apache/arrow-adbc/pull/2563">#2563</a> and gosnowflake options in <a href="https://github.com/apache/arrow-adbc/pull/2558">#2558</a>).  The Flight SQL driver should now provide column-level metadata (<a href="https://github.com/apache/arrow-adbc/pull/2481">#2481</a>).  The Snowflake driver now no longer requires setting the current schema to get metadata (<a href="https://github.com/apache/arrow-adbc/issues/2517">#2517</a>).</p>

<h2 id="contributors">Contributors</h2>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ git shortlog --perl-regexp --author='^((?!dependabot\[bot\]).*)$' -sn apache-arrow-adbc-16..apache-arrow-adbc-17
    15	David Li
     6	Matthijs Brobbel
     2	H√©lder Greg√≥rio
     2	Matt Topol
     2	Matthias Kuhn
     2	Sutou Kouhei
     2	davidhcoe
     1	Curt Hagenlocher
     1	Felipe Oliveira Carvalho
     1	Felipe Vianna
     1	Marius van Niekerk
     1	Shuoze Li
     1	amangoyal
</code></pre></div></div>

<h2 id="roadmap">Roadmap</h2>

<p>There is some discussion on a potential second revision of ADBC to include more missing functionality and asynchronous API support.  For more, see the <a href="https://github.com/apache/arrow-adbc/milestone/8">milestone</a>.  We would welcome suggestions on APIs that could be added or extended.</p>

<h2 id="getting-involved">Getting Involved</h2>

<p>We welcome questions and contributions from all interested.  Issues
can be filed on <a href="https://github.com/apache/arrow-adbc/issues">GitHub</a>, and questions can be directed to GitHub
or the <a href="/community/">Arrow mailing lists</a>.</p>]]></content><author><name>pmc</name></author><category term="release" /><summary type="html"><![CDATA[The Apache Arrow team is pleased to announce the version 17 release of the Apache Arrow ADBC libraries. This release includes 18 resolved issues from 13 distinct contributors. This is a release of the libraries, which are at version 17. The API specification is versioned separately and is at version 1.1.0. The subcomponents are versioned independently: C/C++/GLib/Go/Python/Ruby: 1.5.0 C#: 0.17.0 Java: 0.17.0 R: 0.17.0 Rust: 0.17.0 The release notes below are not exhaustive and only expose selected highlights of the release. Many other bugfixes and improvements have been made: we refer you to the complete changelog. Release Highlights The CMake config can now build against system dependencies instead of forcing vendored dependencies (#2546). Also, CMake files are now installed even for the drivers written in Go (#2506). Packages for Ubuntu 24.04 LTS are now available (#2482). The performance of AdbcDataReader and ValueAt has been improved in C# (#2534). The C# BigQuery driver will now use a default project ID if one is not specified (#2471). The Flight SQL and Snowflake drivers allow passing low-level options in Go (gRPC dial options in #2563 and gosnowflake options in #2558). The Flight SQL driver should now provide column-level metadata (#2481). The Snowflake driver now no longer requires setting the current schema to get metadata (#2517). Contributors $ git shortlog --perl-regexp --author='^((?!dependabot\[bot\]).*)$' -sn apache-arrow-adbc-16..apache-arrow-adbc-17 15 David Li 6 Matthijs Brobbel 2 H√©lder Greg√≥rio 2 Matt Topol 2 Matthias Kuhn 2 Sutou Kouhei 2 davidhcoe 1 Curt Hagenlocher 1 Felipe Oliveira Carvalho 1 Felipe Vianna 1 Marius van Niekerk 1 Shuoze Li 1 amangoyal Roadmap There is some discussion on a potential second revision of ADBC to include more missing functionality and asynchronous API support. For more, see the milestone. We would welcome suggestions on APIs that could be added or extended. Getting Involved We welcome questions and contributions from all interested. Issues can be filed on GitHub, and questions can be directed to GitHub or the Arrow mailing lists.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>