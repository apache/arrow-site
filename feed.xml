<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.0">Jekyll</generator><link href="https://arrow.apache.org/feed.xml" rel="self" type="application/atom+xml" /><link href="https://arrow.apache.org/" rel="alternate" type="text/html" /><updated>2021-02-11T19:49:41-05:00</updated><id>https://arrow.apache.org/feed.xml</id><title type="html">Apache Arrow</title><subtitle>Apache Arrow is a cross-language development platform for in-memory data. It specifies a standardized language-independent columnar memory format for flat and hierarchical data, organized for efficient analytic operations on modern hardware. It also provides computational libraries and zero-copy streaming messaging and interprocess communication. Languages currently supported include C, C++, C#, Go, Java, JavaScript, MATLAB, Python, R, Ruby, and Rust.</subtitle><entry><title type="html">Apache Arrow 3.0.0 Release</title><link href="https://arrow.apache.org/blog/2021/01/25/3.0.0-release/" rel="alternate" type="text/html" title="Apache Arrow 3.0.0 Release" /><published>2021-01-25T01:00:00-05:00</published><updated>2021-01-25T01:00:00-05:00</updated><id>https://arrow.apache.org/blog/2021/01/25/3.0.0-release</id><content type="html" xml:base="https://arrow.apache.org/blog/2021/01/25/3.0.0-release/">&lt;!--

--&gt;

&lt;p&gt;The Apache Arrow team is pleased to announce the 3.0.0 release. This covers
over 3 months of development work and includes &lt;a href=&quot;https://issues.apache.org/jira/issues/?jql=project%20%3D%20ARROW%20AND%20status%20%3D%20Resolved%20AND%20fixVersion%20%3D%203.0.0&quot;&gt;&lt;strong&gt;666 resolved issues&lt;/strong&gt;&lt;/a&gt;
from &lt;a href=&quot;/release/3.0.0.html#contributors&quot;&gt;&lt;strong&gt;106 distinct contributors&lt;/strong&gt;&lt;/a&gt;. See the Install Page to learn how to
get the libraries for your platform.&lt;/p&gt;

&lt;p&gt;The release notes below are not exhaustive and only expose selected highlights
of the release. Many other bugfixes and improvements have been made: we refer
you to the &lt;a href=&quot;/release/3.0.0.html&quot;&gt;complete changelog&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;columnar-format-notes&quot;&gt;Columnar Format Notes&lt;/h2&gt;

&lt;p&gt;The Decimal256 data type, which was already supported by the Arrow columnar
format specification, is now implemented in C++ and Java (ARROW-9747).&lt;/p&gt;

&lt;h2 id=&quot;arrow-flight-rpc-notes&quot;&gt;Arrow Flight RPC notes&lt;/h2&gt;

&lt;p&gt;Authentication in C++/Java/Python has been overhauled, allowing more flexible authentication methods and use of standard headers.
Support for cookies has also been added.
The C++/Java implementations are now more permissive when parsing messages in order to interoperate better with other Flight implementations.&lt;/p&gt;

&lt;p&gt;A basic Flight implementation for C#/.NET has been added.
See the &lt;a href=&quot;https://arrow.apache.org/docs/status.html#flight-rpc&quot;&gt;implementation status matrix&lt;/a&gt; for details.&lt;/p&gt;

&lt;h2 id=&quot;c-notes&quot;&gt;C++ notes&lt;/h2&gt;

&lt;p&gt;The default memory pool can now be changed at runtime using the environment
variable &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ARROW_DEFAULT_MEMORY_POOL&lt;/code&gt; (ARROW-11009).  The environment variable
is inspected at process startup.  This is useful when trying to diagnose memory
consumption issues with Arrow.&lt;/p&gt;

&lt;p&gt;STL-like iterators are now provided over concrete arrays. Those are useful for
non-performance critical tasks, for example testing (ARROW-10776).&lt;/p&gt;

&lt;p&gt;It is now possible to concatenate dictionary arrays with unequal dictionaries.
The dictionaries are unified when concatenating, for supported data types
(ARROW-5336).&lt;/p&gt;

&lt;p&gt;Threads in a thread pool are now spawned lazily as needed for enqueued
tasks, up to the configured capacity. They used to be spawned upfront on
creation of the thread pool (ARROW-10038).&lt;/p&gt;

&lt;h3 id=&quot;compute-layer&quot;&gt;Compute layer&lt;/h3&gt;

&lt;p&gt;Comprehensive documentation for compute functions is now available:
https://arrow.apache.org/docs/cpp/compute.html&lt;/p&gt;

&lt;p&gt;Compute functions for string processing have been added for:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;splitting on whitespace (ASCII and Unicode flavors) and splitting on a
pattern (ARROW-9991);&lt;/li&gt;
  &lt;li&gt;trimming characters (ARROW-9128).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Behavior of the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;index_in&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;is_in&lt;/code&gt; compute functions with nulls has been
changed for consistency (ARROW-10663).&lt;/p&gt;

&lt;p&gt;Multiple-column sort kernels are now available for tables and record batches
(ARROW-8199, ARROW-10796, ARROW-10790).&lt;/p&gt;

&lt;p&gt;Performance of table filtering has been vastly improved (ARROW-10569).&lt;/p&gt;

&lt;p&gt;Scalar arguments are now accepted for more compute functions.&lt;/p&gt;

&lt;p&gt;Compute functions &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;quantile&lt;/code&gt; (ARROW-10831) and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;is_nan&lt;/code&gt; (ARROW-11043) have been
added for numeric data.&lt;/p&gt;

&lt;p&gt;Aggregation functions &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;any&lt;/code&gt; (ARROW-1846) and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;all&lt;/code&gt; (ARROW-10301) have been
added for boolean data.&lt;/p&gt;

&lt;h3 id=&quot;dataset&quot;&gt;Dataset&lt;/h3&gt;

&lt;p&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Expression&lt;/code&gt; hierarchy has simplified to a wrapper around literals, field references,
or calls to named functions. This enables usage of any compute function while filtering
with no boilerplate.&lt;/p&gt;

&lt;p&gt;Parquet statistics are lazily parsed in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ParquetDatasetFactory&lt;/code&gt; and
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ParquetFileFragment&lt;/code&gt; for shorter construction time.&lt;/p&gt;

&lt;h3 id=&quot;csv&quot;&gt;CSV&lt;/h3&gt;

&lt;p&gt;Conversion of string columns is now faster thanks to faster UTF-8 validation
of small strings (ARROW-10313).&lt;/p&gt;

&lt;p&gt;Conversion of floating-point columns is now faster thanks to optimized
string-to-double conversion routines (ARROW-10328).&lt;/p&gt;

&lt;p&gt;Parsing of ISO8601 timestamps is now more liberal: trailing zeros can
be omitted in the fractional part (ARROW-10337).&lt;/p&gt;

&lt;p&gt;Fixed a bug where null detection could give the wrong results on some platforms
(ARROW-11067).&lt;/p&gt;

&lt;p&gt;Added type inference for Date32 columns for values in the form &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;YYYY-MM-DD&lt;/code&gt;
(ARROW-11247).&lt;/p&gt;

&lt;h3 id=&quot;feather&quot;&gt;Feather&lt;/h3&gt;

&lt;p&gt;Fixed reading of compressed Feather files written with Arrow 0.17 (ARROW-11163).&lt;/p&gt;

&lt;h3 id=&quot;filesystem-layer&quot;&gt;Filesystem layer&lt;/h3&gt;

&lt;p&gt;S3 recursive tree walks now benefit from a parallel implementation, where reads
of multiple child directories are now issued concurrently (ARROW-10788).&lt;/p&gt;

&lt;p&gt;Improved empty directory detection to be mindful of differences between Amazon
and Minio S3 implementations (ARROW-10942).&lt;/p&gt;

&lt;h3 id=&quot;flight-rpc&quot;&gt;Flight RPC&lt;/h3&gt;

&lt;p&gt;IPv6 host addresses are now supported (ARROW-10475).&lt;/p&gt;

&lt;h3 id=&quot;ipc&quot;&gt;IPC&lt;/h3&gt;

&lt;p&gt;It is now possible to emit dictionary deltas where possible using the IPC
stream writer. This is governed by a new variable in the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;IpcWriteOptions&lt;/code&gt; class
(ARROW-6883).&lt;/p&gt;

&lt;p&gt;It is now possible to read wider tables, which used to fail due to reaching a
limit during Flatbuffers verification (ARROW-10056).&lt;/p&gt;

&lt;h3 id=&quot;parquet&quot;&gt;Parquet&lt;/h3&gt;

&lt;p&gt;Fixed reading of LZ4-compressed Parquet columns emitted by the Java Parquet
implementation (ARROW-11301).&lt;/p&gt;

&lt;p&gt;Fixed a bug where writing multiple batches of nullable nested strings to Parquet
would not write any data in batches after the first one (ARROW-10493)&lt;/p&gt;

&lt;p&gt;The Decimal256 data type can be read from and written to Parquet (ARROW-10607).&lt;/p&gt;

&lt;p&gt;LargeString and LargeBinary data can now be written to Parquet (ARROW-10426).&lt;/p&gt;

&lt;h2 id=&quot;c-notes-1&quot;&gt;C# notes&lt;/h2&gt;

&lt;p&gt;The .NET package added initial support for Arrow Flight clients and servers. Support is enabled through two new NuGet packages &lt;a href=&quot;https://www.nuget.org/packages/Apache.Arrow.Flight/&quot;&gt;Apache.Arrow.Flight&lt;/a&gt; (client) and &lt;a href=&quot;https://www.nuget.org/packages/Apache.Arrow.Flight.AspNetCore/&quot;&gt;Apache.Arrow.Flight.AspNetCore&lt;/a&gt; (server).&lt;/p&gt;

&lt;p&gt;Also fixed an issue where ArrowStreamWriter wasn’t writing schema metadata to Arrow streams.&lt;/p&gt;

&lt;h2 id=&quot;julia-notes&quot;&gt;Julia notes&lt;/h2&gt;

&lt;p&gt;This is the first release to officially include
&lt;a href=&quot;https://github.com/apache/arrow/tree/master/julia/Arrow&quot;&gt;an implementation&lt;/a&gt;
for the Julia language. The pure Julia implementation includes support
for &lt;a href=&quot;https://arrow.apache.org/docs/status.html&quot;&gt;wide coverage of the format specification&lt;/a&gt;.
Additional details can be found in the
&lt;a href=&quot;https://julialang.org/blog/2021/01/arrow/&quot;&gt;julialang.org blog post&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;python-notes&quot;&gt;Python notes&lt;/h2&gt;

&lt;p&gt;Support for Python 3.9 was added (ARROW-10224), and support for Python 3.5
was removed (ARROW-5679).&lt;/p&gt;

&lt;p&gt;Support for building manylinux1 packages has been removed (ARROW-11212).
PyArrow continues to be available as manylinux2010 and manylinux2014 wheels.&lt;/p&gt;

&lt;p&gt;The minimal required version for NumPy is now 1.16.6. Note that when upgrading
NumPy to 1.20, you also need to upgrade pyarrow to 3.0.0 to ensure compatibility,
as this pyarrow release fixed a compatibility issue with NumPy 1.20 (ARROW-10833).&lt;/p&gt;

&lt;p&gt;Compute functions are now automatically exported from C++ to the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pyarrow.compute&lt;/code&gt;
module, and they have docstrings matching their C++ definition.&lt;/p&gt;

&lt;p&gt;An &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;iter_batches()&lt;/code&gt; method is now available for reading a Parquet file iteratively
(ARROW-7800).&lt;/p&gt;

&lt;p&gt;Alternate memory pools (such as mimalloc, jemalloc or the C malloc-based memory
pool) are now available from Python (ARROW-11049).&lt;/p&gt;

&lt;p&gt;Fixed a potential deadlock when importing pandas from several threads (ARROW-10519).&lt;/p&gt;

&lt;p&gt;See the C++ notes above for additional details.&lt;/p&gt;

&lt;h2 id=&quot;r-notes&quot;&gt;R notes&lt;/h2&gt;

&lt;p&gt;This release contains new features for the Flight RPC wrapper, better
support for saving R metadata (including &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sf&lt;/code&gt; spatial data) to Feather and
Parquet files, several significant improvements to speed and memory management,
and many other enhancements.&lt;/p&gt;

&lt;p&gt;For more on what’s in the 3.0.0 R package, see the &lt;a href=&quot;/docs/r/news/&quot;&gt;R changelog&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;ruby-and-c-glib-notes&quot;&gt;Ruby and C GLib notes&lt;/h2&gt;

&lt;h3 id=&quot;ruby&quot;&gt;Ruby&lt;/h3&gt;

&lt;p&gt;In Ruby binding, 256-bit decimal support and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Arrow::FixedBinaryArrayBuilder&lt;/code&gt; are added likewise C GLib below.&lt;/p&gt;

&lt;h3 id=&quot;c-glib&quot;&gt;C GLib&lt;/h3&gt;

&lt;p&gt;In the version 3.0.0 of C GLib consists of many new features.&lt;/p&gt;

&lt;p&gt;A chunked array, a record batch, and a table support &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sort_indices&lt;/code&gt; function as well as an array.
These functions including array’s support to specify sorting option.
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;garrow_array_sort_to_indices&lt;/code&gt; has been renamed to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;garrow_array_sort_indices&lt;/code&gt; and the previous name has been deprecated.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;GArrowField&lt;/code&gt; supports functions to handle metadata.
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;GArrowSchema&lt;/code&gt; supports &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;garrow_schema_has_metadata()&lt;/code&gt; function.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;GArrowArrayBuilder&lt;/code&gt; supports to add single null, multiple nulls, single empty value, and multiple empty values.
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;GArrowFixedSizedBinaryArrayBuilder&lt;/code&gt; is newly supported.&lt;/p&gt;

&lt;p&gt;256-bit decimal and extension types are newly supported.
Filesystem module supports Mock, HDFS, S3 file systems.
Dataset module supports CSV, IPC, and Parquet file formats.&lt;/p&gt;

&lt;h2 id=&quot;rust-notes&quot;&gt;Rust notes&lt;/h2&gt;
&lt;h3 id=&quot;core-arrow-crate&quot;&gt;Core Arrow Crate&lt;/h3&gt;

&lt;p&gt;The development of the arrow crate was focused on four main aspects:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Make the crate usable in stable Rust&lt;/li&gt;
  &lt;li&gt;Bug fixing and removal of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;unsafe&lt;/code&gt; code&lt;/li&gt;
  &lt;li&gt;Extend functionality to keep up with the specification&lt;/li&gt;
  &lt;li&gt;Increase performance of existing kernels&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;stable-rust&quot;&gt;Stable Rust&lt;/h4&gt;

&lt;p&gt;Possibly the biggest news for this release is that all project crates, including arrow, parquet, and datafusion now
build with stable Rust by default. Nightly / unstable Rust is still required when enabling the SIMD feature.&lt;/p&gt;

&lt;h4 id=&quot;parquet-arrow-writer&quot;&gt;Parquet Arrow writer&lt;/h4&gt;

&lt;p&gt;The Parquet Writer for Arrow arrays is now available, allowing the Rust programs to easily read and write Parquet
files and making it easier to integrate with the overall Arrow ecosystem. The reader and writer include both basic
and nested type support (List, Dictionary, Struct)&lt;/p&gt;

&lt;h4 id=&quot;first-class-arrow-flight-ipc-support&quot;&gt;First Class Arrow Flight IPC Support&lt;/h4&gt;

&lt;p&gt;This release the Arrow Flight IPC implementation in Rust became fully-featured enough to participate in the regular
cross-language integration tests, thus ensuring Rust applications written using Arrow can interoperate with the rest
of the ecosystem&lt;/p&gt;

&lt;h4 id=&quot;performance&quot;&gt;Performance&lt;/h4&gt;

&lt;p&gt;There have been numerous performance improvements in this release across the board. This includes both kernel
operations, such as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;take&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;filter&lt;/code&gt;, and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cast&lt;/code&gt;, as well as more fundamental parts such as bitwise comparison
and reading and writing to CSV.&lt;/p&gt;

&lt;h4 id=&quot;increased-data-type-support&quot;&gt;Increased Data Type Support&lt;/h4&gt;

&lt;p&gt;New DataTypes:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Decimal data type for fixed-width decimal values&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Improved operation support for nested structures Dictionary, and Lists (filter, take, etc)&lt;/p&gt;

&lt;h4 id=&quot;other-improvements&quot;&gt;Other improvements:&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;Added support for Date and time on FFI&lt;/li&gt;
  &lt;li&gt;Added support for Binary type on FFI&lt;/li&gt;
  &lt;li&gt;Added support for i64 sized arrays to “take” kernel&lt;/li&gt;
  &lt;li&gt;Support for the i128 Decimal Type&lt;/li&gt;
  &lt;li&gt;Added support to cast string to date&lt;/li&gt;
  &lt;li&gt;Added API to create arrays out of existing arrays (e.g. for join, merge-sort, concatenate)&lt;/li&gt;
  &lt;li&gt;The simd feature is now also available on aarch64&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;api-changes&quot;&gt;API Changes&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;BooleanArray is no longer a PrimitiveArray&lt;/li&gt;
  &lt;li&gt;ArrowNativeType no longer includes bool since arrows boolean type is represented using bitpacking&lt;/li&gt;
  &lt;li&gt;Several Buffer methods are now infallible instead of returning a Result&lt;/li&gt;
  &lt;li&gt;DataType::List now contains a Field to track metadata about the contained elements&lt;/li&gt;
  &lt;li&gt;PrimitiveArray::raw_values, values_slice and values methods got replaced by a values method returning a slice&lt;/li&gt;
  &lt;li&gt;Buffer::data and raw_data were renamed to as_slice and as_ptr&lt;/li&gt;
  &lt;li&gt;MutableBuffer::data_mut and freeze were renamed to as_slice_mut and into to be more consistent with the stdlib naming conventions&lt;/li&gt;
  &lt;li&gt;The generic type parameter for BufferBuilder was changed from ArrowPrimitiveType to ArrowNativeType&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;datafusion&quot;&gt;DataFusion&lt;/h3&gt;

&lt;h4 id=&quot;sql&quot;&gt;SQL&lt;/h4&gt;

&lt;p&gt;In this release, we clarified that DataFusion will standardize on the PostgreSQL SQL dialect.&lt;/p&gt;

&lt;p&gt;New SQL support:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;JOIN, LEFT JOIN, RIGHT JOIN&lt;/li&gt;
  &lt;li&gt;COUNT DISTINCT&lt;/li&gt;
  &lt;li&gt;CASE WHEN&lt;/li&gt;
  &lt;li&gt;USING&lt;/li&gt;
  &lt;li&gt;BETWEEN&lt;/li&gt;
  &lt;li&gt;IS IN&lt;/li&gt;
  &lt;li&gt;Nested SELECT statements&lt;/li&gt;
  &lt;li&gt;Nested expressions in aggregations&lt;/li&gt;
  &lt;li&gt;LOWER(), UPPER(), TRIM()&lt;/li&gt;
  &lt;li&gt;NULLIF()&lt;/li&gt;
  &lt;li&gt;SHA224(), SHA256(), SHA384(), SHA512()&lt;/li&gt;
  &lt;li&gt;DATE_TRUNC()&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;performance-1&quot;&gt;Performance&lt;/h4&gt;

&lt;p&gt;There have been numerous performance improvements in this release:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Optimizations for JOINs such as using vectorized hashing.&lt;/li&gt;
  &lt;li&gt;We started with adding statistics and cost-based optimizations. We choose the smaller side of a join as the build
side if possible.&lt;/li&gt;
  &lt;li&gt;Improved parallelism when reading partitioned Parquet data sources&lt;/li&gt;
  &lt;li&gt;Concurrent writes of CSV and Parquet partitions to file&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;parquet-crate&quot;&gt;Parquet Crate&lt;/h3&gt;

&lt;p&gt;The Parquet has the following improvements:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Nested reading&lt;/li&gt;
  &lt;li&gt;Support to write booleans&lt;/li&gt;
  &lt;li&gt;Add support to write temporal types&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;roadmap-for-400&quot;&gt;Roadmap for 4.0.0&lt;/h3&gt;

&lt;p&gt;We have also started building up a shared community roadmap for 4.0: &lt;a href=&quot;https://docs.google.com/document/d/1qspsOM_dknOxJKdGvKbC1aoVoO0M3i6x1CIo58mmN2Y/edit#heading=h.kstb571j5g5j&quot;&gt;Apache Arrow: Crowd Sourced Rust Roadmap for
Arrow 4.0, January 2021&lt;/a&gt;.&lt;/p&gt;</content><author><name>pmc</name></author><category term="release" /><summary type="html">The Apache Arrow team is pleased to announce the 3.0.0 release. This covers over 3 months of development work and includes 666 resolved issues from 106 distinct contributors. See the Install Page to learn how to get the libraries for your platform. The release notes below are not exhaustive and only expose selected highlights of the release. Many other bugfixes and improvements have been made: we refer you to the complete changelog. Columnar Format Notes The Decimal256 data type, which was already supported by the Arrow columnar format specification, is now implemented in C++ and Java (ARROW-9747). Arrow Flight RPC notes Authentication in C++/Java/Python has been overhauled, allowing more flexible authentication methods and use of standard headers. Support for cookies has also been added. The C++/Java implementations are now more permissive when parsing messages in order to interoperate better with other Flight implementations. A basic Flight implementation for C#/.NET has been added. See the implementation status matrix for details. C++ notes The default memory pool can now be changed at runtime using the environment variable ARROW_DEFAULT_MEMORY_POOL (ARROW-11009). The environment variable is inspected at process startup. This is useful when trying to diagnose memory consumption issues with Arrow. STL-like iterators are now provided over concrete arrays. Those are useful for non-performance critical tasks, for example testing (ARROW-10776). It is now possible to concatenate dictionary arrays with unequal dictionaries. The dictionaries are unified when concatenating, for supported data types (ARROW-5336). Threads in a thread pool are now spawned lazily as needed for enqueued tasks, up to the configured capacity. They used to be spawned upfront on creation of the thread pool (ARROW-10038). Compute layer Comprehensive documentation for compute functions is now available: https://arrow.apache.org/docs/cpp/compute.html Compute functions for string processing have been added for: splitting on whitespace (ASCII and Unicode flavors) and splitting on a pattern (ARROW-9991); trimming characters (ARROW-9128). Behavior of the index_in and is_in compute functions with nulls has been changed for consistency (ARROW-10663). Multiple-column sort kernels are now available for tables and record batches (ARROW-8199, ARROW-10796, ARROW-10790). Performance of table filtering has been vastly improved (ARROW-10569). Scalar arguments are now accepted for more compute functions. Compute functions quantile (ARROW-10831) and is_nan (ARROW-11043) have been added for numeric data. Aggregation functions any (ARROW-1846) and all (ARROW-10301) have been added for boolean data. Dataset The Expression hierarchy has simplified to a wrapper around literals, field references, or calls to named functions. This enables usage of any compute function while filtering with no boilerplate. Parquet statistics are lazily parsed in ParquetDatasetFactory and ParquetFileFragment for shorter construction time. CSV Conversion of string columns is now faster thanks to faster UTF-8 validation of small strings (ARROW-10313). Conversion of floating-point columns is now faster thanks to optimized string-to-double conversion routines (ARROW-10328). Parsing of ISO8601 timestamps is now more liberal: trailing zeros can be omitted in the fractional part (ARROW-10337). Fixed a bug where null detection could give the wrong results on some platforms (ARROW-11067). Added type inference for Date32 columns for values in the form YYYY-MM-DD (ARROW-11247). Feather Fixed reading of compressed Feather files written with Arrow 0.17 (ARROW-11163). Filesystem layer S3 recursive tree walks now benefit from a parallel implementation, where reads of multiple child directories are now issued concurrently (ARROW-10788). Improved empty directory detection to be mindful of differences between Amazon and Minio S3 implementations (ARROW-10942). Flight RPC IPv6 host addresses are now supported (ARROW-10475). IPC It is now possible to emit dictionary deltas where possible using the IPC stream writer. This is governed by a new variable in the IpcWriteOptions class (ARROW-6883). It is now possible to read wider tables, which used to fail due to reaching a limit during Flatbuffers verification (ARROW-10056). Parquet Fixed reading of LZ4-compressed Parquet columns emitted by the Java Parquet implementation (ARROW-11301). Fixed a bug where writing multiple batches of nullable nested strings to Parquet would not write any data in batches after the first one (ARROW-10493) The Decimal256 data type can be read from and written to Parquet (ARROW-10607). LargeString and LargeBinary data can now be written to Parquet (ARROW-10426). C# notes The .NET package added initial support for Arrow Flight clients and servers. Support is enabled through two new NuGet packages Apache.Arrow.Flight (client) and Apache.Arrow.Flight.AspNetCore (server). Also fixed an issue where ArrowStreamWriter wasn’t writing schema metadata to Arrow streams. Julia notes This is the first release to officially include an implementation for the Julia language. The pure Julia implementation includes support for wide coverage of the format specification. Additional details can be found in the julialang.org blog post. Python notes Support for Python 3.9 was added (ARROW-10224), and support for Python 3.5 was removed (ARROW-5679). Support for building manylinux1 packages has been removed (ARROW-11212). PyArrow continues to be available as manylinux2010 and manylinux2014 wheels. The minimal required version for NumPy is now 1.16.6. Note that when upgrading NumPy to 1.20, you also need to upgrade pyarrow to 3.0.0 to ensure compatibility, as this pyarrow release fixed a compatibility issue with NumPy 1.20 (ARROW-10833). Compute functions are now automatically exported from C++ to the pyarrow.compute module, and they have docstrings matching their C++ definition. An iter_batches() method is now available for reading a Parquet file iteratively (ARROW-7800). Alternate memory pools (such as mimalloc, jemalloc or the C malloc-based memory pool) are now available from Python (ARROW-11049). Fixed a potential deadlock when importing pandas from several threads (ARROW-10519). See the C++ notes above for additional details. R notes This release contains new features for the Flight RPC wrapper, better support for saving R metadata (including sf spatial data) to Feather and Parquet files, several significant improvements to speed and memory management, and many other enhancements. For more on what’s in the 3.0.0 R package, see the R changelog. Ruby and C GLib notes Ruby In Ruby binding, 256-bit decimal support and Arrow::FixedBinaryArrayBuilder are added likewise C GLib below. C GLib In the version 3.0.0 of C GLib consists of many new features. A chunked array, a record batch, and a table support sort_indices function as well as an array. These functions including array’s support to specify sorting option. garrow_array_sort_to_indices has been renamed to garrow_array_sort_indices and the previous name has been deprecated. GArrowField supports functions to handle metadata. GArrowSchema supports garrow_schema_has_metadata() function. GArrowArrayBuilder supports to add single null, multiple nulls, single empty value, and multiple empty values. GArrowFixedSizedBinaryArrayBuilder is newly supported. 256-bit decimal and extension types are newly supported. Filesystem module supports Mock, HDFS, S3 file systems. Dataset module supports CSV, IPC, and Parquet file formats. Rust notes Core Arrow Crate The development of the arrow crate was focused on four main aspects: Make the crate usable in stable Rust Bug fixing and removal of unsafe code Extend functionality to keep up with the specification Increase performance of existing kernels Stable Rust Possibly the biggest news for this release is that all project crates, including arrow, parquet, and datafusion now build with stable Rust by default. Nightly / unstable Rust is still required when enabling the SIMD feature. Parquet Arrow writer The Parquet Writer for Arrow arrays is now available, allowing the Rust programs to easily read and write Parquet files and making it easier to integrate with the overall Arrow ecosystem. The reader and writer include both basic and nested type support (List, Dictionary, Struct) First Class Arrow Flight IPC Support This release the Arrow Flight IPC implementation in Rust became fully-featured enough to participate in the regular cross-language integration tests, thus ensuring Rust applications written using Arrow can interoperate with the rest of the ecosystem Performance There have been numerous performance improvements in this release across the board. This includes both kernel operations, such as take, filter, and cast, as well as more fundamental parts such as bitwise comparison and reading and writing to CSV. Increased Data Type Support New DataTypes: Decimal data type for fixed-width decimal values Improved operation support for nested structures Dictionary, and Lists (filter, take, etc) Other improvements: Added support for Date and time on FFI Added support for Binary type on FFI Added support for i64 sized arrays to “take” kernel Support for the i128 Decimal Type Added support to cast string to date Added API to create arrays out of existing arrays (e.g. for join, merge-sort, concatenate) The simd feature is now also available on aarch64 API Changes BooleanArray is no longer a PrimitiveArray ArrowNativeType no longer includes bool since arrows boolean type is represented using bitpacking Several Buffer methods are now infallible instead of returning a Result DataType::List now contains a Field to track metadata about the contained elements PrimitiveArray::raw_values, values_slice and values methods got replaced by a values method returning a slice Buffer::data and raw_data were renamed to as_slice and as_ptr MutableBuffer::data_mut and freeze were renamed to as_slice_mut and into to be more consistent with the stdlib naming conventions The generic type parameter for BufferBuilder was changed from ArrowPrimitiveType to ArrowNativeType DataFusion SQL In this release, we clarified that DataFusion will standardize on the PostgreSQL SQL dialect. New SQL support: JOIN, LEFT JOIN, RIGHT JOIN COUNT DISTINCT CASE WHEN USING BETWEEN IS IN Nested SELECT statements Nested expressions in aggregations LOWER(), UPPER(), TRIM() NULLIF() SHA224(), SHA256(), SHA384(), SHA512() DATE_TRUNC() Performance There have been numerous performance improvements in this release: Optimizations for JOINs such as using vectorized hashing. We started with adding statistics and cost-based optimizations. We choose the smaller side of a join as the build side if possible. Improved parallelism when reading partitioned Parquet data sources Concurrent writes of CSV and Parquet partitions to file Parquet Crate The Parquet has the following improvements: Nested reading Support to write booleans Add support to write temporal types Roadmap for 4.0.0 We have also started building up a shared community roadmap for 4.0: Apache Arrow: Crowd Sourced Rust Roadmap for Arrow 4.0, January 2021.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Apache Arrow 2.0.0 Rust Highlights</title><link href="https://arrow.apache.org/blog/2020/10/27/rust-2.0.0-release/" rel="alternate" type="text/html" title="Apache Arrow 2.0.0 Rust Highlights" /><published>2020-10-27T02:00:00-04:00</published><updated>2020-10-27T02:00:00-04:00</updated><id>https://arrow.apache.org/blog/2020/10/27/rust-2.0.0-release</id><content type="html" xml:base="https://arrow.apache.org/blog/2020/10/27/rust-2.0.0-release/">&lt;!--

--&gt;

&lt;p&gt;Apache Arrow 2.0.0 is a significant release for the Apache Arrow project in general (&lt;a href=&quot;https://arrow.apache.org/blog/2020/10/22/2.0.0-release/&quot;&gt;release notes&lt;/a&gt;), and the Rust subproject
in particular, with almost 200 issues resolved by 15 contributors. In this blog post, we will go through the main changes 
affecting core Arrow, Parquet support, and DataFusion query engine. The full list of resolved issues can be found 
&lt;a href=&quot;https://issues.apache.org/jira/browse/ARROW-10295?jql=project%20%3D%20ARROW%20AND%20status%20not%20in%20%20(Open%2C%20%22In%20Progress%22%2C%20Reopened)%20and%20fixVersion%20%3D%202.0.0%20AND%20text%20~%20rust%20ORDER%20BY%20created%20DESC&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;While the Java and C/C++ (used by Python and R) Arrow implementations likely remain the most feature-rich, with the 
2.0.0 release, the Rust implementation is closing the feature gap quickly. Here are some of the highlights for this 
release.&lt;/p&gt;

&lt;h1 id=&quot;core-arrow-crate&quot;&gt;Core Arrow Crate&lt;/h1&gt;

&lt;h2 id=&quot;iterator-trait&quot;&gt;Iterator Trait&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Primitive arrays (e.g., array of integers) can now be converted to and initialized from an iterator. This exposes a 
very popular API - iterators - to arrow arrays. Work for other types will continue throughout 3.0.0.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;improved-variable-sized-arrays&quot;&gt;Improved Variable-sized Arrays&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Variable sized arrays (e.g., array of strings) have been internally refactored to more easily support their larger (64-bit 
size offset) version. This allowed us to generalize some of the kernels to both (32 and 64) versions and 
perform type checks when building them.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;kernels&quot;&gt;Kernels&lt;/h2&gt;

&lt;p&gt;There have been numerous improvements in the Arrow compute kernels, including:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;New kernels have been added for string operations, including substring, min, max, concat, and length.&lt;/li&gt;
  &lt;li&gt;Aggregate sum is now implemented for SIMD with a 5x improvement over the non-SIMD operation&lt;/li&gt;
  &lt;li&gt;Many kernels have been improved to support dictionary-encoded arrays&lt;/li&gt;
  &lt;li&gt;Some kernels were optimized for arrays without nulls, making them significantly faster in that case.&lt;/li&gt;
  &lt;li&gt;Many kernels were optimized in the number of memory copies that are needed to apply them and also on their 
implementation.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;other-improvements&quot;&gt;Other Improvements&lt;/h2&gt;

&lt;p&gt;The Array trait now has &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;get_buffer_memory_size&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;get_array_memory_size&lt;/code&gt; methods for determining the amount of 
memory allocated for the array.&lt;/p&gt;

&lt;h1 id=&quot;parquet&quot;&gt;Parquet&lt;/h1&gt;

&lt;p&gt;A significant effort is underway to create a Parquet writer for Arrow data. This work has not been released as part of 
2.0.0, and is planned for the 3.0.0 release. The development of this writer is being carried out on the 
&lt;a href=&quot;https://github.com/apache/arrow/tree/rust-parquet-arrow-writer&quot;&gt;rust-parquet-arrow-writer&lt;/a&gt; branch, and the branch is regularly synchronized with the main branch.
As part of the writer, the necessary improvements and features are being added to the reader.&lt;/p&gt;

&lt;p&gt;The main focus areas are:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Supporting nested Arrow types, such as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;List&amp;lt;Struct&amp;lt;[Dictionary, String]&amp;gt;&amp;gt;&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Ensuring correct round-trip between the reader and writer by encoding Arrow schemas in the Parquet metadata&lt;/li&gt;
  &lt;li&gt;Improve null value writing for Parquet&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;A new &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;parquet_derive&lt;/code&gt; crate has been created, which allows users to derive Parquet records for simple structs. Refer to 
the &lt;a href=&quot;https://github.com/apache/arrow/tree/master/rust/parquet_derive&quot;&gt;parquet_derive crate&lt;/a&gt; for usage examples.&lt;/p&gt;

&lt;h1 id=&quot;datafusion&quot;&gt;DataFusion&lt;/h1&gt;

&lt;p&gt;DataFusion is an in-memory query engine with DataFrame and SQL APIs, built on top of base Arrow support.&lt;/p&gt;

&lt;h2 id=&quot;dataframe-api&quot;&gt;DataFrame API&lt;/h2&gt;

&lt;p&gt;DataFusion now has a richer &lt;a href=&quot;https://docs.rs/datafusion/2.0.0/datafusion/dataframe/trait.DataFrame.html&quot;&gt;DataFrame API&lt;/a&gt; with improved documentation showing example usage, 
supporting the following operations:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;select_columns&lt;/li&gt;
  &lt;li&gt;select&lt;/li&gt;
  &lt;li&gt;filter&lt;/li&gt;
  &lt;li&gt;aggregate&lt;/li&gt;
  &lt;li&gt;limit&lt;/li&gt;
  &lt;li&gt;sort&lt;/li&gt;
  &lt;li&gt;collect&lt;/li&gt;
  &lt;li&gt;explain&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;performance--scalability&quot;&gt;Performance &amp;amp; Scalability&lt;/h2&gt;

&lt;p&gt;DataFusion query execution now uses &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;async&lt;/code&gt;/&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;await&lt;/code&gt; with the tokio threaded runtime rather than launching dedicated 
threads, making queries scale much better across available cores.&lt;/p&gt;

&lt;p&gt;The hash aggregate physical operator has been largely re-written, resulting in significant performance improvements.&lt;/p&gt;

&lt;h2 id=&quot;expressions-and-compute&quot;&gt;Expressions and Compute&lt;/h2&gt;

&lt;h3 id=&quot;improved-scalar-functions&quot;&gt;Improved Scalar Functions&lt;/h3&gt;

&lt;p&gt;DataFusion has many new functions, both in the SQL and the DataFrame API:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Length of an string&lt;/li&gt;
  &lt;li&gt;COUNT(DISTINCT column)&lt;/li&gt;
  &lt;li&gt;to_timestamp&lt;/li&gt;
  &lt;li&gt;IsNull and IsNotNull&lt;/li&gt;
  &lt;li&gt;Min/Max for strings (lexicographic order)&lt;/li&gt;
  &lt;li&gt;Array of columns&lt;/li&gt;
  &lt;li&gt;Concatenation of strings&lt;/li&gt;
  &lt;li&gt;Aliases of aggregate expressions&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Many existing expressions were also significantly optimized (2-3x speedups) by avoiding memory copies and leveraging 
Arrow format’s invariants.&lt;/p&gt;

&lt;p&gt;Unary mathematical functions (such as sqrt) now support both 32 and 64-bit floats and return the corresponding type, 
thereby allowing faster operations when higher precision is not needed.&lt;/p&gt;

&lt;h3 id=&quot;improved-user-defined-functions-udfs&quot;&gt;Improved User-defined Functions (UDFs)&lt;/h3&gt;
&lt;p&gt;The API to use and register UDFs has been significantly improved, allowing users to register UDFs and call them both 
via SQL and the DataFrame API. UDFs now also have the same generality as DataFusion’s functions, including variadic 
and dynamically typed arguments.&lt;/p&gt;

&lt;h3 id=&quot;user-defined-aggregate-functions-udafs&quot;&gt;User-defined Aggregate Functions (UDAFs)&lt;/h3&gt;
&lt;p&gt;DataFusion now supports user-defined aggregate functions that can be used to perform operations than span multiple 
rows, batches, and partitions. UDAFs have the same generality as DataFusion’s functions and support both row updates 
and batch updates. You can check out &lt;a href=&quot;https://github.com/apache/arrow/blob/master/rust/datafusion/examples/simple_udaf.rs&quot;&gt;this example&lt;/a&gt; to learn how to declare and use a UDAF.&lt;/p&gt;

&lt;h3 id=&quot;user-defined-constants&quot;&gt;User-defined Constants&lt;/h3&gt;
&lt;p&gt;DataFusion now supports registering constants (e.g. “@version”) that live for the duration of the execution context 
and can be accessed from SQL.&lt;/p&gt;

&lt;h2 id=&quot;query-planning--optimization&quot;&gt;Query Planning &amp;amp; Optimization&lt;/h2&gt;

&lt;h3 id=&quot;user-defined-logical-plans&quot;&gt;User-defined logical plans&lt;/h3&gt;

&lt;p&gt;The Logical Plan enum is now extensible through an Extension variant which accepts a UserDefinedLogicalPlan trait using 
dynamic dispatch. Consequently, DataFusion now supports user-defined logical nodes, thereby allowing complex nodes to 
be planned and executed. You can check this example to learn how to declare a new node.&lt;/p&gt;

&lt;h3 id=&quot;predicate-push-down&quot;&gt;Predicate push-down&lt;/h3&gt;

&lt;p&gt;DataFusion now has a Predicate push-down optimizer rule that pushes filter operations as close as possible to scans, 
thereby speeding up the physical execution of suboptimal queries created via the DataFrame API.&lt;/p&gt;

&lt;h3 id=&quot;sql&quot;&gt;SQL&lt;/h3&gt;

&lt;p&gt;DataFusion now uses a more recent release of the sqlparser crate, which has much more comprehensive support for SQL 
syntax and also supports multiple dialects (Postgres, MS SQL, and MySQL).&lt;/p&gt;

&lt;p&gt;It is now possible to see the query plan for a SQL statement using EXPLAIN syntax.&lt;/p&gt;

&lt;h1 id=&quot;benchmarks&quot;&gt;Benchmarks&lt;/h1&gt;

&lt;p&gt;The benchmark crate now contains a new benchmark based on TPC-H that can execute TPC-H query 1 against CSV, Parquet, 
and memory data sources. This is useful for running benchmarks against larger data sets.&lt;/p&gt;

&lt;h1 id=&quot;integration-testing--ipc&quot;&gt;Integration Testing / IPC&lt;/h1&gt;

&lt;p&gt;Arrow IPC is the format for serialization and interprocess communication. It is described in &lt;a href=&quot;https://arrow.apache.org/&quot;&gt;arrow.apache.org&lt;/a&gt; and is 
the format used for file and stream I/O between applications wishing to interchange Arrow data.&lt;/p&gt;

&lt;p&gt;The Arrow project released IPC version 5 of the Arrow IPC format in version 1.0.0. Before that, a message padding 
change was made in version 0.15.0 to change the default padding to 8 bytes, while remaining in IPC version 4. Arrow 
release 0.14.1 and earlier were the last releases to use the legacy 4 byte alignment.
As part of 2.0.0, the Rust implementation was updated to comply with the changes up to release 0.15.0 of Arrow. 
Work on supporting IPC version 5 is underway, and is expected to be completed in time for 3.0.0.&lt;/p&gt;

&lt;p&gt;As part of the conformance work, Rust is being added to the Arrow integration suite, which tests that supported 
language implementations &lt;a href=&quot;https://issues.apache.org/jira/browse/ARROW-3690&quot;&gt;(ARROW-3690)&lt;/a&gt;:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Comply with the Arrow IPC format&lt;/li&gt;
  &lt;li&gt;Can read and write each other’s generated data&lt;/li&gt;
  &lt;li&gt;IPC version 4 is being verified through the above work.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;roadmap-for-300-and-beyond&quot;&gt;Roadmap for 3.0.0 and Beyond&lt;/h1&gt;

&lt;p&gt;Here are some of the initiatives that contributors are currently working on for future releases:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Support stable Rust&lt;/li&gt;
  &lt;li&gt;Improved DictionaryArray support and performance&lt;/li&gt;
  &lt;li&gt;Implement inner equijoins&lt;/li&gt;
  &lt;li&gt;Support for various platforms like ARMv8&lt;/li&gt;
  &lt;li&gt;Supporting the C Data Interface from Rust to better support interoperability with other Arrow implementations&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;how-to-get-involved&quot;&gt;How to Get Involved&lt;/h1&gt;

&lt;p&gt;If you are interested in contributing to the Rust subproject in Apache Arrow, you can find a list of open issues 
suitable for beginners &lt;a href=&quot;https://issues.apache.org/jira/issues/?jql=project%20%3D%20ARROW%20AND%20resolution%20%3D%20Unresolved%20AND%20component%20in%20(Rust%2C%20%22Rust%20-%20DataFusion%22)%20AND%20labels%20%3D%20beginner&quot;&gt;here&lt;/a&gt; and the full list &lt;a href=&quot;https://issues.apache.org/jira/issues/?jql=project%20%3D%20ARROW%20AND%20resolution%20%3D%20Unresolved%20AND%20component%20in%20(Rust%2C%20%22Rust%20-%20DataFusion%22)%20ORDER%20BY%20updated%20DESC%2C%20created%20DESC%2C%20priority%20DESC&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Other ways to get involved include trying out Arrow on some of your data and filing bug reports, and helping to 
improve the documentation.&lt;/p&gt;</content><author><name>pmc</name></author><category term="release" /><summary type="html">Apache Arrow 2.0.0 is a significant release for the Apache Arrow project in general (release notes), and the Rust subproject in particular, with almost 200 issues resolved by 15 contributors. In this blog post, we will go through the main changes affecting core Arrow, Parquet support, and DataFusion query engine. The full list of resolved issues can be found here. While the Java and C/C++ (used by Python and R) Arrow implementations likely remain the most feature-rich, with the 2.0.0 release, the Rust implementation is closing the feature gap quickly. Here are some of the highlights for this release. Core Arrow Crate Iterator Trait Primitive arrays (e.g., array of integers) can now be converted to and initialized from an iterator. This exposes a very popular API - iterators - to arrow arrays. Work for other types will continue throughout 3.0.0. Improved Variable-sized Arrays Variable sized arrays (e.g., array of strings) have been internally refactored to more easily support their larger (64-bit size offset) version. This allowed us to generalize some of the kernels to both (32 and 64) versions and perform type checks when building them. Kernels There have been numerous improvements in the Arrow compute kernels, including: New kernels have been added for string operations, including substring, min, max, concat, and length. Aggregate sum is now implemented for SIMD with a 5x improvement over the non-SIMD operation Many kernels have been improved to support dictionary-encoded arrays Some kernels were optimized for arrays without nulls, making them significantly faster in that case. Many kernels were optimized in the number of memory copies that are needed to apply them and also on their implementation. Other Improvements The Array trait now has get_buffer_memory_size and get_array_memory_size methods for determining the amount of memory allocated for the array. Parquet A significant effort is underway to create a Parquet writer for Arrow data. This work has not been released as part of 2.0.0, and is planned for the 3.0.0 release. The development of this writer is being carried out on the rust-parquet-arrow-writer branch, and the branch is regularly synchronized with the main branch. As part of the writer, the necessary improvements and features are being added to the reader. The main focus areas are: Supporting nested Arrow types, such as List&amp;lt;Struct&amp;lt;[Dictionary, String]&amp;gt;&amp;gt; Ensuring correct round-trip between the reader and writer by encoding Arrow schemas in the Parquet metadata Improve null value writing for Parquet A new parquet_derive crate has been created, which allows users to derive Parquet records for simple structs. Refer to the parquet_derive crate for usage examples. DataFusion DataFusion is an in-memory query engine with DataFrame and SQL APIs, built on top of base Arrow support. DataFrame API DataFusion now has a richer DataFrame API with improved documentation showing example usage, supporting the following operations: select_columns select filter aggregate limit sort collect explain Performance &amp;amp; Scalability DataFusion query execution now uses async/await with the tokio threaded runtime rather than launching dedicated threads, making queries scale much better across available cores. The hash aggregate physical operator has been largely re-written, resulting in significant performance improvements. Expressions and Compute Improved Scalar Functions DataFusion has many new functions, both in the SQL and the DataFrame API: Length of an string COUNT(DISTINCT column) to_timestamp IsNull and IsNotNull Min/Max for strings (lexicographic order) Array of columns Concatenation of strings Aliases of aggregate expressions Many existing expressions were also significantly optimized (2-3x speedups) by avoiding memory copies and leveraging Arrow format’s invariants. Unary mathematical functions (such as sqrt) now support both 32 and 64-bit floats and return the corresponding type, thereby allowing faster operations when higher precision is not needed. Improved User-defined Functions (UDFs) The API to use and register UDFs has been significantly improved, allowing users to register UDFs and call them both via SQL and the DataFrame API. UDFs now also have the same generality as DataFusion’s functions, including variadic and dynamically typed arguments. User-defined Aggregate Functions (UDAFs) DataFusion now supports user-defined aggregate functions that can be used to perform operations than span multiple rows, batches, and partitions. UDAFs have the same generality as DataFusion’s functions and support both row updates and batch updates. You can check out this example to learn how to declare and use a UDAF. User-defined Constants DataFusion now supports registering constants (e.g. “@version”) that live for the duration of the execution context and can be accessed from SQL. Query Planning &amp;amp; Optimization User-defined logical plans The Logical Plan enum is now extensible through an Extension variant which accepts a UserDefinedLogicalPlan trait using dynamic dispatch. Consequently, DataFusion now supports user-defined logical nodes, thereby allowing complex nodes to be planned and executed. You can check this example to learn how to declare a new node. Predicate push-down DataFusion now has a Predicate push-down optimizer rule that pushes filter operations as close as possible to scans, thereby speeding up the physical execution of suboptimal queries created via the DataFrame API. SQL DataFusion now uses a more recent release of the sqlparser crate, which has much more comprehensive support for SQL syntax and also supports multiple dialects (Postgres, MS SQL, and MySQL). It is now possible to see the query plan for a SQL statement using EXPLAIN syntax. Benchmarks The benchmark crate now contains a new benchmark based on TPC-H that can execute TPC-H query 1 against CSV, Parquet, and memory data sources. This is useful for running benchmarks against larger data sets. Integration Testing / IPC Arrow IPC is the format for serialization and interprocess communication. It is described in arrow.apache.org and is the format used for file and stream I/O between applications wishing to interchange Arrow data. The Arrow project released IPC version 5 of the Arrow IPC format in version 1.0.0. Before that, a message padding change was made in version 0.15.0 to change the default padding to 8 bytes, while remaining in IPC version 4. Arrow release 0.14.1 and earlier were the last releases to use the legacy 4 byte alignment. As part of 2.0.0, the Rust implementation was updated to comply with the changes up to release 0.15.0 of Arrow. Work on supporting IPC version 5 is underway, and is expected to be completed in time for 3.0.0. As part of the conformance work, Rust is being added to the Arrow integration suite, which tests that supported language implementations (ARROW-3690): Comply with the Arrow IPC format Can read and write each other’s generated data IPC version 4 is being verified through the above work. Roadmap for 3.0.0 and Beyond Here are some of the initiatives that contributors are currently working on for future releases: Support stable Rust Improved DictionaryArray support and performance Implement inner equijoins Support for various platforms like ARMv8 Supporting the C Data Interface from Rust to better support interoperability with other Arrow implementations How to Get Involved If you are interested in contributing to the Rust subproject in Apache Arrow, you can find a list of open issues suitable for beginners here and the full list here. Other ways to get involved include trying out Arrow on some of your data and filing bug reports, and helping to improve the documentation.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Apache Arrow 2.0.0 Release</title><link href="https://arrow.apache.org/blog/2020/10/22/2.0.0-release/" rel="alternate" type="text/html" title="Apache Arrow 2.0.0 Release" /><published>2020-10-22T02:00:00-04:00</published><updated>2020-10-22T02:00:00-04:00</updated><id>https://arrow.apache.org/blog/2020/10/22/2.0.0-release</id><content type="html" xml:base="https://arrow.apache.org/blog/2020/10/22/2.0.0-release/">&lt;!--

--&gt;

&lt;p&gt;The Apache Arrow team is pleased to announce the 2.0.0 release. This covers
over 3 months of development work and includes &lt;a href=&quot;https://issues.apache.org/jira/issues/?jql=project%20%3D%20ARROW%20AND%20status%20%3D%20Resolved%20AND%20fixVersion%20%3D%202.0.0&quot;&gt;&lt;strong&gt;511 resolved issues&lt;/strong&gt;&lt;/a&gt;
from &lt;a href=&quot;/release/2.0.0.html#contributors&quot;&gt;&lt;strong&gt;81 distinct contributors&lt;/strong&gt;&lt;/a&gt;. See the Install Page to learn how to
get the libraries for your platform.&lt;/p&gt;

&lt;p&gt;The release notes below are not exhaustive and only expose selected highlights
of the release. Many other bugfixes and improvements have been made: we refer
you to the &lt;a href=&quot;/release/2.0.0.html&quot;&gt;complete changelog&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;community&quot;&gt;Community&lt;/h2&gt;

&lt;p&gt;Since the 1.0.0 release, &lt;a href=&quot;https://github.com/jorgecarleitao&quot;&gt;Jorge Leitão&lt;/a&gt; has been added as a committer. Thank
you for your contributions!&lt;/p&gt;

&lt;h2 id=&quot;columnar-format&quot;&gt;Columnar Format&lt;/h2&gt;

&lt;p&gt;As this is the first major release since 1.0.0, we remind everyone that we have
moved to a “split” versioning system where the Library version (which is now
2.0.0) will now evolve separate from the Format version (which is still
1.0.0). Major releases of the libraries may contain non-backward-compatible API
changes, but they will not contain any incompatible format changes. See the
&lt;a href=&quot;http://arrow.apache.org/docs/format/Versioning.html&quot;&gt;Versioning and Stability&lt;/a&gt; page in the documentation for more.&lt;/p&gt;

&lt;p&gt;The columnar format metadata has been updated to permit 256-bit decimal values
in addition to 128-bit decimals. This change is backward and forward
compatible.&lt;/p&gt;

&lt;h2 id=&quot;arrow-flight-rpc-notes&quot;&gt;Arrow Flight RPC notes&lt;/h2&gt;

&lt;p&gt;For Arrow Flight, 2.0.0 mostly brings bugfixes. In Java, some memory leaks in
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;FlightStream&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;DoPut&lt;/code&gt; have been addressed. In C++ and Python, a deadlock
has been fixed in an edge case. Additionally, when supported by gRPC, TLS
verification can be disabled.&lt;/p&gt;

&lt;h2 id=&quot;c-notes&quot;&gt;C++ notes&lt;/h2&gt;

&lt;p&gt;Parquet reading now fully supports round trip of arbitrarily nested data,
including extension types with a nested storage type. In the process, several
bugs in writing nested data and FixedSizeList were fixed.  If writing data with
these type we recommend upgrading to this release and validating old data as
there is potential data loss.&lt;/p&gt;

&lt;p&gt;Datasets can now be written with partitions, including these features:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Writing to Parquet, including control over accumulation of statistics for
individual columns.&lt;/li&gt;
  &lt;li&gt;Writing to IPC/Feather, including body buffer compression.&lt;/li&gt;
  &lt;li&gt;Basenames of written files can be specified with a string template, allowing
non-colliding writes into the same partitioned dataset.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Other notable features in the release include&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Compute kernels for standard deviation, variance, and mode&lt;/li&gt;
  &lt;li&gt;Improvements to S3 support, including automatic region detection&lt;/li&gt;
  &lt;li&gt;CSV reading now parses Date type and creating Dictionary types&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;c-notes-1&quot;&gt;C# notes&lt;/h2&gt;

&lt;p&gt;The .NET package has added a number of new features this release.&lt;/p&gt;

&lt;p&gt;Full support for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Struct&lt;/code&gt; types.&lt;/p&gt;

&lt;p&gt;Synchronous write APIs for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ArrowStreamWriter&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ArrowFileWriter&lt;/code&gt;. These are
complimentary to the existing async write APIs, and can be used in situations
where the async APIs can’t be used.&lt;/p&gt;

&lt;p&gt;The ability to use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;DateTime&lt;/code&gt; instances with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Date32Array&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Date64Array&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id=&quot;java-notes&quot;&gt;Java notes&lt;/h2&gt;

&lt;p&gt;The Java package has supported a number of new features.  Users can validate
vectors in a wider range of aspects, if they are willing to take more time.  In
dictionary encoding, dictionary indices can be expressed as unsigned integers.
A framework for data compression has been setup for IPC.&lt;/p&gt;

&lt;p&gt;The calculation for vector capacity has been simplified, so users should
experience notable performance improvements for various ‘setSafe’ methods.&lt;/p&gt;

&lt;p&gt;Bugs for JDBC adapters, sort algorithms, and ComplexCopier have been resolved
to make them more usable.&lt;/p&gt;

&lt;h2 id=&quot;javascript-notes&quot;&gt;JavaScript notes&lt;/h2&gt;

&lt;p&gt;Upgrades Arrow’s build to use TypeScript 3.9, fixing generated &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;.d.ts&lt;/code&gt; typings.&lt;/p&gt;

&lt;h2 id=&quot;python-notes&quot;&gt;Python notes&lt;/h2&gt;

&lt;p&gt;Parquet reading now supports round trip of arbitrarily nested data. Several bug
fixes for writing nested data and FixedSizeList.  If writing data with these
type we recommend validating old data (there is potential some data loss) and
upgrade to 2.0.&lt;/p&gt;

&lt;p&gt;Extension types with a nested storage type now round trip through Parquet.&lt;/p&gt;

&lt;p&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pyarrow.filesystem&lt;/code&gt; submodule is deprecated in favor of new filesystem
implementations in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pyarrow.fs&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;The custom serialization functionality (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pyarrow.serialize()&lt;/code&gt;,
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pyarrow.deserialize()&lt;/code&gt;, etc) is deprecated. Those functions provided a
Python-specific (not cross-language) serialization format which were not
compatible with the standardized Arrow (IPC) serialization format.  For
arbitrary objects, you can use the standard library &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pickle&lt;/code&gt; functionality
instead. For pyarrow objects, you can use the IPC serialization format through
the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pyarrow.ipc&lt;/code&gt; module, as explained above.&lt;/p&gt;

&lt;p&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pyarrow.compute&lt;/code&gt; module now has a complete coverage of the available C++
compute kernels in the python API. Several new kernels have been added.&lt;/p&gt;

&lt;p&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pyarrow.dataset&lt;/code&gt; module was further improved. In addition to reading, it
is now also possible to write partitioned datasets (with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;write_dataset()&lt;/code&gt;).&lt;/p&gt;

&lt;p&gt;The Arrow &amp;lt;-&amp;gt; Python conversion code was refactored, fixing several bugs and
corner cases.&lt;/p&gt;

&lt;p&gt;Conversion of an array of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pyarrow.MapType&lt;/code&gt; to Pandas has been added.&lt;/p&gt;

&lt;p&gt;Conversion of timezone aware datetimes to and/from pyarrow arrays including
pandas now round-trip preserving timezone. To use the old behavior (e.g. for
spark) set the environment variable PYARROW_IGNORE_TIMEZONE to a truthy value
(i.e.  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;PYARROW_IGNORE_TIMEZONE=1&lt;/code&gt;)&lt;/p&gt;

&lt;h2 id=&quot;r-notes&quot;&gt;R notes&lt;/h2&gt;

&lt;p&gt;Highlights of the R release include&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Writing multi-file datasets with partitioning to Parquet or Feather&lt;/li&gt;
  &lt;li&gt;Reading and writing directly to AWS S3, both individual files and multi-file
datasets&lt;/li&gt;
  &lt;li&gt;Bindings for Flight which use reticulate&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In addition, the R package benefits from the various improvements in the C++
library listed above, including the ability to read and write Parquet files
with nested struct and list types.&lt;/p&gt;

&lt;p&gt;For more on what’s in the 2.0.0 R package, see the &lt;a href=&quot;/docs/r/news/&quot;&gt;R changelog&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;ruby-and-c-glib-notes&quot;&gt;Ruby and C GLib notes&lt;/h2&gt;

&lt;h3 id=&quot;ruby&quot;&gt;Ruby&lt;/h3&gt;

&lt;p&gt;In Ruby binding, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Arrow::Table#save&lt;/code&gt; uses the number of rows as the
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;chunk_size&lt;/code&gt; parameter by default when the table is saved in a Parquet file.&lt;/p&gt;

&lt;h3 id=&quot;c-glib&quot;&gt;C GLib&lt;/h3&gt;

&lt;p&gt;The GLib binding newly supports &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;GArrowStringDictionaryArrayBuilder&lt;/code&gt; and
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;GArrowBinaryDictionaryArrayBuilder&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Moreover the GLib binding supports new accessors of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;GArrowListArray&lt;/code&gt; and
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;GArrowLargeListArray&lt;/code&gt;.  They are &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;get_values&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;get_value_offset&lt;/code&gt;,
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;get_value_length&lt;/code&gt;, and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;get_value_offsets&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id=&quot;rust-notes&quot;&gt;Rust notes&lt;/h2&gt;

&lt;p&gt;Due to the high volume of activity in the Rust subproject in this release,
we’re writing a separate blog post dedicated to those changes.&lt;/p&gt;</content><author><name>pmc</name></author><category term="release" /><summary type="html">The Apache Arrow team is pleased to announce the 2.0.0 release. This covers over 3 months of development work and includes 511 resolved issues from 81 distinct contributors. See the Install Page to learn how to get the libraries for your platform. The release notes below are not exhaustive and only expose selected highlights of the release. Many other bugfixes and improvements have been made: we refer you to the complete changelog. Community Since the 1.0.0 release, Jorge Leitão has been added as a committer. Thank you for your contributions! Columnar Format As this is the first major release since 1.0.0, we remind everyone that we have moved to a “split” versioning system where the Library version (which is now 2.0.0) will now evolve separate from the Format version (which is still 1.0.0). Major releases of the libraries may contain non-backward-compatible API changes, but they will not contain any incompatible format changes. See the Versioning and Stability page in the documentation for more. The columnar format metadata has been updated to permit 256-bit decimal values in addition to 128-bit decimals. This change is backward and forward compatible. Arrow Flight RPC notes For Arrow Flight, 2.0.0 mostly brings bugfixes. In Java, some memory leaks in FlightStream and DoPut have been addressed. In C++ and Python, a deadlock has been fixed in an edge case. Additionally, when supported by gRPC, TLS verification can be disabled. C++ notes Parquet reading now fully supports round trip of arbitrarily nested data, including extension types with a nested storage type. In the process, several bugs in writing nested data and FixedSizeList were fixed. If writing data with these type we recommend upgrading to this release and validating old data as there is potential data loss. Datasets can now be written with partitions, including these features: Writing to Parquet, including control over accumulation of statistics for individual columns. Writing to IPC/Feather, including body buffer compression. Basenames of written files can be specified with a string template, allowing non-colliding writes into the same partitioned dataset. Other notable features in the release include Compute kernels for standard deviation, variance, and mode Improvements to S3 support, including automatic region detection CSV reading now parses Date type and creating Dictionary types C# notes The .NET package has added a number of new features this release. Full support for Struct types. Synchronous write APIs for ArrowStreamWriter and ArrowFileWriter. These are complimentary to the existing async write APIs, and can be used in situations where the async APIs can’t be used. The ability to use DateTime instances with Date32Array and Date64Array. Java notes The Java package has supported a number of new features. Users can validate vectors in a wider range of aspects, if they are willing to take more time. In dictionary encoding, dictionary indices can be expressed as unsigned integers. A framework for data compression has been setup for IPC. The calculation for vector capacity has been simplified, so users should experience notable performance improvements for various ‘setSafe’ methods. Bugs for JDBC adapters, sort algorithms, and ComplexCopier have been resolved to make them more usable. JavaScript notes Upgrades Arrow’s build to use TypeScript 3.9, fixing generated .d.ts typings. Python notes Parquet reading now supports round trip of arbitrarily nested data. Several bug fixes for writing nested data and FixedSizeList. If writing data with these type we recommend validating old data (there is potential some data loss) and upgrade to 2.0. Extension types with a nested storage type now round trip through Parquet. The pyarrow.filesystem submodule is deprecated in favor of new filesystem implementations in pyarrow.fs. The custom serialization functionality (pyarrow.serialize(), pyarrow.deserialize(), etc) is deprecated. Those functions provided a Python-specific (not cross-language) serialization format which were not compatible with the standardized Arrow (IPC) serialization format. For arbitrary objects, you can use the standard library pickle functionality instead. For pyarrow objects, you can use the IPC serialization format through the pyarrow.ipc module, as explained above. The pyarrow.compute module now has a complete coverage of the available C++ compute kernels in the python API. Several new kernels have been added. The pyarrow.dataset module was further improved. In addition to reading, it is now also possible to write partitioned datasets (with write_dataset()). The Arrow &amp;lt;-&amp;gt; Python conversion code was refactored, fixing several bugs and corner cases. Conversion of an array of pyarrow.MapType to Pandas has been added. Conversion of timezone aware datetimes to and/from pyarrow arrays including pandas now round-trip preserving timezone. To use the old behavior (e.g. for spark) set the environment variable PYARROW_IGNORE_TIMEZONE to a truthy value (i.e. PYARROW_IGNORE_TIMEZONE=1) R notes Highlights of the R release include Writing multi-file datasets with partitioning to Parquet or Feather Reading and writing directly to AWS S3, both individual files and multi-file datasets Bindings for Flight which use reticulate In addition, the R package benefits from the various improvements in the C++ library listed above, including the ability to read and write Parquet files with nested struct and list types. For more on what’s in the 2.0.0 R package, see the R changelog. Ruby and C GLib notes Ruby In Ruby binding, Arrow::Table#save uses the number of rows as the chunk_size parameter by default when the table is saved in a Parquet file. C GLib The GLib binding newly supports GArrowStringDictionaryArrayBuilder and GArrowBinaryDictionaryArrayBuilder. Moreover the GLib binding supports new accessors of GArrowListArray and GArrowLargeListArray. They are get_values, get_value_offset, get_value_length, and get_value_offsets. Rust notes Due to the high volume of activity in the Rust subproject in this release, we’re writing a separate blog post dedicated to those changes.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Making Arrow C++ Builds Simpler, Smaller, and Faster</title><link href="https://arrow.apache.org/blog/2020/07/29/cpp-build-simplification/" rel="alternate" type="text/html" title="Making Arrow C++ Builds Simpler, Smaller, and Faster" /><published>2020-07-29T02:00:00-04:00</published><updated>2020-07-29T02:00:00-04:00</updated><id>https://arrow.apache.org/blog/2020/07/29/cpp-build-simplification</id><content type="html" xml:base="https://arrow.apache.org/blog/2020/07/29/cpp-build-simplification/">&lt;!--

--&gt;

&lt;p&gt;Over the last four and a half years, we’ve worked to build a
“batteries-included” development platform for high-performance analytics
applications in C++. As the scope of the project has grown, we have sometimes
taken on additional library dependencies to support a wide variety of systems
and data processing tasks.&lt;/p&gt;

&lt;p&gt;While these dependencies give us leverage on hard problems, in some cases they
have added complexity for projects that depend on Arrow. Some projects have thus
been concerned about depending on the Arrow C++ library, particularly if their
use of the Arrow library’s features is limited. Indeed, in the earlier stages of
the Arrow project development, dependency management issues did cause problems
for early adopters.&lt;/p&gt;

&lt;p&gt;We want developers to trust that they can use and depend on our libraries, and
that doing so doesn’t add a burden for their own project maintenance or for
their users. Over the last year, we have undertaken a number of significant
projects to accommodate the different ways that people want to depend on Arrow
C++. We’ve aimed to make the build process simple by default, without requiring
special environment setup, yet also highly configurable for those who need to
specialize. This includes a &lt;em&gt;zero-dependency option&lt;/em&gt; for projects that wish to use
the Arrow C++ core but take on no transitive dependencies. We’ve also worked to
make builds faster and more compact, even as we continue to add new
functionality.&lt;/p&gt;

&lt;p&gt;This post covers many of the efforts we’ve made, both in the C++ libraries and
in the Arrow Python and R packages that depend on them. Compared to a year ago,
the build experience is much more reliable on a wider range of platforms,
requires fewer dependencies, and yields smaller package sizes.&lt;/p&gt;

&lt;h2 id=&quot;minimal-default-build-options&quot;&gt;Minimal default build options&lt;/h2&gt;

&lt;p&gt;One rough edge for people using Arrow as a dependency was that many optional
project components were enabled in the build by default, thus requiring any
extra dependencies of those optional components. Rather than expecting users to
disable optional components one by one, we have made the default for all
optional components to be &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;OFF&lt;/code&gt; so that the default configuration is a
dependency-free minimal core build.&lt;/p&gt;

&lt;p&gt;The only third-party library enabled by default is
&lt;a href=&quot;http://jemalloc.net/&quot;&gt;jemalloc&lt;/a&gt;, the project’s recommended memory allocator
(except on Windows, where it is also disabled). Given that Arrow applications
often process large volumes of data, we have found additionally that using
memory allocators provided by projects like jemalloc and
&lt;a href=&quot;https://microsoft.github.io/mimalloc/&quot;&gt;mimalloc&lt;/a&gt; yield significantly better
performance over the default system allocators. Even so, this can also be
disabled if desired.&lt;/p&gt;

&lt;p&gt;To demonstrate a minimal build, we have provided a
&lt;a href=&quot;https://github.com/apache/arrow/blob/master/cpp/examples/minimal_build/Dockerfile&quot;&gt;Dockerfile&lt;/a&gt;
which can be used to build the project requiring only CMake and a C++ compiler
with zero dependencies. Additionally, we have included an
&lt;a href=&quot;https://github.com/apache/arrow/tree/master/cpp/examples/minimal_build&quot;&gt;example&lt;/a&gt;
of including Arrow as an external project dependency in another CMake project.&lt;/p&gt;

&lt;h2 id=&quot;flexible-dependency-configuration-in-cmake&quot;&gt;Flexible dependency configuration in CMake&lt;/h2&gt;

&lt;p&gt;As part of improving our CMake-based build system, we have made the
configuration of build dependencies both flexible and consistent for different
users’ needs. In some cases, developers want Arrow to build against dependencies
provided by an external package manager, such as apt in Debian-based Linux
distributions. In other cases, developers may want to avoid any quirks of system
libraries and build all dependencies together with the Arrow build.&lt;/p&gt;

&lt;p&gt;For each package, the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;${Library}_SOURCE&lt;/code&gt; CMake option can be set to one of three
values:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;SYSTEM&lt;/code&gt;, when the dependency is to be provided externally (such as by a Linux distribution or Homebrew)&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;BUNDLED&lt;/code&gt;, when you want the dependency to be built from source while building Arrow, and then statically-linked with the resulting libraries&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;AUTO&lt;/code&gt;, which tries the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;SYSTEM&lt;/code&gt; approach but falls back on &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;BUNDLED&lt;/code&gt; if the dependency cannot be located&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We additionally have provided &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;CONDA&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;BREW&lt;/code&gt; source types for the common
scenarios when developers are using the conda or Homebrew package managers.
These dependency sources can be configured on an individual dependency basis or
globally using the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ARROW_DEPENDENCY_SOURCE&lt;/code&gt; CMake option. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;AUTO&lt;/code&gt; is default,
which enables builds to be faster by using pre-built system libraries where
possible but still succeed even if all dependencies are not available on the
system.&lt;/p&gt;

&lt;h2 id=&quot;reduced-external-dependencies&quot;&gt;Reduced external dependencies&lt;/h2&gt;

&lt;p&gt;Another area of focus was to audit our dependencies. We went through and found
places where we could drop external dependencies without losing useful
functionality and without having to rewrite a lot or copy too much code into our
codebase.&lt;/p&gt;

&lt;p&gt;We have eliminated Boost as a dependency of the core Arrow library, and in other
components (Gandiva, Parquet, etc.), the use of Boost has been greatly reduced.
In addition, when building Boost “bundled” in the Arrow build, we stripped down
the Boost package we download to the minimum needed, cutting out 90 percent of
the download size.&lt;/p&gt;

&lt;p&gt;We vendored a few small dependencies, such as the double-conversion and
uriparser libraries, so that they do not need to be downloaded and built
separately.&lt;/p&gt;

&lt;p&gt;We also compiled the Flatbuffers and Thrift definitions (which are needed to
implement the Arrow and Parquet formats, respectively) and checked in the
resulting C++ code to the Arrow repository. This means that Flatbuffers is no
longer a build or runtime dependency of Arrow, and we only need the Thrift C++
library, not the Thrift compiler, which has additional dependencies on flex and
bison.&lt;/p&gt;

&lt;h2 id=&quot;c-library-size-reductions&quot;&gt;C++ library size reductions&lt;/h2&gt;

&lt;p&gt;As the C++ codebase grows in size, so too does compilation times and the amount
of binary code generated by the C++ compiler. Over the last several months, we
have begun analyzing the Arrow libraries both compile times and generated code
sizes. This has yielded both significant size reductions (more than 30 percent
code size reduction since 0.17.0). We have also restructured header files to
avoid including unneeded header files, thus lightening the load on C++ compilers
and improving compilation times.&lt;/p&gt;

&lt;h2 id=&quot;python-wheels&quot;&gt;Python wheels&lt;/h2&gt;

&lt;p&gt;The expectation for binary wheel packages on the Python Package Index (PyPI) is
that they are self-contained and have no external dependencies except on other
Python packages. Additionally, each user of pyarrow may need different things
from the project. Some users just want to read Parquet files and convert them to
pandas data frames while others want to use
&lt;a href=&quot;/blog/2019/10/13/introducing-arrow-flight/&quot;&gt;Flight&lt;/a&gt; for
moving around large datasets. Thus, the “pyarrow” wheel has from the beginning
of the project been a fairly comprehensive build including as many optional
components as is practical for us to maintain.&lt;/p&gt;

&lt;p&gt;A comprehensive wheel package has some downsides: most notably for users, it is large.
Additionally, through a snafu relating to C++ shared libraries, for several
releases the wheel packages would create two copies of each C++ library on disk,
resulting in double the amount of disk usage. This has caused problems for
people using pyarrow in space-constrained environments like AWS Lambda.&lt;/p&gt;

&lt;p&gt;In the 1.0.0 release, we have implemented some changes that have reduced the
size of the wheels (both in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;.whl&lt;/code&gt; form and installed on disk) by about 75 percent:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Working around the problems resulting in two copies of each shared library being created in the site-packages directory.&lt;/li&gt;
  &lt;li&gt;Disabling Gandiva, which required the LLVM runtime, the largest statically-linked dependency. Gandiva is still available to conda users now–it’s just not included in the wheels–and we hope to package it as a separate &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pyarrow-llvm&lt;/code&gt; package in the future.&lt;/li&gt;
  &lt;li&gt;Reducing the size of the C++ shared libraries as discussed above&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Now pyarrow is about the size of NumPy and thus much easier for Python projects
to take on as a hard dependency without worrying about large on-disk size.&lt;/p&gt;

&lt;p&gt;Looking ahead, we have discussed &lt;a href=&quot;https://issues.apache.org/jira/browse/ARROW-8518&quot;&gt;strategies&lt;/a&gt;
for breaking up pyarrow into multiple wheel packages, sort of a “hub and spoke”
model where some optional pieces are installed as separate wheels so people only
needing some “core” functionality only have to install a small package. This
would be a significant project, though, so for now we’ve focused on improvements
to the comprehensive wheel package.&lt;/p&gt;

&lt;h2 id=&quot;r-packaging&quot;&gt;R packaging&lt;/h2&gt;

&lt;p&gt;Packaging Arrow for R involves similar challenges to Python wheels, though the
technical details are unique. Like how &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pip install pyarrow&lt;/code&gt; should just work
everywhere, so should &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;install.packages(&quot;arrow&quot;)&lt;/code&gt; in R, and we have invested
significant effort to get there. Because the R package depends on a C++ library
in active development, this is not trivial, particularly for all of the
combinations of C++ compilers and standard libraries on Linux.&lt;/p&gt;

&lt;p&gt;In the initial CRAN release last year, version 0.14.1, only Windows and macOS
binary packages worked out of the box. For Linux, you had to install the C++
library separately, before installing the R package. While Python wheels contain
binary libraries even on Linux, CRAN only hosts source packages that must be
compiled on the user’s machine at install time. This led to an experience that
was less than ideal for Linux users.&lt;/p&gt;

&lt;p&gt;Starting in version 0.16, a source package installation on Linux handles its C++
dependencies automatically. By default, the package executes a &lt;a href=&quot;https://github.com/apache/arrow/blob/master/r/inst/build_arrow_static.sh&quot;&gt;bundled
script&lt;/a&gt;
that downloads and builds the Arrow C++ library with no system dependencies
beyond what R requires. On many common Linux distributions and versions, this
can be sped up by
&lt;a href=&quot;/docs/r/articles/install.html&quot;&gt;setting an environment variable&lt;/a&gt;
to download a prebuilt static C++ library for inclusion in the package.&lt;/p&gt;

&lt;p&gt;To accompany these improvements and to ensure that they succeeded on a wide
range of platforms, we added
&lt;a href=&quot;https://github.com/apache/arrow/blob/bebcc5db3cc2890a9c53ebd53bc60863ae5ebb49/dev/tasks/tasks.yml#L1704-L1785&quot;&gt;extensive&lt;/a&gt;
&lt;a href=&quot;https://github.com/ursa-labs/arrow-r-nightly/blob/master/.github/workflows/test-binary.yml&quot;&gt;nightly builds&lt;/a&gt;
to our continuous integration system. These are also easily extensible–all we
need is a Docker image containing R, and we can plug new environments into our
regular nightly testing.&lt;/p&gt;

&lt;p&gt;Since then, we’ve continued to improve the installation experience and look for
ways to reduce build time and package size. The C++ library improvements
discussed above help the R package since most installations of the R package
either build or otherwise include the C++ library. Within the R package itself,
we’ve looked for ways to include just what is needed and nothing more. These
efforts have resulted in smaller downloads and installed package sizes. From
0.17.1 to 1.0.0, installed library sizes for macOS and Windows CRAN binaries are
down 10 percent, and the prebuilt static C++ libraries for Linux are 33 percent
smaller compared to 0.16.0, despite the addition of many new features.&lt;/p&gt;

&lt;!-- macOS build 0.17.1:

checking installed package size ... NOTE
  installed size is 38.1Mb
  sub-directories of 1Mb or more:
    R 3.2Mb
    libs 34.5Mb

autobrew libs on master: 8.9mb

macOS 1.0.0
checking installed package size ... NOTE
  installed size is 35.0Mb
  sub-directories of 1Mb or more:
    R 3.2Mb
    libs 31.3Mb

windows 0.17.1:
checking installed package size ... NOTE
  installed size is 27.9Mb
  sub-directories of 1Mb or more:
    R 3.2Mb
    libs 24.3Mb

windows libs on 1.0.0:
checking installed package size ... NOTE
  installed size is 24.9Mb
  sub-directories of 1Mb or more:
    R 3.2Mb
    libs 21.2Mb

ubuntu-18.04 libarrow binaries:
0.16.0.2 18.84 MB
0.17.0 	  12.81 MB
1.0.0      12.45 MB --&gt;

&lt;h2 id=&quot;c-interface&quot;&gt;C Interface&lt;/h2&gt;

&lt;p&gt;Finally, we have observed that some projects may wish to produce or consume a
subset of the Arrow format and do not want to take on any additional code
dependencies. There are also scenarios where two libraries need to share
in-memory Arrow data structures but are unable to depend on a common Arrow
library such as the reference C++ implementation. To address these use cases, we
designed the &lt;a href=&quot;/docs/format/CDataInterface.html&quot;&gt;C Interface&lt;/a&gt;
to provide a lightweight way to exchange Arrow data at the C level without any
memory copying.&lt;/p&gt;

&lt;p&gt;When using the C interface, a developer populates simple C data structures that
contain the schema (data type) information about an Arrow data structure and the
addresses of the pieces of memory that constitute the data. This permits
libraries to be plugged together easily in-memory without any shared code
(except the C interface structure definitions). Most programming languages have
the ability to manipulate C structures and so this interface can even be used
without having to write or compile C code. We have used the C interface to
&lt;a href=&quot;/docs/r/articles/python.html&quot;&gt;transfer data structures between Python and R&lt;/a&gt;
in-memory using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;reticulate&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;One exciting use case for the Arrow C interface is to add Arrow import and
export to database driver libraries which often contain a C API.&lt;/p&gt;

&lt;h2 id=&quot;looking-ahead&quot;&gt;Looking ahead&lt;/h2&gt;

&lt;p&gt;As the project grows, we will continue working to make the build process as
fast and reliable as possible. If you see ways we can improve it further, or if
you run into trouble, please bring it up on our
&lt;a href=&quot;https://arrow.apache.org/community/#mailing-lists&quot;&gt;mailing list&lt;/a&gt; or
&lt;a href=&quot;https://issues.apache.org/jira/browse/ARROW&quot;&gt;report an issue&lt;/a&gt;.&lt;/p&gt;</content><author><name>pmc</name></author><category term="application" /><summary type="html">Over the last four and a half years, we’ve worked to build a “batteries-included” development platform for high-performance analytics applications in C++. As the scope of the project has grown, we have sometimes taken on additional library dependencies to support a wide variety of systems and data processing tasks. While these dependencies give us leverage on hard problems, in some cases they have added complexity for projects that depend on Arrow. Some projects have thus been concerned about depending on the Arrow C++ library, particularly if their use of the Arrow library’s features is limited. Indeed, in the earlier stages of the Arrow project development, dependency management issues did cause problems for early adopters. We want developers to trust that they can use and depend on our libraries, and that doing so doesn’t add a burden for their own project maintenance or for their users. Over the last year, we have undertaken a number of significant projects to accommodate the different ways that people want to depend on Arrow C++. We’ve aimed to make the build process simple by default, without requiring special environment setup, yet also highly configurable for those who need to specialize. This includes a zero-dependency option for projects that wish to use the Arrow C++ core but take on no transitive dependencies. We’ve also worked to make builds faster and more compact, even as we continue to add new functionality. This post covers many of the efforts we’ve made, both in the C++ libraries and in the Arrow Python and R packages that depend on them. Compared to a year ago, the build experience is much more reliable on a wider range of platforms, requires fewer dependencies, and yields smaller package sizes. Minimal default build options One rough edge for people using Arrow as a dependency was that many optional project components were enabled in the build by default, thus requiring any extra dependencies of those optional components. Rather than expecting users to disable optional components one by one, we have made the default for all optional components to be OFF so that the default configuration is a dependency-free minimal core build. The only third-party library enabled by default is jemalloc, the project’s recommended memory allocator (except on Windows, where it is also disabled). Given that Arrow applications often process large volumes of data, we have found additionally that using memory allocators provided by projects like jemalloc and mimalloc yield significantly better performance over the default system allocators. Even so, this can also be disabled if desired. To demonstrate a minimal build, we have provided a Dockerfile which can be used to build the project requiring only CMake and a C++ compiler with zero dependencies. Additionally, we have included an example of including Arrow as an external project dependency in another CMake project. Flexible dependency configuration in CMake As part of improving our CMake-based build system, we have made the configuration of build dependencies both flexible and consistent for different users’ needs. In some cases, developers want Arrow to build against dependencies provided by an external package manager, such as apt in Debian-based Linux distributions. In other cases, developers may want to avoid any quirks of system libraries and build all dependencies together with the Arrow build. For each package, the ${Library}_SOURCE CMake option can be set to one of three values: SYSTEM, when the dependency is to be provided externally (such as by a Linux distribution or Homebrew) BUNDLED, when you want the dependency to be built from source while building Arrow, and then statically-linked with the resulting libraries AUTO, which tries the SYSTEM approach but falls back on BUNDLED if the dependency cannot be located We additionally have provided CONDA and BREW source types for the common scenarios when developers are using the conda or Homebrew package managers. These dependency sources can be configured on an individual dependency basis or globally using the ARROW_DEPENDENCY_SOURCE CMake option. AUTO is default, which enables builds to be faster by using pre-built system libraries where possible but still succeed even if all dependencies are not available on the system. Reduced external dependencies Another area of focus was to audit our dependencies. We went through and found places where we could drop external dependencies without losing useful functionality and without having to rewrite a lot or copy too much code into our codebase. We have eliminated Boost as a dependency of the core Arrow library, and in other components (Gandiva, Parquet, etc.), the use of Boost has been greatly reduced. In addition, when building Boost “bundled” in the Arrow build, we stripped down the Boost package we download to the minimum needed, cutting out 90 percent of the download size. We vendored a few small dependencies, such as the double-conversion and uriparser libraries, so that they do not need to be downloaded and built separately. We also compiled the Flatbuffers and Thrift definitions (which are needed to implement the Arrow and Parquet formats, respectively) and checked in the resulting C++ code to the Arrow repository. This means that Flatbuffers is no longer a build or runtime dependency of Arrow, and we only need the Thrift C++ library, not the Thrift compiler, which has additional dependencies on flex and bison. C++ library size reductions As the C++ codebase grows in size, so too does compilation times and the amount of binary code generated by the C++ compiler. Over the last several months, we have begun analyzing the Arrow libraries both compile times and generated code sizes. This has yielded both significant size reductions (more than 30 percent code size reduction since 0.17.0). We have also restructured header files to avoid including unneeded header files, thus lightening the load on C++ compilers and improving compilation times. Python wheels The expectation for binary wheel packages on the Python Package Index (PyPI) is that they are self-contained and have no external dependencies except on other Python packages. Additionally, each user of pyarrow may need different things from the project. Some users just want to read Parquet files and convert them to pandas data frames while others want to use Flight for moving around large datasets. Thus, the “pyarrow” wheel has from the beginning of the project been a fairly comprehensive build including as many optional components as is practical for us to maintain. A comprehensive wheel package has some downsides: most notably for users, it is large. Additionally, through a snafu relating to C++ shared libraries, for several releases the wheel packages would create two copies of each C++ library on disk, resulting in double the amount of disk usage. This has caused problems for people using pyarrow in space-constrained environments like AWS Lambda. In the 1.0.0 release, we have implemented some changes that have reduced the size of the wheels (both in .whl form and installed on disk) by about 75 percent: Working around the problems resulting in two copies of each shared library being created in the site-packages directory. Disabling Gandiva, which required the LLVM runtime, the largest statically-linked dependency. Gandiva is still available to conda users now–it’s just not included in the wheels–and we hope to package it as a separate pyarrow-llvm package in the future. Reducing the size of the C++ shared libraries as discussed above Now pyarrow is about the size of NumPy and thus much easier for Python projects to take on as a hard dependency without worrying about large on-disk size. Looking ahead, we have discussed strategies for breaking up pyarrow into multiple wheel packages, sort of a “hub and spoke” model where some optional pieces are installed as separate wheels so people only needing some “core” functionality only have to install a small package. This would be a significant project, though, so for now we’ve focused on improvements to the comprehensive wheel package. R packaging Packaging Arrow for R involves similar challenges to Python wheels, though the technical details are unique. Like how pip install pyarrow should just work everywhere, so should install.packages(&quot;arrow&quot;) in R, and we have invested significant effort to get there. Because the R package depends on a C++ library in active development, this is not trivial, particularly for all of the combinations of C++ compilers and standard libraries on Linux. In the initial CRAN release last year, version 0.14.1, only Windows and macOS binary packages worked out of the box. For Linux, you had to install the C++ library separately, before installing the R package. While Python wheels contain binary libraries even on Linux, CRAN only hosts source packages that must be compiled on the user’s machine at install time. This led to an experience that was less than ideal for Linux users. Starting in version 0.16, a source package installation on Linux handles its C++ dependencies automatically. By default, the package executes a bundled script that downloads and builds the Arrow C++ library with no system dependencies beyond what R requires. On many common Linux distributions and versions, this can be sped up by setting an environment variable to download a prebuilt static C++ library for inclusion in the package. To accompany these improvements and to ensure that they succeeded on a wide range of platforms, we added extensive nightly builds to our continuous integration system. These are also easily extensible–all we need is a Docker image containing R, and we can plug new environments into our regular nightly testing. Since then, we’ve continued to improve the installation experience and look for ways to reduce build time and package size. The C++ library improvements discussed above help the R package since most installations of the R package either build or otherwise include the C++ library. Within the R package itself, we’ve looked for ways to include just what is needed and nothing more. These efforts have resulted in smaller downloads and installed package sizes. From 0.17.1 to 1.0.0, installed library sizes for macOS and Windows CRAN binaries are down 10 percent, and the prebuilt static C++ libraries for Linux are 33 percent smaller compared to 0.16.0, despite the addition of many new features. C Interface Finally, we have observed that some projects may wish to produce or consume a subset of the Arrow format and do not want to take on any additional code dependencies. There are also scenarios where two libraries need to share in-memory Arrow data structures but are unable to depend on a common Arrow library such as the reference C++ implementation. To address these use cases, we designed the C Interface to provide a lightweight way to exchange Arrow data at the C level without any memory copying. When using the C interface, a developer populates simple C data structures that contain the schema (data type) information about an Arrow data structure and the addresses of the pieces of memory that constitute the data. This permits libraries to be plugged together easily in-memory without any shared code (except the C interface structure definitions). Most programming languages have the ability to manipulate C structures and so this interface can even be used without having to write or compile C code. We have used the C interface to transfer data structures between Python and R in-memory using reticulate. One exciting use case for the Arrow C interface is to add Arrow import and export to database driver libraries which often contain a C API. Looking ahead As the project grows, we will continue working to make the build process as fast and reliable as possible. If you see ways we can improve it further, or if you run into trouble, please bring it up on our mailing list or report an issue.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Apache Arrow 1.0.0 Release</title><link href="https://arrow.apache.org/blog/2020/07/24/1.0.0-release/" rel="alternate" type="text/html" title="Apache Arrow 1.0.0 Release" /><published>2020-07-24T02:00:00-04:00</published><updated>2020-07-24T02:00:00-04:00</updated><id>https://arrow.apache.org/blog/2020/07/24/1.0.0-release</id><content type="html" xml:base="https://arrow.apache.org/blog/2020/07/24/1.0.0-release/">&lt;!--

--&gt;

&lt;p&gt;The Apache Arrow team is pleased to announce the 1.0.0 release. This covers
over 3 months of development work and includes &lt;a href=&quot;https://issues.apache.org/jira/issues/?jql=project%20%3D%20ARROW%20AND%20status%20%3D%20Resolved%20AND%20fixVersion%20%3D%201.0.0&quot;&gt;&lt;strong&gt;810 resolved issues&lt;/strong&gt;&lt;/a&gt; from
&lt;a href=&quot;/release/1.0.0.html#contributors&quot;&gt;&lt;strong&gt;100 distinct contributors&lt;/strong&gt;&lt;/a&gt;. See the Install Page to learn how to get the
libraries for your platform.&lt;/p&gt;

&lt;p&gt;Despite a “1.0.0” version, this is the 18th major release of Apache Arrow and
marks a transition to binary stability of the columnar format (which was
already informally backward-compatible going back to December 2017) and a
transition to Semantic Versioning for the Arrow software libraries.&lt;/p&gt;

&lt;p&gt;The release notes below are not exhaustive and only expose selected highlights
of the release. Many other bugfixes and improvements have been made: we refer
you to the &lt;a href=&quot;/release/1.0.0.html&quot;&gt;complete changelog&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;100-columnar-format-and-stability-guarantees&quot;&gt;1.0.0 Columnar Format and Stability Guarantees&lt;/h2&gt;

&lt;p&gt;The 1.0.0 release indicates that the Arrow columnar format is declared stable,
with &lt;a href=&quot;/docs/format/Versioning.html&quot;&gt;forward and backward compatibility guarantees&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The Arrow columnar format received several recent changes and additions,
leading to the 1.0.0 format version:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The metadata version was bumped to a new version V5, indicating an
incompatible change in the buffer layout of Union types. All
other types keep the same layout as in V4. V5 also includes format additions
to assist with forward compatibility (detecting unsupported changes sent by
future library versions). Libraries remain backward compatible with data
generated by all libraries back to 0.8.0 (December 2017) and the Java and C++
libraries are capable of generating V4-compatible messages (for sending data
to applications using 0.8.0 to 0.17.1).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Dictionary indices are now allowed to be unsigned integers rather than only
signed integers. Using UInt64 is still discouraged because of
poor Java support.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A “Feature” enum has been added to announce the use of specific optional
features in an IPC stream, such as buffer compression.  This
new field is not used by any implementation yet.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Optional buffer compression using LZ4 or ZStandard was added to the IPC
format.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Decimal types now have an optional “bitWidth” field, defaulting to 128.&lt;br /&gt;
This will allow for future support of other decimal widths
such as 32- and 64-bit.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The validity bitmap buffer has been removed from Union types. The nullity of
a slot in a Union array is determined exclusively by the constituent arrays
forming the union.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Integration testing has been expanded to test for extension types and
nested dictionaries. See the &lt;a href=&quot;/docs/status.html&quot;&gt;implementation matrix&lt;/a&gt; for details.&lt;/p&gt;

&lt;h2 id=&quot;community&quot;&gt;Community&lt;/h2&gt;

&lt;p&gt;Since the last release, we have added two new committers:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Liya Fan&lt;/li&gt;
  &lt;li&gt;Ji Liu&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Thank you for all your contributions!&lt;/p&gt;

&lt;h2 id=&quot;arrow-flight-rpc-notes&quot;&gt;Arrow Flight RPC notes&lt;/h2&gt;

&lt;p&gt;Flight now offers DoExchange, a fully bidirectional data endpoint, in addition
to DoGet and DoPut, in C++, Java, and Python. Middlewares in all languages now
expose binary-valued headers. Additionally, servers and clients can set Arrow
IPC read/write options in all languages, making compatibility easier with earlier
versions of Arrow Flight.&lt;/p&gt;

&lt;p&gt;In C++ and Python, Flight now exposes more options from gRPC, including the
address of the client (on the server) and the ability to set low-level gRPC
client options. Flight also supports mutual TLS authentication and the ability
for a client to control the size of a data message on the wire.&lt;/p&gt;

&lt;h2 id=&quot;c-notes&quot;&gt;C++ notes&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Support for static linking with Arrow has been vastly improved, including the
introduction of a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;libarrow_bundled_dependencies.a&lt;/code&gt; library bundling all
external dependencies that are built from source by Arrow’s build system
rather than installed by an external package manager. This makes
it significantly easier to create dependency-free applications with all
libraries statically-linked.&lt;/li&gt;
  &lt;li&gt;Following the Arrow format changes, Union arrays cannot have a top-level
bitmap anymore.&lt;/li&gt;
  &lt;li&gt;A number of improvements were made to reduce the overall generated binary
size in the Arrow library.&lt;/li&gt;
  &lt;li&gt;A convenience API &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;GetBuildInfo&lt;/code&gt; allows querying the characteristics of the
Arrow library.  We encourage you to suggest any desired addition to the
returned information.&lt;/li&gt;
  &lt;li&gt;We added an optional dependency to the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;utf8proc&lt;/code&gt; library, used in several
compute functions (see below).&lt;/li&gt;
  &lt;li&gt;Instead of sharing the same concrete classes, sparse and dense unions now
have separated classes (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;SparseUnionType&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;DenseUnionType&lt;/code&gt;, as well as
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;SparseUnionArray&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;DenseUnionArray&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;SparseUnionScalar&lt;/code&gt;,
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;DenseUnionScalar&lt;/code&gt;).&lt;/li&gt;
  &lt;li&gt;Arrow can now be built for iOS using the right set of CMake options, though
we don’t officially support it.  See &lt;a href=&quot;https://github.com/UnfoldedInc/deck.gl-native-dependencies/blob/master/docs/iOS-BUILD.md#arrow-v0170&quot;&gt;this writeup&lt;/a&gt; for details.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;compute-functions&quot;&gt;Compute functions&lt;/h3&gt;

&lt;p&gt;The compute kernel layer was extensively reworked.  It now offers
a generic function lookup, dispatch and execution mechanism.  Furthermore, new
internal scaffoldings make it vastly easier to write new function kernels, with
many common details like type checking and function dispatch based on type
combinations handled by the framework rather than implemented manually by the
function developer.&lt;/p&gt;

&lt;p&gt;Around 30 new array compute functions have been added. For example,
Unicode-compliant predicates and transforms, such as lowercase and uppercase
transforms, are now available.&lt;/p&gt;

&lt;p&gt;The available compute functions are listed exhaustively in the Sphinx-generated
&lt;a href=&quot;/docs/cpp/compute.html&quot;&gt;documentation&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;datasets&quot;&gt;Datasets&lt;/h3&gt;

&lt;p&gt;Datasets can now be read from CSV files.&lt;/p&gt;

&lt;p&gt;Datasets can be expanded to their component fragments, enabling fine grained
interoperability with other consumers of data files. Where applicable, metadata
is available as a property of the fragment, including partition information and
(for the parquet format) per-column statistics.&lt;/p&gt;

&lt;p&gt;Datasets of parquet files can now be assembled from a single &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;_metadata&lt;/code&gt; file,
such as those created by systems like Dask and Spark. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;_metadata&lt;/code&gt;
contains the metadata of all fragments, allowing construction of a statistics-
aware dataset with a single IO call.&lt;/p&gt;

&lt;h3 id=&quot;feather&quot;&gt;Feather&lt;/h3&gt;

&lt;p&gt;The Feather format is now available in version 2, which is simply the Arrow
IPC file format with another name.&lt;/p&gt;

&lt;h3 id=&quot;ipc&quot;&gt;IPC&lt;/h3&gt;

&lt;p&gt;By default, we now write IPC streams with metadata V5.  However, metadata V4
can be requested by setting the appropriate member in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;IpcWriteOptions&lt;/code&gt;. V4 as
well as V5 metadata IPC streams can be read properly, with one exception: a V4
metadata stream containing Union arrays with top-level null values will refuse
reading.&lt;/p&gt;

&lt;p&gt;As noted above, there are no changes between V4 and V5 that break
backwards compatibility. For forward compatibility scenarios (where you need to
generate data to be read by an older Arrow library), you can set the V4
compatibility mode.&lt;/p&gt;

&lt;p&gt;Support for dictionary replacement and dictionary delta was implemented.&lt;/p&gt;

&lt;h3 id=&quot;parquet&quot;&gt;Parquet&lt;/h3&gt;

&lt;p&gt;Writing files with the LZ4 codec is disabled because it produces files
incompatible with the widely-used Hadoop Parquet implementation.  Support will
be reenabled once we align the LZ4 implementation with the special buffer
encoding expected by Hadoop.&lt;/p&gt;

&lt;h2 id=&quot;java-notes&quot;&gt;Java notes&lt;/h2&gt;

&lt;p&gt;The Java package introduces a number of low level changes in this release.
Most notable are the work in support of allocating large arrow buffers and
removing Netty from the public API. Users will have to update their
dependencies to use one of the two supported allocators Netty:
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;arrow-memory-netty&lt;/code&gt; or Unsafe (internal java api for direct memory)
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;arrow-memory-unsafe&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;The Java Vector implementation has improved its interoperability having
verified &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;LargeVarChar&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;LargeBinary&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;LargeList&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Union&lt;/code&gt;, Extension types
and duplicate field names in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Structs&lt;/code&gt; are binary compatible with C++ and the
specification.&lt;/p&gt;

&lt;h2 id=&quot;python-notes&quot;&gt;Python notes&lt;/h2&gt;

&lt;p&gt;The size of wheel packages is significantly reduced, up to 75%.  One side
effect is that these wheels do not enable Gandiva anymore (which requires the
LLVM runtime to be statically-linked). We are interested in providing Gandiva
as an add-on package as a separate Python wheel in the future.&lt;/p&gt;

&lt;p&gt;The Scalar class hierarchy was reworked to more closely follow its C++
counterpart.&lt;/p&gt;

&lt;p&gt;TLS CA certificates are looked up more reliably when using the S3 filesystem,
especially with manylinux wheels.&lt;/p&gt;

&lt;p&gt;The encoding of CSV files can now be specified explicitly, defaulting to UTF8.
Custom timestamp parsers can now be used for CSV files.&lt;/p&gt;

&lt;p&gt;Filesystems can now be implemented in pure Python.  As a result,
&lt;a href=&quot;https://filesystem-spec.readthedocs.io&quot;&gt;fsspec&lt;/a&gt;-based filesystems can now
be used in datasets.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;parquet.read_table&lt;/code&gt; is now backed by the dataset API by default, enabling
filters on any column and more flexible partitioning.&lt;/p&gt;

&lt;h2 id=&quot;r-notes&quot;&gt;R notes&lt;/h2&gt;

&lt;p&gt;The R package added support for converting to and from many additional Arrow
types. Tables showing how R types are mapped to Arrow types and vice versa have
been added to the &lt;a href=&quot;/docs/r/articles/arrow.html&quot;&gt;introductory vignette&lt;/a&gt;, and nearly all types are handled.
In addition, R &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;attributes&lt;/code&gt; like custom classes and metadata are now preserved
when converting a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;data.frame&lt;/code&gt; to an Arrow Table and are restored when loading
them back into R.&lt;/p&gt;

&lt;p&gt;For more on what’s in the 1.0.0 R package, see the &lt;a href=&quot;/docs/r/news/&quot;&gt;R changelog&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;ruby-and-c-glib-notes&quot;&gt;Ruby and C GLib notes&lt;/h2&gt;

&lt;p&gt;The Ruby and C GLib packages added support for the new compute function
framework, in which users can find a
compute function dynamically and call it. Users don’t need to wait for a C
GLib binding for new compute functions: if the C++ package provides a
new compute function, users can use it
without additional code in the Ruby and C GLib packages.&lt;/p&gt;

&lt;p&gt;The Ruby and C GLib packages added support for Apache Arrow
Dataset. The Ruby package provides a new gem for Apache Arrow Dataset,
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;red-arrow-dataset&lt;/code&gt;. The C GLib package provides a new module for
Apache Arrow Dataset, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;arrow-dataset-glib&lt;/code&gt;. They just have a few
features for now but we will add more in future releases.&lt;/p&gt;

&lt;p&gt;The Ruby and C GLib packages added support for reading only the
specified row group in an Apache Parquet file.&lt;/p&gt;

&lt;h3 id=&quot;ruby&quot;&gt;Ruby&lt;/h3&gt;

&lt;p&gt;The Ruby package added support for column level compression in writing
Apache Parquet files.&lt;/p&gt;

&lt;p&gt;The Ruby package changed the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Arrow::DictionaryArray#[]&lt;/code&gt; behavior. It now
returns the dictionary value instead of the dictionary index. This is a
backwards-incompatible change.&lt;/p&gt;

&lt;h2 id=&quot;rust-notes&quot;&gt;Rust notes&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;A new integration test crate has been added, allowing the Rust
implementation to participate in integration testing.&lt;/li&gt;
  &lt;li&gt;A new benchmark crate has been added for benchmarking performance
against popular data sets. The initial examples run SQL queries against
the NYC Taxi data set using DataFusion. This is useful for comparing
performance against other Arrow implementations.&lt;/li&gt;
  &lt;li&gt;Rust toolchain has been upgraded to 1.44 nightly.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;arrow-core&quot;&gt;Arrow Core&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Support for binary, string, and list arrays with i64 offsets to support
large lists.&lt;/li&gt;
  &lt;li&gt;A new sort kernel has been added.&lt;/li&gt;
  &lt;li&gt;There have been various improvements to dictionary array support.&lt;/li&gt;
  &lt;li&gt;CSV reader enhancements include a new CsvReadOptions struct and support
for schema inference from multiple CSV files.&lt;/li&gt;
  &lt;li&gt;There are significant (10x - 40x) performance improvements to SIMD
comparison kernels.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;datafusion&quot;&gt;DataFusion&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;There are numerous UX improvements to LogicalPlan and LogicalPlanBuilder,
including support for named columns.&lt;/li&gt;
  &lt;li&gt;General improvements to code base, such as removing many uses of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Arc&lt;/code&gt;
and using slices instead of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;amp;Vec&lt;/code&gt; as function arguments.&lt;/li&gt;
  &lt;li&gt;ParquetScanExec performance improvement (almost 2x).&lt;/li&gt;
  &lt;li&gt;ExecutionContext can now be shared between threads.&lt;/li&gt;
  &lt;li&gt;Rust closures can now be used as Scalar UDFs.&lt;/li&gt;
  &lt;li&gt;Sort support has been added to SQL and LogicalPlan.&lt;/li&gt;
&lt;/ul&gt;</content><author><name>pmc</name></author><category term="release" /><summary type="html">The Apache Arrow team is pleased to announce the 1.0.0 release. This covers over 3 months of development work and includes 810 resolved issues from 100 distinct contributors. See the Install Page to learn how to get the libraries for your platform. Despite a “1.0.0” version, this is the 18th major release of Apache Arrow and marks a transition to binary stability of the columnar format (which was already informally backward-compatible going back to December 2017) and a transition to Semantic Versioning for the Arrow software libraries. The release notes below are not exhaustive and only expose selected highlights of the release. Many other bugfixes and improvements have been made: we refer you to the complete changelog. 1.0.0 Columnar Format and Stability Guarantees The 1.0.0 release indicates that the Arrow columnar format is declared stable, with forward and backward compatibility guarantees. The Arrow columnar format received several recent changes and additions, leading to the 1.0.0 format version: The metadata version was bumped to a new version V5, indicating an incompatible change in the buffer layout of Union types. All other types keep the same layout as in V4. V5 also includes format additions to assist with forward compatibility (detecting unsupported changes sent by future library versions). Libraries remain backward compatible with data generated by all libraries back to 0.8.0 (December 2017) and the Java and C++ libraries are capable of generating V4-compatible messages (for sending data to applications using 0.8.0 to 0.17.1). Dictionary indices are now allowed to be unsigned integers rather than only signed integers. Using UInt64 is still discouraged because of poor Java support. A “Feature” enum has been added to announce the use of specific optional features in an IPC stream, such as buffer compression. This new field is not used by any implementation yet. Optional buffer compression using LZ4 or ZStandard was added to the IPC format. Decimal types now have an optional “bitWidth” field, defaulting to 128. This will allow for future support of other decimal widths such as 32- and 64-bit. The validity bitmap buffer has been removed from Union types. The nullity of a slot in a Union array is determined exclusively by the constituent arrays forming the union. Integration testing has been expanded to test for extension types and nested dictionaries. See the implementation matrix for details. Community Since the last release, we have added two new committers: Liya Fan Ji Liu Thank you for all your contributions! Arrow Flight RPC notes Flight now offers DoExchange, a fully bidirectional data endpoint, in addition to DoGet and DoPut, in C++, Java, and Python. Middlewares in all languages now expose binary-valued headers. Additionally, servers and clients can set Arrow IPC read/write options in all languages, making compatibility easier with earlier versions of Arrow Flight. In C++ and Python, Flight now exposes more options from gRPC, including the address of the client (on the server) and the ability to set low-level gRPC client options. Flight also supports mutual TLS authentication and the ability for a client to control the size of a data message on the wire. C++ notes Support for static linking with Arrow has been vastly improved, including the introduction of a libarrow_bundled_dependencies.a library bundling all external dependencies that are built from source by Arrow’s build system rather than installed by an external package manager. This makes it significantly easier to create dependency-free applications with all libraries statically-linked. Following the Arrow format changes, Union arrays cannot have a top-level bitmap anymore. A number of improvements were made to reduce the overall generated binary size in the Arrow library. A convenience API GetBuildInfo allows querying the characteristics of the Arrow library. We encourage you to suggest any desired addition to the returned information. We added an optional dependency to the utf8proc library, used in several compute functions (see below). Instead of sharing the same concrete classes, sparse and dense unions now have separated classes (SparseUnionType and DenseUnionType, as well as SparseUnionArray, DenseUnionArray, SparseUnionScalar, DenseUnionScalar). Arrow can now be built for iOS using the right set of CMake options, though we don’t officially support it. See this writeup for details. Compute functions The compute kernel layer was extensively reworked. It now offers a generic function lookup, dispatch and execution mechanism. Furthermore, new internal scaffoldings make it vastly easier to write new function kernels, with many common details like type checking and function dispatch based on type combinations handled by the framework rather than implemented manually by the function developer. Around 30 new array compute functions have been added. For example, Unicode-compliant predicates and transforms, such as lowercase and uppercase transforms, are now available. The available compute functions are listed exhaustively in the Sphinx-generated documentation. Datasets Datasets can now be read from CSV files. Datasets can be expanded to their component fragments, enabling fine grained interoperability with other consumers of data files. Where applicable, metadata is available as a property of the fragment, including partition information and (for the parquet format) per-column statistics. Datasets of parquet files can now be assembled from a single _metadata file, such as those created by systems like Dask and Spark. _metadata contains the metadata of all fragments, allowing construction of a statistics- aware dataset with a single IO call. Feather The Feather format is now available in version 2, which is simply the Arrow IPC file format with another name. IPC By default, we now write IPC streams with metadata V5. However, metadata V4 can be requested by setting the appropriate member in IpcWriteOptions. V4 as well as V5 metadata IPC streams can be read properly, with one exception: a V4 metadata stream containing Union arrays with top-level null values will refuse reading. As noted above, there are no changes between V4 and V5 that break backwards compatibility. For forward compatibility scenarios (where you need to generate data to be read by an older Arrow library), you can set the V4 compatibility mode. Support for dictionary replacement and dictionary delta was implemented. Parquet Writing files with the LZ4 codec is disabled because it produces files incompatible with the widely-used Hadoop Parquet implementation. Support will be reenabled once we align the LZ4 implementation with the special buffer encoding expected by Hadoop. Java notes The Java package introduces a number of low level changes in this release. Most notable are the work in support of allocating large arrow buffers and removing Netty from the public API. Users will have to update their dependencies to use one of the two supported allocators Netty: arrow-memory-netty or Unsafe (internal java api for direct memory) arrow-memory-unsafe. The Java Vector implementation has improved its interoperability having verified LargeVarChar, LargeBinary, LargeList, Union, Extension types and duplicate field names in Structs are binary compatible with C++ and the specification. Python notes The size of wheel packages is significantly reduced, up to 75%. One side effect is that these wheels do not enable Gandiva anymore (which requires the LLVM runtime to be statically-linked). We are interested in providing Gandiva as an add-on package as a separate Python wheel in the future. The Scalar class hierarchy was reworked to more closely follow its C++ counterpart. TLS CA certificates are looked up more reliably when using the S3 filesystem, especially with manylinux wheels. The encoding of CSV files can now be specified explicitly, defaulting to UTF8. Custom timestamp parsers can now be used for CSV files. Filesystems can now be implemented in pure Python. As a result, fsspec-based filesystems can now be used in datasets. parquet.read_table is now backed by the dataset API by default, enabling filters on any column and more flexible partitioning. R notes The R package added support for converting to and from many additional Arrow types. Tables showing how R types are mapped to Arrow types and vice versa have been added to the introductory vignette, and nearly all types are handled. In addition, R attributes like custom classes and metadata are now preserved when converting a data.frame to an Arrow Table and are restored when loading them back into R. For more on what’s in the 1.0.0 R package, see the R changelog. Ruby and C GLib notes The Ruby and C GLib packages added support for the new compute function framework, in which users can find a compute function dynamically and call it. Users don’t need to wait for a C GLib binding for new compute functions: if the C++ package provides a new compute function, users can use it without additional code in the Ruby and C GLib packages. The Ruby and C GLib packages added support for Apache Arrow Dataset. The Ruby package provides a new gem for Apache Arrow Dataset, red-arrow-dataset. The C GLib package provides a new module for Apache Arrow Dataset, arrow-dataset-glib. They just have a few features for now but we will add more in future releases. The Ruby and C GLib packages added support for reading only the specified row group in an Apache Parquet file. Ruby The Ruby package added support for column level compression in writing Apache Parquet files. The Ruby package changed the Arrow::DictionaryArray#[] behavior. It now returns the dictionary value instead of the dictionary index. This is a backwards-incompatible change. Rust notes A new integration test crate has been added, allowing the Rust implementation to participate in integration testing. A new benchmark crate has been added for benchmarking performance against popular data sets. The initial examples run SQL queries against the NYC Taxi data set using DataFusion. This is useful for comparing performance against other Arrow implementations. Rust toolchain has been upgraded to 1.44 nightly. Arrow Core Support for binary, string, and list arrays with i64 offsets to support large lists. A new sort kernel has been added. There have been various improvements to dictionary array support. CSV reader enhancements include a new CsvReadOptions struct and support for schema inference from multiple CSV files. There are significant (10x - 40x) performance improvements to SIMD comparison kernels. DataFusion There are numerous UX improvements to LogicalPlan and LogicalPlanBuilder, including support for named columns. General improvements to code base, such as removing many uses of Arc and using slices instead of &amp;amp;Vec as function arguments. ParquetScanExec performance improvement (almost 2x). ExecutionContext can now be shared between threads. Rust closures can now be used as Scalar UDFs. Sort support has been added to SQL and LogicalPlan.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Introducing the Apache Arrow C Data Interface</title><link href="https://arrow.apache.org/blog/2020/05/03/introducing-arrow-c-data-interface/" rel="alternate" type="text/html" title="Introducing the Apache Arrow C Data Interface" /><published>2020-05-03T19:00:00-04:00</published><updated>2020-05-03T19:00:00-04:00</updated><id>https://arrow.apache.org/blog/2020/05/03/introducing-arrow-c-data-interface</id><content type="html" xml:base="https://arrow.apache.org/blog/2020/05/03/introducing-arrow-c-data-interface/">&lt;!--

--&gt;

&lt;p&gt;Apache Arrow includes a cross-language, platform-independent in-memory
&lt;a href=&quot;https://arrow.apache.org/docs/format/Columnar.html&quot;&gt;columnar format&lt;/a&gt;
allowing zero-copy data sharing and transfer between heterogenous runtimes
and applications.&lt;/p&gt;

&lt;p&gt;The easiest way to use the Arrow columnar format has always been to depend
on one of the concrete implementations developed by the Apache Arrow community.
The project codebase contains libraries for 11 different programming languages
so far, and will likely grow to include more languages in the future.&lt;/p&gt;

&lt;p&gt;However, some projects may wish to import and export the Arrow columnar format
without taking on a new library dependency, such as the Arrow C++ library.
We have therefore designed an alternative which exchanges data at the C level,
conforming to a simple data definition.  The C Data Interface carries no dependencies
except a shared C ABI between binaries which use it.  C ABIs are platform-wide standards
which are necessarily adhered to by all compilers which generate binaries and are extremely
stable, ensuring portability of libraries and executable binaries.  Two libraries that utilize
the C structures defined by the C Data Interface can do zero-copy data
transfers at runtime without any build-time or link-time dependency
requirements.&lt;/p&gt;

&lt;p&gt;The best way to learn about the C Data Interface is to read the
&lt;a href=&quot;https://arrow.apache.org/docs/format/CDataInterface.html&quot;&gt;spec&lt;/a&gt;.
However, we will quickly go over its strong points.&lt;/p&gt;

&lt;h2 id=&quot;two-simple-struct-definitions&quot;&gt;Two simple struct definitions&lt;/h2&gt;

&lt;p&gt;To interact with the C Data Interface at the C or C++ level, the only
thing you have to include in your code is two struct type declarations
(and a couple of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;#define&lt;/code&gt;s for constant values).  Those declarations
only depend on standard C types, and can simply be pasted in a header
file.  Other languages can also participate as long as they provide a
Foreign Function Interface layer; this is the case for most modern
languages, such as Python (with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ctypes&lt;/code&gt; or &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cffi&lt;/code&gt;), Julia, Rust, Go, etc.&lt;/p&gt;

&lt;h2 id=&quot;zero-copy-data-sharing&quot;&gt;Zero-copy data sharing&lt;/h2&gt;

&lt;p&gt;The C Data Interface passes Arrow data buffers through memory pointers.  So,
by construction, it allows you to share data from one runtime to
another without copying it.  Since the data is in standard
&lt;a href=&quot;https://arrow.apache.org/docs/format/Columnar.html&quot;&gt;Arrow in-memory format&lt;/a&gt;,
its layout is well-defined and unambiguous.&lt;/p&gt;

&lt;p&gt;This design also restricts the C Data Interface to &lt;em&gt;in-process&lt;/em&gt; data sharing.
For interprocess communication, we recommend use of the Arrow
&lt;a href=&quot;https://arrow.apache.org/docs/format/Columnar.html#serialization-and-interprocess-communication-ipc&quot;&gt;IPC format&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;reduced-marshalling&quot;&gt;Reduced marshalling&lt;/h2&gt;

&lt;p&gt;The C Data Interface stays close to the natural way of expressing Arrow-like
data in C or C++.  Only two aspects involve non-trivial marshalling:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;the encoding of data types, using a very simple string-based language&lt;/li&gt;
  &lt;li&gt;the encoding of optional metadata, using a very simple length-prefixed format&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;separate-type-and-data-representation&quot;&gt;Separate type and data representation&lt;/h2&gt;

&lt;p&gt;For applications which produce many instances of data of a single datatype
(for example, as a stream of record batches), repeatedly reconstructing the
datatype from its string encoding would represent unnecessary overhead.  To
address this use case, the C Data Interface defines two independent structures:
one representing a datatype (and optional metadata), one representing a piece
of data.&lt;/p&gt;

&lt;h2 id=&quot;lifetime-handling&quot;&gt;Lifetime handling&lt;/h2&gt;

&lt;p&gt;One common difficulty of data sharing between heterogenous runtimes is to
correctly handle the lifetime of data.  The C Data Interface allows the producer
to define its own memory management scheme through a release callback.
This is a simple function pointer which consumers will call when they are
finished using the data.  For example when used as a producer the Arrow C++
library passes a release callback which simply decrements a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;shared_ptr&lt;/code&gt;’s
reference count.&lt;/p&gt;

&lt;h2 id=&quot;application-passing-data-between-r-and-python&quot;&gt;Application: passing data between R and Python&lt;/h2&gt;

&lt;p&gt;The R and Python Arrow libraries are both based on the Arrow C++ library,
however their respective toolchains (mandated by the R and Python packaging
standards) are ABI-incompatible.  It is therefore impossible to pass data
directly at the C++ level between the R and Python bindings.&lt;/p&gt;

&lt;p&gt;Using the C Data Interface, we have circumvented this restriction and provide
a zero-copy data sharing API between R and Python.  It is based on the R
&lt;a href=&quot;https://rstudio.github.io/reticulate/&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;reticulate&lt;/code&gt;&lt;/a&gt; library.&lt;/p&gt;

&lt;p&gt;Here is an example session mixing R and Python library calls:&lt;/p&gt;

&lt;div class=&quot;language-r highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;library&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;arrow&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;library&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reticulate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;use_virtualenv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;arrow&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pa&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;import&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;pyarrow&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# Create an array in PyArrow&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pa&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;## Array&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;## &amp;lt;double&amp;gt;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;## [&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;##   1,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;##   2,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;##   3&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;## ]&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# Apply R methods on the PyArrow-created array:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;## Array&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;## &amp;lt;double&amp;gt;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;## [&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;##   2,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;##   3&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;## ]&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# Create an array in R and pass it to PyArrow&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Array&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;create&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a_and_b&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pa&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;concat_arrays&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;r_to_py&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a_and_b&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;## Array&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;## &amp;lt;double&amp;gt;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;## [&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;##   1,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;##   2,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;##   3,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;##   5,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;##   6,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;##   7&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;## ]&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content><author><name>apitrou</name></author><category term="application" /><summary type="html">Apache Arrow includes a cross-language, platform-independent in-memory columnar format allowing zero-copy data sharing and transfer between heterogenous runtimes and applications. The easiest way to use the Arrow columnar format has always been to depend on one of the concrete implementations developed by the Apache Arrow community. The project codebase contains libraries for 11 different programming languages so far, and will likely grow to include more languages in the future. However, some projects may wish to import and export the Arrow columnar format without taking on a new library dependency, such as the Arrow C++ library. We have therefore designed an alternative which exchanges data at the C level, conforming to a simple data definition. The C Data Interface carries no dependencies except a shared C ABI between binaries which use it. C ABIs are platform-wide standards which are necessarily adhered to by all compilers which generate binaries and are extremely stable, ensuring portability of libraries and executable binaries. Two libraries that utilize the C structures defined by the C Data Interface can do zero-copy data transfers at runtime without any build-time or link-time dependency requirements. The best way to learn about the C Data Interface is to read the spec. However, we will quickly go over its strong points. Two simple struct definitions To interact with the C Data Interface at the C or C++ level, the only thing you have to include in your code is two struct type declarations (and a couple of #defines for constant values). Those declarations only depend on standard C types, and can simply be pasted in a header file. Other languages can also participate as long as they provide a Foreign Function Interface layer; this is the case for most modern languages, such as Python (with ctypes or cffi), Julia, Rust, Go, etc. Zero-copy data sharing The C Data Interface passes Arrow data buffers through memory pointers. So, by construction, it allows you to share data from one runtime to another without copying it. Since the data is in standard Arrow in-memory format, its layout is well-defined and unambiguous. This design also restricts the C Data Interface to in-process data sharing. For interprocess communication, we recommend use of the Arrow IPC format. Reduced marshalling The C Data Interface stays close to the natural way of expressing Arrow-like data in C or C++. Only two aspects involve non-trivial marshalling: the encoding of data types, using a very simple string-based language the encoding of optional metadata, using a very simple length-prefixed format Separate type and data representation For applications which produce many instances of data of a single datatype (for example, as a stream of record batches), repeatedly reconstructing the datatype from its string encoding would represent unnecessary overhead. To address this use case, the C Data Interface defines two independent structures: one representing a datatype (and optional metadata), one representing a piece of data. Lifetime handling One common difficulty of data sharing between heterogenous runtimes is to correctly handle the lifetime of data. The C Data Interface allows the producer to define its own memory management scheme through a release callback. This is a simple function pointer which consumers will call when they are finished using the data. For example when used as a producer the Arrow C++ library passes a release callback which simply decrements a shared_ptr’s reference count. Application: passing data between R and Python The R and Python Arrow libraries are both based on the Arrow C++ library, however their respective toolchains (mandated by the R and Python packaging standards) are ABI-incompatible. It is therefore impossible to pass data directly at the C++ level between the R and Python bindings. Using the C Data Interface, we have circumvented this restriction and provide a zero-copy data sharing API between R and Python. It is based on the R reticulate library. Here is an example session mixing R and Python library calls: library(arrow) library(reticulate) use_virtualenv(&quot;arrow&quot;) pa &amp;lt;- import(&quot;pyarrow&quot;) # Create an array in PyArrow a &amp;lt;- pa$array(c(1, 2, 3)) a ## Array ## &amp;lt;double&amp;gt; ## [ ## 1, ## 2, ## 3 ## ] # Apply R methods on the PyArrow-created array: a[a &amp;gt; 1] ## Array ## &amp;lt;double&amp;gt; ## [ ## 2, ## 3 ## ] # Create an array in R and pass it to PyArrow b &amp;lt;- Array$create(c(5, 6, 7)) a_and_b &amp;lt;- pa$concat_arrays(r_to_py(list(a, b))) a_and_b ## Array ## &amp;lt;double&amp;gt; ## [ ## 1, ## 2, ## 3, ## 5, ## 6, ## 7 ## ]</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Apache Arrow 0.17.0 Release</title><link href="https://arrow.apache.org/blog/2020/04/21/0.17.0-release/" rel="alternate" type="text/html" title="Apache Arrow 0.17.0 Release" /><published>2020-04-21T02:00:00-04:00</published><updated>2020-04-21T02:00:00-04:00</updated><id>https://arrow.apache.org/blog/2020/04/21/0.17.0-release</id><content type="html" xml:base="https://arrow.apache.org/blog/2020/04/21/0.17.0-release/">&lt;!--

--&gt;

&lt;p&gt;The Apache Arrow team is pleased to announce the 0.17.0 release. This covers
over 2 months of development work and includes &lt;a href=&quot;https://issues.apache.org/jira/issues/?jql=project%20%3D%20ARROW%20AND%20status%20%3D%20Resolved%20AND%20fixVersion%20%3D%200.17.0&quot;&gt;&lt;strong&gt;569 resolved issues&lt;/strong&gt;&lt;/a&gt;
from &lt;a href=&quot;https://arrow.apache.org/release/0.17.0.html#contributors&quot;&gt;&lt;strong&gt;79 distinct contributors&lt;/strong&gt;&lt;/a&gt;. See the Install Page to learn how to
get the libraries for your platform.&lt;/p&gt;

&lt;p&gt;The release notes below are not exhaustive and only expose selected highlights
of the release. Many other bugfixes and improvements have been made: we refer
you to the &lt;a href=&quot;https://arrow.apache.org/release/0.17.0.html&quot;&gt;complete changelog&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;community&quot;&gt;Community&lt;/h2&gt;

&lt;p&gt;Since the 0.16.0 release, two committers have joined the Project Management
Committee (PMC):&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/nealrichardson&quot;&gt;Neal Richardson&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/fsaintjacques&quot;&gt;François Saint-Jacques&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Thank you for all your contributions!&lt;/p&gt;

&lt;h2 id=&quot;columnar-format-notes&quot;&gt;Columnar Format Notes&lt;/h2&gt;

&lt;p&gt;A &lt;a href=&quot;https://arrow.apache.org/docs/format/CDataInterface.html&quot;&gt;C-level Data Interface&lt;/a&gt; was designed to ease data sharing inside a single
process. It allows different runtimes or libraries to share Arrow data using a
well-known binary layout and metadata representation, without any copies. Third
party libraries can use the C interface to import and export the Arrow columnar
format in-process without requiring on any new code dependencies.&lt;/p&gt;

&lt;p&gt;The C++ library now includes an implementation of the C Data Interface, and
Python and R have bindings to that implementation.&lt;/p&gt;

&lt;h2 id=&quot;arrow-flight-rpc-notes&quot;&gt;Arrow Flight RPC notes&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Adopted new DoExchange bi-directional data RPC&lt;/li&gt;
  &lt;li&gt;ListFlights supports being passed a Criteria argument in
Java/C++/Python. This allows applications to search for flights satisfying a
given query.&lt;/li&gt;
  &lt;li&gt;Custom metadata can be attached to errors that the server sends to the
client, which can be used to encode richer application-specific information.&lt;/li&gt;
  &lt;li&gt;A number of minor bugs were fixed, including proper handling of empty null
arrays in Java and round-tripping of certain Arrow status codes in
C++/Python.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;c-notes&quot;&gt;C++ notes&lt;/h2&gt;

&lt;h3 id=&quot;feather-v2&quot;&gt;Feather V2&lt;/h3&gt;

&lt;p&gt;The “Feather V2” format based on the Arrow IPC file format was developed.
Feather V2 features full support for all Arrow data types, and resolves the 2GB
per-column limitation for large amounts of string data that the &lt;a href=&quot;https://github.com/wesm/feather&quot;&gt;original
Feather implementation&lt;/a&gt; had.  Feather V2 also introduces experimental IPC
message compression using LZ4 frame format or ZSTD. This will be formalized
later in the Arrow format.&lt;/p&gt;

&lt;h3 id=&quot;c-datasets&quot;&gt;C++ Datasets&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Improve speed on high latency file system by relaxing discovery validation&lt;/li&gt;
  &lt;li&gt;Better performance with Arrow IPC files using column projection&lt;/li&gt;
  &lt;li&gt;Add the ability to list files in FileSystemDataset&lt;/li&gt;
  &lt;li&gt;Add support for Parquet file reader options&lt;/li&gt;
  &lt;li&gt;Support dictionary columns in partition expression&lt;/li&gt;
  &lt;li&gt;Fix various crashes and other issues&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;c-parquet-notes&quot;&gt;C++ Parquet notes&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Complete support for writing nested types to Parquet format was
completed. The legacy code can be accessed through parquet write option C++
and an environment variable in Python. Read support will come in a future
release.&lt;/li&gt;
  &lt;li&gt;The BYTE_STREAM_SPLIT encoding was implemented for floating-point types. It
helps improve the efficiency of memory compression for high-entropy data.&lt;/li&gt;
  &lt;li&gt;Expose Parquet schema field_id as Arrow field metadata&lt;/li&gt;
  &lt;li&gt;Support for DataPageV2 data page format&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;c-build-notes&quot;&gt;C++ build notes&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;We continued to make the core C++ library build simpler and faster. Among the
improvements are the removal of the dependency on Thrift IDL compiler at
build time; while Parquet still requires the Thrift runtime C++ library, its
dependencies are much lighter. We also further reduced the number of build
configurations that require Boost, and when Boost is needed to be built, we
only download the components we need, reducing the size of the Boost bundle
by 90%.&lt;/li&gt;
  &lt;li&gt;Improved support for building on ARM platforms&lt;/li&gt;
  &lt;li&gt;Upgraded LLVM version from 7 to 8&lt;/li&gt;
  &lt;li&gt;Simplified SIMD build configuration with ARROW_SIMD_LEVEL option allowing no
SIMD, SSE4.2, AVX2, or AVX512 to be selected.&lt;/li&gt;
  &lt;li&gt;Fixed a number of bugs affecting compilation on aarch64 platforms&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;other-c-notes&quot;&gt;Other C++ notes&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Many crashes on invalid input detected by &lt;a href=&quot;https://google.github.io/oss-fuzz/&quot;&gt;OSS-Fuzz&lt;/a&gt; in the IPC reader and
in Parquet-Arrow reading were fixed. See our recent &lt;a href=&quot;https://arrow.apache.org/blog/2020/03/31/fuzzing-arrow-ipc/&quot;&gt;blog post&lt;/a&gt; for more
details.&lt;/li&gt;
  &lt;li&gt;A “Device” abstraction was added to simplify buffer management and movement
across heterogeneous hardware configurations, e.g. CPUs and GPUs.&lt;/li&gt;
  &lt;li&gt;A streaming CSV reader was implemented, yielding individual RecordBatches and
helping limit overall memory occupation.&lt;/li&gt;
  &lt;li&gt;Array casting from Decimal128 to integer types and to Decimal128 with
different scale/precision was added.&lt;/li&gt;
  &lt;li&gt;Sparse CSF tensors are now supported.&lt;/li&gt;
  &lt;li&gt;When creating an Array, the null bitmap is not kept if the null count is known to be zero&lt;/li&gt;
  &lt;li&gt;Compressor support for the LZ4 frame format (LZ4_FRAME) was added&lt;/li&gt;
  &lt;li&gt;An event-driven interface for reading IPC streams was added.&lt;/li&gt;
  &lt;li&gt;Further core APIs that required passing an explicit out-parameter were
migrated to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Result&amp;lt;T&amp;gt;&lt;/code&gt;.&lt;/li&gt;
  &lt;li&gt;New analytics kernels for match, sort indices / argsort, top-k&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;java-notes&quot;&gt;Java notes&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Netty dependencies were removed for BufferAllocator and ReferenceManager
classes. In the future, we plan to move netty related classes to a separate
module.&lt;/li&gt;
  &lt;li&gt;New features were provided to support efficiently appending vector/vector
schema root values in batch.&lt;/li&gt;
  &lt;li&gt;Comparing a range of values in dense union vectors has been supported.&lt;/li&gt;
  &lt;li&gt;The quick sort algorithm was improved to avoid degenerating to the worst case.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;python-notes&quot;&gt;Python notes&lt;/h2&gt;

&lt;h3 id=&quot;datasets&quot;&gt;Datasets&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Updated &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pyarrow.dataset&lt;/code&gt; module following the changes in the C++ Datasets
project. This release also adds &lt;a href=&quot;https://arrow.apache.org/docs/python/dataset.html&quot;&gt;richer documentation&lt;/a&gt; on the datasets
module.&lt;/li&gt;
  &lt;li&gt;Support for the improved dataset functionality in
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pyarrow.parquet.read_table/ParquetDataset&lt;/code&gt;. To enable, pass
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;use_legacy_dataset=False&lt;/code&gt;. Among other things, this allows to specify filters
for all columns and not only the partition keys (using row group statistics)
and enables different partitioning schemes. See the “note” in the
&lt;a href=&quot;https://arrow.apache.org/docs/python/parquet.html#reading-from-partitioned-datasets&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ParquetDataset&lt;/code&gt; documentation&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;packaging&quot;&gt;Packaging&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Wheels for Python 3.8 are now available&lt;/li&gt;
  &lt;li&gt;Support for Python 2.7 has been dropped as Python 2.x reached end-of-life in
January 2020.&lt;/li&gt;
  &lt;li&gt;Nightly wheels and conda packages are now available for testing or other
development purposes. See the &lt;a href=&quot;https://arrow.apache.org/docs/python/install.html#installing-nightly-packages&quot;&gt;installation guide&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;other-improvements&quot;&gt;Other improvements&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Conversion to numpy/pandas for FixedSizeList, LargeString, LargeBinary&lt;/li&gt;
  &lt;li&gt;Sparse CSC matrices and Sparse CSF tensors support was added. (ARROW-7419,
ARROW-7427)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;r-notes&quot;&gt;R notes&lt;/h2&gt;

&lt;p&gt;Highlights include support for the Feather V2 format and the C Data Interface,
both described above. Along with low-level bindings for the C interface, this
release adds tooling to work with Arrow data in Python using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;reticulate&lt;/code&gt;. See
&lt;a href=&quot;https://arrow.apache.org/docs/r/articles/python.html&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;vignette(&quot;python&quot;, package = &quot;arrow&quot;)&lt;/code&gt;&lt;/a&gt; for a guide to getting started.&lt;/p&gt;

&lt;p&gt;Installation on Linux now builds C++ the library from source by default. For a
faster, richer build, set the environment variable &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NOT_CRAN=true&lt;/code&gt;. See
&lt;a href=&quot;https://arrow.apache.org/docs/r/articles/install.html&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;vignette(&quot;install&quot;, package = &quot;arrow&quot;)&lt;/code&gt;&lt;/a&gt; for details and more options.&lt;/p&gt;

&lt;p&gt;For more on what’s in the 0.17 R package, see the &lt;a href=&quot;https://arrow.apache.org/docs/r/news/&quot;&gt;R changelog&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;ruby-and-c-glib-notes&quot;&gt;Ruby and C GLib notes&lt;/h2&gt;

&lt;h3 id=&quot;ruby&quot;&gt;Ruby&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Support Ruby 2.3 again&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;c-glib&quot;&gt;C GLib&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Add GArrowRecordBatchIterator&lt;/li&gt;
  &lt;li&gt;Add support for GArrowFilterOptions&lt;/li&gt;
  &lt;li&gt;Add support for Peek() to GIOInputStream&lt;/li&gt;
  &lt;li&gt;Add some metadata bindings to GArrowSchema&lt;/li&gt;
  &lt;li&gt;Add LocalFileSystem support&lt;/li&gt;
  &lt;li&gt;Add support for writer properties of Parquet&lt;/li&gt;
  &lt;li&gt;Add support for MapArray&lt;/li&gt;
  &lt;li&gt;Add support for BooleanNode&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;rust-notes&quot;&gt;Rust notes&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;DictionayArray support.&lt;/li&gt;
  &lt;li&gt;Various improvements to code safety.&lt;/li&gt;
  &lt;li&gt;Filter kernel now supports temporal types.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;rust-parquet-notes&quot;&gt;Rust Parquet notes&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Array reader now supports temporal types.&lt;/li&gt;
  &lt;li&gt;Parquet writer now supports custom meta-data key/value pairs.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;rust-datafusion-notes&quot;&gt;Rust DataFusion notes&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Logical plans can now reference columns by name (as well as by index) using
the new &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;UnresolvedColumn&lt;/code&gt; expression. There is a new optimizer rule to
resolve these into column indices.&lt;/li&gt;
  &lt;li&gt;Scalar UDFs can now be registered with the execution context and used from
logical query plans as well as from SQL. A number of math scalar functions
have been implemented using this feature (sqrt, cos, sin, tan, asin, acos,
atan, floor, ceil, round, trunc, abs, signum, exp, log, log2, log10).&lt;/li&gt;
  &lt;li&gt;Various SQL improvements, including support for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;SELECT *&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;SELECT
COUNT(*)&lt;/code&gt;, and improvements to parsing of aggregate queries.&lt;/li&gt;
  &lt;li&gt;Flight examples are provided, with a client that sends a SQL statement to a
Flight server and receives the results.&lt;/li&gt;
  &lt;li&gt;The interactive SQL command-line tool now has improved documentation and
better formatting of query results.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;project-operations&quot;&gt;Project Operations&lt;/h2&gt;

&lt;p&gt;We’ve continued our migration of general automation toward GitHub Actions. The
majority of our commit-by-commit continuous integration (CI) is now running on
GitHub Actions. We are working on different solutions for using dedicated
hardware as part of our CI. The &lt;a href=&quot;https://buildkite.com/&quot;&gt;Buildkite&lt;/a&gt; self-hosted CI/CD platform is
now supported on Apache repositories and GitHub Actions also supports
self-hosted workers.&lt;/p&gt;</content><author><name>pmc</name></author><category term="release" /><summary type="html">The Apache Arrow team is pleased to announce the 0.17.0 release. This covers over 2 months of development work and includes 569 resolved issues from 79 distinct contributors. See the Install Page to learn how to get the libraries for your platform. The release notes below are not exhaustive and only expose selected highlights of the release. Many other bugfixes and improvements have been made: we refer you to the complete changelog. Community Since the 0.16.0 release, two committers have joined the Project Management Committee (PMC): Neal Richardson François Saint-Jacques Thank you for all your contributions! Columnar Format Notes A C-level Data Interface was designed to ease data sharing inside a single process. It allows different runtimes or libraries to share Arrow data using a well-known binary layout and metadata representation, without any copies. Third party libraries can use the C interface to import and export the Arrow columnar format in-process without requiring on any new code dependencies. The C++ library now includes an implementation of the C Data Interface, and Python and R have bindings to that implementation. Arrow Flight RPC notes Adopted new DoExchange bi-directional data RPC ListFlights supports being passed a Criteria argument in Java/C++/Python. This allows applications to search for flights satisfying a given query. Custom metadata can be attached to errors that the server sends to the client, which can be used to encode richer application-specific information. A number of minor bugs were fixed, including proper handling of empty null arrays in Java and round-tripping of certain Arrow status codes in C++/Python. C++ notes Feather V2 The “Feather V2” format based on the Arrow IPC file format was developed. Feather V2 features full support for all Arrow data types, and resolves the 2GB per-column limitation for large amounts of string data that the original Feather implementation had. Feather V2 also introduces experimental IPC message compression using LZ4 frame format or ZSTD. This will be formalized later in the Arrow format. C++ Datasets Improve speed on high latency file system by relaxing discovery validation Better performance with Arrow IPC files using column projection Add the ability to list files in FileSystemDataset Add support for Parquet file reader options Support dictionary columns in partition expression Fix various crashes and other issues C++ Parquet notes Complete support for writing nested types to Parquet format was completed. The legacy code can be accessed through parquet write option C++ and an environment variable in Python. Read support will come in a future release. The BYTE_STREAM_SPLIT encoding was implemented for floating-point types. It helps improve the efficiency of memory compression for high-entropy data. Expose Parquet schema field_id as Arrow field metadata Support for DataPageV2 data page format C++ build notes We continued to make the core C++ library build simpler and faster. Among the improvements are the removal of the dependency on Thrift IDL compiler at build time; while Parquet still requires the Thrift runtime C++ library, its dependencies are much lighter. We also further reduced the number of build configurations that require Boost, and when Boost is needed to be built, we only download the components we need, reducing the size of the Boost bundle by 90%. Improved support for building on ARM platforms Upgraded LLVM version from 7 to 8 Simplified SIMD build configuration with ARROW_SIMD_LEVEL option allowing no SIMD, SSE4.2, AVX2, or AVX512 to be selected. Fixed a number of bugs affecting compilation on aarch64 platforms Other C++ notes Many crashes on invalid input detected by OSS-Fuzz in the IPC reader and in Parquet-Arrow reading were fixed. See our recent blog post for more details. A “Device” abstraction was added to simplify buffer management and movement across heterogeneous hardware configurations, e.g. CPUs and GPUs. A streaming CSV reader was implemented, yielding individual RecordBatches and helping limit overall memory occupation. Array casting from Decimal128 to integer types and to Decimal128 with different scale/precision was added. Sparse CSF tensors are now supported. When creating an Array, the null bitmap is not kept if the null count is known to be zero Compressor support for the LZ4 frame format (LZ4_FRAME) was added An event-driven interface for reading IPC streams was added. Further core APIs that required passing an explicit out-parameter were migrated to Result&amp;lt;T&amp;gt;. New analytics kernels for match, sort indices / argsort, top-k Java notes Netty dependencies were removed for BufferAllocator and ReferenceManager classes. In the future, we plan to move netty related classes to a separate module. New features were provided to support efficiently appending vector/vector schema root values in batch. Comparing a range of values in dense union vectors has been supported. The quick sort algorithm was improved to avoid degenerating to the worst case. Python notes Datasets Updated pyarrow.dataset module following the changes in the C++ Datasets project. This release also adds richer documentation on the datasets module. Support for the improved dataset functionality in pyarrow.parquet.read_table/ParquetDataset. To enable, pass use_legacy_dataset=False. Among other things, this allows to specify filters for all columns and not only the partition keys (using row group statistics) and enables different partitioning schemes. See the “note” in the ParquetDataset documentation. Packaging Wheels for Python 3.8 are now available Support for Python 2.7 has been dropped as Python 2.x reached end-of-life in January 2020. Nightly wheels and conda packages are now available for testing or other development purposes. See the installation guide Other improvements Conversion to numpy/pandas for FixedSizeList, LargeString, LargeBinary Sparse CSC matrices and Sparse CSF tensors support was added. (ARROW-7419, ARROW-7427) R notes Highlights include support for the Feather V2 format and the C Data Interface, both described above. Along with low-level bindings for the C interface, this release adds tooling to work with Arrow data in Python using reticulate. See vignette(&quot;python&quot;, package = &quot;arrow&quot;) for a guide to getting started. Installation on Linux now builds C++ the library from source by default. For a faster, richer build, set the environment variable NOT_CRAN=true. See vignette(&quot;install&quot;, package = &quot;arrow&quot;) for details and more options. For more on what’s in the 0.17 R package, see the R changelog. Ruby and C GLib notes Ruby Support Ruby 2.3 again C GLib Add GArrowRecordBatchIterator Add support for GArrowFilterOptions Add support for Peek() to GIOInputStream Add some metadata bindings to GArrowSchema Add LocalFileSystem support Add support for writer properties of Parquet Add support for MapArray Add support for BooleanNode Rust notes DictionayArray support. Various improvements to code safety. Filter kernel now supports temporal types. Rust Parquet notes Array reader now supports temporal types. Parquet writer now supports custom meta-data key/value pairs. Rust DataFusion notes Logical plans can now reference columns by name (as well as by index) using the new UnresolvedColumn expression. There is a new optimizer rule to resolve these into column indices. Scalar UDFs can now be registered with the execution context and used from logical query plans as well as from SQL. A number of math scalar functions have been implemented using this feature (sqrt, cos, sin, tan, asin, acos, atan, floor, ceil, round, trunc, abs, signum, exp, log, log2, log10). Various SQL improvements, including support for SELECT * and SELECT COUNT(*), and improvements to parsing of aggregate queries. Flight examples are provided, with a client that sends a SQL statement to a Flight server and receives the results. The interactive SQL command-line tool now has improved documentation and better formatting of query results. Project Operations We’ve continued our migration of general automation toward GitHub Actions. The majority of our commit-by-commit continuous integration (CI) is now running on GitHub Actions. We are working on different solutions for using dedicated hardware as part of our CI. The Buildkite self-hosted CI/CD platform is now supported on Apache repositories and GitHub Actions also supports self-hosted workers.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Fuzzing the Arrow C++ IPC implementation</title><link href="https://arrow.apache.org/blog/2020/03/31/fuzzing-arrow-ipc/" rel="alternate" type="text/html" title="Fuzzing the Arrow C++ IPC implementation" /><published>2020-03-31T19:00:00-04:00</published><updated>2020-03-31T19:00:00-04:00</updated><id>https://arrow.apache.org/blog/2020/03/31/fuzzing-arrow-ipc</id><content type="html" xml:base="https://arrow.apache.org/blog/2020/03/31/fuzzing-arrow-ipc/">&lt;!--

--&gt;

&lt;p&gt;Apache Arrow aims to allow fast and seamless data interchange between
heterogenous runtimes and environments.  Whether using the columnar
&lt;a href=&quot;https://arrow.apache.org/docs/format/Columnar.html&quot;&gt;IPC stream protocol&lt;/a&gt;,
the &lt;a href=&quot;https://arrow.apache.org/docs/format/Flight.html&quot;&gt;Flight&lt;/a&gt; RPC layer,
the Feather file format, the
&lt;a href=&quot;https://arrow.apache.org/docs/python/plasma.html&quot;&gt;Plasma&lt;/a&gt; shared object
store, or any application-specific data distribution mechanism, Arrow IPC
implementations may try to decode data from untrusted input.  While it is ok
to report an error in that case, Arrow shouldn’t crash or engage in risky
behaviour while reading such data.&lt;/p&gt;

&lt;p&gt;To validate the robustness of the Arrow C++ IPC reader (which also underlies
the Python, C/GLib, R and Ruby bindings), we
&lt;a href=&quot;https://github.com/google/oss-fuzz/pull/3233&quot;&gt;successfully submitted&lt;/a&gt;
the Arrow project to OSS-Fuzz, a continuous fuzzing initiative for critical
open source projects, provided by Google.&lt;/p&gt;

&lt;h2 id=&quot;what-is-being-fuzzed&quot;&gt;What is being fuzzed&lt;/h2&gt;

&lt;p&gt;As of this writing, the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;RecordBatchStreamReader&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;RecordBatchFileReader&lt;/code&gt;
C++ classes are being fuzzed by feeding them data generated by the fuzzer.&lt;/p&gt;

&lt;p&gt;When a record batch is successfully read by one of those classes, the
fuzzing setup then validates it using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;RecordBatch::ValidateFull&lt;/code&gt;.  This
method can either succeed or fail, but it shouldn’t crash.&lt;/p&gt;

&lt;p&gt;By ensuring that reading a record batch from IPC, then validating it, always
shows deterministic behaviour, we hope to make it relatively safe to ingest
Arrow IPC data coming from untrusted sources.&lt;/p&gt;

&lt;p&gt;(of course, it is still recommended for security-critical applications
 to use cryptographic means of authentication and integrity control – for
 example, to enable TLS with the Flight RPC protocol)&lt;/p&gt;

&lt;h2 id=&quot;how-we-help-the-fuzzer-find-problems&quot;&gt;How we help the fuzzer find problems&lt;/h2&gt;

&lt;p&gt;Fuzzing is a brute force process that tries to devise invalid data to
exercise an implementation’s response.  By default, the fuzzer does not know
anything about the data representation expected by the program under test.
Fuzzing can therefore be extremely inefficient, testing tons of uninteresting
variations while missing critical ones.&lt;/p&gt;

&lt;p&gt;To help guide the fuzzing process, we added a seed corpus of valid Arrow IPC
files with various data types.  By starting from this data and mutating it to
find invalid variations, OSS-Fuzz was able to find tens of issues with data
validation.  All of them have been fixed.  As of this writing, no new issue
in the IPC layer was found since March 4th 2020.&lt;/p&gt;

&lt;h2 id=&quot;what-comes-next&quot;&gt;What comes next&lt;/h2&gt;

&lt;p&gt;Of course, we still monitor OSS-Fuzz for any new problem that could be found
in the C++ IPC implementation.  Such problems might for example appear when adding
features to the Arrow &lt;a href=&quot;https://arrow.apache.org/docs/format/Columnar.html&quot;&gt;IPC format&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;We have started fuzzing the Parquet C++ implementation.  Several issues have
been found and fixed, but more are still coming.  We hope to stabilize the
situation in the next month or two.&lt;/p&gt;

&lt;p&gt;The tensor and sparse tensor IPC read paths are not being exercised yet.
They will be once a motivated core developer wants to own the topic.&lt;/p&gt;</content><author><name>apitrou</name></author><category term="application" /><summary type="html">Apache Arrow aims to allow fast and seamless data interchange between heterogenous runtimes and environments. Whether using the columnar IPC stream protocol, the Flight RPC layer, the Feather file format, the Plasma shared object store, or any application-specific data distribution mechanism, Arrow IPC implementations may try to decode data from untrusted input. While it is ok to report an error in that case, Arrow shouldn’t crash or engage in risky behaviour while reading such data. To validate the robustness of the Arrow C++ IPC reader (which also underlies the Python, C/GLib, R and Ruby bindings), we successfully submitted the Arrow project to OSS-Fuzz, a continuous fuzzing initiative for critical open source projects, provided by Google. What is being fuzzed As of this writing, the RecordBatchStreamReader and RecordBatchFileReader C++ classes are being fuzzed by feeding them data generated by the fuzzer. When a record batch is successfully read by one of those classes, the fuzzing setup then validates it using RecordBatch::ValidateFull. This method can either succeed or fail, but it shouldn’t crash. By ensuring that reading a record batch from IPC, then validating it, always shows deterministic behaviour, we hope to make it relatively safe to ingest Arrow IPC data coming from untrusted sources. (of course, it is still recommended for security-critical applications to use cryptographic means of authentication and integrity control – for example, to enable TLS with the Flight RPC protocol) How we help the fuzzer find problems Fuzzing is a brute force process that tries to devise invalid data to exercise an implementation’s response. By default, the fuzzer does not know anything about the data representation expected by the program under test. Fuzzing can therefore be extremely inefficient, testing tons of uninteresting variations while missing critical ones. To help guide the fuzzing process, we added a seed corpus of valid Arrow IPC files with various data types. By starting from this data and mutating it to find invalid variations, OSS-Fuzz was able to find tens of issues with data validation. All of them have been fixed. As of this writing, no new issue in the IPC layer was found since March 4th 2020. What comes next Of course, we still monitor OSS-Fuzz for any new problem that could be found in the C++ IPC implementation. Such problems might for example appear when adding features to the Arrow IPC format. We have started fuzzing the Parquet C++ implementation. Several issues have been found and fixed, but more are still coming. We hope to stabilize the situation in the next month or two. The tensor and sparse tensor IPC read paths are not being exercised yet. They will be once a motivated core developer wants to own the topic.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Apache Arrow 0.16.0 Release</title><link href="https://arrow.apache.org/blog/2020/02/12/0.16.0-release/" rel="alternate" type="text/html" title="Apache Arrow 0.16.0 Release" /><published>2020-02-12T01:00:00-05:00</published><updated>2020-02-12T01:00:00-05:00</updated><id>https://arrow.apache.org/blog/2020/02/12/0.16.0-release</id><content type="html" xml:base="https://arrow.apache.org/blog/2020/02/12/0.16.0-release/">&lt;!--

--&gt;

&lt;p&gt;The Apache Arrow team is pleased to announce the 0.16.0 release. This covers
about 4 months of development work and includes &lt;a href=&quot;https://issues.apache.org/jira/issues/?jql=project%20%3D%20ARROW%20AND%20status%20%3D%20Resolved%20AND%20fixVersion%20%3D%200.16.0&quot;&gt;&lt;strong&gt;735 resolved issues&lt;/strong&gt;&lt;/a&gt;
from &lt;a href=&quot;https://arrow.apache.org/release/0.16.0.html#contributors&quot;&gt;&lt;strong&gt;99 distinct contributors&lt;/strong&gt;&lt;/a&gt;.  See the Install Page to learn how to
get the libraries for your platform.&lt;/p&gt;

&lt;p&gt;The release notes below are not exhaustive and only expose selected highlights
of the release.  Many other bugfixes and improvements have been made: we refer
you to the &lt;a href=&quot;https://arrow.apache.org/release/0.16.0.html&quot;&gt;complete changelog&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;new-committers&quot;&gt;New committers&lt;/h2&gt;

&lt;p&gt;Since the 0.15.0 release, we’ve added two new committers:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/eerhardt&quot;&gt;Eric Erhardt&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/jorisvandenbossche&quot;&gt;Joris Van den Bossche&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Thank you for all your contributions!&lt;/p&gt;

&lt;h2 id=&quot;columnar-format-notes&quot;&gt;Columnar Format Notes&lt;/h2&gt;

&lt;p&gt;We still have work to do to complete comprehensive columnar format integration
testing between the Java and C++ libraries. Once this work is completed, we
intend to make a 1.0.0 release with &lt;a href=&quot;https://arrow.apache.org/docs/format/Versioning.html&quot;&gt;forward and backward compatibility
guarantees&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;We &lt;a href=&quot;https://github.com/apache/arrow/commit/0ddc1f4737c35008cd06be1ee28472ebd7da68e2&quot;&gt;clarified some ambiguity&lt;/a&gt; on dictionary encoding in the specification.
Work is on going to implement the features in Arrow libraries.&lt;/p&gt;

&lt;h2 id=&quot;arrow-flight-rpc-notes&quot;&gt;Arrow Flight RPC notes&lt;/h2&gt;

&lt;p&gt;Flight development work has recently focused on robustness and stability. If
you are not yet familiar with Flight, read the &lt;a href=&quot;https://arrow.apache.org/blog/2019/10/13/introducing-arrow-flight/&quot;&gt;introductory blog post from
October&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;We are also discussing adding a “bidirectional RPC” which enables
request-response workflows requiring both client and server to send data
streams to be performed a single RPC request.&lt;/p&gt;

&lt;h2 id=&quot;c-notes&quot;&gt;C++ notes&lt;/h2&gt;

&lt;p&gt;Some work has been done to make the default build configuration of Arrow C++ as
lean as possible. The Arrow C++ core can now be built without any external
dependencies other than a new enough C++ compiler (gcc 4.9 or higher). Notably,
Boost is no longer required. We invested effort to vendor some small essential
dependencies: Flatbuffers, double-conversion, and uriparser. Many optional
features requiring external libraries, like compression and GLog integration,
are now disabled by default. Several subcomponents of the C++ project like the
filesystem API, CSV, compute, dataset and JSON layers, as well as command-line
utilities, are now disabled by default. The only toolchain dependency enabled
by default is jemalloc, the recommended memory allocator, but this can also be
disabled if desired. For illustration, see the &lt;a href=&quot;https://github.com/apache/arrow/blob/master/cpp/examples/minimal_build/build.sh&quot;&gt;example minimal build script and
Dockerfile&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;When enabled, the default jemalloc configuration has been tweaked to return
memory more aggressively to the OS (ARROW-6910, ARROW-6994). We welcome
feedback from users about our memory allocation configuration and performance
in applications.&lt;/p&gt;

&lt;p&gt;The array validation facilities have been vastly expanded and now exist in
two flavors: the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Validate&lt;/code&gt; method does a light-weight validation that’s
O(1) in array size, while the potentially O(N) method &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ValidateFull&lt;/code&gt; does
thorough data validation (ARROW-6157).&lt;/p&gt;

&lt;p&gt;The IO APIs now use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Result&amp;lt;T&amp;gt;&lt;/code&gt; when returning both a Status
and result value, rather than taking a pointer-out function parameter
(ARROW-7235).&lt;/p&gt;

&lt;h3 id=&quot;c-csv&quot;&gt;C++: CSV&lt;/h3&gt;

&lt;p&gt;An option is added to attempt automatic dictionary encoding of string columns
during reading a CSV file, until a cardinality limit is reached. When
successful, it can make reading faster and the resulting Arrow data is
much more memory-efficient (ARROW-3408).&lt;/p&gt;

&lt;p&gt;The CSV APIs now use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Result&amp;lt;T&amp;gt;&lt;/code&gt; when returning both a Status
and result value, rather than taking a pointer-out function parameter
(ARROW-7236).&lt;/p&gt;

&lt;h3 id=&quot;c-datasets&quot;&gt;C++: Datasets&lt;/h3&gt;

&lt;p&gt;The 0.16 release introduces the Datasets API to the C++ library, along with
bindings in Python and R.  This API allows you to treat multiple files as a
single logical dataset entity and make efficient selection queries against it.
This release includes support for Parquet and Arrow IPC file formats.  Factory
objects allow you to discover files in a directory recursively, inspect the
schemas in the files, and performs some basic schema unification.  You may
specify how file path segments map to partition, and there is support for
auto-detecting some partition information, including Hive-style partitioning.
The Datasets API includes a filter expression syntax as well as column
selection.  These are evaluated with predicate pushdown, and for Parquet,
evaluation is pushed down to row groups.&lt;/p&gt;

&lt;h3 id=&quot;c-filesystem-layer&quot;&gt;C++: Filesystem layer&lt;/h3&gt;

&lt;p&gt;An HDFS implementation of the FileSystem class is available (ARROW-6720). We
plan to deprecate the prior bespoke C++ HDFS class in favor of the standardized
filesystem API.&lt;/p&gt;

&lt;p&gt;The filesystem APIs now use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Result&amp;lt;T&amp;gt;&lt;/code&gt; when returning both a Status
and result value, rather than taking a pointer-out function parameter
(ARROW-7161).&lt;/p&gt;

&lt;h3 id=&quot;c-ipc&quot;&gt;C++: IPC&lt;/h3&gt;

&lt;p&gt;The Arrow IPC reader is being fuzzed continuously by the &lt;a href=&quot;https://google.github.io/oss-fuzz/&quot;&gt;OSS-Fuzz&lt;/a&gt;
infrastructure, to detect undesirable behavior on invalid or malicious input.
Several issues have already been found and fixed.&lt;/p&gt;

&lt;h3 id=&quot;c-parquet&quot;&gt;C++: Parquet&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/apache/parquet-format/blob/master/Encryption.md&quot;&gt;Modular encryption&lt;/a&gt; is now supported (PARQUET-1300).&lt;/p&gt;

&lt;p&gt;A performance regression when reading a file with a large number of columns has
been fixed (ARROW-6876, ARROW-7059), as well as several bugs (PARQUET-1766,
ARROW-6895).&lt;/p&gt;

&lt;h3 id=&quot;c-tensors&quot;&gt;C++: Tensors&lt;/h3&gt;

&lt;p&gt;CSC sparse matrices are supported (ARROW-4225).&lt;/p&gt;

&lt;p&gt;The Tensor APIs now use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Result&amp;lt;T&amp;gt;&lt;/code&gt; when returning both a Status
and result value, rather than taking a pointer-out function parameter
(ARROW-7420).&lt;/p&gt;

&lt;h2 id=&quot;c-notes-1&quot;&gt;C# Notes&lt;/h2&gt;

&lt;p&gt;There were a number of C# bug fixes this release. Note that the C# library is
not yet being tested in CI against the other native Arrow implementations
(integration tests). We are looking for more contributors for the C# project to
help with this and other new feature development.&lt;/p&gt;

&lt;h2 id=&quot;java-notes&quot;&gt;Java notes&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Added prose documentation describing how to work with the Java libraries.&lt;/li&gt;
  &lt;li&gt;Some additional algorithms have been added to the “contrib” algorithms package:  multithreaded searching of ValueVectors,&lt;/li&gt;
  &lt;li&gt;The memory modules have been refactored so non-netty allocators can now be used.&lt;/li&gt;
  &lt;li&gt;A new utility for populating ValueVectors  more concisely for testing was introduced&lt;/li&gt;
  &lt;li&gt;The “contrib” Avro adapter now supports Avro Logical type conversion to corresponding Arrow type.&lt;/li&gt;
  &lt;li&gt;Various bug fixes across all packages&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;python-notes&quot;&gt;Python notes&lt;/h2&gt;

&lt;p&gt;pyarrow 0.16 will be the last release to support Python 2.7.&lt;/p&gt;

&lt;p&gt;Python now has bindings for the datasets API (ARROW-6341) as well as the S3
(ARROW-6655) and HDFS (ARROW-7310) filesystem implementations.&lt;/p&gt;

&lt;p&gt;The Duration (ARROW-5855) and Fixed Size List (ARROW-7261) types are exposed
in Python.&lt;/p&gt;

&lt;p&gt;Sparse tensors can be converted to dense tensors (ARROW-6624).  They are
also interoperable with the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pydata/sparse&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;scipy.sparse&lt;/code&gt; libraries
(ARROW-4223, ARROW-4224).&lt;/p&gt;

&lt;p&gt;Pandas extension arrays now are able to roundtrip through Arrow conversion
(ARROW-2428).&lt;/p&gt;

&lt;p&gt;A memory leak when converting Arrow data to Pandas “object” data has been
fixed (ARROW-6874).&lt;/p&gt;

&lt;p&gt;Arrow is now tested against Python 3.8, and we now build manylinux2014 wheels
for Python 3 (ARROW-7344).&lt;/p&gt;

&lt;h2 id=&quot;r-notes&quot;&gt;R notes&lt;/h2&gt;

&lt;p&gt;This release includes a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;dplyr&lt;/code&gt; interface to Arrow Datasets,
which let you work efficiently with large, multi-file datasets as a single entity.
See &lt;a href=&quot;https://arrow.apache.org/docs/r/articles/dataset.html&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;vignette(&quot;dataset&quot;, package = &quot;arrow&quot;)&lt;/code&gt;&lt;/a&gt; for more.&lt;/p&gt;

&lt;p&gt;Another major area of work in this release was to improve the installation
experience on Linux. A source package installation (as from CRAN) will now
handle its C++ dependencies automatically, with no system dependencies beyond what R requires.
For common Linux distributions and versions, installation will retrieve a prebuilt static
C++ library for inclusion in the package; where this binary is not available,
the package executes a bundled script that should build the Arrow C++ library.
See &lt;a href=&quot;https://arrow.apache.org/docs/r/articles/install.html&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;vignette(&quot;install&quot;, package = &quot;arrow&quot;)&lt;/code&gt;&lt;/a&gt; for details.&lt;/p&gt;

&lt;p&gt;For more on what’s in the 0.16 R package, see the R &lt;a href=&quot;https://arrow.apache.org/docs/r/news/&quot;&gt;changelog&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;ruby-and-c-glib-notes&quot;&gt;Ruby and C GLib notes&lt;/h2&gt;

&lt;p&gt;Ruby and C GLib continues to follow the features in the C++ project.&lt;/p&gt;

&lt;h3 id=&quot;ruby&quot;&gt;Ruby&lt;/h3&gt;

&lt;p&gt;Ruby includes the following improvements.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Improve CSV save performance (ARROW-7474).&lt;/li&gt;
  &lt;li&gt;Add support for saving/loading TSV (ARROW-7454).&lt;/li&gt;
  &lt;li&gt;Add &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Arrow::Schema#build_expression&lt;/code&gt; to improve building &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Gandiva::Expression&lt;/code&gt; (ARROW-6619).&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;c-glib&quot;&gt;C GLib&lt;/h3&gt;

&lt;p&gt;C GLib includes the following changes.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Add support for LargeList, LargeBinary, and LargeString (ARROW-6285, ARROW-6286).&lt;/li&gt;
  &lt;li&gt;Add filter and take API for GArrowTable, GArrowChunkedArray, and GArrowRecordBatch (ARROW-7110, ARROW-7111).&lt;/li&gt;
  &lt;li&gt;Add &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;garrow_table_combine_chunks()&lt;/code&gt; API (ARROW-7369).&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;rust-notes&quot;&gt;Rust notes&lt;/h2&gt;

&lt;p&gt;Support for Arrow data types has been improved, with the following array types now supported (ARROW-3690):&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Fixed Size List and Fixed Size Binary&lt;/li&gt;
  &lt;li&gt;Adding a String Array for utf-8 strings, and keeping the Binary Array for general binary data&lt;/li&gt;
  &lt;li&gt;Duration and interval arrays.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Initial work on Arrow IPC support has been completed, with readers and writers for streams and files implemented (ARROW-5180).&lt;/p&gt;

&lt;h3 id=&quot;rust-datafusion&quot;&gt;Rust: DataFusion&lt;/h3&gt;

&lt;p&gt;Query execution has been reimplemented with an extensible physical query plan. This allows other projects to add other plans, such as for distributed computing or for specific database servers (ARROW-5227).&lt;/p&gt;

&lt;p&gt;Added support for writing query results to CSV (ARROW-6274).&lt;/p&gt;

&lt;p&gt;The new Parquet -&amp;gt; Arrow reader is now used to read Parquet files (ARROW-6700).&lt;/p&gt;

&lt;p&gt;Various other query improvements have been implemented, especially on grouping and aggregate queries (ARROW-6689).&lt;/p&gt;

&lt;h3 id=&quot;rust-parquet&quot;&gt;Rust: Parquet&lt;/h3&gt;

&lt;p&gt;The Arrow reader integration has been completed, allowing Parquet files to be
read into Arrow memory (ARROW-4059).&lt;/p&gt;

&lt;h2 id=&quot;development-notes&quot;&gt;Development notes&lt;/h2&gt;

&lt;p&gt;Arrow has moved away from Travis-CI and is now using Github Actions for
PR-based continuous integration.  This new CI configuration relies heavily
on &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;docker-compose&lt;/code&gt;, making it easier for developers to reproduce builds
locally, thanks to tremendous work by Krisztián Szűcs (ARROW-7101).&lt;/p&gt;

&lt;h2 id=&quot;community-discussions-ongoing&quot;&gt;Community Discussions Ongoing&lt;/h2&gt;

&lt;p&gt;There are a number of active discussions ongoing on the developer
dev@arrow.apache.org mailing list. We look forward to hearing from the
community there.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Mandatory fields in IPC format: to ease input validation, it is being
&lt;a href=&quot;https://mail-archives.apache.org/mod_mbox/arrow-dev/202001.mbox/%3C0dd13489-9221-459a-3560-1426738d3bb4%40python.org%3E&quot;&gt;proposed&lt;/a&gt; to mark some fields in our Flatbuffers schema “required”.
Those fields are already semantically required, but are not considered so by
the generated Flatbuffers verifier.  Before accepting this proposal, we need
to ensure that it does not break binary compatibility with existing valid
data.&lt;/li&gt;
  &lt;li&gt;The C Data Interface has not yet been formally adopted, though the community
has reached consensus to move forward after addressing various design
questions and concerns.&lt;/li&gt;
  &lt;li&gt;Guidelines for the use of “unsafe” in the Rust implementation are being
&lt;a href=&quot;https://mail-archives.apache.org/mod_mbox/arrow-dev/202001.mbox/%3cMN2PR19MB347050FB22046485C0B2525EDD380@MN2PR19MB3470.namprd19.prod.outlook.com%3e&quot;&gt;discussed&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;</content><author><name>pmc</name></author><category term="release" /><summary type="html">The Apache Arrow team is pleased to announce the 0.16.0 release. This covers about 4 months of development work and includes 735 resolved issues from 99 distinct contributors. See the Install Page to learn how to get the libraries for your platform. The release notes below are not exhaustive and only expose selected highlights of the release. Many other bugfixes and improvements have been made: we refer you to the complete changelog. New committers Since the 0.15.0 release, we’ve added two new committers: Eric Erhardt Joris Van den Bossche Thank you for all your contributions! Columnar Format Notes We still have work to do to complete comprehensive columnar format integration testing between the Java and C++ libraries. Once this work is completed, we intend to make a 1.0.0 release with forward and backward compatibility guarantees. We clarified some ambiguity on dictionary encoding in the specification. Work is on going to implement the features in Arrow libraries. Arrow Flight RPC notes Flight development work has recently focused on robustness and stability. If you are not yet familiar with Flight, read the introductory blog post from October. We are also discussing adding a “bidirectional RPC” which enables request-response workflows requiring both client and server to send data streams to be performed a single RPC request. C++ notes Some work has been done to make the default build configuration of Arrow C++ as lean as possible. The Arrow C++ core can now be built without any external dependencies other than a new enough C++ compiler (gcc 4.9 or higher). Notably, Boost is no longer required. We invested effort to vendor some small essential dependencies: Flatbuffers, double-conversion, and uriparser. Many optional features requiring external libraries, like compression and GLog integration, are now disabled by default. Several subcomponents of the C++ project like the filesystem API, CSV, compute, dataset and JSON layers, as well as command-line utilities, are now disabled by default. The only toolchain dependency enabled by default is jemalloc, the recommended memory allocator, but this can also be disabled if desired. For illustration, see the example minimal build script and Dockerfile. When enabled, the default jemalloc configuration has been tweaked to return memory more aggressively to the OS (ARROW-6910, ARROW-6994). We welcome feedback from users about our memory allocation configuration and performance in applications. The array validation facilities have been vastly expanded and now exist in two flavors: the Validate method does a light-weight validation that’s O(1) in array size, while the potentially O(N) method ValidateFull does thorough data validation (ARROW-6157). The IO APIs now use Result&amp;lt;T&amp;gt; when returning both a Status and result value, rather than taking a pointer-out function parameter (ARROW-7235). C++: CSV An option is added to attempt automatic dictionary encoding of string columns during reading a CSV file, until a cardinality limit is reached. When successful, it can make reading faster and the resulting Arrow data is much more memory-efficient (ARROW-3408). The CSV APIs now use Result&amp;lt;T&amp;gt; when returning both a Status and result value, rather than taking a pointer-out function parameter (ARROW-7236). C++: Datasets The 0.16 release introduces the Datasets API to the C++ library, along with bindings in Python and R. This API allows you to treat multiple files as a single logical dataset entity and make efficient selection queries against it. This release includes support for Parquet and Arrow IPC file formats. Factory objects allow you to discover files in a directory recursively, inspect the schemas in the files, and performs some basic schema unification. You may specify how file path segments map to partition, and there is support for auto-detecting some partition information, including Hive-style partitioning. The Datasets API includes a filter expression syntax as well as column selection. These are evaluated with predicate pushdown, and for Parquet, evaluation is pushed down to row groups. C++: Filesystem layer An HDFS implementation of the FileSystem class is available (ARROW-6720). We plan to deprecate the prior bespoke C++ HDFS class in favor of the standardized filesystem API. The filesystem APIs now use Result&amp;lt;T&amp;gt; when returning both a Status and result value, rather than taking a pointer-out function parameter (ARROW-7161). C++: IPC The Arrow IPC reader is being fuzzed continuously by the OSS-Fuzz infrastructure, to detect undesirable behavior on invalid or malicious input. Several issues have already been found and fixed. C++: Parquet Modular encryption is now supported (PARQUET-1300). A performance regression when reading a file with a large number of columns has been fixed (ARROW-6876, ARROW-7059), as well as several bugs (PARQUET-1766, ARROW-6895). C++: Tensors CSC sparse matrices are supported (ARROW-4225). The Tensor APIs now use Result&amp;lt;T&amp;gt; when returning both a Status and result value, rather than taking a pointer-out function parameter (ARROW-7420). C# Notes There were a number of C# bug fixes this release. Note that the C# library is not yet being tested in CI against the other native Arrow implementations (integration tests). We are looking for more contributors for the C# project to help with this and other new feature development. Java notes Added prose documentation describing how to work with the Java libraries. Some additional algorithms have been added to the “contrib” algorithms package: multithreaded searching of ValueVectors, The memory modules have been refactored so non-netty allocators can now be used. A new utility for populating ValueVectors more concisely for testing was introduced The “contrib” Avro adapter now supports Avro Logical type conversion to corresponding Arrow type. Various bug fixes across all packages Python notes pyarrow 0.16 will be the last release to support Python 2.7. Python now has bindings for the datasets API (ARROW-6341) as well as the S3 (ARROW-6655) and HDFS (ARROW-7310) filesystem implementations. The Duration (ARROW-5855) and Fixed Size List (ARROW-7261) types are exposed in Python. Sparse tensors can be converted to dense tensors (ARROW-6624). They are also interoperable with the pydata/sparse and scipy.sparse libraries (ARROW-4223, ARROW-4224). Pandas extension arrays now are able to roundtrip through Arrow conversion (ARROW-2428). A memory leak when converting Arrow data to Pandas “object” data has been fixed (ARROW-6874). Arrow is now tested against Python 3.8, and we now build manylinux2014 wheels for Python 3 (ARROW-7344). R notes This release includes a dplyr interface to Arrow Datasets, which let you work efficiently with large, multi-file datasets as a single entity. See vignette(&quot;dataset&quot;, package = &quot;arrow&quot;) for more. Another major area of work in this release was to improve the installation experience on Linux. A source package installation (as from CRAN) will now handle its C++ dependencies automatically, with no system dependencies beyond what R requires. For common Linux distributions and versions, installation will retrieve a prebuilt static C++ library for inclusion in the package; where this binary is not available, the package executes a bundled script that should build the Arrow C++ library. See vignette(&quot;install&quot;, package = &quot;arrow&quot;) for details. For more on what’s in the 0.16 R package, see the R changelog. Ruby and C GLib notes Ruby and C GLib continues to follow the features in the C++ project. Ruby Ruby includes the following improvements. Improve CSV save performance (ARROW-7474). Add support for saving/loading TSV (ARROW-7454). Add Arrow::Schema#build_expression to improve building Gandiva::Expression (ARROW-6619). C GLib C GLib includes the following changes. Add support for LargeList, LargeBinary, and LargeString (ARROW-6285, ARROW-6286). Add filter and take API for GArrowTable, GArrowChunkedArray, and GArrowRecordBatch (ARROW-7110, ARROW-7111). Add garrow_table_combine_chunks() API (ARROW-7369). Rust notes Support for Arrow data types has been improved, with the following array types now supported (ARROW-3690): Fixed Size List and Fixed Size Binary Adding a String Array for utf-8 strings, and keeping the Binary Array for general binary data Duration and interval arrays. Initial work on Arrow IPC support has been completed, with readers and writers for streams and files implemented (ARROW-5180). Rust: DataFusion Query execution has been reimplemented with an extensible physical query plan. This allows other projects to add other plans, such as for distributed computing or for specific database servers (ARROW-5227). Added support for writing query results to CSV (ARROW-6274). The new Parquet -&amp;gt; Arrow reader is now used to read Parquet files (ARROW-6700). Various other query improvements have been implemented, especially on grouping and aggregate queries (ARROW-6689). Rust: Parquet The Arrow reader integration has been completed, allowing Parquet files to be read into Arrow memory (ARROW-4059). Development notes Arrow has moved away from Travis-CI and is now using Github Actions for PR-based continuous integration. This new CI configuration relies heavily on docker-compose, making it easier for developers to reproduce builds locally, thanks to tremendous work by Krisztián Szűcs (ARROW-7101). Community Discussions Ongoing There are a number of active discussions ongoing on the developer dev@arrow.apache.org mailing list. We look forward to hearing from the community there. Mandatory fields in IPC format: to ease input validation, it is being proposed to mark some fields in our Flatbuffers schema “required”. Those fields are already semantically required, but are not considered so by the generated Flatbuffers verifier. Before accepting this proposal, we need to ensure that it does not break binary compatibility with existing valid data. The C Data Interface has not yet been formally adopted, though the community has reached consensus to move forward after addressing various design questions and concerns. Guidelines for the use of “unsafe” in the Rust implementation are being discussed.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Apache Arrow Flightの紹介：高速データトランスポートフレームワーク</title><link href="https://arrow.apache.org/blog/2019/10/13/introducing-arrow-flight-japanese/" rel="alternate" type="text/html" title="Apache Arrow Flightの紹介：高速データトランスポートフレームワーク" /><published>2019-10-13T02:00:00-04:00</published><updated>2019-10-13T02:00:00-04:00</updated><id>https://arrow.apache.org/blog/2019/10/13/introducing-arrow-flight-japanese</id><content type="html" xml:base="https://arrow.apache.org/blog/2019/10/13/introducing-arrow-flight-japanese/">&lt;!--

--&gt;

&lt;p&gt;この1.5年、Apache Arrowコミュニティーは&lt;strong&gt;Flight&lt;/strong&gt;の設計と実装を進めてきました。Flightは高速なデータトランスポートを実現するための新しいクライアント・サーバー型のフレームワークです。Flightを使うとネットワーク越しに大きなデータセットを送る処理を簡単に実現できます。Flightは特定用途向けに設計されたものではないため、幅広い用途で利用できます。&lt;/p&gt;

&lt;p&gt;Flightの実装は、まず、&lt;a href=&quot;https://grpc.io/&quot;&gt;gRPC&lt;/a&gt;を使ったArrow列指向フォーマット（つまり「Arrowレコードバッチ」）のトランスポートの最適化に注力しました。gRPCはGoogleが開発しているHTTP/2ベースのRPCライブラリー・フレームワークで、広く利用されています。gRPCも特定用途向けではなく幅広い用途で使えるように設計されています。これまでFlightをgRPCベースで実装することに注力してきましたが、gRPCでだけ使えるようにしたいわけではありません。&lt;/p&gt;

&lt;p&gt;Flightと他のデータトランスポートフレームワークとの大きな違いは並列転送機能です。クライアントとサーバークラスター間で同時にデータをストリームで転送できます。この機能により、簡単にスケーラブルなデータサービスを開発できます。スケーラブルなデータサービスとはクライアント数が増えても大丈夫なサービスです。&lt;/p&gt;

&lt;p&gt;Apache Arrow 0.15.0でC++（Pytonバインディングあり）とJavaでFlightを使えるようになっています。 現時点ではベータユーザー向けです。ベータユーザーとはFlight内部の低レベルの改良によりAPIやプロトコルが変わっても適応できるユーザーのことです。&lt;/p&gt;

&lt;h2 id=&quot;モチベーション&quot;&gt;モチベーション&lt;/h2&gt;

&lt;p&gt;多くの人がネットワーク越しに大きなデータセットにアクセスすることに関して困っています。リモートのデータサービスからデータセットを読むためのさまざまな転送プロトコルやツールがたくさんあります。たとえばODBCやJDBCです。この10年、ファイルベースでデータを保管することが多くなりました。このときにはCSVやAvroやParquetといったフォーマットがよく使われます。しかし、この方法ではデシリアライズする前に生データをローカルのホストに転送しなければいけないという問題があります。&lt;/p&gt;

&lt;p&gt;Apache Arrowの初期からやってきた作業によりさまざまな方法でデータトランスポートを加速できます。&lt;a href=&quot;https://github.com/apache/arrow/blob/master/docs/source/format/Columnar.rst&quot;&gt;Arrow列指向フォーマット&lt;/a&gt;には次の重要な機能があります。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;表形式データの「転送用の」表現です。この表現はデータ受信側でデシリアライズが必要ありません。&lt;/li&gt;
  &lt;li&gt;標準で「バッチをストリーム送信」するためのモードがあります。このモードでは、大きなデータセットを複数の行ごとにまとめて転送します。（Arrowの用語では「レコードバッチ」と呼んでいます。）この記事では「データストリーム」について話します。データストリームとはApache Arrowプロジェクトのバイナリープロトコルを使った一連のArrowレコードバッチです。&lt;/li&gt;
  &lt;li&gt;このフォーマットはプログラミング言語に依存していません。このフォーマットは現在11のプログラミング言語がサポートしています。サポートしているプログラミング言語は増え続けています。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;ODBCのような標準的なプロトコルの各実装は、通常、それぞれ独自の転送用バイナリープロトコルを実装します。これらのプロトコルは各ライブラリーの公開インターフェイスの表現と相互に変換しなければいけません。ODBC・JDBCライブラリーのパフォーマンスは場合によって大きく異なります。&lt;/p&gt;

&lt;p&gt;私たちのFlightの設計で目指していることは、データサービス用の新しいプロトコルを作ることです。このプロトコルは転送用のデータ表現にも開発者向けの公開APIにもArrow列指向フォーマットを使います。こうすることで、データトランスポート関連のシリアライズコストを減らし、分散データシステム全体を効率化できます。さらに、すでに別の用途にApache Arrowを使っているシステム間では非常に効率的にデータをやりとりできます。&lt;/p&gt;

&lt;h2 id=&quot;flightの基礎&quot;&gt;Flightの基礎&lt;/h2&gt;

&lt;p&gt;Arrow Flightライブラリーはデータストリームを送受信できるサービスを実装するための開発者向けフレームワークを提供します。Flightサーバーは次の基本的なリクエストをサポートしています。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Handshake&lt;/strong&gt;：クライアントが認証済みかを確認するシンプルなリクエスト。いくつかのケースでは、以降のリクエストのために実装定義のセッショントークンを確立します。&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;ListFlights&lt;/strong&gt;：利用可能なデータストリームのリストを返します。&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;GetSchema&lt;/strong&gt;：データストリームのスキーマを返します。&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;GetFlightInfo&lt;/strong&gt;：対象のデータセット用の「アクセスプラン」を返します。複数のデータストリームを消費しなければいけないかもしれません。このリクエストにはシリアライズしたカスタムコマンドを含めることができます。たとえば、アプリケーション固有のパラメーターを含めることができます。&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;DoGet&lt;/strong&gt;：クライアントにデータストリームを送信します。&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;DoPut&lt;/strong&gt;: クライアントからデータストリームを受信します。&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;DoAction&lt;/strong&gt;：実装依存のアクションを実行し、結果を返します。つまり、一般的な関数呼び出しです。&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;ListActions&lt;/strong&gt;：利用可能なアクションの種類を返します。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;gRPCの「双方向の」ストリーミングサポート（&lt;a href=&quot;https://grpc.io/docs/guides/concepts/&quot;&gt;HTTP/2ストリーミング&lt;/a&gt;上に実装されています）を活用して、リクエスト処理中でもデータとメタデータをクライアント・サーバー間でやりとりできます。&lt;/p&gt;

&lt;p&gt;単純なFlightの構成は1台のサーバーとそのサーバーに接続し&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;DoGet&lt;/code&gt;リクエストをするクライアントという構成です。&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;/img/20191014_flight_simple.png&quot; alt=&quot;Flight Simple Architecture&quot; width=&quot;50%&quot; class=&quot;img-responsive&quot; /&gt;
&lt;/div&gt;

&lt;h2 id=&quot;grpcごしのデータスループットの最適化&quot;&gt;gRPCごしのデータスループットの最適化&lt;/h2&gt;

&lt;p&gt;gRPCのような汎用メッセージングライブラリーを使うことには多くの利点があります。汎用ライブラリーはすでに多数の問題を解決しているからです。gRPCの場合はGoogleが多数の問題を解決していました。しかし、大きなデータセットのトランスポート性能を改善するためにいくつかの処理を改善する必要がありました。多くのgRPCユーザーは比較的小さなメッセージしか扱っていないからです。&lt;/p&gt;

&lt;p&gt;一番よくサポートされているgRPCを使う方法はサービスを&lt;a href=&quot;https://github.com/protocolbuffers/protobuf&quot;&gt;Protocol Buffers&lt;/a&gt;（「Protobuf」と呼ばれることもあります）の&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;.proto&lt;/code&gt;ファイルで定義する方法です。gRPCのProtobufプラグインはgRPCサービスのスタブを生成します。このスタブを使ってアプリケーションを実装します。RPCコマンドとデータメッセージは&lt;a href=&quot;https://developers.google.com/protocol-buffers/docs/encoding&quot;&gt;Protobufワイヤーフォーマット&lt;/a&gt;を使ってシリアライズします。Flightでは「普通のgRPCとProtocol Buffers」を使っているので、Arrow列指向フォーマットのことを知らないgRPCクライアントでもFlightサービスとやりとりできますし、Arrowデータの中身を気にせずに処理できます。&lt;/p&gt;

&lt;p&gt;Flightの中の主要なデータ関連のProtobufの型は&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;FlightData&lt;/code&gt;と呼ばれています。一般的にProtobufメッセージの読み書きにはコストがかかります。そのため、C++でもJavaでもgRPCにいくつか次のような低レベルの最適化を実装しています。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;FlightData&lt;/code&gt;用のProtobufワイヤーフォーマットを生成します。&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;FlightData&lt;/code&gt;には送信対象のArrowレコードバッチが含まれていますが、メモリーコピー・シリアライズ処理は一切ありません。&lt;/li&gt;
  &lt;li&gt;Protobufで表現された&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;FlightData&lt;/code&gt;からメモリーコピー・デシリアライズ処理なしでArrowレコードバッチを再構築できます。実際には、Protocol Bufersライブラリーにエンコードされたデータペイロードを触らせないようにしています。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Protobufを使うがProtobufのメッセージパースのオーバーヘッドはなくしたいという両立できない2つのことを両立させようとしているということです。Flight実装は上述の最適化をして高速化しています。素のgRPCクライアントでもFlightサービスとやりとりできますが、素のgRPCクライアントにはこのような最適化はないので、Protobufライブラリーを使って&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;FlightData&lt;/code&gt;をデシリアライズすることになります。そのため、素のgRPCクライアントを使うといくらか性能が落ちます。&lt;/p&gt;

&lt;p&gt;FlightのC++実装でのデータスループットベンチマークの結果での絶対的な性能ですが、どちらもローカルホストで動いているサーバー・クライアント間のTCPスループットは2-3GB/sを上回っていました。ただし、TLSは無効にした状態です。このベンチマークは約4秒で12GBのデータを転送できることを示しています。&lt;/p&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;./arrow-flight-benchmark &lt;span class=&quot;nt&quot;&gt;--records_per_stream&lt;/span&gt; 100000000
Bytes &lt;span class=&quot;nb&quot;&gt;read&lt;/span&gt;: 12800000000
Nanos: 3900466413
Speed: 3129.63 MB/s
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;この結果から次の2つのことを言えます。1つはFlightとgRPCを使うと相対的に小さなオーバーヘッドはあるということです。もう1つは多くの実際のFlightアプリケーションではネットワークの帯域がボトルネックになりそうということです。&lt;/p&gt;

&lt;h2 id=&quot;水平方向のスケーラビリティ並列データアクセスとパーティション化したデータアクセス&quot;&gt;水平方向のスケーラビリティ：並列データアクセスとパーティション化したデータアクセス&lt;/h2&gt;

&lt;p&gt;分散型のデータベースシステムの多くは「コーディネーター」を通してクライアントのリクエストを処理するアーキテクチャーパターンを使っています。クライアントへのデータセットを複数回転送するという明らかに効率に課題がある点はさておき、巨大なデータセットへのアクセスに対するスケーラビリティの問題もあります。&lt;/p&gt;

&lt;p&gt;Flightで次のようなシステムを作れるようにしました。それはこのようなボトルネックに取り組まずに水平方向にスケーラブルなデータサービスを作れるシステムです。&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;GetFlightInfo&lt;/code&gt; RPCを使ったクライアントのリクエストは &lt;strong&gt;エンドポイント&lt;/strong&gt; のリストを返します。返ってくる各エンドポイントにはサーバーの位置と &lt;strong&gt;チケット&lt;/strong&gt; の情報が入っています。チケットはデータセットの一部を取得する&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;DoGet&lt;/code&gt;リクエストに入れてサーバーに送ります。データセット全体にアクセスするためにはすべてのエンドポイントを処理する必要があります。どのエンドポイントのFlightのストリームから処理しなければいけないということはありません。どのエンドポイントのFlightのストリームから処理しても構いません。しかし、特定の順序で処理するための仕組みは用意しています。その仕組みとはアプリケーション固有のメタデータを使えるという仕組みです。順序の情報はメタデータで表現できます。&lt;/p&gt;

&lt;p&gt;この複数エンドポイントパターンにはたくさんの利点があります。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;複数のクライアントが複数のエンドポイントから並列にデータを読み込めます。&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;GetFlightInfo&lt;/code&gt;「プランニング」リクエストを提供するサービスは兄弟サービスに処理を移譲できます。これにより、データの局所性の利点を得られたり、単純にロードバランスしやすくなったりします。&lt;/li&gt;
  &lt;li&gt;分散クラスター中のノードは異なる役割を引き受けることができます。たとえば、クラスター内の一部のノードはクエリープランニングに責任を持つかもしれません。一方、他のノードはデータストリームリクエスト（&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;DoGet&lt;/code&gt;または&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;DoPut&lt;/code&gt;）だけを処理するかもしれません。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;次の図はサービスの役割を分けた複数ノードアーキテクチャーの例です。&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;/img/20191014_flight_complex.png&quot; alt=&quot;Flight Complex Architecture&quot; width=&quot;60%&quot; class=&quot;img-responsive&quot; /&gt;
&lt;/div&gt;

&lt;h2 id=&quot;アクションアプリケーション固有のロジックでflightを拡張&quot;&gt;アクション：アプリケーション固有のロジックでFlightを拡張&lt;/h2&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;GetFlightInfo&lt;/code&gt;リクエストはデータセットをリクエストするときにシリアライズしたコマンドを中身を気にせずに送ることができますが、クライアントはデータストリームの送受信以外の操作をサーバーに依頼できなければいけないかもしれません。たとえば、クライアントは特定のデータセットをメモリー上に「ピン止め」することを要求するかもしれません。ピン止めすることで他のクライアントからの後続のリクエストを高速に処理できます。&lt;/p&gt;

&lt;p&gt;Flightサービスは追加で「アクション」を定義できます。&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;DoAction&lt;/code&gt; RPCでアクションを実行できます。アクションリクエストには実行したいアクションの名前と追加情報が入っています。追加情報は省略可能です。アクションの結果はgRPCストリームです。このgRPCストリーム中には任意の結果を入れられます。&lt;/p&gt;

&lt;p&gt;いくつかアクションの例を紹介します。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;メタデータを見つけるアクション。組み込みの&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ListFlights&lt;/code&gt; RPCでも提供されている機能ですが、&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ListFlights&lt;/code&gt;の機能で不十分な場合はアクションで実現できます。&lt;/li&gt;
  &lt;li&gt;セッション固有のパラメーターを設定するアクション。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;サーバーはアクションを1つも実装しなくてもよいことに注意してください。また、アクションは結果を返さなくてもよいです。&lt;/p&gt;

&lt;h2 id=&quot;暗号化と認証&quot;&gt;暗号化と認証&lt;/h2&gt;

&lt;p&gt;Flightは組み込みで暗号化をサポートしています。gRPCの組み込みのTLS/OpenSSLの機能を使っています。&lt;/p&gt;

&lt;p&gt;クライアント側・サーバー側ともに拡張可能な認証ハンドラーがあります。この認証ハンドラーを使えば、ユーザー名とパスワードのようなシンプルな認証スキーマも使えますし、ケルベロスのような複雑な認証も使えます。Flightプロトコルには組み込みの&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;BasicAuth&lt;/code&gt;機能がついています。そのため、追加の開発なしでそのままユーザー名とパスワードの認証を実現できます。&lt;/p&gt;

&lt;h2 id=&quot;ミドルウェアとトレース&quot;&gt;ミドルウェアとトレース&lt;/h2&gt;

&lt;p&gt;gRPCには「インターセプター」というコンセプトがあります。インターセプターを使うと開発者が定義した「ミドルウェア」を開発できます。ミドルウェアを使うと届いたリクエストと送るリクエストに介在することができます。このような処理をするフレームワークに&lt;a href=&quot;https://opentracing.io/&quot;&gt;OpenTracing&lt;/a&gt;があります。&lt;/p&gt;

&lt;p&gt;ミドルウェアの機能は最近Flightに追加された機能です。そのため、今のところはmasterブランチでしか使えません。&lt;/p&gt;

&lt;h2 id=&quot;grpcを使っているがgrpcだけではない&quot;&gt;gRPCを使っているがgRPCだけではない&lt;/h2&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;DoGet&lt;/code&gt;リクエストでサーバーの位置を指定する方法にはRFC 3986準拠のURIを使っています。たとえば、TLSを使ったgRPCは&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;grpc+tls://$HOST:$PORT&lt;/code&gt;というように指定します。&lt;/p&gt;

&lt;p&gt;Flightサーバーの「コマンド」レイヤーにgRPCを使っているのは妥当だと思っていますが、&lt;a href=&quot;https://en.wikipedia.org/wiki/Remote_direct_memory_access&quot;&gt;RDMA&lt;/a&gt;のようなTCP以外のデータトランスポート層もサポートしたくなるかもしれません。設計・開発時間が必要になりますが、おそらく、TCP以外のプロトコル上でデータを転送するときでもgRPCを使えるでしょう。&lt;/p&gt;

&lt;h2 id=&quot;はじめかたと今後の話&quot;&gt;はじめかたと今後の話&lt;/h2&gt;

&lt;p&gt;Flightユーザー向けのドキュメントは作成中です。しかし、このライブラリーはベータユーザー向けには十分に使い物になります。ベータユーザーとは今後1年で発生するだろう軽微なAPI・プロトコルの変更に耐えられるユーザーです。&lt;/p&gt;

&lt;p&gt;Flightをためす簡単な方法はPython APIを使う方法です。なぜならカスタムサーバーもカスタムクライアントもすべてPythonだけで定義できるからです。なにもコンパイルする必要はありません。Arrowのコードにある&lt;a href=&quot;https://github.com/apache/arrow/tree/apache-arrow-0.15.0/python/examples/flight&quot;&gt;PythonでのFlightクライアントとサーバーの例&lt;/a&gt;を参考にできます。&lt;/p&gt;

&lt;p&gt;実際に使っている例もあります。Dremioは&lt;a href=&quot;https://github.com/dremio-hub/dremio-flight-connector&quot;&gt;Arrow Flightベースの&lt;/a&gt;コネクターを開発しました。このコネクターは&lt;a href=&quot;https://www.dremio.com/is-time-to-replace-odbc-jdbc/&quot;&gt;ODBCよりも20-50倍よい性能を発揮する&lt;/a&gt;ことを示しました。ArrowのコントリビューターであるRyan MurrayはApache Sparkユーザー向けにFlight対応エンドポイントに接続する&lt;a href=&quot;https://github.com/rymurr/flight-spark-source&quot;&gt;データソース実装&lt;/a&gt;を作りました。&lt;/p&gt;

&lt;p&gt;最後に今後の話をします。gRPCではない（あるいはTCPではない）データトランスポートをサポートできないか研究開発を進めるかもしれません。Flightの開発が進むとユーザーが使えるFlight対応サービスが増えていくでしょう。Flightは開発フレームワークなので、ユーザーが使うAPIは高レベルなAPIだけになるようにするつもりです。高レベルなAPIではFlightの詳細と特定のFlightアプリケーションに関連する詳細を隠します。&lt;/p&gt;</content><author><name>wesm</name></author><category term="application" /><category term="translation" /><summary type="html">この1.5年、Apache ArrowコミュニティーはFlightの設計と実装を進めてきました。Flightは高速なデータトランスポートを実現するための新しいクライアント・サーバー型のフレームワークです。Flightを使うとネットワーク越しに大きなデータセットを送る処理を簡単に実現できます。Flightは特定用途向けに設計されたものではないため、幅広い用途で利用できます。 Flightの実装は、まず、gRPCを使ったArrow列指向フォーマット（つまり「Arrowレコードバッチ」）のトランスポートの最適化に注力しました。gRPCはGoogleが開発しているHTTP/2ベースのRPCライブラリー・フレームワークで、広く利用されています。gRPCも特定用途向けではなく幅広い用途で使えるように設計されています。これまでFlightをgRPCベースで実装することに注力してきましたが、gRPCでだけ使えるようにしたいわけではありません。 Flightと他のデータトランスポートフレームワークとの大きな違いは並列転送機能です。クライアントとサーバークラスター間で同時にデータをストリームで転送できます。この機能により、簡単にスケーラブルなデータサービスを開発できます。スケーラブルなデータサービスとはクライアント数が増えても大丈夫なサービスです。 Apache Arrow 0.15.0でC++（Pytonバインディングあり）とJavaでFlightを使えるようになっています。 現時点ではベータユーザー向けです。ベータユーザーとはFlight内部の低レベルの改良によりAPIやプロトコルが変わっても適応できるユーザーのことです。 モチベーション 多くの人がネットワーク越しに大きなデータセットにアクセスすることに関して困っています。リモートのデータサービスからデータセットを読むためのさまざまな転送プロトコルやツールがたくさんあります。たとえばODBCやJDBCです。この10年、ファイルベースでデータを保管することが多くなりました。このときにはCSVやAvroやParquetといったフォーマットがよく使われます。しかし、この方法ではデシリアライズする前に生データをローカルのホストに転送しなければいけないという問題があります。 Apache Arrowの初期からやってきた作業によりさまざまな方法でデータトランスポートを加速できます。Arrow列指向フォーマットには次の重要な機能があります。 表形式データの「転送用の」表現です。この表現はデータ受信側でデシリアライズが必要ありません。 標準で「バッチをストリーム送信」するためのモードがあります。このモードでは、大きなデータセットを複数の行ごとにまとめて転送します。（Arrowの用語では「レコードバッチ」と呼んでいます。）この記事では「データストリーム」について話します。データストリームとはApache Arrowプロジェクトのバイナリープロトコルを使った一連のArrowレコードバッチです。 このフォーマットはプログラミング言語に依存していません。このフォーマットは現在11のプログラミング言語がサポートしています。サポートしているプログラミング言語は増え続けています。 ODBCのような標準的なプロトコルの各実装は、通常、それぞれ独自の転送用バイナリープロトコルを実装します。これらのプロトコルは各ライブラリーの公開インターフェイスの表現と相互に変換しなければいけません。ODBC・JDBCライブラリーのパフォーマンスは場合によって大きく異なります。 私たちのFlightの設計で目指していることは、データサービス用の新しいプロトコルを作ることです。このプロトコルは転送用のデータ表現にも開発者向けの公開APIにもArrow列指向フォーマットを使います。こうすることで、データトランスポート関連のシリアライズコストを減らし、分散データシステム全体を効率化できます。さらに、すでに別の用途にApache Arrowを使っているシステム間では非常に効率的にデータをやりとりできます。 Flightの基礎 Arrow Flightライブラリーはデータストリームを送受信できるサービスを実装するための開発者向けフレームワークを提供します。Flightサーバーは次の基本的なリクエストをサポートしています。 Handshake：クライアントが認証済みかを確認するシンプルなリクエスト。いくつかのケースでは、以降のリクエストのために実装定義のセッショントークンを確立します。 ListFlights：利用可能なデータストリームのリストを返します。 GetSchema：データストリームのスキーマを返します。 GetFlightInfo：対象のデータセット用の「アクセスプラン」を返します。複数のデータストリームを消費しなければいけないかもしれません。このリクエストにはシリアライズしたカスタムコマンドを含めることができます。たとえば、アプリケーション固有のパラメーターを含めることができます。 DoGet：クライアントにデータストリームを送信します。 DoPut: クライアントからデータストリームを受信します。 DoAction：実装依存のアクションを実行し、結果を返します。つまり、一般的な関数呼び出しです。 ListActions：利用可能なアクションの種類を返します。 gRPCの「双方向の」ストリーミングサポート（HTTP/2ストリーミング上に実装されています）を活用して、リクエスト処理中でもデータとメタデータをクライアント・サーバー間でやりとりできます。 単純なFlightの構成は1台のサーバーとそのサーバーに接続しDoGetリクエストをするクライアントという構成です。 gRPCごしのデータスループットの最適化 gRPCのような汎用メッセージングライブラリーを使うことには多くの利点があります。汎用ライブラリーはすでに多数の問題を解決しているからです。gRPCの場合はGoogleが多数の問題を解決していました。しかし、大きなデータセットのトランスポート性能を改善するためにいくつかの処理を改善する必要がありました。多くのgRPCユーザーは比較的小さなメッセージしか扱っていないからです。 一番よくサポートされているgRPCを使う方法はサービスをProtocol Buffers（「Protobuf」と呼ばれることもあります）の.protoファイルで定義する方法です。gRPCのProtobufプラグインはgRPCサービスのスタブを生成します。このスタブを使ってアプリケーションを実装します。RPCコマンドとデータメッセージはProtobufワイヤーフォーマットを使ってシリアライズします。Flightでは「普通のgRPCとProtocol Buffers」を使っているので、Arrow列指向フォーマットのことを知らないgRPCクライアントでもFlightサービスとやりとりできますし、Arrowデータの中身を気にせずに処理できます。 Flightの中の主要なデータ関連のProtobufの型はFlightDataと呼ばれています。一般的にProtobufメッセージの読み書きにはコストがかかります。そのため、C++でもJavaでもgRPCにいくつか次のような低レベルの最適化を実装しています。 FlightData用のProtobufワイヤーフォーマットを生成します。FlightDataには送信対象のArrowレコードバッチが含まれていますが、メモリーコピー・シリアライズ処理は一切ありません。 Protobufで表現されたFlightDataからメモリーコピー・デシリアライズ処理なしでArrowレコードバッチを再構築できます。実際には、Protocol Bufersライブラリーにエンコードされたデータペイロードを触らせないようにしています。 Protobufを使うがProtobufのメッセージパースのオーバーヘッドはなくしたいという両立できない2つのことを両立させようとしているということです。Flight実装は上述の最適化をして高速化しています。素のgRPCクライアントでもFlightサービスとやりとりできますが、素のgRPCクライアントにはこのような最適化はないので、Protobufライブラリーを使ってFlightDataをデシリアライズすることになります。そのため、素のgRPCクライアントを使うといくらか性能が落ちます。 FlightのC++実装でのデータスループットベンチマークの結果での絶対的な性能ですが、どちらもローカルホストで動いているサーバー・クライアント間のTCPスループットは2-3GB/sを上回っていました。ただし、TLSは無効にした状態です。このベンチマークは約4秒で12GBのデータを転送できることを示しています。 $ ./arrow-flight-benchmark --records_per_stream 100000000 Bytes read: 12800000000 Nanos: 3900466413 Speed: 3129.63 MB/s この結果から次の2つのことを言えます。1つはFlightとgRPCを使うと相対的に小さなオーバーヘッドはあるということです。もう1つは多くの実際のFlightアプリケーションではネットワークの帯域がボトルネックになりそうということです。 水平方向のスケーラビリティ：並列データアクセスとパーティション化したデータアクセス 分散型のデータベースシステムの多くは「コーディネーター」を通してクライアントのリクエストを処理するアーキテクチャーパターンを使っています。クライアントへのデータセットを複数回転送するという明らかに効率に課題がある点はさておき、巨大なデータセットへのアクセスに対するスケーラビリティの問題もあります。 Flightで次のようなシステムを作れるようにしました。それはこのようなボトルネックに取り組まずに水平方向にスケーラブルなデータサービスを作れるシステムです。GetFlightInfo RPCを使ったクライアントのリクエストは エンドポイント のリストを返します。返ってくる各エンドポイントにはサーバーの位置と チケット の情報が入っています。チケットはデータセットの一部を取得するDoGetリクエストに入れてサーバーに送ります。データセット全体にアクセスするためにはすべてのエンドポイントを処理する必要があります。どのエンドポイントのFlightのストリームから処理しなければいけないということはありません。どのエンドポイントのFlightのストリームから処理しても構いません。しかし、特定の順序で処理するための仕組みは用意しています。その仕組みとはアプリケーション固有のメタデータを使えるという仕組みです。順序の情報はメタデータで表現できます。 この複数エンドポイントパターンにはたくさんの利点があります。 複数のクライアントが複数のエンドポイントから並列にデータを読み込めます。 GetFlightInfo「プランニング」リクエストを提供するサービスは兄弟サービスに処理を移譲できます。これにより、データの局所性の利点を得られたり、単純にロードバランスしやすくなったりします。 分散クラスター中のノードは異なる役割を引き受けることができます。たとえば、クラスター内の一部のノードはクエリープランニングに責任を持つかもしれません。一方、他のノードはデータストリームリクエスト（DoGetまたはDoPut）だけを処理するかもしれません。 次の図はサービスの役割を分けた複数ノードアーキテクチャーの例です。 アクション：アプリケーション固有のロジックでFlightを拡張 GetFlightInfoリクエストはデータセットをリクエストするときにシリアライズしたコマンドを中身を気にせずに送ることができますが、クライアントはデータストリームの送受信以外の操作をサーバーに依頼できなければいけないかもしれません。たとえば、クライアントは特定のデータセットをメモリー上に「ピン止め」することを要求するかもしれません。ピン止めすることで他のクライアントからの後続のリクエストを高速に処理できます。 Flightサービスは追加で「アクション」を定義できます。DoAction RPCでアクションを実行できます。アクションリクエストには実行したいアクションの名前と追加情報が入っています。追加情報は省略可能です。アクションの結果はgRPCストリームです。このgRPCストリーム中には任意の結果を入れられます。 いくつかアクションの例を紹介します。 メタデータを見つけるアクション。組み込みのListFlights RPCでも提供されている機能ですが、ListFlightsの機能で不十分な場合はアクションで実現できます。 セッション固有のパラメーターを設定するアクション。 サーバーはアクションを1つも実装しなくてもよいことに注意してください。また、アクションは結果を返さなくてもよいです。 暗号化と認証 Flightは組み込みで暗号化をサポートしています。gRPCの組み込みのTLS/OpenSSLの機能を使っています。 クライアント側・サーバー側ともに拡張可能な認証ハンドラーがあります。この認証ハンドラーを使えば、ユーザー名とパスワードのようなシンプルな認証スキーマも使えますし、ケルベロスのような複雑な認証も使えます。Flightプロトコルには組み込みのBasicAuth機能がついています。そのため、追加の開発なしでそのままユーザー名とパスワードの認証を実現できます。 ミドルウェアとトレース gRPCには「インターセプター」というコンセプトがあります。インターセプターを使うと開発者が定義した「ミドルウェア」を開発できます。ミドルウェアを使うと届いたリクエストと送るリクエストに介在することができます。このような処理をするフレームワークにOpenTracingがあります。 ミドルウェアの機能は最近Flightに追加された機能です。そのため、今のところはmasterブランチでしか使えません。 gRPCを使っているがgRPCだけではない DoGetリクエストでサーバーの位置を指定する方法にはRFC 3986準拠のURIを使っています。たとえば、TLSを使ったgRPCはgrpc+tls://$HOST:$PORTというように指定します。 Flightサーバーの「コマンド」レイヤーにgRPCを使っているのは妥当だと思っていますが、RDMAのようなTCP以外のデータトランスポート層もサポートしたくなるかもしれません。設計・開発時間が必要になりますが、おそらく、TCP以外のプロトコル上でデータを転送するときでもgRPCを使えるでしょう。 はじめかたと今後の話 Flightユーザー向けのドキュメントは作成中です。しかし、このライブラリーはベータユーザー向けには十分に使い物になります。ベータユーザーとは今後1年で発生するだろう軽微なAPI・プロトコルの変更に耐えられるユーザーです。 Flightをためす簡単な方法はPython APIを使う方法です。なぜならカスタムサーバーもカスタムクライアントもすべてPythonだけで定義できるからです。なにもコンパイルする必要はありません。ArrowのコードにあるPythonでのFlightクライアントとサーバーの例を参考にできます。 実際に使っている例もあります。DremioはArrow Flightベースのコネクターを開発しました。このコネクターはODBCよりも20-50倍よい性能を発揮することを示しました。ArrowのコントリビューターであるRyan MurrayはApache Sparkユーザー向けにFlight対応エンドポイントに接続するデータソース実装を作りました。 最後に今後の話をします。gRPCではない（あるいはTCPではない）データトランスポートをサポートできないか研究開発を進めるかもしれません。Flightの開発が進むとユーザーが使えるFlight対応サービスが増えていくでしょう。Flightは開発フレームワークなので、ユーザーが使うAPIは高レベルなAPIだけになるようにするつもりです。高レベルなAPIではFlightの詳細と特定のFlightアプリケーションに関連する詳細を隠します。</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>