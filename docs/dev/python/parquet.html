

<!DOCTYPE html>


<html lang="en" data-content_root="" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />

    <title>Reading and Writing the Apache Parquet Format &#8212; Apache Arrow v21.0.0.dev194</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!--
    this give us a css class that will be invisible only if js is disabled
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.4cbf315f70debaebd550c87a6162cf0f.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/theme_overrides.css" />
  
  <!-- So that users can add custom icons -->
  <script src="../_static/scripts/fontawesome.js?digest=8878045cc6db502f8baf"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf" />

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/sphinx_highlight.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/design-tabs.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'python/parquet';</script>
    <script>
        DOCUMENTATION_OPTIONS.theme_version = '0.16.1';
        DOCUMENTATION_OPTIONS.theme_switcher_json_url = '/docs/_static/versions.json';
        DOCUMENTATION_OPTIONS.theme_switcher_version_match = 'dev/';
        DOCUMENTATION_OPTIONS.show_version_warning_banner =
            true;
        </script>
    <link rel="canonical" href="https://arrow.apache.org/docs/python/parquet.html" />
    <link rel="icon" href="../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="21.0.0.dev194" />

  <!-- Matomo -->
  <script>
    var _paq = window._paq = window._paq || [];
    /* tracker methods like "setCustomDimension" should be called before "trackPageView" */
    /* We explicitly disable cookie tracking to avoid privacy issues */
    _paq.push(['disableCookies']);
    _paq.push(['trackPageView']);
    _paq.push(['enableLinkTracking']);
    (function() {
      var u="https://analytics.apache.org/";
      _paq.push(['setTrackerUrl', u+'matomo.php']);
      _paq.push(['setSiteId', '20']);
      var d=document, g=d.createElement('script'), s=d.getElementsByTagName('script')[0];
      g.async=true; g.src=u+'matomo.js'; s.parentNode.insertBefore(g,s);
    })();
  </script>
  <!-- End Matomo Code -->

  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
<div class="bd-header__inner bd-page-width">
  <button class="pst-navbar-icon sidebar-toggle primary-toggle" aria-label="Site navigation">
    <span class="fa-solid fa-bars"></span>
  </button>
  
  
  <div class=" navbar-header-items__start">
    
      <div class="navbar-item">

  
    
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/arrow.png" class="logo__image only-light" alt="Apache Arrow v21.0.0.dev194 - Home"/>
    <img src="../_static/arrow-dark.png" class="logo__image only-dark pst-js-only" alt="Apache Arrow v21.0.0.dev194 - Home"/>
  
  
</a></div>
    
  </div>
  
  <div class=" navbar-header-items">
    
    <div class="me-auto navbar-header-items__center">
      
        <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../format/index.html">
    Specifications
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../developers/index.html">
    Development
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../implementations.html">
    Implementations
  </a>
</li>

  </ul>
</nav></div>
      
    </div>
    
    
    <div class="navbar-header-items__end">
      
        <div class="navbar-item navbar-persistent--container">
          

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button>
        </div>
      
      
        <div class="navbar-item"><div class="kapa-ai-bot">
    <script
        async
        src="https://widget.kapa.ai/kapa-widget.bundle.js"
        data-website-id="9db461d5-ac77-4b3f-a5c5-75efa78339d2"
        data-project-name="Apache Arrow"
        data-project-color="#000000"
        data-project-logo="https://arrow.apache.org/img/arrow-logo_chevrons_white-txt_black-bg.png"
        data-modal-disclaimer="This is a custom LLM with access to all [Arrow documentation](https://arrow.apache.org/docs/). Please include the language you are using in your question, e.g., Python, C++, Java, R, etc."
        data-consent-required="true" 
        data-consent-screen-disclaimer="By clicking &quot;I agree, let's chat&quot;, you consent to the use of the AI assistant in accordance with kapa.ai's [Privacy Policy](https://www.kapa.ai/content/privacy-policy). This service uses reCAPTCHA, which requires your consent to Google's [Privacy Policy](https://policies.google.com/privacy) and [Terms of Service](https://policies.google.com/terms). By proceeding, you explicitly agree to both kapa.ai's and Google's privacy policies."
    ></script>
   
</div>

</div>
      
        <div class="navbar-item">
<div class="version-switcher__container dropdown pst-js-only">
  <button id="pst-version-switcher-button-2"
    type="button"
    class="version-switcher__button btn btn-sm dropdown-toggle"
    data-bs-toggle="dropdown"
    aria-haspopup="listbox"
    aria-controls="pst-version-switcher-list-2"
    aria-label="Version switcher list"
  >
    Choose version  <!-- this text may get changed later by javascript -->
    <span class="caret"></span>
  </button>
  <div id="pst-version-switcher-list-2"
    class="version-switcher__menu dropdown-menu list-group-flush py-0"
    role="listbox" aria-labelledby="pst-version-switcher-button-2">
    <!-- dropdown will be populated by javascript on page load -->
  </div>
</div></div>
      
        <div class="navbar-item">

<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button></div>
      
        <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/apache/arrow" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-square-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://www.linkedin.com/company/apache-arrow/" title="LinkedIn" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-linkedin fa-lg" aria-hidden="true"></i>
            <span class="sr-only">LinkedIn</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://bsky.app/profile/arrow.apache.org" title="BlueSky" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-bluesky fa-lg" aria-hidden="true"></i>
            <span class="sr-only">BlueSky</span></a>
        </li>
</ul></div>
      
    </div>
    
  </div>
  
  
    <div class="navbar-persistent--mobile">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button>
    </div>
  

  
    <button class="pst-navbar-icon sidebar-toggle secondary-toggle" aria-label="On this page">
      <span class="fa-solid fa-outdent"></span>
    </button>
  
</div>

    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
        
          
          
            <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../format/index.html">
    Specifications
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../developers/index.html">
    Development
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../implementations.html">
    Implementations
  </a>
</li>

  </ul>
</nav></div>
          
        
      </div>
    
    
    
      <div class="sidebar-header-items__end">
        
          <div class="navbar-item"><div class="kapa-ai-bot">
    <script
        async
        src="https://widget.kapa.ai/kapa-widget.bundle.js"
        data-website-id="9db461d5-ac77-4b3f-a5c5-75efa78339d2"
        data-project-name="Apache Arrow"
        data-project-color="#000000"
        data-project-logo="https://arrow.apache.org/img/arrow-logo_chevrons_white-txt_black-bg.png"
        data-modal-disclaimer="This is a custom LLM with access to all [Arrow documentation](https://arrow.apache.org/docs/). Please include the language you are using in your question, e.g., Python, C++, Java, R, etc."
        data-consent-required="true" 
        data-consent-screen-disclaimer="By clicking &quot;I agree, let's chat&quot;, you consent to the use of the AI assistant in accordance with kapa.ai's [Privacy Policy](https://www.kapa.ai/content/privacy-policy). This service uses reCAPTCHA, which requires your consent to Google's [Privacy Policy](https://policies.google.com/privacy) and [Terms of Service](https://policies.google.com/terms). By proceeding, you explicitly agree to both kapa.ai's and Google's privacy policies."
    ></script>
   
</div>

</div>
        
          <div class="navbar-item">
<div class="version-switcher__container dropdown pst-js-only">
  <button id="pst-version-switcher-button-3"
    type="button"
    class="version-switcher__button btn btn-sm dropdown-toggle"
    data-bs-toggle="dropdown"
    aria-haspopup="listbox"
    aria-controls="pst-version-switcher-list-3"
    aria-label="Version switcher list"
  >
    Choose version  <!-- this text may get changed later by javascript -->
    <span class="caret"></span>
  </button>
  <div id="pst-version-switcher-list-3"
    class="version-switcher__menu dropdown-menu list-group-flush py-0"
    role="listbox" aria-labelledby="pst-version-switcher-button-3">
    <!-- dropdown will be populated by javascript on page load -->
  </div>
</div></div>
        
          <div class="navbar-item">

<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button></div>
        
          <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/apache/arrow" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-square-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://www.linkedin.com/company/apache-arrow/" title="LinkedIn" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-linkedin fa-lg" aria-hidden="true"></i>
            <span class="sr-only">LinkedIn</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://bsky.app/profile/arrow.apache.org" title="BlueSky" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-bluesky fa-lg" aria-hidden="true"></i>
            <span class="sr-only">BlueSky</span></a>
        </li>
</ul></div>
        
      </div>
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
<nav class="bd-docs-nav bd-links"
     aria-label="Section Navigation">
  <p class="bd-links__title" role="heading" aria-level="1">Section Navigation</p>
  <div class="bd-toc-item navbar-nav"></div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
      <div class="sidebar-primary-item">
<div id="ethical-ad-placement"
      class="flat"
      data-ea-publisher="readthedocs"
      data-ea-type="readthedocs-sidebar"
      data-ea-manual="true">
</div></div>
  </div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">

<nav aria-label="Breadcrumb" class="d-print-none">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    <li class="breadcrumb-item active" aria-current="page"><span class="ellipsis">Reading and Writing the Apache Parquet Format</span></li>
  </ul>
</nav>
</div>
      
    </div>
  
  
</div>
</div>
              
              
              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section id="reading-and-writing-the-apache-parquet-format">
<span id="parquet"></span><h1>Reading and Writing the Apache Parquet Format<a class="headerlink" href="#reading-and-writing-the-apache-parquet-format" title="Permalink to this heading">#</a></h1>
<p>The <a class="reference external" href="http://parquet.apache.org/">Apache Parquet</a> project provides a
standardized open-source columnar storage format for use in data analysis
systems. It was created originally for use in <a class="reference external" href="http://hadoop.apache.org/">Apache Hadoop</a> with systems like <a class="reference external" href="http://drill.apache.org">Apache Drill</a>, <a class="reference external" href="http://hive.apache.org">Apache Hive</a>, <a class="reference external" href="http://impala.apache.org">Apache
Impala</a>, and <a class="reference external" href="http://spark.apache.org">Apache Spark</a> adopting it as a shared standard for high
performance data IO.</p>
<p>Apache Arrow is an ideal in-memory transport layer for data that is being read
or written with Parquet files. We have been concurrently developing the <a class="reference external" href="https://github.com/apache/arrow/tree/main/cpp/tools/parquet">C++
implementation of
Apache Parquet</a>,
which includes a native, multithreaded C++ adapter to and from in-memory Arrow
data. PyArrow includes Python bindings to this code, which thus enables reading
and writing Parquet files with pandas as well.</p>
<section id="obtaining-pyarrow-with-parquet-support">
<h2>Obtaining pyarrow with Parquet Support<a class="headerlink" href="#obtaining-pyarrow-with-parquet-support" title="Permalink to this heading">#</a></h2>
<p>If you installed <code class="docutils literal notranslate"><span class="pre">pyarrow</span></code> with pip or conda, it should be built with Parquet
support bundled:</p>
<div class="highlight-ipython notranslate"><div class="highlight"><pre><span></span><span class="gp">In [1]: </span><span class="kn">import</span><span class="w"> </span><span class="nn">pyarrow.parquet</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pq</span>
</pre></div>
</div>
<p>If you are building <code class="docutils literal notranslate"><span class="pre">pyarrow</span></code> from source, you must use <code class="docutils literal notranslate"><span class="pre">-DARROW_PARQUET=ON</span></code>
when compiling the C++ libraries and enable the Parquet extensions when
building <code class="docutils literal notranslate"><span class="pre">pyarrow</span></code>. If you want to use Parquet Encryption, then you must
use <code class="docutils literal notranslate"><span class="pre">-DPARQUET_REQUIRE_ENCRYPTION=ON</span></code> too when compiling the C++ libraries.
See the <a class="reference internal" href="../developers/python.html#python-development"><span class="std std-ref">Python Development</span></a> page for more details.</p>
</section>
<section id="reading-and-writing-single-files">
<h2>Reading and Writing Single Files<a class="headerlink" href="#reading-and-writing-single-files" title="Permalink to this heading">#</a></h2>
<p>The functions <a class="reference internal" href="generated/pyarrow.parquet.read_table.html#pyarrow.parquet.read_table" title="pyarrow.parquet.read_table"><code class="xref py py-func docutils literal notranslate"><span class="pre">read_table()</span></code></a> and <a class="reference internal" href="generated/pyarrow.parquet.write_table.html#pyarrow.parquet.write_table" title="pyarrow.parquet.write_table"><code class="xref py py-func docutils literal notranslate"><span class="pre">write_table()</span></code></a>
read and write the <a class="reference internal" href="data.html#data-table"><span class="std std-ref">pyarrow.Table</span></a> object, respectively.</p>
<p>Let’s look at a simple table:</p>
<div class="highlight-ipython notranslate"><div class="highlight"><pre><span></span><span class="gp">In [2]: </span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="gp">In [3]: </span><span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>

<span class="gp">In [4]: </span><span class="kn">import</span><span class="w"> </span><span class="nn">pyarrow</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pa</span>

<span class="gp">In [5]: </span><span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;one&#39;</span><span class="p">:</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">],</span>
<span class="gp">   ...: </span>                   <span class="s1">&#39;two&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;foo&#39;</span><span class="p">,</span> <span class="s1">&#39;bar&#39;</span><span class="p">,</span> <span class="s1">&#39;baz&#39;</span><span class="p">],</span>
<span class="gp">   ...: </span>                   <span class="s1">&#39;three&#39;</span><span class="p">:</span> <span class="p">[</span><span class="kc">True</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="kc">True</span><span class="p">]},</span>
<span class="gp">   ...: </span>                   <span class="n">index</span><span class="o">=</span><span class="nb">list</span><span class="p">(</span><span class="s1">&#39;abc&#39;</span><span class="p">))</span>
<span class="gp">   ...: </span>

<span class="gp">In [6]: </span><span class="n">table</span> <span class="o">=</span> <span class="n">pa</span><span class="o">.</span><span class="n">Table</span><span class="o">.</span><span class="n">from_pandas</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
</pre></div>
</div>
<p>We write this to Parquet format with <code class="docutils literal notranslate"><span class="pre">write_table</span></code>:</p>
<div class="highlight-ipython notranslate"><div class="highlight"><pre><span></span><span class="gp">In [7]: </span><span class="kn">import</span><span class="w"> </span><span class="nn">pyarrow.parquet</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pq</span>

<span class="gp">In [8]: </span><span class="n">pq</span><span class="o">.</span><span class="n">write_table</span><span class="p">(</span><span class="n">table</span><span class="p">,</span> <span class="s1">&#39;example.parquet&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>This creates a single Parquet file. In practice, a Parquet dataset may consist
of many files in many directories. We can read a single file back with
<code class="docutils literal notranslate"><span class="pre">read_table</span></code>:</p>
<div class="highlight-ipython notranslate"><div class="highlight"><pre><span></span><span class="gp">In [9]: </span><span class="n">table2</span> <span class="o">=</span> <span class="n">pq</span><span class="o">.</span><span class="n">read_table</span><span class="p">(</span><span class="s1">&#39;example.parquet&#39;</span><span class="p">)</span>

<span class="gp">In [10]: </span><span class="n">table2</span><span class="o">.</span><span class="n">to_pandas</span><span class="p">()</span>
<span class="gh">Out[10]: </span>
<span class="go">   one  two  three</span>
<span class="go">a -1.0  foo   True</span>
<span class="go">b  NaN  bar  False</span>
<span class="go">c  2.5  baz   True</span>
</pre></div>
</div>
<p>You can pass a subset of columns to read, which can be much faster than reading
the whole file (due to the columnar layout):</p>
<div class="highlight-ipython notranslate"><div class="highlight"><pre><span></span><span class="gp">In [11]: </span><span class="n">pq</span><span class="o">.</span><span class="n">read_table</span><span class="p">(</span><span class="s1">&#39;example.parquet&#39;</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;one&#39;</span><span class="p">,</span> <span class="s1">&#39;three&#39;</span><span class="p">])</span>
<span class="gh">Out[11]: </span>
<span class="go">pyarrow.Table</span>
<span class="go">one: double</span>
<span class="go">three: bool</span>
<span class="gt">----</span>
<span class="ne">one</span>: [[-1,null,2.5]]
<span class="ne">three</span>: [[true,false,true]]
</pre></div>
</div>
<p>When reading a subset of columns from a file that used a Pandas dataframe as the
source, we use <code class="docutils literal notranslate"><span class="pre">read_pandas</span></code> to maintain any additional index column data:</p>
<div class="highlight-ipython notranslate"><div class="highlight"><pre><span></span><span class="gp">In [12]: </span><span class="n">pq</span><span class="o">.</span><span class="n">read_pandas</span><span class="p">(</span><span class="s1">&#39;example.parquet&#39;</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;two&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">to_pandas</span><span class="p">()</span>
<span class="gh">Out[12]: </span>
<span class="go">   two</span>
<span class="go">a  foo</span>
<span class="go">b  bar</span>
<span class="go">c  baz</span>
</pre></div>
</div>
<p>We do not need to use a string to specify the origin of the file. It can be any of:</p>
<ul class="simple">
<li><p>A file path as a string</p></li>
<li><p>A <a class="reference internal" href="memory.html#io-native-file"><span class="std std-ref">NativeFile</span></a> from PyArrow</p></li>
<li><p>A Python file object</p></li>
</ul>
<p>In general, a Python file object will have the worst read performance, while a
string file path or an instance of <a class="reference internal" href="generated/pyarrow.NativeFile.html#pyarrow.NativeFile" title="pyarrow.NativeFile"><code class="xref py py-class docutils literal notranslate"><span class="pre">NativeFile</span></code></a> (especially memory
maps) will perform the best.</p>
<section id="reading-parquet-and-memory-mapping">
<span id="parquet-mmap"></span><h3>Reading Parquet and Memory Mapping<a class="headerlink" href="#reading-parquet-and-memory-mapping" title="Permalink to this heading">#</a></h3>
<p>Because Parquet data needs to be decoded from the Parquet format
and compression, it can’t be directly mapped from disk.
Thus the <code class="docutils literal notranslate"><span class="pre">memory_map</span></code> option might perform better on some systems
but won’t help much with resident memory consumption.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">pq_array</span> <span class="o">=</span> <span class="n">pa</span><span class="o">.</span><span class="n">parquet</span><span class="o">.</span><span class="n">read_table</span><span class="p">(</span><span class="s2">&quot;area1.parquet&quot;</span><span class="p">,</span> <span class="n">memory_map</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;RSS: </span><span class="si">{}</span><span class="s2">MB&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">pa</span><span class="o">.</span><span class="n">total_allocated_bytes</span><span class="p">()</span> <span class="o">&gt;&gt;</span> <span class="mi">20</span><span class="p">))</span>
<span class="go">RSS: 4299MB</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">pq_array</span> <span class="o">=</span> <span class="n">pa</span><span class="o">.</span><span class="n">parquet</span><span class="o">.</span><span class="n">read_table</span><span class="p">(</span><span class="s2">&quot;area1.parquet&quot;</span><span class="p">,</span> <span class="n">memory_map</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;RSS: </span><span class="si">{}</span><span class="s2">MB&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">pa</span><span class="o">.</span><span class="n">total_allocated_bytes</span><span class="p">()</span> <span class="o">&gt;&gt;</span> <span class="mi">20</span><span class="p">))</span>
<span class="go">RSS: 4299MB</span>
</pre></div>
</div>
<p>If you need to deal with Parquet data bigger than memory,
the <a class="reference internal" href="dataset.html#dataset"><span class="std std-ref">Tabular Datasets</span></a> and partitioning is probably what you are looking for.</p>
</section>
<section id="parquet-file-writing-options">
<h3>Parquet file writing options<a class="headerlink" href="#parquet-file-writing-options" title="Permalink to this heading">#</a></h3>
<p><a class="reference internal" href="generated/pyarrow.parquet.write_table.html#pyarrow.parquet.write_table" title="pyarrow.parquet.write_table"><code class="xref py py-func docutils literal notranslate"><span class="pre">write_table()</span></code></a> has a number of options to
control various settings when writing a Parquet file.</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">version</span></code>, the Parquet format version to use.  <code class="docutils literal notranslate"><span class="pre">'1.0'</span></code> ensures
compatibility with older readers, while <code class="docutils literal notranslate"><span class="pre">'2.4'</span></code> and greater values
enable more Parquet types and encodings.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">data_page_size</span></code>, to control the approximate size of encoded data
pages within a column chunk. This currently defaults to 1MB.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">flavor</span></code>, to set compatibility options particular to a Parquet
consumer like <code class="docutils literal notranslate"><span class="pre">'spark'</span></code> for Apache Spark.</p></li>
</ul>
<p>See the <a class="reference internal" href="generated/pyarrow.parquet.write_table.html#pyarrow.parquet.write_table" title="pyarrow.parquet.write_table"><code class="xref py py-func docutils literal notranslate"><span class="pre">write_table()</span></code></a> docstring for more details.</p>
<p>There are some additional data type handling-specific options
described below.</p>
</section>
<section id="omitting-the-dataframe-index">
<h3>Omitting the DataFrame index<a class="headerlink" href="#omitting-the-dataframe-index" title="Permalink to this heading">#</a></h3>
<p>When using <code class="docutils literal notranslate"><span class="pre">pa.Table.from_pandas</span></code> to convert to an Arrow table, by default
one or more special columns are added to keep track of the index (row
labels). Storing the index takes extra space, so if your index is not valuable,
you may choose to omit it by passing <code class="docutils literal notranslate"><span class="pre">preserve_index=False</span></code></p>
<div class="highlight-ipython notranslate"><div class="highlight"><pre><span></span><span class="gp">In [13]: </span><span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;one&#39;</span><span class="p">:</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">],</span>
<span class="gp">   ....: </span>                   <span class="s1">&#39;two&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;foo&#39;</span><span class="p">,</span> <span class="s1">&#39;bar&#39;</span><span class="p">,</span> <span class="s1">&#39;baz&#39;</span><span class="p">],</span>
<span class="gp">   ....: </span>                   <span class="s1">&#39;three&#39;</span><span class="p">:</span> <span class="p">[</span><span class="kc">True</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="kc">True</span><span class="p">]},</span>
<span class="gp">   ....: </span>                   <span class="n">index</span><span class="o">=</span><span class="nb">list</span><span class="p">(</span><span class="s1">&#39;abc&#39;</span><span class="p">))</span>
<span class="gp">   ....: </span>

<span class="gp">In [14]: </span><span class="n">df</span>
<span class="gh">Out[14]: </span>
<span class="go">   one  two  three</span>
<span class="go">a -1.0  foo   True</span>
<span class="go">b  NaN  bar  False</span>
<span class="go">c  2.5  baz   True</span>

<span class="gp">In [15]: </span><span class="n">table</span> <span class="o">=</span> <span class="n">pa</span><span class="o">.</span><span class="n">Table</span><span class="o">.</span><span class="n">from_pandas</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">preserve_index</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
<p>Then we have:</p>
<div class="highlight-ipython notranslate"><div class="highlight"><pre><span></span><span class="gp">In [16]: </span><span class="n">pq</span><span class="o">.</span><span class="n">write_table</span><span class="p">(</span><span class="n">table</span><span class="p">,</span> <span class="s1">&#39;example_noindex.parquet&#39;</span><span class="p">)</span>

<span class="gp">In [17]: </span><span class="n">t</span> <span class="o">=</span> <span class="n">pq</span><span class="o">.</span><span class="n">read_table</span><span class="p">(</span><span class="s1">&#39;example_noindex.parquet&#39;</span><span class="p">)</span>

<span class="gp">In [18]: </span><span class="n">t</span><span class="o">.</span><span class="n">to_pandas</span><span class="p">()</span>
<span class="gh">Out[18]: </span>
<span class="go">   one  two  three</span>
<span class="go">0 -1.0  foo   True</span>
<span class="go">1  NaN  bar  False</span>
<span class="go">2  2.5  baz   True</span>
</pre></div>
</div>
<p>Here you see the index did not survive the round trip.</p>
</section>
</section>
<section id="finer-grained-reading-and-writing">
<h2>Finer-grained Reading and Writing<a class="headerlink" href="#finer-grained-reading-and-writing" title="Permalink to this heading">#</a></h2>
<p><code class="docutils literal notranslate"><span class="pre">read_table</span></code> uses the <a class="reference internal" href="generated/pyarrow.parquet.ParquetFile.html#pyarrow.parquet.ParquetFile" title="pyarrow.parquet.ParquetFile"><code class="xref py py-class docutils literal notranslate"><span class="pre">ParquetFile</span></code></a> class, which has other features:</p>
<div class="highlight-ipython notranslate"><div class="highlight"><pre><span></span><span class="gp">In [19]: </span><span class="n">parquet_file</span> <span class="o">=</span> <span class="n">pq</span><span class="o">.</span><span class="n">ParquetFile</span><span class="p">(</span><span class="s1">&#39;example.parquet&#39;</span><span class="p">)</span>

<span class="gp">In [20]: </span><span class="n">parquet_file</span><span class="o">.</span><span class="n">metadata</span>
<span class="gh">Out[20]: </span>
<span class="go">&lt;pyarrow._parquet.FileMetaData object at 0x7fa010b7e070&gt;</span>
<span class="go">  created_by: parquet-cpp-arrow version 21.0.0-SNAPSHOT</span>
<span class="go">  num_columns: 4</span>
<span class="go">  num_rows: 3</span>
<span class="go">  num_row_groups: 1</span>
<span class="go">  format_version: 2.6</span>
<span class="go">  serialized_size: 2634</span>

<span class="gp">In [21]: </span><span class="n">parquet_file</span><span class="o">.</span><span class="n">schema</span>
<span class="gh">Out[21]: </span>
<span class="go">&lt;pyarrow._parquet.ParquetSchema object at 0x7fa010d87b40&gt;</span>
<span class="go">required group field_id=-1 schema {</span>
<span class="go">  optional double field_id=-1 one;</span>
<span class="go">  optional binary field_id=-1 two (String);</span>
<span class="go">  optional boolean field_id=-1 three;</span>
<span class="go">  optional binary field_id=-1 __index_level_0__ (String);</span>
<span class="go">}</span>
</pre></div>
</div>
<p>As you can learn more in the <a class="reference external" href="https://github.com/apache/parquet-format">Apache Parquet format</a>, a Parquet file consists of
multiple row groups. <code class="docutils literal notranslate"><span class="pre">read_table</span></code> will read all of the row groups and
concatenate them into a single table. You can read individual row groups with
<code class="docutils literal notranslate"><span class="pre">read_row_group</span></code>:</p>
<div class="highlight-ipython notranslate"><div class="highlight"><pre><span></span><span class="gp">In [22]: </span><span class="n">parquet_file</span><span class="o">.</span><span class="n">num_row_groups</span>
<span class="gh">Out[22]: </span><span class="go">1</span>

<span class="gp">In [23]: </span><span class="n">parquet_file</span><span class="o">.</span><span class="n">read_row_group</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="gh">Out[23]: </span>
<span class="go">pyarrow.Table</span>
<span class="go">one: double</span>
<span class="go">two: string</span>
<span class="go">three: bool</span>
<span class="go">__index_level_0__: string</span>
<span class="gt">----</span>
<span class="ne">one</span>: [[-1,null,2.5]]
<span class="ne">two</span>: [[&quot;foo&quot;,&quot;bar&quot;,&quot;baz&quot;]]
<span class="ne">three</span>: [[true,false,true]]
<span class="ne">__index_level_0__</span>: [[&quot;a&quot;,&quot;b&quot;,&quot;c&quot;]]
</pre></div>
</div>
<p>We can similarly write a Parquet file with multiple row groups by using
<code class="docutils literal notranslate"><span class="pre">ParquetWriter</span></code>:</p>
<div class="highlight-ipython notranslate"><div class="highlight"><pre><span></span><span class="gp">In [24]: </span><span class="k">with</span> <span class="n">pq</span><span class="o">.</span><span class="n">ParquetWriter</span><span class="p">(</span><span class="s1">&#39;example2.parquet&#39;</span><span class="p">,</span> <span class="n">table</span><span class="o">.</span><span class="n">schema</span><span class="p">)</span> <span class="k">as</span> <span class="n">writer</span><span class="p">:</span>
<span class="gp">   ....: </span>   <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">):</span>
<span class="gp">   ....: </span>      <span class="n">writer</span><span class="o">.</span><span class="n">write_table</span><span class="p">(</span><span class="n">table</span><span class="p">)</span>
<span class="gp">   ....: </span>

<span class="gp">In [25]: </span><span class="n">pf2</span> <span class="o">=</span> <span class="n">pq</span><span class="o">.</span><span class="n">ParquetFile</span><span class="p">(</span><span class="s1">&#39;example2.parquet&#39;</span><span class="p">)</span>

<span class="gp">In [26]: </span><span class="n">pf2</span><span class="o">.</span><span class="n">num_row_groups</span>
<span class="gh">Out[26]: </span><span class="go">3</span>
</pre></div>
</div>
</section>
<section id="inspecting-the-parquet-file-metadata">
<h2>Inspecting the Parquet File Metadata<a class="headerlink" href="#inspecting-the-parquet-file-metadata" title="Permalink to this heading">#</a></h2>
<p>The <code class="docutils literal notranslate"><span class="pre">FileMetaData</span></code> of a Parquet file can be accessed through
<a class="reference internal" href="generated/pyarrow.parquet.ParquetFile.html#pyarrow.parquet.ParquetFile" title="pyarrow.parquet.ParquetFile"><code class="xref py py-class docutils literal notranslate"><span class="pre">ParquetFile</span></code></a> as shown above:</p>
<div class="highlight-ipython notranslate"><div class="highlight"><pre><span></span><span class="gp">In [27]: </span><span class="n">parquet_file</span> <span class="o">=</span> <span class="n">pq</span><span class="o">.</span><span class="n">ParquetFile</span><span class="p">(</span><span class="s1">&#39;example.parquet&#39;</span><span class="p">)</span>

<span class="gp">In [28]: </span><span class="n">metadata</span> <span class="o">=</span> <span class="n">parquet_file</span><span class="o">.</span><span class="n">metadata</span>
</pre></div>
</div>
<p>or can also be read directly using <a class="reference internal" href="generated/pyarrow.parquet.read_metadata.html#pyarrow.parquet.read_metadata" title="pyarrow.parquet.read_metadata"><code class="xref py py-func docutils literal notranslate"><span class="pre">read_metadata()</span></code></a>:</p>
<div class="highlight-ipython notranslate"><div class="highlight"><pre><span></span><span class="gp">In [29]: </span><span class="n">metadata</span> <span class="o">=</span> <span class="n">pq</span><span class="o">.</span><span class="n">read_metadata</span><span class="p">(</span><span class="s1">&#39;example.parquet&#39;</span><span class="p">)</span>

<span class="gp">In [30]: </span><span class="n">metadata</span>
<span class="gh">Out[30]: </span>
<span class="go">&lt;pyarrow._parquet.FileMetaData object at 0x7fa0102d0450&gt;</span>
<span class="go">  created_by: parquet-cpp-arrow version 21.0.0-SNAPSHOT</span>
<span class="go">  num_columns: 4</span>
<span class="go">  num_rows: 3</span>
<span class="go">  num_row_groups: 1</span>
<span class="go">  format_version: 2.6</span>
<span class="go">  serialized_size: 2634</span>
</pre></div>
</div>
<p>The returned <code class="docutils literal notranslate"><span class="pre">FileMetaData</span></code> object allows to inspect the
<a class="reference external" href="https://github.com/apache/parquet-format#metadata">Parquet file metadata</a>,
such as the row groups and column chunk metadata and statistics:</p>
<div class="highlight-ipython notranslate"><div class="highlight"><pre><span></span><span class="gp">In [31]: </span><span class="n">metadata</span><span class="o">.</span><span class="n">row_group</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="gh">Out[31]: </span>
<span class="go">&lt;pyarrow._parquet.RowGroupMetaData object at 0x7fa09967f740&gt;</span>
<span class="go">  num_columns: 4</span>
<span class="go">  num_rows: 3</span>
<span class="go">  total_byte_size: 282</span>
<span class="go">  sorting_columns: ()</span>

<span class="gp">In [32]: </span><span class="n">metadata</span><span class="o">.</span><span class="n">row_group</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">column</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="gh">Out[32]: </span>
<span class="go">&lt;pyarrow._parquet.ColumnChunkMetaData object at 0x7fa09967f920&gt;</span>
<span class="go">  file_offset: 0</span>
<span class="go">  file_path: </span>
<span class="go">  physical_type: DOUBLE</span>
<span class="go">  num_values: 3</span>
<span class="go">  path_in_schema: one</span>
<span class="go">  is_stats_set: True</span>
<span class="go">  statistics:</span>
<span class="go">    &lt;pyarrow._parquet.Statistics object at 0x7fa09967f970&gt;</span>
<span class="go">      has_min_max: True</span>
<span class="go">      min: -1.0</span>
<span class="go">      max: 2.5</span>
<span class="go">      null_count: 1</span>
<span class="go">      distinct_count: None</span>
<span class="go">      num_values: 2</span>
<span class="go">      physical_type: DOUBLE</span>
<span class="go">      logical_type: None</span>
<span class="go">      converted_type (legacy): NONE</span>
<span class="go">  geo_statistics:</span>
<span class="go">    None</span>
<span class="go">  compression: SNAPPY</span>
<span class="go">  encodings: (&#39;PLAIN&#39;, &#39;RLE&#39;, &#39;RLE_DICTIONARY&#39;)</span>
<span class="go">  has_dictionary_page: True</span>
<span class="go">  dictionary_page_offset: 4</span>
<span class="go">  data_page_offset: 36</span>
<span class="go">  total_compressed_size: 104</span>
<span class="go">  total_uncompressed_size: 100</span>
</pre></div>
</div>
</section>
<section id="data-type-handling">
<h2>Data Type Handling<a class="headerlink" href="#data-type-handling" title="Permalink to this heading">#</a></h2>
<section id="reading-types-as-dictionaryarray">
<h3>Reading types as DictionaryArray<a class="headerlink" href="#reading-types-as-dictionaryarray" title="Permalink to this heading">#</a></h3>
<p>The <code class="docutils literal notranslate"><span class="pre">read_dictionary</span></code> option in <code class="docutils literal notranslate"><span class="pre">read_table</span></code> and <code class="docutils literal notranslate"><span class="pre">ParquetDataset</span></code> will
cause columns to be read as <code class="docutils literal notranslate"><span class="pre">DictionaryArray</span></code>, which will become
<code class="docutils literal notranslate"><span class="pre">pandas.Categorical</span></code> when converted to pandas. This option is only valid for
string and binary column types, and it can yield significantly lower memory use
and improved performance for columns with many repeated string values.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">pq</span><span class="o">.</span><span class="n">read_table</span><span class="p">(</span><span class="n">table</span><span class="p">,</span> <span class="n">where</span><span class="p">,</span> <span class="n">read_dictionary</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;binary_c0&#39;</span><span class="p">,</span> <span class="s1">&#39;stringb_c2&#39;</span><span class="p">])</span>
</pre></div>
</div>
</section>
<section id="storing-timestamps">
<h3>Storing timestamps<a class="headerlink" href="#storing-timestamps" title="Permalink to this heading">#</a></h3>
<p>Some Parquet readers may only support timestamps stored in millisecond
(<code class="docutils literal notranslate"><span class="pre">'ms'</span></code>) or microsecond (<code class="docutils literal notranslate"><span class="pre">'us'</span></code>) resolution. Since pandas uses nanoseconds
to represent timestamps, this can occasionally be a nuisance. By default
(when writing version 1.0 Parquet files), the nanoseconds will be cast to
microseconds (‘us’).</p>
<p>In addition, We provide the <code class="docutils literal notranslate"><span class="pre">coerce_timestamps</span></code> option to allow you to select
the desired resolution:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">pq</span><span class="o">.</span><span class="n">write_table</span><span class="p">(</span><span class="n">table</span><span class="p">,</span> <span class="n">where</span><span class="p">,</span> <span class="n">coerce_timestamps</span><span class="o">=</span><span class="s1">&#39;ms&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>If a cast to a lower resolution value may result in a loss of data, by default
an exception will be raised. This can be suppressed by passing
<code class="docutils literal notranslate"><span class="pre">allow_truncated_timestamps=True</span></code>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">pq</span><span class="o">.</span><span class="n">write_table</span><span class="p">(</span><span class="n">table</span><span class="p">,</span> <span class="n">where</span><span class="p">,</span> <span class="n">coerce_timestamps</span><span class="o">=</span><span class="s1">&#39;ms&#39;</span><span class="p">,</span>
               <span class="n">allow_truncated_timestamps</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<p>Timestamps with nanoseconds can be stored without casting when using the
more recent Parquet format version 2.6:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">pq</span><span class="o">.</span><span class="n">write_table</span><span class="p">(</span><span class="n">table</span><span class="p">,</span> <span class="n">where</span><span class="p">,</span> <span class="n">version</span><span class="o">=</span><span class="s1">&#39;2.6&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>However, many Parquet readers do not yet support this newer format version, and
therefore the default is to write version 1.0 files. When compatibility across
different processing frameworks is required, it is recommended to use the
default version 1.0.</p>
<p>Older Parquet implementations use <code class="docutils literal notranslate"><span class="pre">INT96</span></code> based storage of
timestamps, but this is now deprecated. This includes some older
versions of Apache Impala and Apache Spark. To write timestamps in
this format, set the <code class="docutils literal notranslate"><span class="pre">use_deprecated_int96_timestamps</span></code> option to
<code class="docutils literal notranslate"><span class="pre">True</span></code> in <code class="docutils literal notranslate"><span class="pre">write_table</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">pq</span><span class="o">.</span><span class="n">write_table</span><span class="p">(</span><span class="n">table</span><span class="p">,</span> <span class="n">where</span><span class="p">,</span> <span class="n">use_deprecated_int96_timestamps</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="compression-encoding-and-file-compatibility">
<h2>Compression, Encoding, and File Compatibility<a class="headerlink" href="#compression-encoding-and-file-compatibility" title="Permalink to this heading">#</a></h2>
<p>The most commonly used Parquet implementations use dictionary encoding when
writing files; if the dictionaries grow too large, then they “fall back” to
plain encoding. Whether dictionary encoding is used can be toggled using the
<code class="docutils literal notranslate"><span class="pre">use_dictionary</span></code> option:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">pq</span><span class="o">.</span><span class="n">write_table</span><span class="p">(</span><span class="n">table</span><span class="p">,</span> <span class="n">where</span><span class="p">,</span> <span class="n">use_dictionary</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
<p>The data pages within a column in a row group can be compressed after the
encoding passes (dictionary, RLE encoding). In PyArrow we use Snappy
compression by default, but Brotli, Gzip, ZSTD, LZ4, and uncompressed are
also supported:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">pq</span><span class="o">.</span><span class="n">write_table</span><span class="p">(</span><span class="n">table</span><span class="p">,</span> <span class="n">where</span><span class="p">,</span> <span class="n">compression</span><span class="o">=</span><span class="s1">&#39;snappy&#39;</span><span class="p">)</span>
<span class="n">pq</span><span class="o">.</span><span class="n">write_table</span><span class="p">(</span><span class="n">table</span><span class="p">,</span> <span class="n">where</span><span class="p">,</span> <span class="n">compression</span><span class="o">=</span><span class="s1">&#39;gzip&#39;</span><span class="p">)</span>
<span class="n">pq</span><span class="o">.</span><span class="n">write_table</span><span class="p">(</span><span class="n">table</span><span class="p">,</span> <span class="n">where</span><span class="p">,</span> <span class="n">compression</span><span class="o">=</span><span class="s1">&#39;brotli&#39;</span><span class="p">)</span>
<span class="n">pq</span><span class="o">.</span><span class="n">write_table</span><span class="p">(</span><span class="n">table</span><span class="p">,</span> <span class="n">where</span><span class="p">,</span> <span class="n">compression</span><span class="o">=</span><span class="s1">&#39;zstd&#39;</span><span class="p">)</span>
<span class="n">pq</span><span class="o">.</span><span class="n">write_table</span><span class="p">(</span><span class="n">table</span><span class="p">,</span> <span class="n">where</span><span class="p">,</span> <span class="n">compression</span><span class="o">=</span><span class="s1">&#39;lz4&#39;</span><span class="p">)</span>
<span class="n">pq</span><span class="o">.</span><span class="n">write_table</span><span class="p">(</span><span class="n">table</span><span class="p">,</span> <span class="n">where</span><span class="p">,</span> <span class="n">compression</span><span class="o">=</span><span class="s1">&#39;none&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>Snappy generally results in better performance, while Gzip may yield smaller
files.</p>
<p>These settings can also be set on a per-column basis:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">pq</span><span class="o">.</span><span class="n">write_table</span><span class="p">(</span><span class="n">table</span><span class="p">,</span> <span class="n">where</span><span class="p">,</span> <span class="n">compression</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;foo&#39;</span><span class="p">:</span> <span class="s1">&#39;snappy&#39;</span><span class="p">,</span> <span class="s1">&#39;bar&#39;</span><span class="p">:</span> <span class="s1">&#39;gzip&#39;</span><span class="p">},</span>
               <span class="n">use_dictionary</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;foo&#39;</span><span class="p">,</span> <span class="s1">&#39;bar&#39;</span><span class="p">])</span>
</pre></div>
</div>
</section>
<section id="partitioned-datasets-multiple-files">
<h2>Partitioned Datasets (Multiple Files)<a class="headerlink" href="#partitioned-datasets-multiple-files" title="Permalink to this heading">#</a></h2>
<p>Multiple Parquet files constitute a Parquet <em>dataset</em>. These may present in a
number of ways:</p>
<ul class="simple">
<li><p>A list of Parquet absolute file paths</p></li>
<li><p>A directory name containing nested directories defining a partitioned dataset</p></li>
</ul>
<p>A dataset partitioned by year and month may look like on disk:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>dataset_name/
  year=2007/
    month=01/
       0.parq
       1.parq
       ...
    month=02/
       0.parq
       1.parq
       ...
    month=03/
    ...
  year=2008/
    month=01/
    ...
  ...
</pre></div>
</div>
</section>
<section id="writing-to-partitioned-datasets">
<h2>Writing to Partitioned Datasets<a class="headerlink" href="#writing-to-partitioned-datasets" title="Permalink to this heading">#</a></h2>
<p>You can write a partitioned dataset for any <code class="docutils literal notranslate"><span class="pre">pyarrow</span></code> file system that is a
file-store (e.g. local, HDFS, S3). The default behaviour when no filesystem is
added is to use the local filesystem.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Local dataset write</span>
<span class="n">pq</span><span class="o">.</span><span class="n">write_to_dataset</span><span class="p">(</span><span class="n">table</span><span class="p">,</span> <span class="n">root_path</span><span class="o">=</span><span class="s1">&#39;dataset_name&#39;</span><span class="p">,</span>
                    <span class="n">partition_cols</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;one&#39;</span><span class="p">,</span> <span class="s1">&#39;two&#39;</span><span class="p">])</span>
</pre></div>
</div>
<p>The root path in this case specifies the parent directory to which data will be
saved. The partition columns are the column names by which to partition the
dataset. Columns are partitioned in the order they are given. The partition
splits are determined by the unique values in the partition columns.</p>
<p>To use another filesystem you only need to add the filesystem parameter, the
individual table writes are wrapped using <code class="docutils literal notranslate"><span class="pre">with</span></code> statements so the
<code class="docutils literal notranslate"><span class="pre">pq.write_to_dataset</span></code> function does not need to be.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Remote file-system example</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">pyarrow.fs</span><span class="w"> </span><span class="kn">import</span> <span class="n">HadoopFileSystem</span>
<span class="n">fs</span> <span class="o">=</span> <span class="n">HadoopFileSystem</span><span class="p">(</span><span class="n">host</span><span class="p">,</span> <span class="n">port</span><span class="p">,</span> <span class="n">user</span><span class="o">=</span><span class="n">user</span><span class="p">,</span> <span class="n">kerb_ticket</span><span class="o">=</span><span class="n">ticket_cache_path</span><span class="p">)</span>
<span class="n">pq</span><span class="o">.</span><span class="n">write_to_dataset</span><span class="p">(</span><span class="n">table</span><span class="p">,</span> <span class="n">root_path</span><span class="o">=</span><span class="s1">&#39;dataset_name&#39;</span><span class="p">,</span>
                    <span class="n">partition_cols</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;one&#39;</span><span class="p">,</span> <span class="s1">&#39;two&#39;</span><span class="p">],</span> <span class="n">filesystem</span><span class="o">=</span><span class="n">fs</span><span class="p">)</span>
</pre></div>
</div>
<p>Compatibility Note: if using <code class="docutils literal notranslate"><span class="pre">pq.write_to_dataset</span></code> to create a table that
will then be used by HIVE then partition column values must be compatible with
the allowed character set of the HIVE version you are running.</p>
<section id="writing-metadata-and-common-metadata-files">
<h3>Writing <code class="docutils literal notranslate"><span class="pre">_metadata</span></code> and <code class="docutils literal notranslate"><span class="pre">_common_metadata</span></code> files<a class="headerlink" href="#writing-metadata-and-common-metadata-files" title="Permalink to this heading">#</a></h3>
<p>Some processing frameworks such as Spark or Dask (optionally) use <code class="docutils literal notranslate"><span class="pre">_metadata</span></code>
and <code class="docutils literal notranslate"><span class="pre">_common_metadata</span></code> files with partitioned datasets.</p>
<p>Those files include information about the schema of the full dataset (for
<code class="docutils literal notranslate"><span class="pre">_common_metadata</span></code>) and potentially all row group metadata of all files in the
partitioned dataset as well (for <code class="docutils literal notranslate"><span class="pre">_metadata</span></code>). The actual files are
metadata-only Parquet files. Note this is not a Parquet standard, but a
convention set in practice by those frameworks.</p>
<p>Using those files can give a more efficient creation of a parquet Dataset,
since it can use the stored schema and file paths of all row groups,
instead of inferring the schema and crawling the directories for all Parquet
files (this is especially the case for filesystems where accessing files
is expensive).</p>
<p>The <a class="reference internal" href="generated/pyarrow.parquet.write_to_dataset.html#pyarrow.parquet.write_to_dataset" title="pyarrow.parquet.write_to_dataset"><code class="xref py py-func docutils literal notranslate"><span class="pre">write_to_dataset()</span></code></a> function does not automatically
write such metadata files, but you can use it to gather the metadata and
combine and write them manually:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Write a dataset and collect metadata information of all written files</span>
<span class="n">metadata_collector</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">pq</span><span class="o">.</span><span class="n">write_to_dataset</span><span class="p">(</span><span class="n">table</span><span class="p">,</span> <span class="n">root_path</span><span class="p">,</span> <span class="n">metadata_collector</span><span class="o">=</span><span class="n">metadata_collector</span><span class="p">)</span>

<span class="c1"># Write the ``_common_metadata`` parquet file without row groups statistics</span>
<span class="n">pq</span><span class="o">.</span><span class="n">write_metadata</span><span class="p">(</span><span class="n">table</span><span class="o">.</span><span class="n">schema</span><span class="p">,</span> <span class="n">root_path</span> <span class="o">/</span> <span class="s1">&#39;_common_metadata&#39;</span><span class="p">)</span>

<span class="c1"># Write the ``_metadata`` parquet file with row groups statistics of all files</span>
<span class="n">pq</span><span class="o">.</span><span class="n">write_metadata</span><span class="p">(</span>
    <span class="n">table</span><span class="o">.</span><span class="n">schema</span><span class="p">,</span> <span class="n">root_path</span> <span class="o">/</span> <span class="s1">&#39;_metadata&#39;</span><span class="p">,</span>
    <span class="n">metadata_collector</span><span class="o">=</span><span class="n">metadata_collector</span>
<span class="p">)</span>
</pre></div>
</div>
<p>When not using the <a class="reference internal" href="generated/pyarrow.parquet.write_to_dataset.html#pyarrow.parquet.write_to_dataset" title="pyarrow.parquet.write_to_dataset"><code class="xref py py-func docutils literal notranslate"><span class="pre">write_to_dataset()</span></code></a> function, but
writing the individual files of the partitioned dataset using
<a class="reference internal" href="generated/pyarrow.parquet.write_table.html#pyarrow.parquet.write_table" title="pyarrow.parquet.write_table"><code class="xref py py-func docutils literal notranslate"><span class="pre">write_table()</span></code></a> or <a class="reference internal" href="generated/pyarrow.parquet.ParquetWriter.html#pyarrow.parquet.ParquetWriter" title="pyarrow.parquet.ParquetWriter"><code class="xref py py-class docutils literal notranslate"><span class="pre">ParquetWriter</span></code></a>,
the <code class="docutils literal notranslate"><span class="pre">metadata_collector</span></code> keyword can also be used to collect the FileMetaData
of the written files. In this case, you need to ensure to set the file path
contained in the row group metadata yourself before combining the metadata, and
the schemas of all different files and collected FileMetaData objects should be
the same:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">metadata_collector</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">pq</span><span class="o">.</span><span class="n">write_table</span><span class="p">(</span>
    <span class="n">table1</span><span class="p">,</span> <span class="n">root_path</span> <span class="o">/</span> <span class="s2">&quot;year=2017/data1.parquet&quot;</span><span class="p">,</span>
    <span class="n">metadata_collector</span><span class="o">=</span><span class="n">metadata_collector</span>
<span class="p">)</span>

<span class="c1"># set the file path relative to the root of the partitioned dataset</span>
<span class="n">metadata_collector</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_file_path</span><span class="p">(</span><span class="s2">&quot;year=2017/data1.parquet&quot;</span><span class="p">)</span>

<span class="c1"># combine and write the metadata</span>
<span class="n">metadata</span> <span class="o">=</span> <span class="n">metadata_collector</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="k">for</span> <span class="n">_meta</span> <span class="ow">in</span> <span class="n">metadata_collector</span><span class="p">[</span><span class="mi">1</span><span class="p">:]:</span>
    <span class="n">metadata</span><span class="o">.</span><span class="n">append_row_groups</span><span class="p">(</span><span class="n">_meta</span><span class="p">)</span>
<span class="n">metadata</span><span class="o">.</span><span class="n">write_metadata_file</span><span class="p">(</span><span class="n">root_path</span> <span class="o">/</span> <span class="s2">&quot;_metadata&quot;</span><span class="p">)</span>

<span class="c1"># or use pq.write_metadata to combine and write in a single step</span>
<span class="n">pq</span><span class="o">.</span><span class="n">write_metadata</span><span class="p">(</span>
    <span class="n">table1</span><span class="o">.</span><span class="n">schema</span><span class="p">,</span> <span class="n">root_path</span> <span class="o">/</span> <span class="s2">&quot;_metadata&quot;</span><span class="p">,</span>
    <span class="n">metadata_collector</span><span class="o">=</span><span class="n">metadata_collector</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="reading-from-partitioned-datasets">
<h2>Reading from Partitioned Datasets<a class="headerlink" href="#reading-from-partitioned-datasets" title="Permalink to this heading">#</a></h2>
<p>The <a class="reference internal" href="generated/pyarrow.parquet.ParquetDataset.html#pyarrow.parquet.ParquetDataset" title="pyarrow.parquet.ParquetDataset"><code class="xref py py-class docutils literal notranslate"><span class="pre">ParquetDataset</span></code></a> class accepts either a directory name or a list
of file paths, and can discover and infer some common partition structures,
such as those produced by Hive:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">dataset</span> <span class="o">=</span> <span class="n">pq</span><span class="o">.</span><span class="n">ParquetDataset</span><span class="p">(</span><span class="s1">&#39;dataset_name/&#39;</span><span class="p">)</span>
<span class="n">table</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">read</span><span class="p">()</span>
</pre></div>
</div>
<p>You can also use the convenience function <code class="docutils literal notranslate"><span class="pre">read_table</span></code> exposed by
<code class="docutils literal notranslate"><span class="pre">pyarrow.parquet</span></code> that avoids the need for an additional Dataset object
creation step.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">table</span> <span class="o">=</span> <span class="n">pq</span><span class="o">.</span><span class="n">read_table</span><span class="p">(</span><span class="s1">&#39;dataset_name&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>Note: the partition columns in the original table will have their types
converted to Arrow dictionary types (pandas categorical) on load. Ordering of
partition columns is not preserved through the save/load process. If reading
from a remote filesystem into a pandas dataframe you may need to run
<code class="docutils literal notranslate"><span class="pre">sort_index</span></code> to maintain row ordering (as long as the <code class="docutils literal notranslate"><span class="pre">preserve_index</span></code>
option was enabled on write).</p>
<p>Other features:</p>
<ul class="simple">
<li><p>Filtering on all columns (using row group statistics) instead of only on
the partition keys.</p></li>
<li><p>Fine-grained partitioning: support for a directory partitioning scheme
in addition to the Hive-like partitioning (e.g. “/2019/11/15/” instead of
“/year=2019/month=11/day=15/”), and the ability to specify a schema for
the partition keys.</p></li>
</ul>
<p>Note:</p>
<ul class="simple">
<li><p>The partition keys need to be explicitly included in the <code class="docutils literal notranslate"><span class="pre">columns</span></code>
keyword when you want to include them in the result while reading a
subset of the columns</p></li>
</ul>
</section>
<section id="using-with-spark">
<h2>Using with Spark<a class="headerlink" href="#using-with-spark" title="Permalink to this heading">#</a></h2>
<p>Spark places some constraints on the types of Parquet files it will read. The
option <code class="docutils literal notranslate"><span class="pre">flavor='spark'</span></code> will set these options automatically and also
sanitize field characters unsupported by Spark SQL.</p>
</section>
<section id="multithreaded-reads">
<h2>Multithreaded Reads<a class="headerlink" href="#multithreaded-reads" title="Permalink to this heading">#</a></h2>
<p>Each of the reading functions by default use multi-threading for reading
columns in parallel. Depending on the speed of IO
and how expensive it is to decode the columns in a particular file
(particularly with GZIP compression), this can yield significantly higher data
throughput.</p>
<p>This can be disabled by specifying <code class="docutils literal notranslate"><span class="pre">use_threads=False</span></code>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The number of threads to use concurrently is automatically inferred by Arrow
and can be inspected using the <a class="reference internal" href="generated/pyarrow.cpu_count.html#pyarrow.cpu_count" title="pyarrow.cpu_count"><code class="xref py py-func docutils literal notranslate"><span class="pre">cpu_count()</span></code></a> function.</p>
</div>
</section>
<section id="reading-from-cloud-storage">
<h2>Reading from cloud storage<a class="headerlink" href="#reading-from-cloud-storage" title="Permalink to this heading">#</a></h2>
<p>In addition to local files, pyarrow supports other filesystems, such as cloud
filesystems, through the <code class="docutils literal notranslate"><span class="pre">filesystem</span></code> keyword:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">pyarrow</span><span class="w"> </span><span class="kn">import</span> <span class="n">fs</span>

<span class="n">s3</span>  <span class="o">=</span> <span class="n">fs</span><span class="o">.</span><span class="n">S3FileSystem</span><span class="p">(</span><span class="n">region</span><span class="o">=</span><span class="s2">&quot;us-east-2&quot;</span><span class="p">)</span>
<span class="n">table</span> <span class="o">=</span> <span class="n">pq</span><span class="o">.</span><span class="n">read_table</span><span class="p">(</span><span class="s2">&quot;bucket/object/key/prefix&quot;</span><span class="p">,</span> <span class="n">filesystem</span><span class="o">=</span><span class="n">s3</span><span class="p">)</span>
</pre></div>
</div>
<p>Currently, <a class="reference internal" href="generated/pyarrow.fs.HadoopFileSystem.html#pyarrow.fs.HadoopFileSystem" title="pyarrow.fs.HadoopFileSystem"><code class="xref py py-class docutils literal notranslate"><span class="pre">HDFS</span></code></a> and
<a class="reference internal" href="generated/pyarrow.fs.S3FileSystem.html#pyarrow.fs.S3FileSystem" title="pyarrow.fs.S3FileSystem"><code class="xref py py-class docutils literal notranslate"><span class="pre">Amazon</span> <span class="pre">S3-compatible</span> <span class="pre">storage</span></code></a> are
supported. See the <a class="reference internal" href="filesystems.html#filesystem"><span class="std std-ref">Filesystem Interface</span></a> docs for more details. For those
built-in filesystems, the filesystem can also be inferred from the file path,
if specified as a URI:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">table</span> <span class="o">=</span> <span class="n">pq</span><span class="o">.</span><span class="n">read_table</span><span class="p">(</span><span class="s2">&quot;s3://bucket/object/key/prefix&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>Other filesystems can still be supported if there is an
<a class="reference external" href="https://filesystem-spec.readthedocs.io/en/latest/">fsspec</a>-compatible
implementation available. See <a class="reference internal" href="filesystems.html#filesystem-fsspec"><span class="std std-ref">Using fsspec-compatible filesystems with Arrow</span></a> for more details.
One example is Azure Blob storage, which can be interfaced through the
<a class="reference external" href="https://github.com/dask/adlfs">adlfs</a> package.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">adlfs</span><span class="w"> </span><span class="kn">import</span> <span class="n">AzureBlobFileSystem</span>

<span class="n">abfs</span> <span class="o">=</span> <span class="n">AzureBlobFileSystem</span><span class="p">(</span><span class="n">account_name</span><span class="o">=</span><span class="s2">&quot;XXXX&quot;</span><span class="p">,</span> <span class="n">account_key</span><span class="o">=</span><span class="s2">&quot;XXXX&quot;</span><span class="p">,</span> <span class="n">container_name</span><span class="o">=</span><span class="s2">&quot;XXXX&quot;</span><span class="p">)</span>
<span class="n">table</span> <span class="o">=</span> <span class="n">pq</span><span class="o">.</span><span class="n">read_table</span><span class="p">(</span><span class="s2">&quot;file.parquet&quot;</span><span class="p">,</span> <span class="n">filesystem</span><span class="o">=</span><span class="n">abfs</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="parquet-modular-encryption-columnar-encryption">
<h2>Parquet Modular Encryption (Columnar Encryption)<a class="headerlink" href="#parquet-modular-encryption-columnar-encryption" title="Permalink to this heading">#</a></h2>
<p>Columnar encryption is supported for Parquet files in C++ starting from
Apache Arrow 4.0.0 and in PyArrow starting from Apache Arrow 6.0.0.</p>
<p>Parquet uses the envelope encryption practice, where file parts are encrypted
with “data encryption keys” (DEKs), and the DEKs are encrypted with “master
encryption keys” (MEKs). The DEKs are randomly generated by Parquet for each
encrypted file/column. The MEKs are generated, stored and managed in a Key
Management Service (KMS) of user’s choice.</p>
<p>Reading and writing encrypted Parquet files involves passing file encryption
and decryption properties to <a class="reference internal" href="generated/pyarrow.parquet.ParquetWriter.html#pyarrow.parquet.ParquetWriter" title="pyarrow.parquet.ParquetWriter"><code class="xref py py-class docutils literal notranslate"><span class="pre">ParquetWriter</span></code></a> and to
<a class="reference internal" href="generated/pyarrow.parquet.ParquetFile.html#pyarrow.parquet.ParquetFile" title="pyarrow.parquet.ParquetFile"><code class="xref py py-class docutils literal notranslate"><span class="pre">ParquetFile</span></code></a>, respectively.</p>
<p>Writing an encrypted Parquet file:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">encryption_properties</span> <span class="o">=</span> <span class="n">crypto_factory</span><span class="o">.</span><span class="n">file_encryption_properties</span><span class="p">(</span>
                                 <span class="n">kms_connection_config</span><span class="p">,</span> <span class="n">encryption_config</span><span class="p">)</span>
<span class="k">with</span> <span class="n">pq</span><span class="o">.</span><span class="n">ParquetWriter</span><span class="p">(</span><span class="n">filename</span><span class="p">,</span> <span class="n">schema</span><span class="p">,</span>
                     <span class="n">encryption_properties</span><span class="o">=</span><span class="n">encryption_properties</span><span class="p">)</span> <span class="k">as</span> <span class="n">writer</span><span class="p">:</span>
   <span class="n">writer</span><span class="o">.</span><span class="n">write_table</span><span class="p">(</span><span class="n">table</span><span class="p">)</span>
</pre></div>
</div>
<p>Reading an encrypted Parquet file:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">decryption_properties</span> <span class="o">=</span> <span class="n">crypto_factory</span><span class="o">.</span><span class="n">file_decryption_properties</span><span class="p">(</span>
                                                 <span class="n">kms_connection_config</span><span class="p">)</span>
<span class="n">parquet_file</span> <span class="o">=</span> <span class="n">pq</span><span class="o">.</span><span class="n">ParquetFile</span><span class="p">(</span><span class="n">filename</span><span class="p">,</span>
                              <span class="n">decryption_properties</span><span class="o">=</span><span class="n">decryption_properties</span><span class="p">)</span>
</pre></div>
</div>
<p>In order to create the encryption and decryption properties, a
<a class="reference internal" href="generated/pyarrow.parquet.encryption.CryptoFactory.html#pyarrow.parquet.encryption.CryptoFactory" title="pyarrow.parquet.encryption.CryptoFactory"><code class="xref py py-class docutils literal notranslate"><span class="pre">pyarrow.parquet.encryption.CryptoFactory</span></code></a> should be created and
initialized with KMS Client details, as described below.</p>
<section id="kms-client">
<h3>KMS Client<a class="headerlink" href="#kms-client" title="Permalink to this heading">#</a></h3>
<p>The master encryption keys should be kept and managed in a production-grade
Key Management System (KMS), deployed in the user’s organization. Using Parquet
encryption requires implementation of a client class for the KMS server.
Any KmsClient implementation should implement the informal interface
defined by <a class="reference internal" href="generated/pyarrow.parquet.encryption.KmsClient.html#pyarrow.parquet.encryption.KmsClient" title="pyarrow.parquet.encryption.KmsClient"><code class="xref py py-class docutils literal notranslate"><span class="pre">pyarrow.parquet.encryption.KmsClient</span></code></a> as following:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">pyarrow.parquet.encryption</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pe</span>

<span class="k">class</span><span class="w"> </span><span class="nc">MyKmsClient</span><span class="p">(</span><span class="n">pe</span><span class="o">.</span><span class="n">KmsClient</span><span class="p">):</span>

<span class="w">   </span><span class="sd">&quot;&quot;&quot;An example KmsClient implementation skeleton&quot;&quot;&quot;</span>
   <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">kms_connection_configuration</span><span class="p">):</span>
      <span class="n">pe</span><span class="o">.</span><span class="n">KmsClient</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
      <span class="c1"># Any KMS-specific initialization based on</span>
      <span class="c1"># kms_connection_configuration comes here</span>

   <span class="k">def</span><span class="w"> </span><span class="nf">wrap_key</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">key_bytes</span><span class="p">,</span> <span class="n">master_key_identifier</span><span class="p">):</span>
      <span class="n">wrapped_key</span> <span class="o">=</span> <span class="o">...</span> <span class="c1"># call KMS to wrap key_bytes with key specified by</span>
                        <span class="c1"># master_key_identifier</span>
      <span class="k">return</span> <span class="n">wrapped_key</span>

   <span class="k">def</span><span class="w"> </span><span class="nf">unwrap_key</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">wrapped_key</span><span class="p">,</span> <span class="n">master_key_identifier</span><span class="p">):</span>
      <span class="n">key_bytes</span> <span class="o">=</span> <span class="o">...</span> <span class="c1"># call KMS to unwrap wrapped_key with key specified by</span>
                      <span class="c1"># master_key_identifier</span>
      <span class="k">return</span> <span class="n">key_bytes</span>
</pre></div>
</div>
<p>The concrete implementation will be loaded at runtime by a factory function
provided by the user. This factory function will be used to initialize the
<a class="reference internal" href="generated/pyarrow.parquet.encryption.CryptoFactory.html#pyarrow.parquet.encryption.CryptoFactory" title="pyarrow.parquet.encryption.CryptoFactory"><code class="xref py py-class docutils literal notranslate"><span class="pre">pyarrow.parquet.encryption.CryptoFactory</span></code></a> for creating file encryption
and decryption properties.</p>
<p>For example, in order to use the <code class="docutils literal notranslate"><span class="pre">MyKmsClient</span></code> defined above:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">kms_client_factory</span><span class="p">(</span><span class="n">kms_connection_configuration</span><span class="p">):</span>
   <span class="k">return</span> <span class="n">MyKmsClient</span><span class="p">(</span><span class="n">kms_connection_configuration</span><span class="p">)</span>

<span class="n">crypto_factory</span> <span class="o">=</span> <span class="n">CryptoFactory</span><span class="p">(</span><span class="n">kms_client_factory</span><span class="p">)</span>
</pre></div>
</div>
<p>An <code class="xref download docutils literal notranslate"><span class="pre">example</span></code>
of such a class for an open source
<a class="reference external" href="https://www.vaultproject.io/api/secret/transit">KMS</a> can be found in the Apache
Arrow GitHub repository. The production KMS client should be designed in
cooperation with an organization’s security administrators, and built by
developers with experience in access control management. Once such a class is
created, it can be passed to applications via a factory method and leveraged
by general PyArrow users as shown in the encrypted parquet write/read sample
above.</p>
</section>
<section id="kms-connection-configuration">
<h3>KMS connection configuration<a class="headerlink" href="#kms-connection-configuration" title="Permalink to this heading">#</a></h3>
<p>Configuration of connection to KMS (<a class="reference internal" href="generated/pyarrow.parquet.encryption.KmsConnectionConfig.html#pyarrow.parquet.encryption.KmsConnectionConfig" title="pyarrow.parquet.encryption.KmsConnectionConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">pyarrow.parquet.encryption.KmsConnectionConfig</span></code></a>
used when creating file encryption and decryption properties) includes the
following options:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">kms_instance_url</span></code>, URL of the KMS instance.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">kms_instance_id</span></code>, ID of the KMS instance that will be used for encryption
(if multiple KMS instances are available).</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">key_access_token</span></code>, authorization token that will be passed to KMS.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">custom_kms_conf</span></code>, a string dictionary with KMS-type-specific configuration.</p></li>
</ul>
</section>
<section id="encryption-configuration">
<h3>Encryption configuration<a class="headerlink" href="#encryption-configuration" title="Permalink to this heading">#</a></h3>
<p><a class="reference internal" href="generated/pyarrow.parquet.encryption.EncryptionConfiguration.html#pyarrow.parquet.encryption.EncryptionConfiguration" title="pyarrow.parquet.encryption.EncryptionConfiguration"><code class="xref py py-class docutils literal notranslate"><span class="pre">pyarrow.parquet.encryption.EncryptionConfiguration</span></code></a> (used when
creating file encryption properties) includes the following options:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">footer_key</span></code>, the ID of the master key for footer encryption/signing.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">column_keys</span></code>, which columns to encrypt with which key. Dictionary with
master key IDs as the keys, and column name lists as the values,
e.g. <code class="docutils literal notranslate"><span class="pre">{key1:</span> <span class="pre">[col1,</span> <span class="pre">col2],</span> <span class="pre">key2:</span> <span class="pre">[col3]}</span></code>. See notes on nested fields below.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">encryption_algorithm</span></code>, the Parquet encryption algorithm.
Can be <code class="docutils literal notranslate"><span class="pre">AES_GCM_V1</span></code> (default) or <code class="docutils literal notranslate"><span class="pre">AES_GCM_CTR_V1</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">plaintext_footer</span></code>, whether to write the file footer in plain text (otherwise it is encrypted).</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">double_wrapping</span></code>, whether to use double wrapping - where data encryption keys (DEKs)
are encrypted with key encryption keys (KEKs), which in turn are encrypted
with master encryption keys (MEKs). If set to <code class="docutils literal notranslate"><span class="pre">false</span></code>, single wrapping is
used - where DEKs are encrypted directly with MEKs.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">cache_lifetime</span></code>, the lifetime of cached entities (key encryption keys,
local wrapping keys, KMS client objects) represented as a <code class="docutils literal notranslate"><span class="pre">datetime.timedelta</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">internal_key_material</span></code>, whether to store key material inside Parquet file footers;
this mode doesn’t produce additional files. If set to <code class="docutils literal notranslate"><span class="pre">false</span></code>, key material is
stored in separate files in the same folder, which enables key rotation for
immutable Parquet files.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">data_key_length_bits</span></code>, the length of data encryption keys (DEKs), randomly
generated by Parquet key management tools. Can be 128, 192 or 256 bits.</p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>When <code class="docutils literal notranslate"><span class="pre">double_wrapping</span></code> is true, Parquet implements a “double envelope
encryption” mode that minimizes the interaction of the program with a KMS
server. In this mode, the DEKs are encrypted with “key encryption keys”
(KEKs, randomly generated by Parquet). The KEKs are encrypted with “master
encryption keys” (MEKs) in the KMS; the result and the KEK itself are
cached in the process memory.</p>
</div>
<p>An example encryption configuration:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">encryption_config</span> <span class="o">=</span> <span class="n">pq</span><span class="o">.</span><span class="n">EncryptionConfiguration</span><span class="p">(</span>
   <span class="n">footer_key</span><span class="o">=</span><span class="s2">&quot;footer_key_name&quot;</span><span class="p">,</span>
   <span class="n">column_keys</span><span class="o">=</span><span class="p">{</span>
      <span class="s2">&quot;column_key_name&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;Column1&quot;</span><span class="p">,</span> <span class="s2">&quot;Column2&quot;</span><span class="p">],</span>
   <span class="p">},</span>
<span class="p">)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Encrypting columns that have nested fields (struct, map or list data types)
requires column keys for the inner fields, not the outer column itself.
Configuring a column key for the outer column causes
this error (here the column name is <code class="docutils literal notranslate"><span class="pre">col</span></code>):</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="ne">OSError</span><span class="p">:</span> <span class="n">Encrypted</span> <span class="n">column</span> <span class="n">col</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">file</span> <span class="n">schema</span>
</pre></div>
</div>
</div>
<p>An example encryption configuration for columns with nested fields, where
all columns will be encrypted with the same key identified by <code class="docutils literal notranslate"><span class="pre">column_key_id</span></code>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">pyarrow.parquet.encryption</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pe</span>

<span class="n">schema</span> <span class="o">=</span> <span class="n">pa</span><span class="o">.</span><span class="n">schema</span><span class="p">([</span>
  <span class="p">(</span><span class="s2">&quot;ListColumn&quot;</span><span class="p">,</span> <span class="n">pa</span><span class="o">.</span><span class="n">list_</span><span class="p">(</span><span class="n">pa</span><span class="o">.</span><span class="n">int32</span><span class="p">())),</span>
  <span class="p">(</span><span class="s2">&quot;MapColumn&quot;</span><span class="p">,</span> <span class="n">pa</span><span class="o">.</span><span class="n">map_</span><span class="p">(</span><span class="n">pa</span><span class="o">.</span><span class="n">string</span><span class="p">(),</span> <span class="n">pa</span><span class="o">.</span><span class="n">int32</span><span class="p">())),</span>
  <span class="p">(</span><span class="s2">&quot;StructColumn&quot;</span><span class="p">,</span> <span class="n">pa</span><span class="o">.</span><span class="n">struct</span><span class="p">([(</span><span class="s2">&quot;f1&quot;</span><span class="p">,</span> <span class="n">pa</span><span class="o">.</span><span class="n">int32</span><span class="p">()),</span> <span class="p">(</span><span class="s2">&quot;f2&quot;</span><span class="p">,</span> <span class="n">pa</span><span class="o">.</span><span class="n">string</span><span class="p">())])),</span>
<span class="p">])</span>

<span class="n">encryption_config</span> <span class="o">=</span> <span class="n">pe</span><span class="o">.</span><span class="n">EncryptionConfiguration</span><span class="p">(</span>
   <span class="n">footer_key</span><span class="o">=</span><span class="s2">&quot;footer_key_name&quot;</span><span class="p">,</span>
   <span class="n">column_keys</span><span class="o">=</span><span class="p">{</span>
      <span class="s2">&quot;column_key_id&quot;</span><span class="p">:</span> <span class="p">[</span>
        <span class="s2">&quot;ListColumn.list.element&quot;</span><span class="p">,</span>
        <span class="s2">&quot;MapColumn.key_value.key&quot;</span><span class="p">,</span> <span class="s2">&quot;MapColumn.key_value.value&quot;</span><span class="p">,</span>
        <span class="s2">&quot;StructColumn.f1&quot;</span><span class="p">,</span> <span class="s2">&quot;StructColumn.f2&quot;</span>
      <span class="p">],</span>
   <span class="p">},</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
<section id="decryption-configuration">
<h3>Decryption configuration<a class="headerlink" href="#decryption-configuration" title="Permalink to this heading">#</a></h3>
<p><a class="reference internal" href="generated/pyarrow.parquet.encryption.DecryptionConfiguration.html#pyarrow.parquet.encryption.DecryptionConfiguration" title="pyarrow.parquet.encryption.DecryptionConfiguration"><code class="xref py py-class docutils literal notranslate"><span class="pre">pyarrow.parquet.encryption.DecryptionConfiguration</span></code></a> (used when creating
file decryption properties) is optional and it includes the following options:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">cache_lifetime</span></code>, the lifetime of cached entities (key encryption keys, local
wrapping keys, KMS client objects) represented as a <code class="docutils literal notranslate"><span class="pre">datetime.timedelta</span></code>.</p></li>
</ul>
</section>
</section>
<section id="content-defined-chunking">
<h2>Content-Defined Chunking<a class="headerlink" href="#content-defined-chunking" title="Permalink to this heading">#</a></h2>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This feature is experimental and may change in future releases.</p>
</div>
<p>PyArrow introduces an experimental feature for optimizing Parquet files for content
addressable storage (CAS) systems using content-defined chunking (CDC). This feature
enables efficient deduplication of data across files, improving network transfers and
storage efficiency.</p>
<p>When enabled, data pages are written according to content-defined chunk boundaries,
determined by a rolling hash algorithm that identifies chunk boundaries based on the
actual content of the data. When data in a column is modified (e.g., inserted, deleted,
or updated), this approach minimizes the number of changed data pages.</p>
<p>The feature can be enabled by setting the <code class="docutils literal notranslate"><span class="pre">use_content_defined_chunking</span></code> parameter in
the Parquet writer. It accepts either a boolean or a dictionary for configuration:</p>
<ul class="simple">
<li><dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">True</span></code>: Uses the default configuration with:</dt><dd><ul>
<li><p>Minimum chunk size: 256 KiB</p></li>
<li><p>Maximum chunk size: 1024 KiB</p></li>
<li><p>Normalization level: 0</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">dict</span></code>: Allows customization of the chunking parameters:</dt><dd><ul>
<li><p><code class="docutils literal notranslate"><span class="pre">min_chunk_size</span></code>: Minimum chunk size in bytes (default: 256 KiB).</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">max_chunk_size</span></code>: Maximum chunk size in bytes (default: 1024 KiB).</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">norm_level</span></code>: Normalization level to adjust chunk size distribution (default: 0).</p></li>
</ul>
</dd>
</dl>
</li>
</ul>
<p>Note that the chunk size is calculated on the logical values before applying any encoding
or compression. The actual size of the data pages may vary based on the encoding and
compression used.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>To make the most of this feature, you should ensure that Parquet write options
remain consistent across writes and files.
Using different write options (like compression, encoding, or row group size)
for different files may prevent proper deduplication and lead to suboptimal
storage efficiency.</p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">pyarrow</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pa</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pyarrow.parquet</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">p</span>

<span class="n">table</span> <span class="o">=</span> <span class="n">pa</span><span class="o">.</span><span class="n">Table</span><span class="o">.</span><span class="n">from_pandas</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>

<span class="c1"># Enable content-defined chunking with default settings</span>
<span class="n">pq</span><span class="o">.</span><span class="n">write_table</span><span class="p">(</span><span class="n">table</span><span class="p">,</span> <span class="s1">&#39;example.parquet&#39;</span><span class="p">,</span> <span class="n">use_content_defined_chunking</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Enable content-defined chunking with custom settings</span>
<span class="n">pq</span><span class="o">.</span><span class="n">write_table</span><span class="p">(</span>
    <span class="n">table</span><span class="p">,</span>
    <span class="s1">&#39;example_custom.parquet&#39;</span><span class="p">,</span>
    <span class="n">use_content_defined_chunking</span><span class="o">=</span><span class="p">{</span>
        <span class="s1">&#39;min_chunk_size&#39;</span><span class="p">:</span> <span class="mi">128</span> <span class="o">*</span> <span class="mi">1024</span><span class="p">,</span>  <span class="c1"># 128 KiB</span>
        <span class="s1">&#39;max_chunk_size&#39;</span><span class="p">:</span> <span class="mi">512</span> <span class="o">*</span> <span class="mi">1024</span><span class="p">,</span>  <span class="c1"># 512 KiB</span>
    <span class="p">}</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
</section>


                </article>
              
              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
</div>
                </footer>
              
            </div>
            
            
              
                <dialog id="pst-secondary-sidebar-modal"></dialog>
                <div id="pst-secondary-sidebar" class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
<div
    id="pst-page-navigation-heading-2"
    class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> On this page
  </div>
  <nav class="bd-toc-nav page-toc" aria-labelledby="pst-page-navigation-heading-2">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#obtaining-pyarrow-with-parquet-support">Obtaining pyarrow with Parquet Support</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#reading-and-writing-single-files">Reading and Writing Single Files</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#reading-parquet-and-memory-mapping">Reading Parquet and Memory Mapping</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#parquet-file-writing-options">Parquet file writing options</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#omitting-the-dataframe-index">Omitting the DataFrame index</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#finer-grained-reading-and-writing">Finer-grained Reading and Writing</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#inspecting-the-parquet-file-metadata">Inspecting the Parquet File Metadata</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#data-type-handling">Data Type Handling</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#reading-types-as-dictionaryarray">Reading types as DictionaryArray</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#storing-timestamps">Storing timestamps</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#compression-encoding-and-file-compatibility">Compression, Encoding, and File Compatibility</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#partitioned-datasets-multiple-files">Partitioned Datasets (Multiple Files)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#writing-to-partitioned-datasets">Writing to Partitioned Datasets</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#writing-metadata-and-common-metadata-files">Writing <code class="docutils literal notranslate"><span class="pre">_metadata</span></code> and <code class="docutils literal notranslate"><span class="pre">_common_metadata</span></code> files</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#reading-from-partitioned-datasets">Reading from Partitioned Datasets</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#using-with-spark">Using with Spark</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#multithreaded-reads">Multithreaded Reads</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#reading-from-cloud-storage">Reading from cloud storage</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#parquet-modular-encryption-columnar-encryption">Parquet Modular Encryption (Columnar Encryption)</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#kms-client">KMS Client</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#kms-connection-configuration">KMS connection configuration</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#encryption-configuration">Encryption configuration</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#decryption-configuration">Decryption configuration</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#content-defined-chunking">Content-Defined Chunking</a></li>
</ul>
  </nav></div>

  <div class="sidebar-secondary-item">

  
  <div class="tocsection editthispage">
    <a href="https://github.com/apache/arrow/edit/main/docs/source/python/parquet.rst">
      <i class="fa-solid fa-pencil"></i>
      
      
        
          Edit on GitHub
        
      
    </a>
  </div>
</div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
          </footer>
        
      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf"></script>
<script defer src="../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf"></script>

  <footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
  
    <div class="footer-items__start">
      
        <div class="footer-item">

  <p class="copyright">
    
      © Copyright 2016-2025 Apache Software Foundation.
Apache Arrow, Arrow, Apache, the Apache feather logo, and the Apache Arrow project logo are either registered trademarks or trademarks of The Apache Software Foundation in the United States and other countries.
      <br/>
    
  </p>
</div>
      
        <div class="footer-item">

  <p class="sphinx-version">
    Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 6.2.0.
    <br/>
  </p>
</div>
      
    </div>
  
  
  
    <div class="footer-items__end">
      
        <div class="footer-item">
<p class="theme-version">
  <!-- # L10n: Setting the PST URL as an argument as this does not need to be localized -->
  Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.16.1.
</p></div>
      
    </div>
  
</div>

  </footer>
  </body>
</html>