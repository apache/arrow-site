<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- The above meta tags *must* come first in the head; any other head content must come *after* these tags -->
    
    <title>Querying Parquet with Millisecond Latency | Apache Arrow</title>
    

    <!-- Begin Jekyll SEO tag v2.8.0 -->
<meta name="generator" content="Jekyll v4.2.0" />
<meta property="og:title" content="Querying Parquet with Millisecond Latency" />
<meta name="author" content="tustvold and alamb" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Querying Parquet with Millisecond Latency Note: this article was originally published on the InfluxData Blog. We believe that querying data in Apache Parquet files directly can achieve similar or better storage efficiency and query performance than most specialized file formats. While it requires significant engineering effort, the benefits of Parquet’s open format and broad ecosystem support make it the obvious choice for a wide class of data systems. In this article we explain several advanced techniques needed to query data stored in the Parquet format quickly that we implemented in the Apache Arrow Rust Parquet reader. Together these techniques make the Rust implementation one of, if not the fastest implementation for querying Parquet files — be it on local disk or remote object storage. It is able to query GBs of Parquet in a matter of milliseconds. We would like to acknowledge and thank InfluxData for their support of this work. InfluxData has a deep and continuing commitment to Open source software, and it sponsored much of our time for writing this blog post as well as many contributions as part of building the InfluxDB IOx Storage Engine. Background Apache Parquet is an increasingly popular open format for storing analytic datasets, and has become the de-facto standard for cost-effective, DBMS-agnostic data storage. Initially created for the Hadoop ecosystem, Parquet’s reach now expands broadly across the data analytics ecosystem due to its compelling combination of: High compression ratios Amenability to commodity blob-storage such as S3 Broad ecosystem and tooling support Portability across many different platforms and tools Support for arbitrarily structured data Increasingly other systems, such as DuckDB and Redshift allow querying data stored in Parquet directly, but support is still often a secondary consideration compared to their native (custom) file formats. Such formats include the DuckDB .duckdb file format, the Apache IOT TsFile, the Gorilla format, and others. For the first time, access to the same sophisticated query techniques, previously only available in closed source commercial implementations, are now available as open source. The required engineering capacity comes from large, well-run open source projects with global contributor communities, such as Apache Arrow and Apache Impala. Parquet file format Before diving into the details of efficiently reading from Parquet, it is important to understand the file layout. The file format is carefully designed to quickly locate the desired information, skip irrelevant portions, and decode what remains efficiently. The data in a Parquet file is broken into horizontal slices called RowGroups Each RowGroup contains a single ColumnChunk for each column in the schema For example, the following diagram illustrates a Parquet file with three columns “A”, “B” and “C” stored in two RowGroups for a total of 6 ColumnChunks. ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ ┃┏━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━┓ ┃ ┃┃┌ ─ ─ ─ ─ ─ ─ ┌ ─ ─ ─ ─ ─ ─ ┐┌ ─ ─ ─ ─ ─ ─ ┃ ┃ ┃┃ │ │ ┃ ┃ ┃┃│ │ ││ ┃ ┃ ┃┃ │ │ ┃ ┃ ┃┃│ │ ││ ┃ RowGroup ┃ ┃┃ │ │ ┃ 1 ┃ ┃┃│ │ ││ ┃ ┃ ┃┃ │ │ ┃ ┃ ┃┃└ ─ ─ ─ ─ ─ ─ └ ─ ─ ─ ─ ─ ─ ┘└ ─ ─ ─ ─ ─ ─ ┃ ┃ ┃┃ColumnChunk 1 ColumnChunk 2 ColumnChunk 3 ┃ ┃ ┃┃ (Column &quot;A&quot;) (Column &quot;B&quot;) (Column &quot;C&quot;) ┃ ┃ ┃┗━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━┛ ┃ ┃┏━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━┓ ┃ ┃┃┌ ─ ─ ─ ─ ─ ─ ┌ ─ ─ ─ ─ ─ ─ ┐┌ ─ ─ ─ ─ ─ ─ ┃ ┃ ┃┃ │ │ ┃ ┃ ┃┃│ │ ││ ┃ ┃ ┃┃ │ │ ┃ ┃ ┃┃│ │ ││ ┃ RowGroup ┃ ┃┃ │ │ ┃ 2 ┃ ┃┃│ │ ││ ┃ ┃ ┃┃ │ │ ┃ ┃ ┃┃└ ─ ─ ─ ─ ─ ─ └ ─ ─ ─ ─ ─ ─ ┘└ ─ ─ ─ ─ ─ ─ ┃ ┃ ┃┃ColumnChunk 4 ColumnChunk 5 ColumnChunk 6 ┃ ┃ ┃┃ (Column &quot;A&quot;) (Column &quot;B&quot;) (Column &quot;C&quot;) ┃ ┃ ┃┗━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━┛ ┃ ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ The logical values for a ColumnChunk are written using one of the many available encodings into one or more Data Pages appended sequentially in the file. At the end of a Parquet file is a footer, which contains important metadata, such as: The file’s schema information such as column names and types The locations of the RowGroup and ColumnChunks in the file The footer may also contain other specialized data structures: Optional statistics for each ColumnChunk including min/max values and null counts Optional pointers to OffsetIndexes containing the location of each individual Page Optional pointers to ColumnIndex containing row counts and summary statistics for each Page Optional pointers to BloomFilterData, which can quickly check if a value is present in a ColumnChunk For example, the logical structure of 2 Row Groups and 6 ColumnChunks in the previous diagram might be stored in a Parquet file as shown in the following diagram (not to scale). The pages for the ColumnChunks come first, followed by the footer. The data, the effectiveness of the encoding scheme, and the settings of the Parquet encoder determine the number of and size of the pages needed for each ColumnChunk. In this case, ColumnChunk 1 required 2 pages while ColumnChunk 6 required only 1 page. In addition to other information, the footer contains the locations of each Data Page and the types of the columns. ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐ ┃ ┃ Data Page for ColumnChunk 1 (&quot;A&quot;) ◀─┃─ ─ ─│ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘ ┃ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐ ┃ │ ┃ Data Page for ColumnChunk 1 (&quot;A&quot;) ┃ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘ ┃ │ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐ ┃ ┃ Data Page for ColumnChunk 2 (&quot;B&quot;) ┃ │ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘ ┃ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐ ┃ │ ┃ Data Page for ColumnChunk 3 (&quot;C&quot;) ┃ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘ ┃ │ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐ ┃ ┃ Data Page for ColumnChunk 3 (&quot;C&quot;) ┃ │ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘ ┃ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐ ┃ │ ┃ Data Page for ColumnChunk 3 (&quot;C&quot;) ┃ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘ ┃ │ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐ ┃ ┃ Data Page for ColumnChunk 4 (&quot;A&quot;) ◀─┃─ ─ ─│─ ┐ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘ ┃ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐ ┃ │ │ ┃ Data Page for ColumnChunk 5 (&quot;B&quot;) ┃ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘ ┃ │ │ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐ ┃ ┃ Data Page for ColumnChunk 5 (&quot;B&quot;) ┃ │ │ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘ ┃ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐ ┃ │ │ ┃ Data Page for ColumnChunk 5 (&quot;B&quot;) ┃ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘ ┃ │ │ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐ ┃ ┃ Data Page for ColumnChunk 6 (&quot;C&quot;) ┃ │ │ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘ ┃ ┃┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ ┃ │ │ ┃┃Footer ┃ ┃ ┃┃ ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ ┃ ┃ │ │ ┃┃ ┃File Metadata ┃ ┃ ┃ ┃┃ ┃ Schema, etc ┃ ┃ ┃ │ │ ┃┃ ┃ ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ ┃ ┃ ┃ ┃┃ ┃ ┃Row Group 1 Metadata ┃ ┃ ┃ ┃ │ │ ┃┃ ┃ ┃┏━━━━━━━━━━━━━━━━━━━┓ ┃ ┃ ┃ ┃ ┃┃ ┃ ┃┃Column &quot;A&quot; Metadata┃ Location of ┃ ┃ ┃ ┃ │ │ ┃┃ ┃ ┃┗━━━━━━━━━━━━━━━━━━━┛ first Data ┣ ─ ─ ╋ ╋ ╋ ─ ─ ┃┃ ┃ ┃┏━━━━━━━━━━━━━━━━━━━┓ Page, row ┃ ┃ ┃ ┃ │ ┃┃ ┃ ┃┃Column &quot;B&quot; Metadata┃ counts, ┃ ┃ ┃ ┃ ┃┃ ┃ ┃┗━━━━━━━━━━━━━━━━━━━┛ sizes, ┃ ┃ ┃ ┃ │ ┃┃ ┃ ┃┏━━━━━━━━━━━━━━━━━━━┓ min/max ┃ ┃ ┃ ┃ ┃┃ ┃ ┃┃Column &quot;C&quot; Metadata┃ values, etc ┃ ┃ ┃ ┃ │ ┃┃ ┃ ┃┗━━━━━━━━━━━━━━━━━━━┛ ┃ ┃ ┃ ┃ ┃┃ ┃ ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ ┃ ┃ ┃ │ ┃┃ ┃ ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ ┃ ┃ ┃ ┃┃ ┃ ┃Row Group 2 Metadata ┃ ┃ ┃ ┃ │ ┃┃ ┃ ┃┏━━━━━━━━━━━━━━━━━━━┓ Location of ┃ ┃ ┃ ┃ ┃┃ ┃ ┃┃Column &quot;A&quot; Metadata┃ first Data ┃ ┃ ┃ ┃ │ ┃┃ ┃ ┃┗━━━━━━━━━━━━━━━━━━━┛ Page, row ┣ ─ ─ ╋ ╋ ╋ ─ ─ ─ ─ ┃┃ ┃ ┃┏━━━━━━━━━━━━━━━━━━━┓ counts, ┃ ┃ ┃ ┃ ┃┃ ┃ ┃┃Column &quot;B&quot; Metadata┃ sizes, ┃ ┃ ┃ ┃ ┃┃ ┃ ┃┗━━━━━━━━━━━━━━━━━━━┛ min/max ┃ ┃ ┃ ┃ ┃┃ ┃ ┃┏━━━━━━━━━━━━━━━━━━━┓ values, etc ┃ ┃ ┃ ┃ ┃┃ ┃ ┃┃Column &quot;C&quot; Metadata┃ ┃ ┃ ┃ ┃ ┃┃ ┃ ┃┗━━━━━━━━━━━━━━━━━━━┛ ┃ ┃ ┃ ┃ ┃┃ ┃ ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ ┃ ┃ ┃ ┃┃ ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ ┃ ┃ ┃┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ ┃ ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ There are many important criteria to consider when creating Parquet files such as how to optimally order/cluster data and structure it into RowGroups and Data Pages. Such “physical design” considerations are complex, worthy of their own series of articles, and not addressed in this blog post. Instead, we focus on how to use the available structure to make queries very fast. Optimizing queries In any query processing system, the following techniques generally improve performance: Reduce the data that must be transferred from secondary storage for processing (reduce I/O) Reduce the computational load for decoding the data (reduce CPU) Interleave/pipeline the reading and decoding of the data (improve parallelism) The same principles apply to querying Parquet files, as we describe below: Decode optimization Parquet achieves impressive compression ratios by using sophisticated encoding techniques such as run length compression, dictionary encoding, delta encoding, and others. Consequently, the CPU-bound task of decoding can dominate query latency. Parquet readers can use a number of techniques to improve the latency and throughput of this task, as we have done in the Rust implementation. Vectorized decode Most analytic systems decode multiple values at a time to a columnar memory format, such as Apache Arrow, rather than processing data row-by-row. This is often called vectorized or columnar processing, and is beneficial because it: Amortizes dispatch overheads to switch on the type of column being decoded Improves cache locality by reading consecutive values from a ColumnChunk Often allows multiple values to be decoded in a single instruction. Avoid many small heap allocations with a single large allocation, yielding significant savings for variable length types such as strings and byte arrays Thus, Rust Parquet Reader implements specialized decoders for reading Parquet directly into a columnar memory format (Arrow Arrays). Streaming decode There is no relationship between which rows are stored in which Pages across ColumnChunks. For example, the logical values for the 10,000th row may be in the first page of column A and in the third page of column B. The simplest approach to vectorized decoding, and the one often initially implemented in Parquet decoders, is to decode an entire RowGroup (or ColumnChunk) at a time. However, given Parquet’s high compression ratios, a single RowGroup may well contain millions of rows. Decoding so many rows at once is non-optimal because it: Requires large amounts of intermediate RAM: typical in-memory formats optimized for processing, such as Apache Arrow, require much more than their Parquet-encoded form. Increases query latency: Subsequent processing steps (like filtering or aggregation) can only begin once the entire RowGroup (or ColumnChunk) is decoded. As such, the best Parquet readers support “streaming” data out in by producing configurable sized batches of rows on demand. The batch size must be large enough to amortize decode overhead, but small enough for efficient memory usage and to allow downstream processing to begin concurrently while the subsequent batch is decoded. ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┃ ┃ Data Page for ColumnChunk 1 │◀┃─ ┌── ─── ─── ─── ─── ┐ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┃ │ ┏━━━━━━━┓ ┌ ─ ┐ ┌ ─ ┐ ┌ ─ ┐ │ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┃ ┃ ┃ │ │ ┃ Data Page for ColumnChunk 1 │ ┃ │ ┃ ┃ ─ ▶│ │ │ │ │ │ │ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┃ ─ ─┃ ┃─ ┤ │ ─ ─ ─ ─ ─ ─ │ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┃ │ ┃ ┃ A B C │ ┃ Data Page for ColumnChunk 2 │◀┃─ ┗━━━━━━━┛ │ └── ─── ─── ─── ─── ┘ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┃ │ Parquet ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┃ Decoder │ ... ┃ Data Page for ColumnChunk 3 │ ┃ │ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┃ │ ┌── ─── ─── ─── ─── ┐ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┃ │ ┌ ─ ┐ ┌ ─ ┐ ┌ ─ ┐ │ ┃ Data Page for ColumnChunk 3 │◀┃─ │ │ │ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┃ ─ ▶│ │ │ │ │ │ │ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┃ │ ─ ─ ─ ─ ─ ─ │ ┃ Data Page for ColumnChunk 3 │ ┃ A B C │ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┃ └── ─── ─── ─── ─── ┘ ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ Parquet file Smaller in memory batches for processing While streaming is not a complicated feature to explain, the stateful nature of decoding, especially across multiple columns and arbitrarily nested data, where the relationship between rows and values is not fixed, requires complex intermediate buffering and significant engineering effort to handle correctly. Dictionary preservation Dictionary Encoding, also called categorical encoding, is a technique where each value in a column is not stored directly, but instead, an index in a separate list called a “Dictionary” is stored. This technique achieves many of the benefits of third normal form for columns that have repeated values (low cardinality) and is especially effective for columns of strings such as “City”. The first page in a ColumnChunk can optionally be a dictionary page, containing a list of values of the column’s type. Subsequent pages within this ColumnChunk can then encode an index into this dictionary, instead of encoding the values directly. Given the effectiveness of this encoding, if a Parquet decoder simply decodes dictionary data into the native type, it will inefficiently replicate the same value over and over again, which is especially disastrous for string data. To handle dictionary-encoded data efficiently, the encoding must be preserved during decode. Conveniently, many columnar formats, such as the Arrow DictionaryArray, support such compatible encodings. Preserving dictionary encoding drastically improves performance when reading to an Arrow array, in some cases in excess of 60x, as well as using significantly less memory. The major complicating factor for preserving dictionaries is that the dictionaries are stored per ColumnChunk, and therefore the dictionary changes between RowGroups. The reader must automatically recompute a dictionary for batches that span multiple RowGroups, while also optimizing for the case that batch sizes divide evenly into the number of rows per RowGroup. Additionally a column may be only partly dictionary encoded, further complicating implementation. More information on this technique and its complications can be found in the blog post on applying this technique to the C++ Parquet reader. Projection pushdown The most basic Parquet optimization, and the one most commonly described for Parquet files, is projection pushdown, which reduces both I/Oand CPU requirements. Projection in this context means “selecting some but not all of the columns.” Given how Parquet organizes data, it is straightforward to read and decode only the ColumnChunks required for the referenced columns. For example, consider a SQL query of the form SELECT B from table where A &gt; 35 This query only needs data for columns A and B (and not C) and the projection can be “pushed down” to the Parquet reader. Specifically, using the information in the footer, the Parquet reader can entirely skip fetching (I/O) and decoding (CPU) the Data Pages that store data for column C (ColumnChunk 3 and ColumnChunk 6 in our example). ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐┃ ┌─────▶ Data Page for ColumnChunk 1 (&quot;A&quot;) ┃ │ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘┃ │ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐┃ ├─────▶ Data Page for ColumnChunk 1 (&quot;A&quot;) ┃ │ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘┃ │ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐┃ ├─────▶ Data Page for ColumnChunk 2 (&quot;B&quot;) ┃ │ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘┃ │ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐┃ │ ┃ Data Page for ColumnChunk 3 (&quot;C&quot;) ┃ │ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘┃ A query that │ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐┃ accesses only │ ┃ Data Page for ColumnChunk 3 (&quot;C&quot;) ┃ columns A and B │ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘┃ can read only the │ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐┃ relevant pages, ─────┤ ┃ Data Page for ColumnChunk 3 (&quot;C&quot;) ┃ skipping any Data │ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘┃ Page for column C │ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐┃ ├─────▶ Data Page for ColumnChunk 4 (&quot;A&quot;) ┃ │ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘┃ │ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐┃ ├─────▶ Data Page for ColumnChunk 5 (&quot;B&quot;) ┃ │ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘┃ │ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐┃ ├─────▶ Data Page for ColumnChunk 5 (&quot;B&quot;) ┃ │ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘┃ │ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐┃ └─────▶ Data Page for ColumnChunk 5 (&quot;B&quot;) ┃ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘┃ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐┃ ┃ Data Page for ColumnChunk 6 (&quot;C&quot;) ┃ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘┃ ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ Predicate pushdown Similar to projection pushdown, predicate pushdown also avoids fetching and decoding data from Parquet files, but does so using filter expressions. This technique typically requires closer integration with a query engine such as DataFusion, to determine valid predicates and evaluate them during the scan. Unfortunately without careful API design, the Parquet decoder and query engine can end up tightly coupled, preventing reuse (e.g. there are different Impala and Spark implementations in Cloudera Parquet Predicate Pushdown docs). The Rust Parquet reader uses the RowSelection API to avoid this coupling. RowGroup pruning The simplest form of predicate pushdown, supported by many Parquet based query engines, uses the statistics stored in the footer to skip entire RowGroups. We call this operation RowGroup pruning, and it is analogous to partition pruning in many classical data warehouse systems. For the example query above, if the maximum value for A in a particular RowGroup is less than 35, the decoder can skip fetching and decoding any ColumnChunks from that entire RowGroup. ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ ┃Row Group 1 Metadata ┃ ┃ ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ ┃ ┃ ┃Column &quot;A&quot; Metadata Min:0 Max:15 ┃◀╋ ┐ ┃ ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ ┃ Using the min ┃ ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ ┃ │ and max values ┃ ┃Column &quot;B&quot; Metadata ┃ ┃ from the ┃ ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ ┃ │ metadata, ┃ ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ ┃ RowGroup 1 can ┃ ┃Column &quot;C&quot; Metadata ┃ ┃ ├ ─ ─ be entirely ┃ ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ ┃ skipped ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ │ (pruned) when ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ searching for ┃Row Group 2 Metadata ┃ │ rows with A &gt; ┃ ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ ┃ 35, ┃ ┃Column &quot;A&quot; Metadata Min:10 Max:50 ┃◀╋ ┘ ┃ ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ ┃ ┃ ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ ┃ ┃ ┃Column &quot;B&quot; Metadata ┃ ┃ ┃ ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ ┃ ┃ ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ ┃ ┃ ┃Column &quot;C&quot; Metadata ┃ ┃ ┃ ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ ┃ ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ Note that pruning on minimum and maximum values is effective for many data layouts and column types, but not all. Specifically, it is not as effective for columns with many distinct pseudo-random values (e.g. identifiers or uuids). Thankfully for this use case, Parquet also supports per ColumnChunk Bloom Filters. We are actively working on adding bloom filter support in Apache Rust’s implementation. Page pruning A more sophisticated form of predicate pushdown uses the optional page index in the footer metadata to rule out entire Data Pages. The decoder decodes only the corresponding rows from other columns, often skipping entire pages. The fact that pages in different ColumnChunks often contain different numbers of rows, due to various reasons, complicates this optimization. While the page index may identify the needed pages from one column, pruning a page from one column doesn’t immediately rule out entire pages in other columns. Page pruning proceeds as follows: Uses the predicates in combination with the page index to identify pages to skip Uses the offset index to determine what row ranges correspond to non-skipped pages Computes the intersection of ranges across non-skipped pages, and decodes only those rows This last point is highly non-trivial to implement, especially for nested lists where a single row may correspond to multiple values. Fortunately, the Rust Parquet reader hides this complexity internally, and can decode arbitrary RowSelections. For example, to scan Columns A and B, stored in 5 Data Pages as shown in the figure below: If the predicate is A &gt; 35, Page 1 is pruned using the page index (max value is 20), leaving a RowSelection of [200-&gt;onwards], Parquet reader skips Page 3 entirely (as its last row index is 99) (Only) the relevant rows are read by reading pages 2, 4, and 5. If the predicate is instead A &gt; 35 AND B = &quot;F&quot; the page index is even more effective Using A &gt; 35, yields a RowSelection of [200-&gt;onwards] as before Using B = &quot;F&quot;, on the remaining Page 4 and Page 5 of B, yields a RowSelection of [100-244] Intersecting the two RowSelections leaves a combined RowSelection [200-244] Parquet reader only decodes those 50 rows from Page 2 and Page 4. ┏━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┃ ┃ ┌──────────────┐ │ ┌──────────────┐ │ ┃ ┃ │ │ │ │ │ │ ┃ ┃ │ │ │ │ Page │ │ │ │ │ │ │ 3 │ ┃ ┃ │ │ │ │ min: &quot;A&quot; │ │ ┃ ┃ │ │ │ │ │ max: &quot;C&quot; │ ┃ ┃ │ Page │ │ │ first_row: 0 │ │ │ │ 1 │ │ │ │ ┃ ┃ │ min: 10 │ │ └──────────────┘ │ ┃ ┃ │ │ max: 20 │ │ ┌──────────────┐ ┃ ┃ │ first_row: 0 │ │ │ │ │ │ │ │ │ │ Page │ ┃ ┃ │ │ │ │ 4 │ │ ┃ ┃ │ │ │ │ │ min: &quot;D&quot; │ ┃ ┃ │ │ │ │ max: &quot;G&quot; │ │ │ │ │ │ │first_row: 100│ ┃ ┃ └──────────────┘ │ │ │ │ ┃ ┃ │ ┌──────────────┐ │ │ │ ┃ ┃ │ │ │ └──────────────┘ │ │ │ Page │ │ ┌──────────────┐ ┃ ┃ │ 2 │ │ │ │ │ ┃ ┃ │ │ min: 30 │ │ │ Page │ ┃ ┃ │ max: 40 │ │ │ 5 │ │ │ │first_row: 200│ │ │ min: &quot;H&quot; │ ┃ ┃ │ │ │ │ max: &quot;Z&quot; │ │ ┃ ┃ │ │ │ │ │first_row: 250│ ┃ ┃ └──────────────┘ │ │ │ │ │ │ └──────────────┘ ┃ ┃ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘ ┃ ┃ ColumnChunk ColumnChunk ┃ ┃ A B ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━┛ Support for reading and writing these indexes from Arrow C++, and by extension pyarrow/pandas, is tracked in PARQUET-1404. Late materialization The two previous forms of predicate pushdown only operated on metadata stored for RowGroups, ColumnChunks, and Data Pages prior to decoding values. However, the same techniques also extend to values of one or more columns after decoding them but prior to decoding other columns, which is often called “late materialization”. This technique is especially effective when: The predicate is very selective, i.e. filters out large numbers of rows Each row is large, either due to wide rows (e.g. JSON blobs) or many columns The selected data is clustered together The columns required by the predicate are relatively inexpensive to decode, e.g. PrimitiveArray / DictionaryArray There is additional discussion about the benefits of this technique in SPARK-36527 and Impala. For example, given the predicate A &gt; 35 AND B = &quot;F&quot; from above where the engine uses the page index to determine only 50 rows within RowSelection of [100-244] could match, using late materialization, the Parquet decoder: Decodes the 50 values of Column A Evaluates A &gt; 35 on those 50 values In this case, only 5 rows pass, resulting in the RowSelection: RowSelection[205-206] RowSelection[238-240] Only decodes the 5 rows for Column B for those selections Row Index ┌────────────────────┐ ┌────────────────────┐ 200 │ 30 │ │ &quot;F&quot; │ └────────────────────┘ └────────────────────┘ ... ... ┌────────────────────┐ ┌────────────────────┐ 205 │ 37 │─ ─ ─ ─ ─ ─▶│ &quot;F&quot; │ ├────────────────────┤ ├────────────────────┤ 206 │ 36 │─ ─ ─ ─ ─ ─▶│ &quot;G&quot; │ └────────────────────┘ └────────────────────┘ ... ... ┌────────────────────┐ ┌────────────────────┐ 238 │ 36 │─ ─ ─ ─ ─ ─▶│ &quot;F&quot; │ ├────────────────────┤ ├────────────────────┤ 239 │ 36 │─ ─ ─ ─ ─ ─▶│ &quot;G&quot; │ ├────────────────────┤ ├────────────────────┤ 240 │ 40 │─ ─ ─ ─ ─ ─▶│ &quot;G&quot; │ └────────────────────┘ └────────────────────┘ ... ... ┌────────────────────┐ ┌────────────────────┐ 244 │ 26 │ │ &quot;D&quot; │ └────────────────────┘ └────────────────────┘ Column A Column B Values Values In certain cases, such as our example where B stores single character values, the cost of late materialization machinery can outweigh the savings in decoding. However, the savings can be substantial when some of the conditions listed above are fulfilled. The query engine must decide which predicates to push down and in which order to apply them for optimal results. While it is outside the scope of this document, the same technique can be applied for multiple predicates as well as predicates on multiple columns. See the RowFilter interface in the Parquet crate for more information, and the row_filter implementation in DataFusion. I/O pushdown While Parquet was designed for efficient access on the HDFS distributed file system, it works very well with commodity blob storage systems such as AWS S3 as they have very similar characteristics: Relatively slow “random access” reads: it is much more efficient to read large (MBs) sections of data in each request than issue many requests for smaller portions Significant latency before retrieving the first byte High per-request cost: Often billed per request, regardless of number of bytes read, which incentivizes fewer requests that each read a large contiguous section of data. To read optimally from such systems, a Parquet reader must: Minimize the number of I/O requests, while also applying the various pushdown techniques to avoid fetching large amounts of unused data. Integrate with the appropriate task scheduling mechanism to interleave I/O and processing on the data that is fetched to avoid pipeline bottlenecks. As these are substantial engineering and integration challenges, many Parquet readers still require the files to be fetched in their entirety to local storage. Fetching the entire files in order to process them is not ideal for several reasons: High Latency: Decoding cannot begin until the entire file is fetched (Parquet metadata is at the end of the file, so the decoder must see the end prior to decoding the rest) Wasted work: Fetching the entire file fetches all necessary data, but also potentially lots of unnecessary data that will be skipped after reading the footer. This increases the cost unnecessarily. Requires costly “locally attached” storage (or memory): Many cloud environments do not offer computing resources with locally attached storage – they either rely on expensive network block storage such as AWS EBS or else restrict local storage to certain classes of VMs. Avoiding the need to buffer the entire file requires a sophisticated Parquet decoder, integrated with the I/O subsystem, that can initially fetch and decode the metadata followed by ranged fetches for the relevant data blocks, interleaved with the decoding of Parquet data. This optimization requires careful engineering to fetch large enough blocks of data from the object store that the per request overhead doesn’t dominate gains from reducing the bytes transferred. SPARK-36529 describes the challenges of sequential processing in more detail. ┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ │ │ Step 1: Fetch │ Parquet Parquet metadata file on ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━▼━━━━━━━┓ Remote ┃ ▒▒▒▒▒▒▒▒▒▒ ▒▒▒▒▒▒▒▒▒▒ ░░░░░░░░░░ ┃ Object ┃ ▒▒▒data▒▒▒ ▒▒▒data▒▒▒ ░metadata░ ┃ Store ┃ ▒▒▒▒▒▒▒▒▒▒ ▒▒▒▒▒▒▒▒▒▒ ░░░░░░░░░░ ┃ ┗━━━━━━━━━━━▲━━━━━━━━━━━━━━━━━━━━━▲━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ │ └ ─ ─ ─ │ │ Step 2: Fetch only ─ ─ ─ ─ ─ ─ ─ ─ ─ relevant data blocks Not included in this diagram picture are details like coalescing requests and ensuring minimum request sizes needed for an actual implementation. The Rust Parquet crate provides an async Parquet reader, to efficiently read from any AsyncFileReader that: Efficiently reads from any storage medium that supports range requests Integrates with Rust’s futures ecosystem to avoid blocking threads waiting on network I/O and easily can interleave CPU and network Requests multiple ranges simultaneously, to allow the implementation to coalesce adjacent ranges, fetch ranges in parallel, etc. Uses the pushdown techniques described previously to eliminate fetching data where possible Integrates easily with the Apache Arrow object_store crate which you can read more about here To give a sense of what is possible, the following picture shows a timeline of fetching the footer metadata from remote files, using that metadata to determine what Data Pages to read, and then fetching data and decoding simultaneously. This process often must be done for more than one file at a time in order to match network latency, bandwidth, and available CPU. begin metadata read of end read read ─ ─ ─ ┐ data of data │ begin complete block block read of │ │ │ │ metadata ─ ─ ─ ┐ At any time, there are │ │ │ │ │ multiple network │ ▼ ▼ ▼ ▼ requests outstanding to file 1 │ ░░░░░░░░░░ ▒▒▒read▒▒▒ ▒▒▒read▒▒▒ │ hide the individual │ ░░░read░░░ ▒▒▒data▒▒▒ ▒▒▒data▒▒▒ request latency │ ░metadata░ ▓▓decode▓▓ │ ░░░░░░░░░░ ▓▓▓data▓▓▓ │ │ │ │ ░░░░░░░░░░ ▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒read▒▒▒▒│▒▒▒▒▒▒▒▒▒▒▒▒▒▒ file 2 │ ░░░read░░░ ▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒data▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒ │ ░metadata░ │ ▓▓▓▓▓decode▓▓▓▓▓▓ │ ░░░░░░░░░░ ▓▓▓▓▓▓data▓▓▓▓▓▓▓ │ │ │ │ ░░│░░░░░░░ ▒▒▒read▒▒▒ ▒▒▒▒read▒▒▒▒▒ file 3 │ ░░░read░░░ ▒▒▒data▒▒▒ ▒▒▒▒data▒▒▒▒▒ ... │ ░m│tadata░ ▓▓decode▓▓ │ ░░░░░░░░░░ ▓▓▓data▓▓▓ └───────────────────────────────────────┼──────────────────────────────▶Time │ Conclusion We hope you enjoyed reading about the Parquet file format, and the various techniques used to quickly query parquet files. We believe that the reason most open source implementations of Parquet do not have the breadth of features described in this post is that it takes a monumental effort, that was previously only possible at well-financed commercial enterprises which kept their implementations closed source. However, with the growth and quality of the Apache Arrow community, both Rust practitioners and the wider Arrow community, our ability to collaborate and build a cutting-edge open source implementation is exhilarating and immensely satisfying. The technology described in this blog is the result of the contributions of many engineers spread across companies, hobbyists, and the world in several repositories, notably Apache Arrow DataFusion, Apache Arrow and Apache Arrow Ballista. If you are interested in joining the DataFusion Community, please get in touch." />
<meta property="og:description" content="Querying Parquet with Millisecond Latency Note: this article was originally published on the InfluxData Blog. We believe that querying data in Apache Parquet files directly can achieve similar or better storage efficiency and query performance than most specialized file formats. While it requires significant engineering effort, the benefits of Parquet’s open format and broad ecosystem support make it the obvious choice for a wide class of data systems. In this article we explain several advanced techniques needed to query data stored in the Parquet format quickly that we implemented in the Apache Arrow Rust Parquet reader. Together these techniques make the Rust implementation one of, if not the fastest implementation for querying Parquet files — be it on local disk or remote object storage. It is able to query GBs of Parquet in a matter of milliseconds. We would like to acknowledge and thank InfluxData for their support of this work. InfluxData has a deep and continuing commitment to Open source software, and it sponsored much of our time for writing this blog post as well as many contributions as part of building the InfluxDB IOx Storage Engine. Background Apache Parquet is an increasingly popular open format for storing analytic datasets, and has become the de-facto standard for cost-effective, DBMS-agnostic data storage. Initially created for the Hadoop ecosystem, Parquet’s reach now expands broadly across the data analytics ecosystem due to its compelling combination of: High compression ratios Amenability to commodity blob-storage such as S3 Broad ecosystem and tooling support Portability across many different platforms and tools Support for arbitrarily structured data Increasingly other systems, such as DuckDB and Redshift allow querying data stored in Parquet directly, but support is still often a secondary consideration compared to their native (custom) file formats. Such formats include the DuckDB .duckdb file format, the Apache IOT TsFile, the Gorilla format, and others. For the first time, access to the same sophisticated query techniques, previously only available in closed source commercial implementations, are now available as open source. The required engineering capacity comes from large, well-run open source projects with global contributor communities, such as Apache Arrow and Apache Impala. Parquet file format Before diving into the details of efficiently reading from Parquet, it is important to understand the file layout. The file format is carefully designed to quickly locate the desired information, skip irrelevant portions, and decode what remains efficiently. The data in a Parquet file is broken into horizontal slices called RowGroups Each RowGroup contains a single ColumnChunk for each column in the schema For example, the following diagram illustrates a Parquet file with three columns “A”, “B” and “C” stored in two RowGroups for a total of 6 ColumnChunks. ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ ┃┏━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━┓ ┃ ┃┃┌ ─ ─ ─ ─ ─ ─ ┌ ─ ─ ─ ─ ─ ─ ┐┌ ─ ─ ─ ─ ─ ─ ┃ ┃ ┃┃ │ │ ┃ ┃ ┃┃│ │ ││ ┃ ┃ ┃┃ │ │ ┃ ┃ ┃┃│ │ ││ ┃ RowGroup ┃ ┃┃ │ │ ┃ 1 ┃ ┃┃│ │ ││ ┃ ┃ ┃┃ │ │ ┃ ┃ ┃┃└ ─ ─ ─ ─ ─ ─ └ ─ ─ ─ ─ ─ ─ ┘└ ─ ─ ─ ─ ─ ─ ┃ ┃ ┃┃ColumnChunk 1 ColumnChunk 2 ColumnChunk 3 ┃ ┃ ┃┃ (Column &quot;A&quot;) (Column &quot;B&quot;) (Column &quot;C&quot;) ┃ ┃ ┃┗━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━┛ ┃ ┃┏━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━┓ ┃ ┃┃┌ ─ ─ ─ ─ ─ ─ ┌ ─ ─ ─ ─ ─ ─ ┐┌ ─ ─ ─ ─ ─ ─ ┃ ┃ ┃┃ │ │ ┃ ┃ ┃┃│ │ ││ ┃ ┃ ┃┃ │ │ ┃ ┃ ┃┃│ │ ││ ┃ RowGroup ┃ ┃┃ │ │ ┃ 2 ┃ ┃┃│ │ ││ ┃ ┃ ┃┃ │ │ ┃ ┃ ┃┃└ ─ ─ ─ ─ ─ ─ └ ─ ─ ─ ─ ─ ─ ┘└ ─ ─ ─ ─ ─ ─ ┃ ┃ ┃┃ColumnChunk 4 ColumnChunk 5 ColumnChunk 6 ┃ ┃ ┃┃ (Column &quot;A&quot;) (Column &quot;B&quot;) (Column &quot;C&quot;) ┃ ┃ ┃┗━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━┛ ┃ ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ The logical values for a ColumnChunk are written using one of the many available encodings into one or more Data Pages appended sequentially in the file. At the end of a Parquet file is a footer, which contains important metadata, such as: The file’s schema information such as column names and types The locations of the RowGroup and ColumnChunks in the file The footer may also contain other specialized data structures: Optional statistics for each ColumnChunk including min/max values and null counts Optional pointers to OffsetIndexes containing the location of each individual Page Optional pointers to ColumnIndex containing row counts and summary statistics for each Page Optional pointers to BloomFilterData, which can quickly check if a value is present in a ColumnChunk For example, the logical structure of 2 Row Groups and 6 ColumnChunks in the previous diagram might be stored in a Parquet file as shown in the following diagram (not to scale). The pages for the ColumnChunks come first, followed by the footer. The data, the effectiveness of the encoding scheme, and the settings of the Parquet encoder determine the number of and size of the pages needed for each ColumnChunk. In this case, ColumnChunk 1 required 2 pages while ColumnChunk 6 required only 1 page. In addition to other information, the footer contains the locations of each Data Page and the types of the columns. ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐ ┃ ┃ Data Page for ColumnChunk 1 (&quot;A&quot;) ◀─┃─ ─ ─│ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘ ┃ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐ ┃ │ ┃ Data Page for ColumnChunk 1 (&quot;A&quot;) ┃ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘ ┃ │ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐ ┃ ┃ Data Page for ColumnChunk 2 (&quot;B&quot;) ┃ │ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘ ┃ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐ ┃ │ ┃ Data Page for ColumnChunk 3 (&quot;C&quot;) ┃ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘ ┃ │ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐ ┃ ┃ Data Page for ColumnChunk 3 (&quot;C&quot;) ┃ │ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘ ┃ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐ ┃ │ ┃ Data Page for ColumnChunk 3 (&quot;C&quot;) ┃ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘ ┃ │ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐ ┃ ┃ Data Page for ColumnChunk 4 (&quot;A&quot;) ◀─┃─ ─ ─│─ ┐ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘ ┃ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐ ┃ │ │ ┃ Data Page for ColumnChunk 5 (&quot;B&quot;) ┃ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘ ┃ │ │ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐ ┃ ┃ Data Page for ColumnChunk 5 (&quot;B&quot;) ┃ │ │ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘ ┃ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐ ┃ │ │ ┃ Data Page for ColumnChunk 5 (&quot;B&quot;) ┃ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘ ┃ │ │ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐ ┃ ┃ Data Page for ColumnChunk 6 (&quot;C&quot;) ┃ │ │ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘ ┃ ┃┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ ┃ │ │ ┃┃Footer ┃ ┃ ┃┃ ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ ┃ ┃ │ │ ┃┃ ┃File Metadata ┃ ┃ ┃ ┃┃ ┃ Schema, etc ┃ ┃ ┃ │ │ ┃┃ ┃ ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ ┃ ┃ ┃ ┃┃ ┃ ┃Row Group 1 Metadata ┃ ┃ ┃ ┃ │ │ ┃┃ ┃ ┃┏━━━━━━━━━━━━━━━━━━━┓ ┃ ┃ ┃ ┃ ┃┃ ┃ ┃┃Column &quot;A&quot; Metadata┃ Location of ┃ ┃ ┃ ┃ │ │ ┃┃ ┃ ┃┗━━━━━━━━━━━━━━━━━━━┛ first Data ┣ ─ ─ ╋ ╋ ╋ ─ ─ ┃┃ ┃ ┃┏━━━━━━━━━━━━━━━━━━━┓ Page, row ┃ ┃ ┃ ┃ │ ┃┃ ┃ ┃┃Column &quot;B&quot; Metadata┃ counts, ┃ ┃ ┃ ┃ ┃┃ ┃ ┃┗━━━━━━━━━━━━━━━━━━━┛ sizes, ┃ ┃ ┃ ┃ │ ┃┃ ┃ ┃┏━━━━━━━━━━━━━━━━━━━┓ min/max ┃ ┃ ┃ ┃ ┃┃ ┃ ┃┃Column &quot;C&quot; Metadata┃ values, etc ┃ ┃ ┃ ┃ │ ┃┃ ┃ ┃┗━━━━━━━━━━━━━━━━━━━┛ ┃ ┃ ┃ ┃ ┃┃ ┃ ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ ┃ ┃ ┃ │ ┃┃ ┃ ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ ┃ ┃ ┃ ┃┃ ┃ ┃Row Group 2 Metadata ┃ ┃ ┃ ┃ │ ┃┃ ┃ ┃┏━━━━━━━━━━━━━━━━━━━┓ Location of ┃ ┃ ┃ ┃ ┃┃ ┃ ┃┃Column &quot;A&quot; Metadata┃ first Data ┃ ┃ ┃ ┃ │ ┃┃ ┃ ┃┗━━━━━━━━━━━━━━━━━━━┛ Page, row ┣ ─ ─ ╋ ╋ ╋ ─ ─ ─ ─ ┃┃ ┃ ┃┏━━━━━━━━━━━━━━━━━━━┓ counts, ┃ ┃ ┃ ┃ ┃┃ ┃ ┃┃Column &quot;B&quot; Metadata┃ sizes, ┃ ┃ ┃ ┃ ┃┃ ┃ ┃┗━━━━━━━━━━━━━━━━━━━┛ min/max ┃ ┃ ┃ ┃ ┃┃ ┃ ┃┏━━━━━━━━━━━━━━━━━━━┓ values, etc ┃ ┃ ┃ ┃ ┃┃ ┃ ┃┃Column &quot;C&quot; Metadata┃ ┃ ┃ ┃ ┃ ┃┃ ┃ ┃┗━━━━━━━━━━━━━━━━━━━┛ ┃ ┃ ┃ ┃ ┃┃ ┃ ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ ┃ ┃ ┃ ┃┃ ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ ┃ ┃ ┃┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ ┃ ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ There are many important criteria to consider when creating Parquet files such as how to optimally order/cluster data and structure it into RowGroups and Data Pages. Such “physical design” considerations are complex, worthy of their own series of articles, and not addressed in this blog post. Instead, we focus on how to use the available structure to make queries very fast. Optimizing queries In any query processing system, the following techniques generally improve performance: Reduce the data that must be transferred from secondary storage for processing (reduce I/O) Reduce the computational load for decoding the data (reduce CPU) Interleave/pipeline the reading and decoding of the data (improve parallelism) The same principles apply to querying Parquet files, as we describe below: Decode optimization Parquet achieves impressive compression ratios by using sophisticated encoding techniques such as run length compression, dictionary encoding, delta encoding, and others. Consequently, the CPU-bound task of decoding can dominate query latency. Parquet readers can use a number of techniques to improve the latency and throughput of this task, as we have done in the Rust implementation. Vectorized decode Most analytic systems decode multiple values at a time to a columnar memory format, such as Apache Arrow, rather than processing data row-by-row. This is often called vectorized or columnar processing, and is beneficial because it: Amortizes dispatch overheads to switch on the type of column being decoded Improves cache locality by reading consecutive values from a ColumnChunk Often allows multiple values to be decoded in a single instruction. Avoid many small heap allocations with a single large allocation, yielding significant savings for variable length types such as strings and byte arrays Thus, Rust Parquet Reader implements specialized decoders for reading Parquet directly into a columnar memory format (Arrow Arrays). Streaming decode There is no relationship between which rows are stored in which Pages across ColumnChunks. For example, the logical values for the 10,000th row may be in the first page of column A and in the third page of column B. The simplest approach to vectorized decoding, and the one often initially implemented in Parquet decoders, is to decode an entire RowGroup (or ColumnChunk) at a time. However, given Parquet’s high compression ratios, a single RowGroup may well contain millions of rows. Decoding so many rows at once is non-optimal because it: Requires large amounts of intermediate RAM: typical in-memory formats optimized for processing, such as Apache Arrow, require much more than their Parquet-encoded form. Increases query latency: Subsequent processing steps (like filtering or aggregation) can only begin once the entire RowGroup (or ColumnChunk) is decoded. As such, the best Parquet readers support “streaming” data out in by producing configurable sized batches of rows on demand. The batch size must be large enough to amortize decode overhead, but small enough for efficient memory usage and to allow downstream processing to begin concurrently while the subsequent batch is decoded. ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┃ ┃ Data Page for ColumnChunk 1 │◀┃─ ┌── ─── ─── ─── ─── ┐ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┃ │ ┏━━━━━━━┓ ┌ ─ ┐ ┌ ─ ┐ ┌ ─ ┐ │ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┃ ┃ ┃ │ │ ┃ Data Page for ColumnChunk 1 │ ┃ │ ┃ ┃ ─ ▶│ │ │ │ │ │ │ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┃ ─ ─┃ ┃─ ┤ │ ─ ─ ─ ─ ─ ─ │ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┃ │ ┃ ┃ A B C │ ┃ Data Page for ColumnChunk 2 │◀┃─ ┗━━━━━━━┛ │ └── ─── ─── ─── ─── ┘ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┃ │ Parquet ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┃ Decoder │ ... ┃ Data Page for ColumnChunk 3 │ ┃ │ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┃ │ ┌── ─── ─── ─── ─── ┐ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┃ │ ┌ ─ ┐ ┌ ─ ┐ ┌ ─ ┐ │ ┃ Data Page for ColumnChunk 3 │◀┃─ │ │ │ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┃ ─ ▶│ │ │ │ │ │ │ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┃ │ ─ ─ ─ ─ ─ ─ │ ┃ Data Page for ColumnChunk 3 │ ┃ A B C │ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┃ └── ─── ─── ─── ─── ┘ ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ Parquet file Smaller in memory batches for processing While streaming is not a complicated feature to explain, the stateful nature of decoding, especially across multiple columns and arbitrarily nested data, where the relationship between rows and values is not fixed, requires complex intermediate buffering and significant engineering effort to handle correctly. Dictionary preservation Dictionary Encoding, also called categorical encoding, is a technique where each value in a column is not stored directly, but instead, an index in a separate list called a “Dictionary” is stored. This technique achieves many of the benefits of third normal form for columns that have repeated values (low cardinality) and is especially effective for columns of strings such as “City”. The first page in a ColumnChunk can optionally be a dictionary page, containing a list of values of the column’s type. Subsequent pages within this ColumnChunk can then encode an index into this dictionary, instead of encoding the values directly. Given the effectiveness of this encoding, if a Parquet decoder simply decodes dictionary data into the native type, it will inefficiently replicate the same value over and over again, which is especially disastrous for string data. To handle dictionary-encoded data efficiently, the encoding must be preserved during decode. Conveniently, many columnar formats, such as the Arrow DictionaryArray, support such compatible encodings. Preserving dictionary encoding drastically improves performance when reading to an Arrow array, in some cases in excess of 60x, as well as using significantly less memory. The major complicating factor for preserving dictionaries is that the dictionaries are stored per ColumnChunk, and therefore the dictionary changes between RowGroups. The reader must automatically recompute a dictionary for batches that span multiple RowGroups, while also optimizing for the case that batch sizes divide evenly into the number of rows per RowGroup. Additionally a column may be only partly dictionary encoded, further complicating implementation. More information on this technique and its complications can be found in the blog post on applying this technique to the C++ Parquet reader. Projection pushdown The most basic Parquet optimization, and the one most commonly described for Parquet files, is projection pushdown, which reduces both I/Oand CPU requirements. Projection in this context means “selecting some but not all of the columns.” Given how Parquet organizes data, it is straightforward to read and decode only the ColumnChunks required for the referenced columns. For example, consider a SQL query of the form SELECT B from table where A &gt; 35 This query only needs data for columns A and B (and not C) and the projection can be “pushed down” to the Parquet reader. Specifically, using the information in the footer, the Parquet reader can entirely skip fetching (I/O) and decoding (CPU) the Data Pages that store data for column C (ColumnChunk 3 and ColumnChunk 6 in our example). ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐┃ ┌─────▶ Data Page for ColumnChunk 1 (&quot;A&quot;) ┃ │ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘┃ │ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐┃ ├─────▶ Data Page for ColumnChunk 1 (&quot;A&quot;) ┃ │ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘┃ │ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐┃ ├─────▶ Data Page for ColumnChunk 2 (&quot;B&quot;) ┃ │ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘┃ │ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐┃ │ ┃ Data Page for ColumnChunk 3 (&quot;C&quot;) ┃ │ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘┃ A query that │ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐┃ accesses only │ ┃ Data Page for ColumnChunk 3 (&quot;C&quot;) ┃ columns A and B │ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘┃ can read only the │ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐┃ relevant pages, ─────┤ ┃ Data Page for ColumnChunk 3 (&quot;C&quot;) ┃ skipping any Data │ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘┃ Page for column C │ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐┃ ├─────▶ Data Page for ColumnChunk 4 (&quot;A&quot;) ┃ │ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘┃ │ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐┃ ├─────▶ Data Page for ColumnChunk 5 (&quot;B&quot;) ┃ │ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘┃ │ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐┃ ├─────▶ Data Page for ColumnChunk 5 (&quot;B&quot;) ┃ │ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘┃ │ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐┃ └─────▶ Data Page for ColumnChunk 5 (&quot;B&quot;) ┃ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘┃ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐┃ ┃ Data Page for ColumnChunk 6 (&quot;C&quot;) ┃ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘┃ ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ Predicate pushdown Similar to projection pushdown, predicate pushdown also avoids fetching and decoding data from Parquet files, but does so using filter expressions. This technique typically requires closer integration with a query engine such as DataFusion, to determine valid predicates and evaluate them during the scan. Unfortunately without careful API design, the Parquet decoder and query engine can end up tightly coupled, preventing reuse (e.g. there are different Impala and Spark implementations in Cloudera Parquet Predicate Pushdown docs). The Rust Parquet reader uses the RowSelection API to avoid this coupling. RowGroup pruning The simplest form of predicate pushdown, supported by many Parquet based query engines, uses the statistics stored in the footer to skip entire RowGroups. We call this operation RowGroup pruning, and it is analogous to partition pruning in many classical data warehouse systems. For the example query above, if the maximum value for A in a particular RowGroup is less than 35, the decoder can skip fetching and decoding any ColumnChunks from that entire RowGroup. ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ ┃Row Group 1 Metadata ┃ ┃ ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ ┃ ┃ ┃Column &quot;A&quot; Metadata Min:0 Max:15 ┃◀╋ ┐ ┃ ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ ┃ Using the min ┃ ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ ┃ │ and max values ┃ ┃Column &quot;B&quot; Metadata ┃ ┃ from the ┃ ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ ┃ │ metadata, ┃ ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ ┃ RowGroup 1 can ┃ ┃Column &quot;C&quot; Metadata ┃ ┃ ├ ─ ─ be entirely ┃ ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ ┃ skipped ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ │ (pruned) when ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ searching for ┃Row Group 2 Metadata ┃ │ rows with A &gt; ┃ ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ ┃ 35, ┃ ┃Column &quot;A&quot; Metadata Min:10 Max:50 ┃◀╋ ┘ ┃ ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ ┃ ┃ ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ ┃ ┃ ┃Column &quot;B&quot; Metadata ┃ ┃ ┃ ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ ┃ ┃ ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ ┃ ┃ ┃Column &quot;C&quot; Metadata ┃ ┃ ┃ ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ ┃ ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ Note that pruning on minimum and maximum values is effective for many data layouts and column types, but not all. Specifically, it is not as effective for columns with many distinct pseudo-random values (e.g. identifiers or uuids). Thankfully for this use case, Parquet also supports per ColumnChunk Bloom Filters. We are actively working on adding bloom filter support in Apache Rust’s implementation. Page pruning A more sophisticated form of predicate pushdown uses the optional page index in the footer metadata to rule out entire Data Pages. The decoder decodes only the corresponding rows from other columns, often skipping entire pages. The fact that pages in different ColumnChunks often contain different numbers of rows, due to various reasons, complicates this optimization. While the page index may identify the needed pages from one column, pruning a page from one column doesn’t immediately rule out entire pages in other columns. Page pruning proceeds as follows: Uses the predicates in combination with the page index to identify pages to skip Uses the offset index to determine what row ranges correspond to non-skipped pages Computes the intersection of ranges across non-skipped pages, and decodes only those rows This last point is highly non-trivial to implement, especially for nested lists where a single row may correspond to multiple values. Fortunately, the Rust Parquet reader hides this complexity internally, and can decode arbitrary RowSelections. For example, to scan Columns A and B, stored in 5 Data Pages as shown in the figure below: If the predicate is A &gt; 35, Page 1 is pruned using the page index (max value is 20), leaving a RowSelection of [200-&gt;onwards], Parquet reader skips Page 3 entirely (as its last row index is 99) (Only) the relevant rows are read by reading pages 2, 4, and 5. If the predicate is instead A &gt; 35 AND B = &quot;F&quot; the page index is even more effective Using A &gt; 35, yields a RowSelection of [200-&gt;onwards] as before Using B = &quot;F&quot;, on the remaining Page 4 and Page 5 of B, yields a RowSelection of [100-244] Intersecting the two RowSelections leaves a combined RowSelection [200-244] Parquet reader only decodes those 50 rows from Page 2 and Page 4. ┏━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┃ ┃ ┌──────────────┐ │ ┌──────────────┐ │ ┃ ┃ │ │ │ │ │ │ ┃ ┃ │ │ │ │ Page │ │ │ │ │ │ │ 3 │ ┃ ┃ │ │ │ │ min: &quot;A&quot; │ │ ┃ ┃ │ │ │ │ │ max: &quot;C&quot; │ ┃ ┃ │ Page │ │ │ first_row: 0 │ │ │ │ 1 │ │ │ │ ┃ ┃ │ min: 10 │ │ └──────────────┘ │ ┃ ┃ │ │ max: 20 │ │ ┌──────────────┐ ┃ ┃ │ first_row: 0 │ │ │ │ │ │ │ │ │ │ Page │ ┃ ┃ │ │ │ │ 4 │ │ ┃ ┃ │ │ │ │ │ min: &quot;D&quot; │ ┃ ┃ │ │ │ │ max: &quot;G&quot; │ │ │ │ │ │ │first_row: 100│ ┃ ┃ └──────────────┘ │ │ │ │ ┃ ┃ │ ┌──────────────┐ │ │ │ ┃ ┃ │ │ │ └──────────────┘ │ │ │ Page │ │ ┌──────────────┐ ┃ ┃ │ 2 │ │ │ │ │ ┃ ┃ │ │ min: 30 │ │ │ Page │ ┃ ┃ │ max: 40 │ │ │ 5 │ │ │ │first_row: 200│ │ │ min: &quot;H&quot; │ ┃ ┃ │ │ │ │ max: &quot;Z&quot; │ │ ┃ ┃ │ │ │ │ │first_row: 250│ ┃ ┃ └──────────────┘ │ │ │ │ │ │ └──────────────┘ ┃ ┃ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘ ┃ ┃ ColumnChunk ColumnChunk ┃ ┃ A B ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━┛ Support for reading and writing these indexes from Arrow C++, and by extension pyarrow/pandas, is tracked in PARQUET-1404. Late materialization The two previous forms of predicate pushdown only operated on metadata stored for RowGroups, ColumnChunks, and Data Pages prior to decoding values. However, the same techniques also extend to values of one or more columns after decoding them but prior to decoding other columns, which is often called “late materialization”. This technique is especially effective when: The predicate is very selective, i.e. filters out large numbers of rows Each row is large, either due to wide rows (e.g. JSON blobs) or many columns The selected data is clustered together The columns required by the predicate are relatively inexpensive to decode, e.g. PrimitiveArray / DictionaryArray There is additional discussion about the benefits of this technique in SPARK-36527 and Impala. For example, given the predicate A &gt; 35 AND B = &quot;F&quot; from above where the engine uses the page index to determine only 50 rows within RowSelection of [100-244] could match, using late materialization, the Parquet decoder: Decodes the 50 values of Column A Evaluates A &gt; 35 on those 50 values In this case, only 5 rows pass, resulting in the RowSelection: RowSelection[205-206] RowSelection[238-240] Only decodes the 5 rows for Column B for those selections Row Index ┌────────────────────┐ ┌────────────────────┐ 200 │ 30 │ │ &quot;F&quot; │ └────────────────────┘ └────────────────────┘ ... ... ┌────────────────────┐ ┌────────────────────┐ 205 │ 37 │─ ─ ─ ─ ─ ─▶│ &quot;F&quot; │ ├────────────────────┤ ├────────────────────┤ 206 │ 36 │─ ─ ─ ─ ─ ─▶│ &quot;G&quot; │ └────────────────────┘ └────────────────────┘ ... ... ┌────────────────────┐ ┌────────────────────┐ 238 │ 36 │─ ─ ─ ─ ─ ─▶│ &quot;F&quot; │ ├────────────────────┤ ├────────────────────┤ 239 │ 36 │─ ─ ─ ─ ─ ─▶│ &quot;G&quot; │ ├────────────────────┤ ├────────────────────┤ 240 │ 40 │─ ─ ─ ─ ─ ─▶│ &quot;G&quot; │ └────────────────────┘ └────────────────────┘ ... ... ┌────────────────────┐ ┌────────────────────┐ 244 │ 26 │ │ &quot;D&quot; │ └────────────────────┘ └────────────────────┘ Column A Column B Values Values In certain cases, such as our example where B stores single character values, the cost of late materialization machinery can outweigh the savings in decoding. However, the savings can be substantial when some of the conditions listed above are fulfilled. The query engine must decide which predicates to push down and in which order to apply them for optimal results. While it is outside the scope of this document, the same technique can be applied for multiple predicates as well as predicates on multiple columns. See the RowFilter interface in the Parquet crate for more information, and the row_filter implementation in DataFusion. I/O pushdown While Parquet was designed for efficient access on the HDFS distributed file system, it works very well with commodity blob storage systems such as AWS S3 as they have very similar characteristics: Relatively slow “random access” reads: it is much more efficient to read large (MBs) sections of data in each request than issue many requests for smaller portions Significant latency before retrieving the first byte High per-request cost: Often billed per request, regardless of number of bytes read, which incentivizes fewer requests that each read a large contiguous section of data. To read optimally from such systems, a Parquet reader must: Minimize the number of I/O requests, while also applying the various pushdown techniques to avoid fetching large amounts of unused data. Integrate with the appropriate task scheduling mechanism to interleave I/O and processing on the data that is fetched to avoid pipeline bottlenecks. As these are substantial engineering and integration challenges, many Parquet readers still require the files to be fetched in their entirety to local storage. Fetching the entire files in order to process them is not ideal for several reasons: High Latency: Decoding cannot begin until the entire file is fetched (Parquet metadata is at the end of the file, so the decoder must see the end prior to decoding the rest) Wasted work: Fetching the entire file fetches all necessary data, but also potentially lots of unnecessary data that will be skipped after reading the footer. This increases the cost unnecessarily. Requires costly “locally attached” storage (or memory): Many cloud environments do not offer computing resources with locally attached storage – they either rely on expensive network block storage such as AWS EBS or else restrict local storage to certain classes of VMs. Avoiding the need to buffer the entire file requires a sophisticated Parquet decoder, integrated with the I/O subsystem, that can initially fetch and decode the metadata followed by ranged fetches for the relevant data blocks, interleaved with the decoding of Parquet data. This optimization requires careful engineering to fetch large enough blocks of data from the object store that the per request overhead doesn’t dominate gains from reducing the bytes transferred. SPARK-36529 describes the challenges of sequential processing in more detail. ┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ │ │ Step 1: Fetch │ Parquet Parquet metadata file on ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━▼━━━━━━━┓ Remote ┃ ▒▒▒▒▒▒▒▒▒▒ ▒▒▒▒▒▒▒▒▒▒ ░░░░░░░░░░ ┃ Object ┃ ▒▒▒data▒▒▒ ▒▒▒data▒▒▒ ░metadata░ ┃ Store ┃ ▒▒▒▒▒▒▒▒▒▒ ▒▒▒▒▒▒▒▒▒▒ ░░░░░░░░░░ ┃ ┗━━━━━━━━━━━▲━━━━━━━━━━━━━━━━━━━━━▲━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ │ └ ─ ─ ─ │ │ Step 2: Fetch only ─ ─ ─ ─ ─ ─ ─ ─ ─ relevant data blocks Not included in this diagram picture are details like coalescing requests and ensuring minimum request sizes needed for an actual implementation. The Rust Parquet crate provides an async Parquet reader, to efficiently read from any AsyncFileReader that: Efficiently reads from any storage medium that supports range requests Integrates with Rust’s futures ecosystem to avoid blocking threads waiting on network I/O and easily can interleave CPU and network Requests multiple ranges simultaneously, to allow the implementation to coalesce adjacent ranges, fetch ranges in parallel, etc. Uses the pushdown techniques described previously to eliminate fetching data where possible Integrates easily with the Apache Arrow object_store crate which you can read more about here To give a sense of what is possible, the following picture shows a timeline of fetching the footer metadata from remote files, using that metadata to determine what Data Pages to read, and then fetching data and decoding simultaneously. This process often must be done for more than one file at a time in order to match network latency, bandwidth, and available CPU. begin metadata read of end read read ─ ─ ─ ┐ data of data │ begin complete block block read of │ │ │ │ metadata ─ ─ ─ ┐ At any time, there are │ │ │ │ │ multiple network │ ▼ ▼ ▼ ▼ requests outstanding to file 1 │ ░░░░░░░░░░ ▒▒▒read▒▒▒ ▒▒▒read▒▒▒ │ hide the individual │ ░░░read░░░ ▒▒▒data▒▒▒ ▒▒▒data▒▒▒ request latency │ ░metadata░ ▓▓decode▓▓ │ ░░░░░░░░░░ ▓▓▓data▓▓▓ │ │ │ │ ░░░░░░░░░░ ▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒read▒▒▒▒│▒▒▒▒▒▒▒▒▒▒▒▒▒▒ file 2 │ ░░░read░░░ ▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒data▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒ │ ░metadata░ │ ▓▓▓▓▓decode▓▓▓▓▓▓ │ ░░░░░░░░░░ ▓▓▓▓▓▓data▓▓▓▓▓▓▓ │ │ │ │ ░░│░░░░░░░ ▒▒▒read▒▒▒ ▒▒▒▒read▒▒▒▒▒ file 3 │ ░░░read░░░ ▒▒▒data▒▒▒ ▒▒▒▒data▒▒▒▒▒ ... │ ░m│tadata░ ▓▓decode▓▓ │ ░░░░░░░░░░ ▓▓▓data▓▓▓ └───────────────────────────────────────┼──────────────────────────────▶Time │ Conclusion We hope you enjoyed reading about the Parquet file format, and the various techniques used to quickly query parquet files. We believe that the reason most open source implementations of Parquet do not have the breadth of features described in this post is that it takes a monumental effort, that was previously only possible at well-financed commercial enterprises which kept their implementations closed source. However, with the growth and quality of the Apache Arrow community, both Rust practitioners and the wider Arrow community, our ability to collaborate and build a cutting-edge open source implementation is exhilarating and immensely satisfying. The technology described in this blog is the result of the contributions of many engineers spread across companies, hobbyists, and the world in several repositories, notably Apache Arrow DataFusion, Apache Arrow and Apache Arrow Ballista. If you are interested in joining the DataFusion Community, please get in touch." />
<link rel="canonical" href="https://arrow.apache.org/blog/2022/12/26/querying-parquet-with-millisecond-latency/" />
<meta property="og:url" content="https://arrow.apache.org/blog/2022/12/26/querying-parquet-with-millisecond-latency/" />
<meta property="og:site_name" content="Apache Arrow" />
<meta property="og:image" content="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2022-12-26T00:00:00-05:00" />
<meta name="twitter:card" content="summary_large_image" />
<meta property="twitter:image" content="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" />
<meta property="twitter:title" content="Querying Parquet with Millisecond Latency" />
<meta name="twitter:site" content="@ApacheArrow" />
<meta name="twitter:creator" content="@tustvold and alamb" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"tustvold and alamb"},"dateModified":"2022-12-26T00:00:00-05:00","datePublished":"2022-12-26T00:00:00-05:00","description":"Querying Parquet with Millisecond Latency Note: this article was originally published on the InfluxData Blog. We believe that querying data in Apache Parquet files directly can achieve similar or better storage efficiency and query performance than most specialized file formats. While it requires significant engineering effort, the benefits of Parquet’s open format and broad ecosystem support make it the obvious choice for a wide class of data systems. In this article we explain several advanced techniques needed to query data stored in the Parquet format quickly that we implemented in the Apache Arrow Rust Parquet reader. Together these techniques make the Rust implementation one of, if not the fastest implementation for querying Parquet files — be it on local disk or remote object storage. It is able to query GBs of Parquet in a matter of milliseconds. We would like to acknowledge and thank InfluxData for their support of this work. InfluxData has a deep and continuing commitment to Open source software, and it sponsored much of our time for writing this blog post as well as many contributions as part of building the InfluxDB IOx Storage Engine. Background Apache Parquet is an increasingly popular open format for storing analytic datasets, and has become the de-facto standard for cost-effective, DBMS-agnostic data storage. Initially created for the Hadoop ecosystem, Parquet’s reach now expands broadly across the data analytics ecosystem due to its compelling combination of: High compression ratios Amenability to commodity blob-storage such as S3 Broad ecosystem and tooling support Portability across many different platforms and tools Support for arbitrarily structured data Increasingly other systems, such as DuckDB and Redshift allow querying data stored in Parquet directly, but support is still often a secondary consideration compared to their native (custom) file formats. Such formats include the DuckDB .duckdb file format, the Apache IOT TsFile, the Gorilla format, and others. For the first time, access to the same sophisticated query techniques, previously only available in closed source commercial implementations, are now available as open source. The required engineering capacity comes from large, well-run open source projects with global contributor communities, such as Apache Arrow and Apache Impala. Parquet file format Before diving into the details of efficiently reading from Parquet, it is important to understand the file layout. The file format is carefully designed to quickly locate the desired information, skip irrelevant portions, and decode what remains efficiently. The data in a Parquet file is broken into horizontal slices called RowGroups Each RowGroup contains a single ColumnChunk for each column in the schema For example, the following diagram illustrates a Parquet file with three columns “A”, “B” and “C” stored in two RowGroups for a total of 6 ColumnChunks. ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ ┃┏━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━┓ ┃ ┃┃┌ ─ ─ ─ ─ ─ ─ ┌ ─ ─ ─ ─ ─ ─ ┐┌ ─ ─ ─ ─ ─ ─ ┃ ┃ ┃┃ │ │ ┃ ┃ ┃┃│ │ ││ ┃ ┃ ┃┃ │ │ ┃ ┃ ┃┃│ │ ││ ┃ RowGroup ┃ ┃┃ │ │ ┃ 1 ┃ ┃┃│ │ ││ ┃ ┃ ┃┃ │ │ ┃ ┃ ┃┃└ ─ ─ ─ ─ ─ ─ └ ─ ─ ─ ─ ─ ─ ┘└ ─ ─ ─ ─ ─ ─ ┃ ┃ ┃┃ColumnChunk 1 ColumnChunk 2 ColumnChunk 3 ┃ ┃ ┃┃ (Column &quot;A&quot;) (Column &quot;B&quot;) (Column &quot;C&quot;) ┃ ┃ ┃┗━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━┛ ┃ ┃┏━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━┓ ┃ ┃┃┌ ─ ─ ─ ─ ─ ─ ┌ ─ ─ ─ ─ ─ ─ ┐┌ ─ ─ ─ ─ ─ ─ ┃ ┃ ┃┃ │ │ ┃ ┃ ┃┃│ │ ││ ┃ ┃ ┃┃ │ │ ┃ ┃ ┃┃│ │ ││ ┃ RowGroup ┃ ┃┃ │ │ ┃ 2 ┃ ┃┃│ │ ││ ┃ ┃ ┃┃ │ │ ┃ ┃ ┃┃└ ─ ─ ─ ─ ─ ─ └ ─ ─ ─ ─ ─ ─ ┘└ ─ ─ ─ ─ ─ ─ ┃ ┃ ┃┃ColumnChunk 4 ColumnChunk 5 ColumnChunk 6 ┃ ┃ ┃┃ (Column &quot;A&quot;) (Column &quot;B&quot;) (Column &quot;C&quot;) ┃ ┃ ┃┗━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━┛ ┃ ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ The logical values for a ColumnChunk are written using one of the many available encodings into one or more Data Pages appended sequentially in the file. At the end of a Parquet file is a footer, which contains important metadata, such as: The file’s schema information such as column names and types The locations of the RowGroup and ColumnChunks in the file The footer may also contain other specialized data structures: Optional statistics for each ColumnChunk including min/max values and null counts Optional pointers to OffsetIndexes containing the location of each individual Page Optional pointers to ColumnIndex containing row counts and summary statistics for each Page Optional pointers to BloomFilterData, which can quickly check if a value is present in a ColumnChunk For example, the logical structure of 2 Row Groups and 6 ColumnChunks in the previous diagram might be stored in a Parquet file as shown in the following diagram (not to scale). The pages for the ColumnChunks come first, followed by the footer. The data, the effectiveness of the encoding scheme, and the settings of the Parquet encoder determine the number of and size of the pages needed for each ColumnChunk. In this case, ColumnChunk 1 required 2 pages while ColumnChunk 6 required only 1 page. In addition to other information, the footer contains the locations of each Data Page and the types of the columns. ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐ ┃ ┃ Data Page for ColumnChunk 1 (&quot;A&quot;) ◀─┃─ ─ ─│ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘ ┃ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐ ┃ │ ┃ Data Page for ColumnChunk 1 (&quot;A&quot;) ┃ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘ ┃ │ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐ ┃ ┃ Data Page for ColumnChunk 2 (&quot;B&quot;) ┃ │ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘ ┃ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐ ┃ │ ┃ Data Page for ColumnChunk 3 (&quot;C&quot;) ┃ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘ ┃ │ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐ ┃ ┃ Data Page for ColumnChunk 3 (&quot;C&quot;) ┃ │ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘ ┃ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐ ┃ │ ┃ Data Page for ColumnChunk 3 (&quot;C&quot;) ┃ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘ ┃ │ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐ ┃ ┃ Data Page for ColumnChunk 4 (&quot;A&quot;) ◀─┃─ ─ ─│─ ┐ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘ ┃ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐ ┃ │ │ ┃ Data Page for ColumnChunk 5 (&quot;B&quot;) ┃ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘ ┃ │ │ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐ ┃ ┃ Data Page for ColumnChunk 5 (&quot;B&quot;) ┃ │ │ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘ ┃ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐ ┃ │ │ ┃ Data Page for ColumnChunk 5 (&quot;B&quot;) ┃ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘ ┃ │ │ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐ ┃ ┃ Data Page for ColumnChunk 6 (&quot;C&quot;) ┃ │ │ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘ ┃ ┃┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ ┃ │ │ ┃┃Footer ┃ ┃ ┃┃ ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ ┃ ┃ │ │ ┃┃ ┃File Metadata ┃ ┃ ┃ ┃┃ ┃ Schema, etc ┃ ┃ ┃ │ │ ┃┃ ┃ ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ ┃ ┃ ┃ ┃┃ ┃ ┃Row Group 1 Metadata ┃ ┃ ┃ ┃ │ │ ┃┃ ┃ ┃┏━━━━━━━━━━━━━━━━━━━┓ ┃ ┃ ┃ ┃ ┃┃ ┃ ┃┃Column &quot;A&quot; Metadata┃ Location of ┃ ┃ ┃ ┃ │ │ ┃┃ ┃ ┃┗━━━━━━━━━━━━━━━━━━━┛ first Data ┣ ─ ─ ╋ ╋ ╋ ─ ─ ┃┃ ┃ ┃┏━━━━━━━━━━━━━━━━━━━┓ Page, row ┃ ┃ ┃ ┃ │ ┃┃ ┃ ┃┃Column &quot;B&quot; Metadata┃ counts, ┃ ┃ ┃ ┃ ┃┃ ┃ ┃┗━━━━━━━━━━━━━━━━━━━┛ sizes, ┃ ┃ ┃ ┃ │ ┃┃ ┃ ┃┏━━━━━━━━━━━━━━━━━━━┓ min/max ┃ ┃ ┃ ┃ ┃┃ ┃ ┃┃Column &quot;C&quot; Metadata┃ values, etc ┃ ┃ ┃ ┃ │ ┃┃ ┃ ┃┗━━━━━━━━━━━━━━━━━━━┛ ┃ ┃ ┃ ┃ ┃┃ ┃ ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ ┃ ┃ ┃ │ ┃┃ ┃ ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ ┃ ┃ ┃ ┃┃ ┃ ┃Row Group 2 Metadata ┃ ┃ ┃ ┃ │ ┃┃ ┃ ┃┏━━━━━━━━━━━━━━━━━━━┓ Location of ┃ ┃ ┃ ┃ ┃┃ ┃ ┃┃Column &quot;A&quot; Metadata┃ first Data ┃ ┃ ┃ ┃ │ ┃┃ ┃ ┃┗━━━━━━━━━━━━━━━━━━━┛ Page, row ┣ ─ ─ ╋ ╋ ╋ ─ ─ ─ ─ ┃┃ ┃ ┃┏━━━━━━━━━━━━━━━━━━━┓ counts, ┃ ┃ ┃ ┃ ┃┃ ┃ ┃┃Column &quot;B&quot; Metadata┃ sizes, ┃ ┃ ┃ ┃ ┃┃ ┃ ┃┗━━━━━━━━━━━━━━━━━━━┛ min/max ┃ ┃ ┃ ┃ ┃┃ ┃ ┃┏━━━━━━━━━━━━━━━━━━━┓ values, etc ┃ ┃ ┃ ┃ ┃┃ ┃ ┃┃Column &quot;C&quot; Metadata┃ ┃ ┃ ┃ ┃ ┃┃ ┃ ┃┗━━━━━━━━━━━━━━━━━━━┛ ┃ ┃ ┃ ┃ ┃┃ ┃ ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ ┃ ┃ ┃ ┃┃ ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ ┃ ┃ ┃┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ ┃ ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ There are many important criteria to consider when creating Parquet files such as how to optimally order/cluster data and structure it into RowGroups and Data Pages. Such “physical design” considerations are complex, worthy of their own series of articles, and not addressed in this blog post. Instead, we focus on how to use the available structure to make queries very fast. Optimizing queries In any query processing system, the following techniques generally improve performance: Reduce the data that must be transferred from secondary storage for processing (reduce I/O) Reduce the computational load for decoding the data (reduce CPU) Interleave/pipeline the reading and decoding of the data (improve parallelism) The same principles apply to querying Parquet files, as we describe below: Decode optimization Parquet achieves impressive compression ratios by using sophisticated encoding techniques such as run length compression, dictionary encoding, delta encoding, and others. Consequently, the CPU-bound task of decoding can dominate query latency. Parquet readers can use a number of techniques to improve the latency and throughput of this task, as we have done in the Rust implementation. Vectorized decode Most analytic systems decode multiple values at a time to a columnar memory format, such as Apache Arrow, rather than processing data row-by-row. This is often called vectorized or columnar processing, and is beneficial because it: Amortizes dispatch overheads to switch on the type of column being decoded Improves cache locality by reading consecutive values from a ColumnChunk Often allows multiple values to be decoded in a single instruction. Avoid many small heap allocations with a single large allocation, yielding significant savings for variable length types such as strings and byte arrays Thus, Rust Parquet Reader implements specialized decoders for reading Parquet directly into a columnar memory format (Arrow Arrays). Streaming decode There is no relationship between which rows are stored in which Pages across ColumnChunks. For example, the logical values for the 10,000th row may be in the first page of column A and in the third page of column B. The simplest approach to vectorized decoding, and the one often initially implemented in Parquet decoders, is to decode an entire RowGroup (or ColumnChunk) at a time. However, given Parquet’s high compression ratios, a single RowGroup may well contain millions of rows. Decoding so many rows at once is non-optimal because it: Requires large amounts of intermediate RAM: typical in-memory formats optimized for processing, such as Apache Arrow, require much more than their Parquet-encoded form. Increases query latency: Subsequent processing steps (like filtering or aggregation) can only begin once the entire RowGroup (or ColumnChunk) is decoded. As such, the best Parquet readers support “streaming” data out in by producing configurable sized batches of rows on demand. The batch size must be large enough to amortize decode overhead, but small enough for efficient memory usage and to allow downstream processing to begin concurrently while the subsequent batch is decoded. ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┃ ┃ Data Page for ColumnChunk 1 │◀┃─ ┌── ─── ─── ─── ─── ┐ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┃ │ ┏━━━━━━━┓ ┌ ─ ┐ ┌ ─ ┐ ┌ ─ ┐ │ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┃ ┃ ┃ │ │ ┃ Data Page for ColumnChunk 1 │ ┃ │ ┃ ┃ ─ ▶│ │ │ │ │ │ │ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┃ ─ ─┃ ┃─ ┤ │ ─ ─ ─ ─ ─ ─ │ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┃ │ ┃ ┃ A B C │ ┃ Data Page for ColumnChunk 2 │◀┃─ ┗━━━━━━━┛ │ └── ─── ─── ─── ─── ┘ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┃ │ Parquet ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┃ Decoder │ ... ┃ Data Page for ColumnChunk 3 │ ┃ │ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┃ │ ┌── ─── ─── ─── ─── ┐ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┃ │ ┌ ─ ┐ ┌ ─ ┐ ┌ ─ ┐ │ ┃ Data Page for ColumnChunk 3 │◀┃─ │ │ │ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┃ ─ ▶│ │ │ │ │ │ │ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┃ │ ─ ─ ─ ─ ─ ─ │ ┃ Data Page for ColumnChunk 3 │ ┃ A B C │ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┃ └── ─── ─── ─── ─── ┘ ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ Parquet file Smaller in memory batches for processing While streaming is not a complicated feature to explain, the stateful nature of decoding, especially across multiple columns and arbitrarily nested data, where the relationship between rows and values is not fixed, requires complex intermediate buffering and significant engineering effort to handle correctly. Dictionary preservation Dictionary Encoding, also called categorical encoding, is a technique where each value in a column is not stored directly, but instead, an index in a separate list called a “Dictionary” is stored. This technique achieves many of the benefits of third normal form for columns that have repeated values (low cardinality) and is especially effective for columns of strings such as “City”. The first page in a ColumnChunk can optionally be a dictionary page, containing a list of values of the column’s type. Subsequent pages within this ColumnChunk can then encode an index into this dictionary, instead of encoding the values directly. Given the effectiveness of this encoding, if a Parquet decoder simply decodes dictionary data into the native type, it will inefficiently replicate the same value over and over again, which is especially disastrous for string data. To handle dictionary-encoded data efficiently, the encoding must be preserved during decode. Conveniently, many columnar formats, such as the Arrow DictionaryArray, support such compatible encodings. Preserving dictionary encoding drastically improves performance when reading to an Arrow array, in some cases in excess of 60x, as well as using significantly less memory. The major complicating factor for preserving dictionaries is that the dictionaries are stored per ColumnChunk, and therefore the dictionary changes between RowGroups. The reader must automatically recompute a dictionary for batches that span multiple RowGroups, while also optimizing for the case that batch sizes divide evenly into the number of rows per RowGroup. Additionally a column may be only partly dictionary encoded, further complicating implementation. More information on this technique and its complications can be found in the blog post on applying this technique to the C++ Parquet reader. Projection pushdown The most basic Parquet optimization, and the one most commonly described for Parquet files, is projection pushdown, which reduces both I/Oand CPU requirements. Projection in this context means “selecting some but not all of the columns.” Given how Parquet organizes data, it is straightforward to read and decode only the ColumnChunks required for the referenced columns. For example, consider a SQL query of the form SELECT B from table where A &gt; 35 This query only needs data for columns A and B (and not C) and the projection can be “pushed down” to the Parquet reader. Specifically, using the information in the footer, the Parquet reader can entirely skip fetching (I/O) and decoding (CPU) the Data Pages that store data for column C (ColumnChunk 3 and ColumnChunk 6 in our example). ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐┃ ┌─────▶ Data Page for ColumnChunk 1 (&quot;A&quot;) ┃ │ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘┃ │ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐┃ ├─────▶ Data Page for ColumnChunk 1 (&quot;A&quot;) ┃ │ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘┃ │ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐┃ ├─────▶ Data Page for ColumnChunk 2 (&quot;B&quot;) ┃ │ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘┃ │ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐┃ │ ┃ Data Page for ColumnChunk 3 (&quot;C&quot;) ┃ │ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘┃ A query that │ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐┃ accesses only │ ┃ Data Page for ColumnChunk 3 (&quot;C&quot;) ┃ columns A and B │ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘┃ can read only the │ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐┃ relevant pages, ─────┤ ┃ Data Page for ColumnChunk 3 (&quot;C&quot;) ┃ skipping any Data │ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘┃ Page for column C │ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐┃ ├─────▶ Data Page for ColumnChunk 4 (&quot;A&quot;) ┃ │ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘┃ │ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐┃ ├─────▶ Data Page for ColumnChunk 5 (&quot;B&quot;) ┃ │ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘┃ │ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐┃ ├─────▶ Data Page for ColumnChunk 5 (&quot;B&quot;) ┃ │ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘┃ │ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐┃ └─────▶ Data Page for ColumnChunk 5 (&quot;B&quot;) ┃ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘┃ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐┃ ┃ Data Page for ColumnChunk 6 (&quot;C&quot;) ┃ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘┃ ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ Predicate pushdown Similar to projection pushdown, predicate pushdown also avoids fetching and decoding data from Parquet files, but does so using filter expressions. This technique typically requires closer integration with a query engine such as DataFusion, to determine valid predicates and evaluate them during the scan. Unfortunately without careful API design, the Parquet decoder and query engine can end up tightly coupled, preventing reuse (e.g. there are different Impala and Spark implementations in Cloudera Parquet Predicate Pushdown docs). The Rust Parquet reader uses the RowSelection API to avoid this coupling. RowGroup pruning The simplest form of predicate pushdown, supported by many Parquet based query engines, uses the statistics stored in the footer to skip entire RowGroups. We call this operation RowGroup pruning, and it is analogous to partition pruning in many classical data warehouse systems. For the example query above, if the maximum value for A in a particular RowGroup is less than 35, the decoder can skip fetching and decoding any ColumnChunks from that entire RowGroup. ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ ┃Row Group 1 Metadata ┃ ┃ ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ ┃ ┃ ┃Column &quot;A&quot; Metadata Min:0 Max:15 ┃◀╋ ┐ ┃ ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ ┃ Using the min ┃ ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ ┃ │ and max values ┃ ┃Column &quot;B&quot; Metadata ┃ ┃ from the ┃ ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ ┃ │ metadata, ┃ ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ ┃ RowGroup 1 can ┃ ┃Column &quot;C&quot; Metadata ┃ ┃ ├ ─ ─ be entirely ┃ ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ ┃ skipped ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ │ (pruned) when ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ searching for ┃Row Group 2 Metadata ┃ │ rows with A &gt; ┃ ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ ┃ 35, ┃ ┃Column &quot;A&quot; Metadata Min:10 Max:50 ┃◀╋ ┘ ┃ ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ ┃ ┃ ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ ┃ ┃ ┃Column &quot;B&quot; Metadata ┃ ┃ ┃ ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ ┃ ┃ ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ ┃ ┃ ┃Column &quot;C&quot; Metadata ┃ ┃ ┃ ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ ┃ ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ Note that pruning on minimum and maximum values is effective for many data layouts and column types, but not all. Specifically, it is not as effective for columns with many distinct pseudo-random values (e.g. identifiers or uuids). Thankfully for this use case, Parquet also supports per ColumnChunk Bloom Filters. We are actively working on adding bloom filter support in Apache Rust’s implementation. Page pruning A more sophisticated form of predicate pushdown uses the optional page index in the footer metadata to rule out entire Data Pages. The decoder decodes only the corresponding rows from other columns, often skipping entire pages. The fact that pages in different ColumnChunks often contain different numbers of rows, due to various reasons, complicates this optimization. While the page index may identify the needed pages from one column, pruning a page from one column doesn’t immediately rule out entire pages in other columns. Page pruning proceeds as follows: Uses the predicates in combination with the page index to identify pages to skip Uses the offset index to determine what row ranges correspond to non-skipped pages Computes the intersection of ranges across non-skipped pages, and decodes only those rows This last point is highly non-trivial to implement, especially for nested lists where a single row may correspond to multiple values. Fortunately, the Rust Parquet reader hides this complexity internally, and can decode arbitrary RowSelections. For example, to scan Columns A and B, stored in 5 Data Pages as shown in the figure below: If the predicate is A &gt; 35, Page 1 is pruned using the page index (max value is 20), leaving a RowSelection of [200-&gt;onwards], Parquet reader skips Page 3 entirely (as its last row index is 99) (Only) the relevant rows are read by reading pages 2, 4, and 5. If the predicate is instead A &gt; 35 AND B = &quot;F&quot; the page index is even more effective Using A &gt; 35, yields a RowSelection of [200-&gt;onwards] as before Using B = &quot;F&quot;, on the remaining Page 4 and Page 5 of B, yields a RowSelection of [100-244] Intersecting the two RowSelections leaves a combined RowSelection [200-244] Parquet reader only decodes those 50 rows from Page 2 and Page 4. ┏━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┃ ┃ ┌──────────────┐ │ ┌──────────────┐ │ ┃ ┃ │ │ │ │ │ │ ┃ ┃ │ │ │ │ Page │ │ │ │ │ │ │ 3 │ ┃ ┃ │ │ │ │ min: &quot;A&quot; │ │ ┃ ┃ │ │ │ │ │ max: &quot;C&quot; │ ┃ ┃ │ Page │ │ │ first_row: 0 │ │ │ │ 1 │ │ │ │ ┃ ┃ │ min: 10 │ │ └──────────────┘ │ ┃ ┃ │ │ max: 20 │ │ ┌──────────────┐ ┃ ┃ │ first_row: 0 │ │ │ │ │ │ │ │ │ │ Page │ ┃ ┃ │ │ │ │ 4 │ │ ┃ ┃ │ │ │ │ │ min: &quot;D&quot; │ ┃ ┃ │ │ │ │ max: &quot;G&quot; │ │ │ │ │ │ │first_row: 100│ ┃ ┃ └──────────────┘ │ │ │ │ ┃ ┃ │ ┌──────────────┐ │ │ │ ┃ ┃ │ │ │ └──────────────┘ │ │ │ Page │ │ ┌──────────────┐ ┃ ┃ │ 2 │ │ │ │ │ ┃ ┃ │ │ min: 30 │ │ │ Page │ ┃ ┃ │ max: 40 │ │ │ 5 │ │ │ │first_row: 200│ │ │ min: &quot;H&quot; │ ┃ ┃ │ │ │ │ max: &quot;Z&quot; │ │ ┃ ┃ │ │ │ │ │first_row: 250│ ┃ ┃ └──────────────┘ │ │ │ │ │ │ └──────────────┘ ┃ ┃ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘ ┃ ┃ ColumnChunk ColumnChunk ┃ ┃ A B ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━┛ Support for reading and writing these indexes from Arrow C++, and by extension pyarrow/pandas, is tracked in PARQUET-1404. Late materialization The two previous forms of predicate pushdown only operated on metadata stored for RowGroups, ColumnChunks, and Data Pages prior to decoding values. However, the same techniques also extend to values of one or more columns after decoding them but prior to decoding other columns, which is often called “late materialization”. This technique is especially effective when: The predicate is very selective, i.e. filters out large numbers of rows Each row is large, either due to wide rows (e.g. JSON blobs) or many columns The selected data is clustered together The columns required by the predicate are relatively inexpensive to decode, e.g. PrimitiveArray / DictionaryArray There is additional discussion about the benefits of this technique in SPARK-36527 and Impala. For example, given the predicate A &gt; 35 AND B = &quot;F&quot; from above where the engine uses the page index to determine only 50 rows within RowSelection of [100-244] could match, using late materialization, the Parquet decoder: Decodes the 50 values of Column A Evaluates A &gt; 35 on those 50 values In this case, only 5 rows pass, resulting in the RowSelection: RowSelection[205-206] RowSelection[238-240] Only decodes the 5 rows for Column B for those selections Row Index ┌────────────────────┐ ┌────────────────────┐ 200 │ 30 │ │ &quot;F&quot; │ └────────────────────┘ └────────────────────┘ ... ... ┌────────────────────┐ ┌────────────────────┐ 205 │ 37 │─ ─ ─ ─ ─ ─▶│ &quot;F&quot; │ ├────────────────────┤ ├────────────────────┤ 206 │ 36 │─ ─ ─ ─ ─ ─▶│ &quot;G&quot; │ └────────────────────┘ └────────────────────┘ ... ... ┌────────────────────┐ ┌────────────────────┐ 238 │ 36 │─ ─ ─ ─ ─ ─▶│ &quot;F&quot; │ ├────────────────────┤ ├────────────────────┤ 239 │ 36 │─ ─ ─ ─ ─ ─▶│ &quot;G&quot; │ ├────────────────────┤ ├────────────────────┤ 240 │ 40 │─ ─ ─ ─ ─ ─▶│ &quot;G&quot; │ └────────────────────┘ └────────────────────┘ ... ... ┌────────────────────┐ ┌────────────────────┐ 244 │ 26 │ │ &quot;D&quot; │ └────────────────────┘ └────────────────────┘ Column A Column B Values Values In certain cases, such as our example where B stores single character values, the cost of late materialization machinery can outweigh the savings in decoding. However, the savings can be substantial when some of the conditions listed above are fulfilled. The query engine must decide which predicates to push down and in which order to apply them for optimal results. While it is outside the scope of this document, the same technique can be applied for multiple predicates as well as predicates on multiple columns. See the RowFilter interface in the Parquet crate for more information, and the row_filter implementation in DataFusion. I/O pushdown While Parquet was designed for efficient access on the HDFS distributed file system, it works very well with commodity blob storage systems such as AWS S3 as they have very similar characteristics: Relatively slow “random access” reads: it is much more efficient to read large (MBs) sections of data in each request than issue many requests for smaller portions Significant latency before retrieving the first byte High per-request cost: Often billed per request, regardless of number of bytes read, which incentivizes fewer requests that each read a large contiguous section of data. To read optimally from such systems, a Parquet reader must: Minimize the number of I/O requests, while also applying the various pushdown techniques to avoid fetching large amounts of unused data. Integrate with the appropriate task scheduling mechanism to interleave I/O and processing on the data that is fetched to avoid pipeline bottlenecks. As these are substantial engineering and integration challenges, many Parquet readers still require the files to be fetched in their entirety to local storage. Fetching the entire files in order to process them is not ideal for several reasons: High Latency: Decoding cannot begin until the entire file is fetched (Parquet metadata is at the end of the file, so the decoder must see the end prior to decoding the rest) Wasted work: Fetching the entire file fetches all necessary data, but also potentially lots of unnecessary data that will be skipped after reading the footer. This increases the cost unnecessarily. Requires costly “locally attached” storage (or memory): Many cloud environments do not offer computing resources with locally attached storage – they either rely on expensive network block storage such as AWS EBS or else restrict local storage to certain classes of VMs. Avoiding the need to buffer the entire file requires a sophisticated Parquet decoder, integrated with the I/O subsystem, that can initially fetch and decode the metadata followed by ranged fetches for the relevant data blocks, interleaved with the decoding of Parquet data. This optimization requires careful engineering to fetch large enough blocks of data from the object store that the per request overhead doesn’t dominate gains from reducing the bytes transferred. SPARK-36529 describes the challenges of sequential processing in more detail. ┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ │ │ Step 1: Fetch │ Parquet Parquet metadata file on ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━▼━━━━━━━┓ Remote ┃ ▒▒▒▒▒▒▒▒▒▒ ▒▒▒▒▒▒▒▒▒▒ ░░░░░░░░░░ ┃ Object ┃ ▒▒▒data▒▒▒ ▒▒▒data▒▒▒ ░metadata░ ┃ Store ┃ ▒▒▒▒▒▒▒▒▒▒ ▒▒▒▒▒▒▒▒▒▒ ░░░░░░░░░░ ┃ ┗━━━━━━━━━━━▲━━━━━━━━━━━━━━━━━━━━━▲━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ │ └ ─ ─ ─ │ │ Step 2: Fetch only ─ ─ ─ ─ ─ ─ ─ ─ ─ relevant data blocks Not included in this diagram picture are details like coalescing requests and ensuring minimum request sizes needed for an actual implementation. The Rust Parquet crate provides an async Parquet reader, to efficiently read from any AsyncFileReader that: Efficiently reads from any storage medium that supports range requests Integrates with Rust’s futures ecosystem to avoid blocking threads waiting on network I/O and easily can interleave CPU and network Requests multiple ranges simultaneously, to allow the implementation to coalesce adjacent ranges, fetch ranges in parallel, etc. Uses the pushdown techniques described previously to eliminate fetching data where possible Integrates easily with the Apache Arrow object_store crate which you can read more about here To give a sense of what is possible, the following picture shows a timeline of fetching the footer metadata from remote files, using that metadata to determine what Data Pages to read, and then fetching data and decoding simultaneously. This process often must be done for more than one file at a time in order to match network latency, bandwidth, and available CPU. begin metadata read of end read read ─ ─ ─ ┐ data of data │ begin complete block block read of │ │ │ │ metadata ─ ─ ─ ┐ At any time, there are │ │ │ │ │ multiple network │ ▼ ▼ ▼ ▼ requests outstanding to file 1 │ ░░░░░░░░░░ ▒▒▒read▒▒▒ ▒▒▒read▒▒▒ │ hide the individual │ ░░░read░░░ ▒▒▒data▒▒▒ ▒▒▒data▒▒▒ request latency │ ░metadata░ ▓▓decode▓▓ │ ░░░░░░░░░░ ▓▓▓data▓▓▓ │ │ │ │ ░░░░░░░░░░ ▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒read▒▒▒▒│▒▒▒▒▒▒▒▒▒▒▒▒▒▒ file 2 │ ░░░read░░░ ▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒data▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒ │ ░metadata░ │ ▓▓▓▓▓decode▓▓▓▓▓▓ │ ░░░░░░░░░░ ▓▓▓▓▓▓data▓▓▓▓▓▓▓ │ │ │ │ ░░│░░░░░░░ ▒▒▒read▒▒▒ ▒▒▒▒read▒▒▒▒▒ file 3 │ ░░░read░░░ ▒▒▒data▒▒▒ ▒▒▒▒data▒▒▒▒▒ ... │ ░m│tadata░ ▓▓decode▓▓ │ ░░░░░░░░░░ ▓▓▓data▓▓▓ └───────────────────────────────────────┼──────────────────────────────▶Time │ Conclusion We hope you enjoyed reading about the Parquet file format, and the various techniques used to quickly query parquet files. We believe that the reason most open source implementations of Parquet do not have the breadth of features described in this post is that it takes a monumental effort, that was previously only possible at well-financed commercial enterprises which kept their implementations closed source. However, with the growth and quality of the Apache Arrow community, both Rust practitioners and the wider Arrow community, our ability to collaborate and build a cutting-edge open source implementation is exhilarating and immensely satisfying. The technology described in this blog is the result of the contributions of many engineers spread across companies, hobbyists, and the world in several repositories, notably Apache Arrow DataFusion, Apache Arrow and Apache Arrow Ballista. If you are interested in joining the DataFusion Community, please get in touch.","headline":"Querying Parquet with Millisecond Latency","image":"https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png","mainEntityOfPage":{"@type":"WebPage","@id":"https://arrow.apache.org/blog/2022/12/26/querying-parquet-with-millisecond-latency/"},"publisher":{"@type":"Organization","logo":{"@type":"ImageObject","url":"https://arrow.apache.org/img/logo.png"},"name":"tustvold and alamb"},"url":"https://arrow.apache.org/blog/2022/12/26/querying-parquet-with-millisecond-latency/"}</script>
<!-- End Jekyll SEO tag -->


    <!-- favicons -->
    <link rel="icon" type="image/png" sizes="16x16" href="/img/favicon-16x16.png" id="light1">
    <link rel="icon" type="image/png" sizes="32x32" href="/img/favicon-32x32.png" id="light2">
    <link rel="apple-touch-icon" type="image/png" sizes="180x180" href="/img/apple-touch-icon.png" id="light3">
    <link rel="apple-touch-icon" type="image/png" sizes="120x120" href="/img/apple-touch-icon-120x120.png" id="light4">
    <link rel="apple-touch-icon" type="image/png" sizes="76x76" href="/img/apple-touch-icon-76x76.png" id="light5">
    <link rel="apple-touch-icon" type="image/png" sizes="60x60" href="/img/apple-touch-icon-60x60.png" id="light6">
    <!-- dark mode favicons -->
    <link rel="icon" type="image/png" sizes="16x16" href="/img/favicon-16x16-dark.png" id="dark1">
    <link rel="icon" type="image/png" sizes="32x32" href="/img/favicon-32x32-dark.png" id="dark2">
    <link rel="apple-touch-icon" type="image/png" sizes="180x180" href="/img/apple-touch-icon-dark.png" id="dark3">
    <link rel="apple-touch-icon" type="image/png" sizes="120x120" href="/img/apple-touch-icon-120x120-dark.png" id="dark4">
    <link rel="apple-touch-icon" type="image/png" sizes="76x76" href="/img/apple-touch-icon-76x76-dark.png" id="dark5">
    <link rel="apple-touch-icon" type="image/png" sizes="60x60" href="/img/apple-touch-icon-60x60-dark.png" id="dark6">

    <script>
      // Switch to the dark-mode favicons if prefers-color-scheme: dark
      function onUpdate() {
        light1 = document.querySelector('link#light1');
        light2 = document.querySelector('link#light2');
        light3 = document.querySelector('link#light3');
        light4 = document.querySelector('link#light4');
        light5 = document.querySelector('link#light5');
        light6 = document.querySelector('link#light6');

        dark1 = document.querySelector('link#dark1');
        dark2 = document.querySelector('link#dark2');
        dark3 = document.querySelector('link#dark3');
        dark4 = document.querySelector('link#dark4');
        dark5 = document.querySelector('link#dark5');
        dark6 = document.querySelector('link#dark6');

        if (matcher.matches) {
          light1.remove();
          light2.remove();
          light3.remove();
          light4.remove();
          light5.remove();
          light6.remove();
          document.head.append(dark1);
          document.head.append(dark2);
          document.head.append(dark3);
          document.head.append(dark4);
          document.head.append(dark5);
          document.head.append(dark6);
        } else {
          dark1.remove();
          dark2.remove();
          dark3.remove();
          dark4.remove();
          dark5.remove();
          dark6.remove();
          document.head.append(light1);
          document.head.append(light2);
          document.head.append(light3);
          document.head.append(light4);
          document.head.append(light5);
          document.head.append(light6);
        }
      }
      matcher = window.matchMedia('(prefers-color-scheme: dark)');
      matcher.addListener(onUpdate);
      onUpdate();
    </script>

    <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic,900">

    <link href="/css/main.css" rel="stylesheet">
    <link href="/css/syntax.css" rel="stylesheet">
    <script src="/javascript/main.js"></script>
    
    <!-- Matomo -->
<script>
  var _paq = window._paq = window._paq || [];
  /* tracker methods like "setCustomDimension" should be called before "trackPageView" */
  /* We explicitly disable cookie tracking to avoid privacy issues */
  _paq.push(['disableCookies']);
  _paq.push(['trackPageView']);
  _paq.push(['enableLinkTracking']);
  (function() {
    var u="https://analytics.apache.org/";
    _paq.push(['setTrackerUrl', u+'matomo.php']);
    _paq.push(['setSiteId', '20']);
    var d=document, g=d.createElement('script'), s=d.getElementsByTagName('script')[0];
    g.async=true; g.src=u+'matomo.js'; s.parentNode.insertBefore(g,s);
  })();
</script>
<!-- End Matomo Code -->

    
  </head>


<body class="wrap">
  <header>
    <nav class="navbar navbar-expand-md navbar-dark bg-dark">
  
  <a class="navbar-brand no-padding" href="/"><img src="/img/arrow-inverse-300px.png" height="40px"/></a>
  
   <button class="navbar-toggler ml-auto" type="button" data-toggle="collapse" data-target="#arrow-navbar" aria-controls="arrow-navbar" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>

    <!-- Collect the nav links, forms, and other content for toggling -->
    <div class="collapse navbar-collapse justify-content-end" id="arrow-navbar">
      <ul class="nav navbar-nav">
        <li class="nav-item"><a class="nav-link" href="/overview/" role="button" aria-haspopup="true" aria-expanded="false">Overview</a></li>
        <li class="nav-item"><a class="nav-link" href="/faq/" role="button" aria-haspopup="true" aria-expanded="false">FAQ</a></li>
        <li class="nav-item"><a class="nav-link" href="/blog" role="button" aria-haspopup="true" aria-expanded="false">Blog</a></li>
        <li class="nav-item dropdown">
          <a class="nav-link dropdown-toggle" href="#"
             id="navbarDropdownGetArrow" role="button" data-toggle="dropdown"
             aria-haspopup="true" aria-expanded="false">
             Get Arrow
          </a>
          <div class="dropdown-menu" aria-labelledby="navbarDropdownGetArrow">
            <a class="dropdown-item" href="/install/">Install</a>
            <a class="dropdown-item" href="/release/">Releases</a>
            <a class="dropdown-item" href="https://github.com/apache/arrow">Source Code</a>
          </div>
        </li>
        <li class="nav-item dropdown">
          <a class="nav-link dropdown-toggle" href="#"
             id="navbarDropdownDocumentation" role="button" data-toggle="dropdown"
             aria-haspopup="true" aria-expanded="false">
             Documentation
          </a>
          <div class="dropdown-menu" aria-labelledby="navbarDropdownDocumentation">
            <a class="dropdown-item" href="/docs">Project Docs</a>
            <a class="dropdown-item" href="/docs/format/Columnar.html">Format</a>
            <hr/>
            <a class="dropdown-item" href="/docs/c_glib">C GLib</a>
            <a class="dropdown-item" href="/docs/cpp">C++</a>
            <a class="dropdown-item" href="https://github.com/apache/arrow/blob/main/csharp/README.md">C#</a>
            <a class="dropdown-item" href="https://godoc.org/github.com/apache/arrow/go/arrow">Go</a>
            <a class="dropdown-item" href="/docs/java">Java</a>
            <a class="dropdown-item" href="/docs/js">JavaScript</a>
            <a class="dropdown-item" href="/julia/">Julia</a>
            <a class="dropdown-item" href="https://github.com/apache/arrow/blob/main/matlab/README.md">MATLAB</a>
            <a class="dropdown-item" href="/docs/python">Python</a>
            <a class="dropdown-item" href="/docs/r">R</a>
            <a class="dropdown-item" href="https://github.com/apache/arrow/blob/main/ruby/README.md">Ruby</a>
            <a class="dropdown-item" href="https://docs.rs/arrow/latest">Rust</a>
          </div>
        </li>
        <li class="nav-item dropdown">
          <a class="nav-link dropdown-toggle" href="#"
             id="navbarDropdownSubprojects" role="button" data-toggle="dropdown"
             aria-haspopup="true" aria-expanded="false">
             Subprojects
          </a>
          <div class="dropdown-menu" aria-labelledby="navbarDropdownSubprojects">
            <a class="dropdown-item" href="/adbc">ADBC</a>
            <a class="dropdown-item" href="/docs/format/Flight.html">Arrow Flight</a>
            <a class="dropdown-item" href="/docs/format/FlightSql.html">Arrow Flight SQL</a>
            <a class="dropdown-item" href="/datafusion">DataFusion</a>
          </div>
        </li>
        <li class="nav-item dropdown">
          <a class="nav-link dropdown-toggle" href="#"
             id="navbarDropdownCommunity" role="button" data-toggle="dropdown"
             aria-haspopup="true" aria-expanded="false">
             Community
          </a>
          <div class="dropdown-menu" aria-labelledby="navbarDropdownCommunity">
            <a class="dropdown-item" href="/community/">Communication</a>
            <a class="dropdown-item" href="/docs/developers/contributing.html">Contributing</a>
            <a class="dropdown-item" href="https://github.com/apache/arrow/issues">Issue Tracker</a>
            <a class="dropdown-item" href="/committers/">Governance</a>
            <a class="dropdown-item" href="/use_cases/">Use Cases</a>
            <a class="dropdown-item" href="/powered_by/">Powered By</a>
            <a class="dropdown-item" href="/visual_identity/">Visual Identity</a>
            <a class="dropdown-item" href="/security/">Security</a>
            <a class="dropdown-item" href="https://www.apache.org/foundation/policies/conduct.html">Code of Conduct</a>
          </div>
        </li>
        <li class="nav-item dropdown">
          <a class="nav-link dropdown-toggle" href="#"
             id="navbarDropdownASF" role="button" data-toggle="dropdown"
             aria-haspopup="true" aria-expanded="false">
             ASF Links
          </a>
          <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdownASF">
            <a class="dropdown-item" href="https://www.apache.org/">ASF Website</a>
            <a class="dropdown-item" href="https://www.apache.org/licenses/">License</a>
            <a class="dropdown-item" href="https://www.apache.org/foundation/sponsorship.html">Donate</a>
            <a class="dropdown-item" href="https://www.apache.org/foundation/thanks.html">Thanks</a>
            <a class="dropdown-item" href="https://www.apache.org/security/">Security</a>
          </div>
        </li>
      </ul>
    </div><!-- /.navbar-collapse -->
  </nav>

  </header>

  <div class="container p-4 pt-5">
    <div class="col-md-8 mx-auto">
      <main role="main" class="pb-5">
        
<h1>
  Querying Parquet with Millisecond Latency
</h1>
<hr class="mt-4 mb-3">



<p class="mb-4 pb-1">
  <span class="badge badge-secondary">Published</span>
  <span class="published mr-3">
    26 Dec 2022
  </span>
  <br />
  <span class="badge badge-secondary">By</span>
  
    tustvold and alamb
  

  
</p>


        <!--

-->

<h1 id="querying-parquet-with-millisecond-latency">Querying Parquet with Millisecond Latency</h1>
<p><em>Note: this article was originally published on the <a href="https://www.influxdata.com/blog/querying-parquet-millisecond-latency">InfluxData Blog</a>.</em></p>

<p>We believe that querying data in <a href="https://parquet.apache.org/">Apache Parquet</a> files directly can achieve similar or better storage efficiency and query performance than most specialized file formats. While it requires significant engineering effort, the benefits of Parquet’s open format and broad ecosystem support make it the obvious choice for a wide class of data systems.</p>

<p>In this article we explain several advanced techniques needed to query data stored in the Parquet format quickly that we implemented in the <a href="https://docs.rs/parquet/27.0.0/parquet/">Apache Arrow Rust Parquet reader</a>. Together these techniques make the Rust implementation one of, if not the fastest implementation for querying Parquet files — be it on local disk or remote object storage. It is able to query GBs of Parquet in a <a href="https://github.com/tustvold/access-log-bench">matter of milliseconds</a>.</p>

<p>We would like to acknowledge and thank <a href="https://www.influxdata.com/">InfluxData</a> for their support of this work. InfluxData has a deep and continuing commitment to Open source software, and it sponsored much of our time for writing this blog post as well as many contributions as part of building the <a href="https://www.influxdata.com/blog/influxdb-engine/">InfluxDB IOx Storage Engine</a>.</p>

<h1 id="background">Background</h1>

<p><a href="https://parquet.apache.org/">Apache Parquet</a> is an increasingly popular open format for storing <a href="https://www.influxdata.com/glossary/olap/">analytic datasets</a>, and has become the de-facto standard for cost-effective, DBMS-agnostic data storage. Initially created for the Hadoop ecosystem, Parquet’s reach now expands broadly across the data analytics ecosystem due to its compelling combination of:</p>

<ul>
  <li>High compression ratios</li>
  <li>Amenability to commodity blob-storage such as S3</li>
  <li>Broad ecosystem and tooling support</li>
  <li>Portability across many different platforms and tools</li>
  <li>Support for <a href="https://arrow.apache.org/blog/2022/10/05/arrow-parquet-encoding-part-1/">arbitrarily structured data</a></li>
</ul>

<p>Increasingly other systems, such as <a href="https://duckdb.org/2021/06/25/querying-parquet.html">DuckDB</a> and <a href="https://docs.aws.amazon.com/redshift/latest/dg/c-using-spectrum.html#c-spectrum-overview">Redshift</a> allow querying data stored in Parquet directly, but support is still often a secondary consideration compared to their native (custom) file formats. Such formats include the DuckDB <code class="language-plaintext highlighter-rouge">.duckdb</code> file format, the Apache IOT <a href="https://github.com/apache/iotdb/blob/master/tsfile/README.md">TsFile</a>, the <a href="https://www.vldb.org/pvldb/vol8/p1816-teller.pdf">Gorilla format</a>, and others.</p>

<p>For the first time, access to the same sophisticated query techniques, previously only available in closed source commercial implementations, are now available as open source. The required engineering capacity comes from large, well-run open source projects with global contributor communities, such as <a href="https://arrow.apache.org/">Apache Arrow</a> and <a href="https://impala.apache.org/">Apache Impala</a>.</p>

<h1 id="parquet-file-format">Parquet file format</h1>

<p>Before diving into the details of efficiently reading from <a href="https://www.influxdata.com/glossary/apache-parquet/">Parquet</a>, it is important to understand the file layout. The file format is carefully designed to quickly locate the desired information, skip irrelevant portions, and decode what remains efficiently.</p>

<ul>
  <li>The data in a Parquet file is broken into horizontal slices called <code class="language-plaintext highlighter-rouge">RowGroup</code>s</li>
  <li>Each <code class="language-plaintext highlighter-rouge">RowGroup</code> contains a single <code class="language-plaintext highlighter-rouge">ColumnChunk</code> for each column in the schema</li>
</ul>

<p>For example, the following diagram illustrates a Parquet file with three columns “A”, “B” and “C” stored in two <code class="language-plaintext highlighter-rouge">RowGroup</code>s for a total of 6 <code class="language-plaintext highlighter-rouge">ColumnChunk</code>s.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃┏━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━┓          ┃
┃┃┌ ─ ─ ─ ─ ─ ─ ┌ ─ ─ ─ ─ ─ ─ ┐┌ ─ ─ ─ ─ ─ ─  ┃          ┃
┃┃             │                            │ ┃          ┃
┃┃│             │             ││              ┃          ┃
┃┃             │                            │ ┃          ┃
┃┃│             │             ││              ┃ RowGroup ┃
┃┃             │                            │ ┃     1    ┃
┃┃│             │             ││              ┃          ┃
┃┃             │                            │ ┃          ┃
┃┃└ ─ ─ ─ ─ ─ ─ └ ─ ─ ─ ─ ─ ─ ┘└ ─ ─ ─ ─ ─ ─  ┃          ┃
┃┃ColumnChunk 1  ColumnChunk 2 ColumnChunk 3  ┃          ┃
┃┃ (Column "A")   (Column "B")  (Column "C")  ┃          ┃
┃┗━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━┛          ┃
┃┏━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━┓          ┃
┃┃┌ ─ ─ ─ ─ ─ ─ ┌ ─ ─ ─ ─ ─ ─ ┐┌ ─ ─ ─ ─ ─ ─  ┃          ┃
┃┃             │                            │ ┃          ┃
┃┃│             │             ││              ┃          ┃
┃┃             │                            │ ┃          ┃
┃┃│             │             ││              ┃ RowGroup ┃
┃┃             │                            │ ┃     2    ┃
┃┃│             │             ││              ┃          ┃
┃┃             │                            │ ┃          ┃
┃┃└ ─ ─ ─ ─ ─ ─ └ ─ ─ ─ ─ ─ ─ ┘└ ─ ─ ─ ─ ─ ─  ┃          ┃
┃┃ColumnChunk 4  ColumnChunk 5 ColumnChunk 6  ┃          ┃
┃┃ (Column "A")   (Column "B")  (Column "C")  ┃          ┃
┃┗━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━┛          ┃
┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛
</code></pre></div></div>

<p>The logical values for a <code class="language-plaintext highlighter-rouge">ColumnChunk</code> are written using one of the many <a href="https://parquet.apache.org/docs/file-format/data-pages/encodings/">available encodings</a> into one or more Data Pages appended sequentially in the file. At the end of a Parquet file is a footer, which contains important metadata, such as:</p>

<ul>
  <li>The file’s schema information such as column names and types</li>
  <li>The locations of the <code class="language-plaintext highlighter-rouge">RowGroup</code> and <code class="language-plaintext highlighter-rouge">ColumnChunk</code>s in the file</li>
</ul>

<p>The footer may also contain other specialized data structures:</p>

<ul>
  <li>Optional statistics for each <code class="language-plaintext highlighter-rouge">ColumnChunk</code> including min/max values and null counts</li>
  <li>Optional pointers to <a href="https://github.com/apache/parquet-format/blob/54e53e5d7794d383529dd30746378f19a12afd58/src/main/thrift/parquet.thrift#L926-L932">OffsetIndexes</a> containing the location of each individual Page</li>
  <li>Optional pointers to <a href="https://github.com/apache/parquet-format/blob/54e53e5d7794d383529dd30746378f19a12afd58/src/main/thrift/parquet.thrift#L938">ColumnIndex</a> containing row counts and summary statistics for each Page</li>
  <li>Optional pointers to <a href="https://github.com/apache/parquet-format/blob/54e53e5d7794d383529dd30746378f19a12afd58/src/main/thrift/parquet.thrift#L621-L630">BloomFilterData</a>, which can quickly check if a value is present in a <code class="language-plaintext highlighter-rouge">ColumnChunk</code></li>
</ul>

<p>For example, the logical structure of 2 Row Groups and 6 <code class="language-plaintext highlighter-rouge">ColumnChunk</code>s in the previous diagram might be stored in a Parquet file as shown in the following diagram (not to scale). The pages for the <code class="language-plaintext highlighter-rouge">ColumnChunk</code>s come first, followed by the footer. The data, the effectiveness of the encoding scheme, and the settings of the Parquet encoder determine the number of and size of the pages needed for each <code class="language-plaintext highlighter-rouge">ColumnChunk</code>. In this case, <code class="language-plaintext highlighter-rouge">ColumnChunk</code> 1 required 2 pages while <code class="language-plaintext highlighter-rouge">ColumnChunk</code> 6 required only 1 page. In addition to other information, the footer contains the locations of each Data Page and the types of the columns.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐  ┃
┃ Data Page for ColumnChunk 1 ("A")             ◀─┃─ ─ ─│
┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘  ┃
┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐  ┃     │
┃ Data Page for ColumnChunk 1 ("A")               ┃
┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘  ┃     │
┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐  ┃
┃ Data Page for ColumnChunk 2 ("B")               ┃     │
┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘  ┃
┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐  ┃     │
┃ Data Page for ColumnChunk 3 ("C")               ┃
┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘  ┃     │
┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐  ┃
┃ Data Page for ColumnChunk 3 ("C")               ┃     │
┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘  ┃
┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐  ┃     │
┃ Data Page for ColumnChunk 3 ("C")               ┃
┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘  ┃     │
┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐  ┃
┃ Data Page for ColumnChunk 4 ("A")             ◀─┃─ ─ ─│─ ┐
┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘  ┃
┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐  ┃     │  │
┃ Data Page for ColumnChunk 5 ("B")               ┃
┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘  ┃     │  │
┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐  ┃
┃ Data Page for ColumnChunk 5 ("B")               ┃     │  │
┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘  ┃
┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐  ┃     │  │
┃ Data Page for ColumnChunk 5 ("B")               ┃
┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘  ┃     │  │
┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐  ┃
┃ Data Page for ColumnChunk 6 ("C")               ┃     │  │
┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘  ┃
┃┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ ┃     │  │
┃┃Footer                                        ┃ ┃
┃┃ ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ ┃ ┃     │  │
┃┃ ┃File Metadata                             ┃ ┃ ┃
┃┃ ┃ Schema, etc                              ┃ ┃ ┃     │  │
┃┃ ┃ ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓     ┃ ┃ ┃
┃┃ ┃ ┃Row Group 1 Metadata              ┃     ┃ ┃ ┃     │  │
┃┃ ┃ ┃┏━━━━━━━━━━━━━━━━━━━┓             ┃     ┃ ┃ ┃
┃┃ ┃ ┃┃Column "A" Metadata┃ Location of ┃     ┃ ┃ ┃     │  │
┃┃ ┃ ┃┗━━━━━━━━━━━━━━━━━━━┛ first Data  ┣ ─ ─ ╋ ╋ ╋ ─ ─
┃┃ ┃ ┃┏━━━━━━━━━━━━━━━━━━━┓ Page, row   ┃     ┃ ┃ ┃        │
┃┃ ┃ ┃┃Column "B" Metadata┃ counts,     ┃     ┃ ┃ ┃
┃┃ ┃ ┃┗━━━━━━━━━━━━━━━━━━━┛ sizes,      ┃     ┃ ┃ ┃        │
┃┃ ┃ ┃┏━━━━━━━━━━━━━━━━━━━┓ min/max     ┃     ┃ ┃ ┃
┃┃ ┃ ┃┃Column "C" Metadata┃ values, etc ┃     ┃ ┃ ┃        │
┃┃ ┃ ┃┗━━━━━━━━━━━━━━━━━━━┛             ┃     ┃ ┃ ┃
┃┃ ┃ ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛     ┃ ┃ ┃        │
┃┃ ┃ ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓     ┃ ┃ ┃
┃┃ ┃ ┃Row Group 2 Metadata              ┃     ┃ ┃ ┃        │
┃┃ ┃ ┃┏━━━━━━━━━━━━━━━━━━━┓ Location of ┃     ┃ ┃ ┃
┃┃ ┃ ┃┃Column "A" Metadata┃ first Data  ┃     ┃ ┃ ┃        │
┃┃ ┃ ┃┗━━━━━━━━━━━━━━━━━━━┛ Page, row   ┣ ─ ─ ╋ ╋ ╋ ─ ─ ─ ─
┃┃ ┃ ┃┏━━━━━━━━━━━━━━━━━━━┓ counts,     ┃     ┃ ┃ ┃
┃┃ ┃ ┃┃Column "B" Metadata┃ sizes,      ┃     ┃ ┃ ┃
┃┃ ┃ ┃┗━━━━━━━━━━━━━━━━━━━┛ min/max     ┃     ┃ ┃ ┃
┃┃ ┃ ┃┏━━━━━━━━━━━━━━━━━━━┓ values, etc ┃     ┃ ┃ ┃
┃┃ ┃ ┃┃Column "C" Metadata┃             ┃     ┃ ┃ ┃
┃┃ ┃ ┃┗━━━━━━━━━━━━━━━━━━━┛             ┃     ┃ ┃ ┃
┃┃ ┃ ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛     ┃ ┃ ┃
┃┃ ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ ┃ ┃
┃┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ ┃
┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛
</code></pre></div></div>

<p>There are many important criteria to consider when creating Parquet files such as how to optimally order/cluster data and structure it into <code class="language-plaintext highlighter-rouge">RowGroup</code>s and Data Pages. Such “physical design” considerations are complex, worthy of their own series of articles, and not addressed in this blog post. Instead, we focus on how to use the available structure to make queries very fast.</p>

<h1 id="optimizing-queries">Optimizing queries</h1>

<p>In any query processing system, the following techniques generally improve performance:</p>

<ol>
  <li>Reduce the data that must be transferred from secondary storage for processing (reduce I/O)</li>
  <li>Reduce the computational load for decoding the data (reduce CPU)</li>
  <li>Interleave/pipeline the reading and decoding of the data (improve parallelism)</li>
</ol>

<p>The same principles apply to querying Parquet files, as we describe below:</p>

<h1 id="decode-optimization">Decode optimization</h1>

<p>Parquet achieves impressive compression ratios by using <a href="https://parquet.apache.org/docs/file-format/data-pages/encodings/">sophisticated encoding techniques</a> such as run length compression, dictionary encoding, delta encoding, and others. Consequently, the CPU-bound task of decoding can dominate query latency. Parquet readers can use a number of techniques to improve the latency and throughput of this task, as we have done in the Rust implementation.</p>

<h2 id="vectorized-decode">Vectorized decode</h2>

<p>Most analytic systems decode multiple values at a time to a columnar memory format, such as Apache Arrow, rather than processing data row-by-row. This is often called vectorized or columnar processing, and is beneficial because it:</p>

<ul>
  <li>Amortizes dispatch overheads to switch on the type of column being decoded</li>
  <li>Improves cache locality by reading consecutive values from a <code class="language-plaintext highlighter-rouge">ColumnChunk</code></li>
  <li>Often allows multiple values to be decoded in a single instruction.</li>
  <li>Avoid many small heap allocations with a single large allocation, yielding significant savings for variable length types such as strings and byte arrays</li>
</ul>

<p>Thus, Rust Parquet Reader implements specialized decoders for reading Parquet directly into a <a href="https://www.influxdata.com/glossary/column-database/">columnar</a> memory format (Arrow Arrays).</p>

<h2 id="streaming-decode">Streaming decode</h2>

<p>There is no relationship between which rows are stored in which Pages across <code class="language-plaintext highlighter-rouge">ColumnChunk</code>s. For example, the logical values for the 10,000th row may be in the first page of column A and in the third page of column B.</p>

<p>The simplest approach to vectorized decoding, and the one often initially implemented in Parquet decoders, is to decode an entire <code class="language-plaintext highlighter-rouge">RowGroup</code> (or <code class="language-plaintext highlighter-rouge">ColumnChunk</code>) at a time.</p>

<p>However, given Parquet’s high compression ratios, a single <code class="language-plaintext highlighter-rouge">RowGroup</code> may well contain millions of rows. Decoding so many rows at once is non-optimal because it:</p>

<ul>
  <li><strong>Requires large amounts of intermediate RAM</strong>: typical in-memory formats optimized for processing, such as Apache Arrow, require much more than their Parquet-encoded form.</li>
  <li><strong>Increases query latency</strong>: Subsequent processing steps (like filtering or aggregation) can only begin once the entire <code class="language-plaintext highlighter-rouge">RowGroup</code> (or <code class="language-plaintext highlighter-rouge">ColumnChunk</code>) is decoded.</li>
</ul>

<p>As such, the best Parquet readers support “streaming” data out in by producing configurable sized batches of rows on demand. The batch size must be large enough to amortize decode overhead, but small enough for efficient memory usage and to allow downstream processing to begin concurrently while the subsequent batch is decoded.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─  ┃
┃ Data Page for ColumnChunk 1 │◀┃─                   ┌── ─── ─── ─── ─── ┐
┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─  ┃ │   ┏━━━━━━━┓        ┌ ─ ┐ ┌ ─ ┐ ┌ ─ ┐ │
┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─  ┃     ┃       ┃      │                   │
┃ Data Page for ColumnChunk 1 │ ┃ │   ┃       ┃   ─ ▶│ │   │ │   │ │   │
┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─  ┃  ─ ─┃       ┃─ ┤   │  ─ ─   ─ ─   ─ ─  │
┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─  ┃ │   ┃       ┃           A    B     C   │
┃ Data Page for ColumnChunk 2 │◀┃─    ┗━━━━━━━┛  │   └── ─── ─── ─── ─── ┘
┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─  ┃ │    Parquet
┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─  ┃      Decoder   │            ...
┃ Data Page for ColumnChunk 3 │ ┃ │
┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─  ┃                │   ┌── ─── ─── ─── ─── ┐
┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─  ┃ │                    ┌ ─ ┐ ┌ ─ ┐ ┌ ─ ┐ │
┃ Data Page for ColumnChunk 3 │◀┃─               │   │                   │
┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─  ┃                 ─ ▶│ │   │ │   │ │   │
┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─  ┃                    │  ─ ─   ─ ─   ─ ─  │
┃ Data Page for ColumnChunk 3 │ ┃                         A    B     C   │
┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─  ┃                    └── ─── ─── ─── ─── ┘
┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛

      Parquet file                                    Smaller in memory
                                                         batches for
                                                         processing
</code></pre></div></div>

<p>While streaming is not a complicated feature to explain, the stateful nature of decoding, especially across multiple columns and <a href="https://arrow.apache.org/blog/2022/10/05/arrow-parquet-encoding-part-1/">arbitrarily nested data</a>, where the relationship between rows and values is not fixed, requires <a href="https://github.com/apache/arrow-rs/blob/b7af85cb8dfe6887bb3fd43d1d76f659473b6927/parquet/src/arrow/record_reader/mod.rs">complex intermediate buffering</a> and significant engineering effort to handle correctly.</p>

<h2 id="dictionary-preservation">Dictionary preservation</h2>

<p>Dictionary Encoding, also called <a href="https://pandas.pydata.org/docs/user_guide/categorical.html">categorical</a> encoding, is a technique where each value in a column is not stored directly, but instead, an index in a separate list called a “Dictionary” is stored. This technique achieves many of the benefits of <a href="https://en.wikipedia.org/wiki/Third_normal_form#:~:text=Third%20normal%20form%20(3NF)%20is,in%201971%20by%20Edgar%20F.">third normal form</a> for columns that have repeated values (low <a href="https://www.influxdata.com/glossary/cardinality/">cardinality</a>) and is especially effective for columns of strings such as “City”.</p>

<p>The first page in a <code class="language-plaintext highlighter-rouge">ColumnChunk</code> can optionally be a dictionary page, containing a list of values of the column’s type. Subsequent pages within this <code class="language-plaintext highlighter-rouge">ColumnChunk</code> can then encode an index into this dictionary, instead of encoding the values directly.</p>

<p>Given the effectiveness of this encoding, if a Parquet decoder simply decodes dictionary data into the native type, it will inefficiently replicate the same value over and over again, which is especially disastrous for string data. To handle dictionary-encoded data efficiently, the encoding must be preserved during decode. Conveniently, many columnar formats, such as the Arrow <a href="https://docs.rs/arrow/27.0.0/arrow/array/struct.DictionaryArray.html">DictionaryArray</a>, support such compatible encodings.</p>

<p>Preserving dictionary encoding drastically improves performance when reading to an Arrow array, in some cases in excess of <a href="https://github.com/apache/arrow-rs/pull/1180">60x</a>, as well as using significantly less memory.</p>

<p>The major complicating factor for preserving dictionaries is that the dictionaries are stored per <code class="language-plaintext highlighter-rouge">ColumnChunk</code>, and therefore the dictionary changes between <code class="language-plaintext highlighter-rouge">RowGroup</code>s. The reader must automatically recompute a dictionary for batches that span multiple <code class="language-plaintext highlighter-rouge">RowGroup</code>s, while also optimizing for the case that batch sizes divide evenly into the number of rows per <code class="language-plaintext highlighter-rouge">RowGroup</code>. Additionally a column may be only <a href="https://github.com/apache/parquet-format/blob/111dbdcf8eff2e9f8e0d4e958cecbc7e00028aca/README.md?plain=1#L194-L199">partly dictionary encoded</a>, further complicating implementation. More information on this technique and its complications can be found in the <a href="https://arrow.apache.org/blog/2019/09/05/faster-strings-cpp-parquet/">blog post</a> on applying this technique to the C++ Parquet reader.</p>

<h1 id="projection-pushdown">Projection pushdown</h1>

<p>The most basic Parquet optimization, and the one most commonly described for Parquet files, is <em>projection pushdown</em>, which reduces both I/Oand CPU requirements. Projection in this context means “selecting some but not all of the columns.” Given how Parquet organizes data, it is straightforward to read and decode only the <code class="language-plaintext highlighter-rouge">ColumnChunk</code>s required for the referenced columns.</p>

<p>For example, consider a SQL query of the form</p>

<pre><code class="language-SQL">SELECT B from table where A &gt; 35
</code></pre>

<p>This query only needs data for columns A and B (and not C) and the projection can be “pushed down” to the Parquet reader.</p>

<p>Specifically, using the information in the footer, the Parquet reader can entirely skip fetching (I/O) and decoding (CPU) the Data Pages that store data for column C (<code class="language-plaintext highlighter-rouge">ColumnChunk</code> 3 and <code class="language-plaintext highlighter-rouge">ColumnChunk</code> 6 in our example).</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>                             ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
                             ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐┃
                       ┌─────▶ Data Page for ColumnChunk 1 ("A") ┃
                       │     ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘┃
                       │     ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐┃
                       ├─────▶ Data Page for ColumnChunk 1 ("A") ┃
                       │     ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘┃
                       │     ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐┃
                       ├─────▶ Data Page for ColumnChunk 2 ("B") ┃
                       │     ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘┃
                       │     ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐┃
                       │     ┃ Data Page for ColumnChunk 3 ("C") ┃
                       │     ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘┃
   A query that        │     ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐┃
  accesses only        │     ┃ Data Page for ColumnChunk 3 ("C") ┃
 columns A and B       │     ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘┃
can read only the      │     ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐┃
 relevant pages,  ─────┤     ┃ Data Page for ColumnChunk 3 ("C") ┃
skipping any Data      │     ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘┃
Page for column C      │     ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐┃
                       ├─────▶ Data Page for ColumnChunk 4 ("A") ┃
                       │     ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘┃
                       │     ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐┃
                       ├─────▶ Data Page for ColumnChunk 5 ("B") ┃
                       │     ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘┃
                       │     ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐┃
                       ├─────▶ Data Page for ColumnChunk 5 ("B") ┃
                       │     ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘┃
                       │     ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐┃
                       └─────▶ Data Page for ColumnChunk 5 ("B") ┃
                             ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘┃
                             ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐┃
                             ┃ Data Page for ColumnChunk 6 ("C") ┃
                             ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘┃
                             ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛
</code></pre></div></div>

<h1 id="predicate-pushdown">Predicate pushdown</h1>

<p>Similar to projection pushdown, <strong>predicate</strong> pushdown also avoids fetching and decoding data from Parquet files, but does so using filter expressions. This technique typically requires closer integration with a query engine such as <a href="https://arrow.apache.org/datafusion/">DataFusion</a>, to determine valid predicates and evaluate them during the scan. Unfortunately without careful API design, the Parquet decoder and query engine can end up tightly coupled, preventing reuse (e.g. there are different Impala and Spark implementations in <a href="https://docs.cloudera.com/documentation/enterprise/6/6.3/topics/cdh_ig_predicate_pushdown_parquet.html#concept_pgs_plb_mgb">Cloudera Parquet Predicate Pushdown docs</a>). The Rust Parquet reader uses the <a href="https://docs.rs/parquet/27.0.0/parquet/arrow/arrow_reader/struct.RowSelector.html">RowSelection</a> API to avoid this coupling.</p>

<h2 id="rowgroup-pruning"><code class="language-plaintext highlighter-rouge">RowGroup</code> pruning</h2>

<p>The simplest form of predicate pushdown, supported by many Parquet based query engines, uses the statistics stored in the footer to skip entire <code class="language-plaintext highlighter-rouge">RowGroup</code>s. We call this operation <code class="language-plaintext highlighter-rouge">RowGroup</code> <em>pruning</em>, and it is analogous to <a href="https://docs.oracle.com/database/121/VLDBG/GUID-E677C85E-C5E3-4927-B3DF-684007A7B05D.htm#VLDBG00401">partition pruning</a> in many classical data warehouse systems.</p>

<p>For the example query above, if the maximum value for A in a particular <code class="language-plaintext highlighter-rouge">RowGroup</code> is less than 35, the decoder can skip fetching and decoding any <code class="language-plaintext highlighter-rouge">ColumnChunk</code>s from that <strong>entire</strong> <code class="language-plaintext highlighter-rouge">RowGroup</code>.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃Row Group 1 Metadata                      ┃
┃ ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ ┃
┃ ┃Column "A" Metadata    Min:0 Max:15   ┃◀╋ ┐
┃ ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ ┃       Using the min
┃ ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ ┃ │     and max values
┃ ┃Column "B" Metadata                   ┃ ┃       from the
┃ ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ ┃ │     metadata,
┃ ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ ┃       RowGroup 1  can
┃ ┃Column "C" Metadata                   ┃ ┃ ├ ─ ─ be entirely
┃ ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ ┃       skipped
┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ │     (pruned) when
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓       searching for
┃Row Group 2 Metadata                      ┃ │     rows with A &gt;
┃ ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ ┃       35,
┃ ┃Column "A" Metadata   Min:10 Max:50   ┃◀╋ ┘
┃ ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ ┃
┃ ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ ┃
┃ ┃Column "B" Metadata                   ┃ ┃
┃ ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ ┃
┃ ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ ┃
┃ ┃Column "C" Metadata                   ┃ ┃
┃ ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ ┃
┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛
</code></pre></div></div>

<p>Note that pruning on minimum and maximum values is effective for many data layouts and column types, but not all. Specifically, it is not as effective for columns with many distinct pseudo-random values (e.g. identifiers or uuids). Thankfully for this use case, Parquet also supports per <code class="language-plaintext highlighter-rouge">ColumnChunk</code> <a href="https://github.com/apache/parquet-format/blob/master/BloomFilter.md">Bloom Filters</a>. We are actively working on <a href="https://github.com/apache/arrow-rs/issues/3023">adding bloom filter</a> support in Apache Rust’s implementation.</p>

<h2 id="page-pruning">Page pruning</h2>

<p>A more sophisticated form of predicate pushdown uses the optional <a href="https://github.com/apache/parquet-format/blob/master/PageIndex.md">page index</a> in the footer metadata to rule out entire Data Pages. The decoder decodes only the corresponding rows from other columns, often skipping entire pages.</p>

<p>The fact that pages in different <code class="language-plaintext highlighter-rouge">ColumnChunk</code>s often contain different numbers of rows, due to various reasons, complicates this optimization. While the page index may identify the needed pages from one column, pruning a page from one column doesn’t immediately rule out entire pages in other columns.</p>

<p>Page pruning proceeds as follows:</p>

<ul>
  <li>Uses the predicates in combination with the page index to identify pages to skip</li>
  <li>Uses the offset index to determine what row ranges correspond to non-skipped pages</li>
  <li>Computes the intersection of ranges across non-skipped pages, and decodes only those rows</li>
</ul>

<p>This last point is highly non-trivial to implement, especially for nested lists where <a href="https://arrow.apache.org/blog/2022/10/08/arrow-parquet-encoding-part-2/">a single row may correspond to multiple values</a>. Fortunately, the Rust Parquet reader hides this complexity internally, and can decode arbitrary <a href="https://docs.rs/parquet/27.0.0/parquet/arrow/arrow_reader/struct.RowSelection.html">RowSelections</a>.</p>

<p>For example, to scan Columns A and B, stored in 5 Data Pages as shown in the figure below:</p>

<p>If the predicate is <code class="language-plaintext highlighter-rouge">A &gt; 35</code>,</p>

<ul>
  <li>Page 1 is pruned using the page index (max value is <code class="language-plaintext highlighter-rouge">20</code>), leaving a RowSelection of  [200-&gt;onwards],</li>
  <li>Parquet reader skips Page 3 entirely (as its last row index is <code class="language-plaintext highlighter-rouge">99</code>)</li>
  <li>(Only) the relevant rows are read by reading pages 2, 4, and 5.</li>
</ul>

<p>If the predicate is instead <code class="language-plaintext highlighter-rouge">A &gt; 35 AND B = "F"</code> the page index is even more effective</p>

<ul>
  <li>Using <code class="language-plaintext highlighter-rouge">A &gt; 35</code>, yields a RowSelection of <code class="language-plaintext highlighter-rouge">[200-&gt;onwards]</code> as before</li>
  <li>Using <code class="language-plaintext highlighter-rouge">B = "F"</code>, on the remaining Page 4 and Page 5 of B, yields a RowSelection of <code class="language-plaintext highlighter-rouge">[100-244]</code></li>
  <li>Intersecting the two RowSelections leaves a combined RowSelection <code class="language-plaintext highlighter-rouge">[200-244]</code></li>
  <li>Parquet reader only decodes those 50 rows from Page 2 and Page 4.</li>
</ul>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>┏━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━
   ┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─   ┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─   ┃
┃     ┌──────────────┐  │     ┌──────────────┐  │  ┃
┃  │  │              │     │  │              │     ┃
┃     │              │  │     │     Page     │  │
   │  │              │     │  │      3       │     ┃
┃     │              │  │     │   min: "A"   │  │  ┃
┃  │  │              │     │  │   max: "C"   │     ┃
┃     │     Page     │  │     │ first_row: 0 │  │
   │  │      1       │     │  │              │     ┃
┃     │   min: 10    │  │     └──────────────┘  │  ┃
┃  │  │   max: 20    │     │  ┌──────────────┐     ┃
┃     │ first_row: 0 │  │     │              │  │
   │  │              │     │  │     Page     │     ┃
┃     │              │  │     │      4       │  │  ┃
┃  │  │              │     │  │   min: "D"   │     ┃
┃     │              │  │     │   max: "G"   │  │
   │  │              │     │  │first_row: 100│     ┃
┃     └──────────────┘  │     │              │  │  ┃
┃  │  ┌──────────────┐     │  │              │     ┃
┃     │              │  │     └──────────────┘  │
   │  │     Page     │     │  ┌──────────────┐     ┃
┃     │      2       │  │     │              │  │  ┃
┃  │  │   min: 30    │     │  │     Page     │     ┃
┃     │   max: 40    │  │     │      5       │  │
   │  │first_row: 200│     │  │   min: "H"   │     ┃
┃     │              │  │     │   max: "Z"   │  │  ┃
┃  │  │              │     │  │first_row: 250│     ┃
┃     └──────────────┘  │     │              │  │
   │                       │  └──────────────┘     ┃
┃   ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘   ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘  ┃
┃       ColumnChunk            ColumnChunk         ┃
┃            A                      B
 ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━┛
</code></pre></div></div>

<p>Support for reading and writing these indexes from Arrow C++, and by extension pyarrow/pandas, is tracked in <a href="https://issues.apache.org/jira/browse/PARQUET-1404">PARQUET-1404</a>.</p>

<h2 id="late-materialization">Late materialization</h2>

<p>The two previous forms of predicate pushdown only operated on metadata stored for <code class="language-plaintext highlighter-rouge">RowGroup</code>s, <code class="language-plaintext highlighter-rouge">ColumnChunk</code>s, and Data Pages prior to decoding values. However, the same techniques also extend to values of one or more columns <em>after</em> decoding them but prior to decoding other columns, which is often called “late materialization”.</p>

<p>This technique is especially effective when:</p>

<ul>
  <li>The predicate is very selective, i.e. filters out large numbers of rows</li>
  <li>Each row is large, either due to wide rows (e.g. JSON blobs) or many columns</li>
  <li>The selected data is clustered together</li>
  <li>The columns required by the predicate are relatively inexpensive to decode, e.g. PrimitiveArray / DictionaryArray</li>
</ul>

<p>There is additional discussion about the benefits of this technique in <a href="https://issues.apache.org/jira/browse/SPARK-36527">SPARK-36527</a> and<a href="https://docs.cloudera.com/cdw-runtime/cloud/impala-reference/topics/impala-lazy-materialization.html"> Impala</a>.</p>

<p>For example, given the predicate <code class="language-plaintext highlighter-rouge">A &gt; 35 AND B = "F"</code> from above where the engine uses the page index to determine only 50 rows within RowSelection of [100-244] could match, using late materialization, the Parquet decoder:</p>

<ul>
  <li>Decodes the 50 values of Column A</li>
  <li>Evaluates  <code class="language-plaintext highlighter-rouge">A &gt; 35 </code> on those 50 values</li>
  <li>In this case, only 5 rows pass, resulting in the RowSelection:
    <ul>
      <li>RowSelection[205-206]</li>
      <li>RowSelection[238-240]</li>
    </ul>
  </li>
  <li>Only decodes the 5 rows for Column B for those selections</li>
</ul>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  Row Index
             ┌────────────────────┐            ┌────────────────────┐
       200   │         30         │            │        "F"         │
             └────────────────────┘            └────────────────────┘
                      ...                               ...
             ┌────────────────────┐            ┌────────────────────┐
       205   │         37         │─ ─ ─ ─ ─ ─▶│        "F"         │
             ├────────────────────┤            ├────────────────────┤
       206   │         36         │─ ─ ─ ─ ─ ─▶│        "G"         │
             └────────────────────┘            └────────────────────┘
                      ...                               ...
             ┌────────────────────┐            ┌────────────────────┐
       238   │         36         │─ ─ ─ ─ ─ ─▶│        "F"         │
             ├────────────────────┤            ├────────────────────┤
       239   │         36         │─ ─ ─ ─ ─ ─▶│        "G"         │
             ├────────────────────┤            ├────────────────────┤
       240   │         40         │─ ─ ─ ─ ─ ─▶│        "G"         │
             └────────────────────┘            └────────────────────┘
                      ...                               ...
             ┌────────────────────┐            ┌────────────────────┐
      244    │         26         │            │        "D"         │
             └────────────────────┘            └────────────────────┘


                   Column A                          Column B
                    Values                            Values
</code></pre></div></div>

<p>In certain cases, such as our example where B stores single character values, the cost of late materialization machinery can outweigh the savings in decoding. However, the savings can be substantial when some of the conditions listed above are fulfilled. The query engine must decide which predicates to push down and in which order to apply them for optimal results.</p>

<p>While it is outside the scope of this document, the same technique can be applied for multiple predicates as well as predicates on multiple columns. See the <a href="https://docs.rs/parquet/latest/parquet/arrow/arrow_reader/struct.RowFilter.html">RowFilter</a> interface in the Parquet crate for more information, and the <a href="https://github.com/apache/arrow-datafusion/blob/58b43f5c0b629be49a3efa0e37052ec51d9ba3fe/datafusion/core/src/physical_plan/file_format/parquet/row_filter.rs#L40-L70">row_filter</a> implementation in DataFusion.</p>

<h1 id="io-pushdown">I/O pushdown</h1>

<p>While Parquet was designed for efficient access on the <a href="https://hadoop.apache.org/docs/r1.2.1/hdfs_design.html">HDFS distributed file system</a>, it works very well with commodity blob storage systems such as AWS S3 as they have very similar characteristics:</p>

<ul>
  <li><strong>Relatively slow “random access” reads</strong>: it is much more efficient to read large (MBs) sections of data in each request than issue many requests for smaller portions</li>
  <li><strong>Significant latency before retrieving the first byte</strong></li>
  <li><strong>High per-request cost:</strong> Often billed per request, regardless of number of bytes read, which incentivizes fewer requests that each read a large contiguous section of data.</li>
</ul>

<p>To read optimally from such systems, a Parquet reader must:</p>

<ol>
  <li>Minimize the number of I/O requests, while also applying the various pushdown techniques to avoid fetching large amounts of unused data.</li>
  <li>Integrate with the appropriate task scheduling mechanism to interleave I/O and processing on the data that is fetched to avoid pipeline bottlenecks.</li>
</ol>

<p>As these are substantial engineering and integration challenges, many Parquet readers still require the files to be fetched in their entirety to local storage.</p>

<p>Fetching the entire files in order to process them is not ideal for several reasons:</p>

<ol>
  <li><strong>High Latency</strong>: Decoding cannot begin until the entire file is fetched (Parquet metadata is at the end of the file, so the decoder must see the end prior to decoding the rest)</li>
  <li><strong>Wasted work</strong>: Fetching the entire file fetches all necessary data, but also potentially lots of unnecessary data that will be skipped after reading the footer. This increases the cost unnecessarily.</li>
  <li><strong>Requires costly “locally attached” storage (or memory)</strong>: Many cloud environments do not offer computing resources with locally attached storage – they either rely on expensive network block storage such as AWS EBS or else restrict local storage to certain classes of VMs.</li>
</ol>

<p>Avoiding the need to buffer the entire file requires a sophisticated Parquet decoder, integrated with the I/O subsystem, that can initially fetch and decode the metadata followed by ranged fetches for the relevant data blocks, interleaved with the decoding of Parquet data. This optimization requires careful engineering to fetch large enough blocks of data from the object store that the per request overhead doesn’t dominate gains from reducing the bytes transferred. <a href="https://issues.apache.org/jira/browse/SPARK-36529">SPARK-36529</a> describes the challenges of sequential processing in more detail.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>                       ┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─
                                                                │
                       │
               Step 1: Fetch                                    │
 Parquet       Parquet metadata
 file on ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━▼━━━━━━━┓
 Remote  ┃      ▒▒▒▒▒▒▒▒▒▒          ▒▒▒▒▒▒▒▒▒▒               ░░░░░░░░░░ ┃
 Object  ┃      ▒▒▒data▒▒▒          ▒▒▒data▒▒▒               ░metadata░ ┃
  Store  ┃      ▒▒▒▒▒▒▒▒▒▒          ▒▒▒▒▒▒▒▒▒▒               ░░░░░░░░░░ ┃
         ┗━━━━━━━━━━━▲━━━━━━━━━━━━━━━━━━━━━▲━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛
                     │                     └ ─ ─ ─
                                                  │
                     │                   Step 2: Fetch only
                      ─ ─ ─ ─ ─ ─ ─ ─ ─ relevant data blocks
</code></pre></div></div>

<p>Not included in this diagram picture are details like coalescing requests and ensuring minimum request sizes needed for an actual implementation.</p>

<p>The Rust Parquet crate provides an async Parquet reader, to efficiently read from any <a href="https://docs.rs/parquet/latest/parquet/arrow/async_reader/trait.AsyncFileReader.html">AsyncFileReader</a> that:</p>

<ul>
  <li>Efficiently reads from any storage medium that supports range requests</li>
  <li>Integrates with Rust’s futures ecosystem to avoid blocking threads waiting on network I/O <a href="https://www.influxdata.com/blog/using-rustlangs-async-tokio-runtime-for-cpu-bound-tasks/">and easily can interleave CPU and network </a></li>
  <li>Requests multiple ranges simultaneously, to allow the implementation to coalesce adjacent ranges, fetch ranges in parallel, etc.</li>
  <li>Uses the pushdown techniques described previously to eliminate fetching data where possible</li>
  <li>Integrates easily with the Apache Arrow <a href="https://docs.rs/object_store/latest/object_store/">object_store</a> crate which you can read more about <a href="https://www.influxdata.com/blog/rust-object-store-donation/">here</a></li>
</ul>

<p>To give a sense of what is possible, the following picture shows a timeline of fetching the footer metadata from remote files, using that metadata to determine what Data Pages to read, and then fetching data and decoding simultaneously. This process often must be done for more than one file at a time in order to match network latency, bandwidth, and available CPU.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>                           begin
          metadata        read of   end read
            read  ─ ─ ─ ┐   data    of data          │
 begin    complete         block     block
read of                 │   │        │               │
metadata  ─ ─ ─ ┐                                       At any time, there are
             │          │   │        │               │     multiple network
             │  ▼       ▼   ▼        ▼                  requests outstanding to
  file 1     │ ░░░░░░░░░░   ▒▒▒read▒▒▒   ▒▒▒read▒▒▒  │    hide the individual
             │ ░░░read░░░   ▒▒▒data▒▒▒   ▒▒▒data▒▒▒        request latency
             │ ░metadata░                         ▓▓decode▓▓
             │ ░░░░░░░░░░                         ▓▓▓data▓▓▓
             │                                       │
             │
             │ ░░░░░░░░░░  ▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒read▒▒▒▒│▒▒▒▒▒▒▒▒▒▒▒▒▒▒
   file 2    │ ░░░read░░░  ▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒data▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒
             │ ░metadata░                            │              ▓▓▓▓▓decode▓▓▓▓▓▓
             │ ░░░░░░░░░░                                           ▓▓▓▓▓▓data▓▓▓▓▓▓▓
             │                                       │
             │
             │                                     ░░│░░░░░░░  ▒▒▒read▒▒▒  ▒▒▒▒read▒▒▒▒▒
   file 3    │                                     ░░░read░░░  ▒▒▒data▒▒▒  ▒▒▒▒data▒▒▒▒▒      ...
             │                                     ░m│tadata░            ▓▓decode▓▓
             │                                     ░░░░░░░░░░            ▓▓▓data▓▓▓
             └───────────────────────────────────────┼──────────────────────────────▶Time


                                                     │
</code></pre></div></div>

<h2 id="conclusion">Conclusion</h2>

<p>We hope you enjoyed reading about the Parquet file format, and the various techniques used to quickly query parquet files.</p>

<p>We believe that the reason most open source implementations of Parquet do not have the breadth of features described in this post is that it takes a monumental effort, that was previously only possible at well-financed commercial enterprises which kept their implementations closed source.</p>

<p>However, with the growth and quality of the Apache Arrow community, both Rust practitioners and the wider Arrow community, our ability to collaborate and build a cutting-edge open source implementation is exhilarating and immensely satisfying. The technology described in this blog is the result of the contributions of many engineers spread across companies, hobbyists, and the world in several repositories, notably <a href="https://github.com/apache/arrow-datafusion">Apache Arrow DataFusion</a>, <a href="https://github.com/apache/arrow-rs">Apache Arrow</a> and <a href="https://github.com/apache/arrow-ballista">Apache Arrow Ballista.</a></p>

<p>If you are interested in joining the DataFusion Community, please <a href="https://arrow.apache.org/datafusion/contributor-guide/communication.html">get in touch</a>.</p>

      </main>
    </div>

    <hr/>
<footer class="footer">
  <div class="row">
    <div class="col-md-9">
      <p>Apache Arrow, Arrow, Apache, the Apache feather logo, and the Apache Arrow project logo are either registered trademarks or trademarks of The Apache Software Foundation in the United States and other countries.</p>
      <p>&copy; 2016-2023 The Apache Software Foundation</p>
    </div>
    <div class="col-md-3">
      <a class="d-sm-none d-md-inline pr-2" href="https://www.apache.org/events/current-event.html">
        <img src="https://www.apache.org/events/current-event-234x60.png"/>
      </a>
    </div>
  </div>
</footer>

  </div>
</body>
</html>
