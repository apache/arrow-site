<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- The above meta tags *must* come first in the head; any other head content must come *after* these tags -->
    
    <title>Apache Arrow 2.0.0 Rust Highlights | Apache Arrow</title>
    

    <!-- Begin Jekyll SEO tag v2.8.0 -->
<meta name="generator" content="Jekyll v4.2.0" />
<meta property="og:title" content="Apache Arrow 2.0.0 Rust Highlights" />
<meta name="author" content="pmc" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Apache Arrow 2.0.0 is a significant release for the Apache Arrow project in general (release notes), and the Rust subproject in particular, with almost 200 issues resolved by 15 contributors. In this blog post, we will go through the main changes affecting core Arrow, Parquet support, and DataFusion query engine. The full list of resolved issues can be found here. While the Java and C/C++ (used by Python and R) Arrow implementations likely remain the most feature-rich, with the 2.0.0 release, the Rust implementation is closing the feature gap quickly. Here are some of the highlights for this release. Core Arrow Crate Iterator Trait Primitive arrays (e.g., array of integers) can now be converted to and initialized from an iterator. This exposes a very popular API - iterators - to arrow arrays. Work for other types will continue throughout 3.0.0. Improved Variable-sized Arrays Variable sized arrays (e.g., array of strings) have been internally refactored to more easily support their larger (64-bit size offset) version. This allowed us to generalize some of the kernels to both (32 and 64) versions and perform type checks when building them. Kernels There have been numerous improvements in the Arrow compute kernels, including: New kernels have been added for string operations, including substring, min, max, concat, and length. Aggregate sum is now implemented for SIMD with a 5x improvement over the non-SIMD operation Many kernels have been improved to support dictionary-encoded arrays Some kernels were optimized for arrays without nulls, making them significantly faster in that case. Many kernels were optimized in the number of memory copies that are needed to apply them and also on their implementation. Other Improvements The Array trait now has get_buffer_memory_size and get_array_memory_size methods for determining the amount of memory allocated for the array. Parquet A significant effort is underway to create a Parquet writer for Arrow data. This work has not been released as part of 2.0.0, and is planned for the 3.0.0 release. The development of this writer is being carried out on the rust-parquet-arrow-writer branch, and the branch is regularly synchronized with the main branch. As part of the writer, the necessary improvements and features are being added to the reader. The main focus areas are: Supporting nested Arrow types, such as List&lt;Struct&lt;[Dictionary, String]&gt;&gt; Ensuring correct round-trip between the reader and writer by encoding Arrow schemas in the Parquet metadata Improve null value writing for Parquet A new parquet_derive crate has been created, which allows users to derive Parquet records for simple structs. Refer to the parquet_derive crate for usage examples. DataFusion DataFusion is an in-memory query engine with DataFrame and SQL APIs, built on top of base Arrow support. DataFrame API DataFusion now has a richer DataFrame API with improved documentation showing example usage, supporting the following operations: select_columns select filter aggregate limit sort collect explain Performance &amp; Scalability DataFusion query execution now uses async/await with the tokio threaded runtime rather than launching dedicated threads, making queries scale much better across available cores. The hash aggregate physical operator has been largely re-written, resulting in significant performance improvements. Expressions and Compute Improved Scalar Functions DataFusion has many new functions, both in the SQL and the DataFrame API: Length of an string COUNT(DISTINCT column) to_timestamp IsNull and IsNotNull Min/Max for strings (lexicographic order) Array of columns Concatenation of strings Aliases of aggregate expressions Many existing expressions were also significantly optimized (2-3x speedups) by avoiding memory copies and leveraging Arrow format’s invariants. Unary mathematical functions (such as sqrt) now support both 32 and 64-bit floats and return the corresponding type, thereby allowing faster operations when higher precision is not needed. Improved User-defined Functions (UDFs) The API to use and register UDFs has been significantly improved, allowing users to register UDFs and call them both via SQL and the DataFrame API. UDFs now also have the same generality as DataFusion’s functions, including variadic and dynamically typed arguments. User-defined Aggregate Functions (UDAFs) DataFusion now supports user-defined aggregate functions that can be used to perform operations than span multiple rows, batches, and partitions. UDAFs have the same generality as DataFusion’s functions and support both row updates and batch updates. You can check out this example to learn how to declare and use a UDAF. User-defined Constants DataFusion now supports registering constants (e.g. “@version”) that live for the duration of the execution context and can be accessed from SQL. Query Planning &amp; Optimization User-defined logical plans The Logical Plan enum is now extensible through an Extension variant which accepts a UserDefinedLogicalPlan trait using dynamic dispatch. Consequently, DataFusion now supports user-defined logical nodes, thereby allowing complex nodes to be planned and executed. You can check this example to learn how to declare a new node. Predicate push-down DataFusion now has a Predicate push-down optimizer rule that pushes filter operations as close as possible to scans, thereby speeding up the physical execution of suboptimal queries created via the DataFrame API. SQL DataFusion now uses a more recent release of the sqlparser crate, which has much more comprehensive support for SQL syntax and also supports multiple dialects (Postgres, MS SQL, and MySQL). It is now possible to see the query plan for a SQL statement using EXPLAIN syntax. Benchmarks The benchmark crate now contains a new benchmark based on TPC-H that can execute TPC-H query 1 against CSV, Parquet, and memory data sources. This is useful for running benchmarks against larger data sets. Integration Testing / IPC Arrow IPC is the format for serialization and interprocess communication. It is described in arrow.apache.org and is the format used for file and stream I/O between applications wishing to interchange Arrow data. The Arrow project released IPC version 5 of the Arrow IPC format in version 1.0.0. Before that, a message padding change was made in version 0.15.0 to change the default padding to 8 bytes, while remaining in IPC version 4. Arrow release 0.14.1 and earlier were the last releases to use the legacy 4 byte alignment. As part of 2.0.0, the Rust implementation was updated to comply with the changes up to release 0.15.0 of Arrow. Work on supporting IPC version 5 is underway, and is expected to be completed in time for 3.0.0. As part of the conformance work, Rust is being added to the Arrow integration suite, which tests that supported language implementations (ARROW-3690): Comply with the Arrow IPC format Can read and write each other’s generated data IPC version 4 is being verified through the above work. Roadmap for 3.0.0 and Beyond Here are some of the initiatives that contributors are currently working on for future releases: Support stable Rust Improved DictionaryArray support and performance Implement inner equijoins Support for various platforms like ARMv8 Supporting the C Data Interface from Rust to better support interoperability with other Arrow implementations How to Get Involved If you are interested in contributing to the Rust subproject in Apache Arrow, you can find a list of open issues suitable for beginners here and the full list here. Other ways to get involved include trying out Arrow on some of your data and filing bug reports, and helping to improve the documentation." />
<meta property="og:description" content="Apache Arrow 2.0.0 is a significant release for the Apache Arrow project in general (release notes), and the Rust subproject in particular, with almost 200 issues resolved by 15 contributors. In this blog post, we will go through the main changes affecting core Arrow, Parquet support, and DataFusion query engine. The full list of resolved issues can be found here. While the Java and C/C++ (used by Python and R) Arrow implementations likely remain the most feature-rich, with the 2.0.0 release, the Rust implementation is closing the feature gap quickly. Here are some of the highlights for this release. Core Arrow Crate Iterator Trait Primitive arrays (e.g., array of integers) can now be converted to and initialized from an iterator. This exposes a very popular API - iterators - to arrow arrays. Work for other types will continue throughout 3.0.0. Improved Variable-sized Arrays Variable sized arrays (e.g., array of strings) have been internally refactored to more easily support their larger (64-bit size offset) version. This allowed us to generalize some of the kernels to both (32 and 64) versions and perform type checks when building them. Kernels There have been numerous improvements in the Arrow compute kernels, including: New kernels have been added for string operations, including substring, min, max, concat, and length. Aggregate sum is now implemented for SIMD with a 5x improvement over the non-SIMD operation Many kernels have been improved to support dictionary-encoded arrays Some kernels were optimized for arrays without nulls, making them significantly faster in that case. Many kernels were optimized in the number of memory copies that are needed to apply them and also on their implementation. Other Improvements The Array trait now has get_buffer_memory_size and get_array_memory_size methods for determining the amount of memory allocated for the array. Parquet A significant effort is underway to create a Parquet writer for Arrow data. This work has not been released as part of 2.0.0, and is planned for the 3.0.0 release. The development of this writer is being carried out on the rust-parquet-arrow-writer branch, and the branch is regularly synchronized with the main branch. As part of the writer, the necessary improvements and features are being added to the reader. The main focus areas are: Supporting nested Arrow types, such as List&lt;Struct&lt;[Dictionary, String]&gt;&gt; Ensuring correct round-trip between the reader and writer by encoding Arrow schemas in the Parquet metadata Improve null value writing for Parquet A new parquet_derive crate has been created, which allows users to derive Parquet records for simple structs. Refer to the parquet_derive crate for usage examples. DataFusion DataFusion is an in-memory query engine with DataFrame and SQL APIs, built on top of base Arrow support. DataFrame API DataFusion now has a richer DataFrame API with improved documentation showing example usage, supporting the following operations: select_columns select filter aggregate limit sort collect explain Performance &amp; Scalability DataFusion query execution now uses async/await with the tokio threaded runtime rather than launching dedicated threads, making queries scale much better across available cores. The hash aggregate physical operator has been largely re-written, resulting in significant performance improvements. Expressions and Compute Improved Scalar Functions DataFusion has many new functions, both in the SQL and the DataFrame API: Length of an string COUNT(DISTINCT column) to_timestamp IsNull and IsNotNull Min/Max for strings (lexicographic order) Array of columns Concatenation of strings Aliases of aggregate expressions Many existing expressions were also significantly optimized (2-3x speedups) by avoiding memory copies and leveraging Arrow format’s invariants. Unary mathematical functions (such as sqrt) now support both 32 and 64-bit floats and return the corresponding type, thereby allowing faster operations when higher precision is not needed. Improved User-defined Functions (UDFs) The API to use and register UDFs has been significantly improved, allowing users to register UDFs and call them both via SQL and the DataFrame API. UDFs now also have the same generality as DataFusion’s functions, including variadic and dynamically typed arguments. User-defined Aggregate Functions (UDAFs) DataFusion now supports user-defined aggregate functions that can be used to perform operations than span multiple rows, batches, and partitions. UDAFs have the same generality as DataFusion’s functions and support both row updates and batch updates. You can check out this example to learn how to declare and use a UDAF. User-defined Constants DataFusion now supports registering constants (e.g. “@version”) that live for the duration of the execution context and can be accessed from SQL. Query Planning &amp; Optimization User-defined logical plans The Logical Plan enum is now extensible through an Extension variant which accepts a UserDefinedLogicalPlan trait using dynamic dispatch. Consequently, DataFusion now supports user-defined logical nodes, thereby allowing complex nodes to be planned and executed. You can check this example to learn how to declare a new node. Predicate push-down DataFusion now has a Predicate push-down optimizer rule that pushes filter operations as close as possible to scans, thereby speeding up the physical execution of suboptimal queries created via the DataFrame API. SQL DataFusion now uses a more recent release of the sqlparser crate, which has much more comprehensive support for SQL syntax and also supports multiple dialects (Postgres, MS SQL, and MySQL). It is now possible to see the query plan for a SQL statement using EXPLAIN syntax. Benchmarks The benchmark crate now contains a new benchmark based on TPC-H that can execute TPC-H query 1 against CSV, Parquet, and memory data sources. This is useful for running benchmarks against larger data sets. Integration Testing / IPC Arrow IPC is the format for serialization and interprocess communication. It is described in arrow.apache.org and is the format used for file and stream I/O between applications wishing to interchange Arrow data. The Arrow project released IPC version 5 of the Arrow IPC format in version 1.0.0. Before that, a message padding change was made in version 0.15.0 to change the default padding to 8 bytes, while remaining in IPC version 4. Arrow release 0.14.1 and earlier were the last releases to use the legacy 4 byte alignment. As part of 2.0.0, the Rust implementation was updated to comply with the changes up to release 0.15.0 of Arrow. Work on supporting IPC version 5 is underway, and is expected to be completed in time for 3.0.0. As part of the conformance work, Rust is being added to the Arrow integration suite, which tests that supported language implementations (ARROW-3690): Comply with the Arrow IPC format Can read and write each other’s generated data IPC version 4 is being verified through the above work. Roadmap for 3.0.0 and Beyond Here are some of the initiatives that contributors are currently working on for future releases: Support stable Rust Improved DictionaryArray support and performance Implement inner equijoins Support for various platforms like ARMv8 Supporting the C Data Interface from Rust to better support interoperability with other Arrow implementations How to Get Involved If you are interested in contributing to the Rust subproject in Apache Arrow, you can find a list of open issues suitable for beginners here and the full list here. Other ways to get involved include trying out Arrow on some of your data and filing bug reports, and helping to improve the documentation." />
<link rel="canonical" href="https://arrow.apache.org/blog/2020/10/27/rust-2.0.0-release/" />
<meta property="og:url" content="https://arrow.apache.org/blog/2020/10/27/rust-2.0.0-release/" />
<meta property="og:site_name" content="Apache Arrow" />
<meta property="og:image" content="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-10-27T02:00:00-04:00" />
<meta name="twitter:card" content="summary_large_image" />
<meta property="twitter:image" content="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" />
<meta property="twitter:title" content="Apache Arrow 2.0.0 Rust Highlights" />
<meta name="twitter:site" content="@ApacheArrow" />
<meta name="twitter:creator" content="@pmc" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"pmc"},"dateModified":"2020-10-27T02:00:00-04:00","datePublished":"2020-10-27T02:00:00-04:00","description":"Apache Arrow 2.0.0 is a significant release for the Apache Arrow project in general (release notes), and the Rust subproject in particular, with almost 200 issues resolved by 15 contributors. In this blog post, we will go through the main changes affecting core Arrow, Parquet support, and DataFusion query engine. The full list of resolved issues can be found here. While the Java and C/C++ (used by Python and R) Arrow implementations likely remain the most feature-rich, with the 2.0.0 release, the Rust implementation is closing the feature gap quickly. Here are some of the highlights for this release. Core Arrow Crate Iterator Trait Primitive arrays (e.g., array of integers) can now be converted to and initialized from an iterator. This exposes a very popular API - iterators - to arrow arrays. Work for other types will continue throughout 3.0.0. Improved Variable-sized Arrays Variable sized arrays (e.g., array of strings) have been internally refactored to more easily support their larger (64-bit size offset) version. This allowed us to generalize some of the kernels to both (32 and 64) versions and perform type checks when building them. Kernels There have been numerous improvements in the Arrow compute kernels, including: New kernels have been added for string operations, including substring, min, max, concat, and length. Aggregate sum is now implemented for SIMD with a 5x improvement over the non-SIMD operation Many kernels have been improved to support dictionary-encoded arrays Some kernels were optimized for arrays without nulls, making them significantly faster in that case. Many kernels were optimized in the number of memory copies that are needed to apply them and also on their implementation. Other Improvements The Array trait now has get_buffer_memory_size and get_array_memory_size methods for determining the amount of memory allocated for the array. Parquet A significant effort is underway to create a Parquet writer for Arrow data. This work has not been released as part of 2.0.0, and is planned for the 3.0.0 release. The development of this writer is being carried out on the rust-parquet-arrow-writer branch, and the branch is regularly synchronized with the main branch. As part of the writer, the necessary improvements and features are being added to the reader. The main focus areas are: Supporting nested Arrow types, such as List&lt;Struct&lt;[Dictionary, String]&gt;&gt; Ensuring correct round-trip between the reader and writer by encoding Arrow schemas in the Parquet metadata Improve null value writing for Parquet A new parquet_derive crate has been created, which allows users to derive Parquet records for simple structs. Refer to the parquet_derive crate for usage examples. DataFusion DataFusion is an in-memory query engine with DataFrame and SQL APIs, built on top of base Arrow support. DataFrame API DataFusion now has a richer DataFrame API with improved documentation showing example usage, supporting the following operations: select_columns select filter aggregate limit sort collect explain Performance &amp; Scalability DataFusion query execution now uses async/await with the tokio threaded runtime rather than launching dedicated threads, making queries scale much better across available cores. The hash aggregate physical operator has been largely re-written, resulting in significant performance improvements. Expressions and Compute Improved Scalar Functions DataFusion has many new functions, both in the SQL and the DataFrame API: Length of an string COUNT(DISTINCT column) to_timestamp IsNull and IsNotNull Min/Max for strings (lexicographic order) Array of columns Concatenation of strings Aliases of aggregate expressions Many existing expressions were also significantly optimized (2-3x speedups) by avoiding memory copies and leveraging Arrow format’s invariants. Unary mathematical functions (such as sqrt) now support both 32 and 64-bit floats and return the corresponding type, thereby allowing faster operations when higher precision is not needed. Improved User-defined Functions (UDFs) The API to use and register UDFs has been significantly improved, allowing users to register UDFs and call them both via SQL and the DataFrame API. UDFs now also have the same generality as DataFusion’s functions, including variadic and dynamically typed arguments. User-defined Aggregate Functions (UDAFs) DataFusion now supports user-defined aggregate functions that can be used to perform operations than span multiple rows, batches, and partitions. UDAFs have the same generality as DataFusion’s functions and support both row updates and batch updates. You can check out this example to learn how to declare and use a UDAF. User-defined Constants DataFusion now supports registering constants (e.g. “@version”) that live for the duration of the execution context and can be accessed from SQL. Query Planning &amp; Optimization User-defined logical plans The Logical Plan enum is now extensible through an Extension variant which accepts a UserDefinedLogicalPlan trait using dynamic dispatch. Consequently, DataFusion now supports user-defined logical nodes, thereby allowing complex nodes to be planned and executed. You can check this example to learn how to declare a new node. Predicate push-down DataFusion now has a Predicate push-down optimizer rule that pushes filter operations as close as possible to scans, thereby speeding up the physical execution of suboptimal queries created via the DataFrame API. SQL DataFusion now uses a more recent release of the sqlparser crate, which has much more comprehensive support for SQL syntax and also supports multiple dialects (Postgres, MS SQL, and MySQL). It is now possible to see the query plan for a SQL statement using EXPLAIN syntax. Benchmarks The benchmark crate now contains a new benchmark based on TPC-H that can execute TPC-H query 1 against CSV, Parquet, and memory data sources. This is useful for running benchmarks against larger data sets. Integration Testing / IPC Arrow IPC is the format for serialization and interprocess communication. It is described in arrow.apache.org and is the format used for file and stream I/O between applications wishing to interchange Arrow data. The Arrow project released IPC version 5 of the Arrow IPC format in version 1.0.0. Before that, a message padding change was made in version 0.15.0 to change the default padding to 8 bytes, while remaining in IPC version 4. Arrow release 0.14.1 and earlier were the last releases to use the legacy 4 byte alignment. As part of 2.0.0, the Rust implementation was updated to comply with the changes up to release 0.15.0 of Arrow. Work on supporting IPC version 5 is underway, and is expected to be completed in time for 3.0.0. As part of the conformance work, Rust is being added to the Arrow integration suite, which tests that supported language implementations (ARROW-3690): Comply with the Arrow IPC format Can read and write each other’s generated data IPC version 4 is being verified through the above work. Roadmap for 3.0.0 and Beyond Here are some of the initiatives that contributors are currently working on for future releases: Support stable Rust Improved DictionaryArray support and performance Implement inner equijoins Support for various platforms like ARMv8 Supporting the C Data Interface from Rust to better support interoperability with other Arrow implementations How to Get Involved If you are interested in contributing to the Rust subproject in Apache Arrow, you can find a list of open issues suitable for beginners here and the full list here. Other ways to get involved include trying out Arrow on some of your data and filing bug reports, and helping to improve the documentation.","headline":"Apache Arrow 2.0.0 Rust Highlights","image":"https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png","mainEntityOfPage":{"@type":"WebPage","@id":"https://arrow.apache.org/blog/2020/10/27/rust-2.0.0-release/"},"publisher":{"@type":"Organization","logo":{"@type":"ImageObject","url":"https://arrow.apache.org/img/logo.png"},"name":"pmc"},"url":"https://arrow.apache.org/blog/2020/10/27/rust-2.0.0-release/"}</script>
<!-- End Jekyll SEO tag -->


    <!-- favicons -->
    <link rel="icon" type="image/png" sizes="16x16" href="/img/favicon-16x16.png" id="light1">
    <link rel="icon" type="image/png" sizes="32x32" href="/img/favicon-32x32.png" id="light2">
    <link rel="apple-touch-icon" type="image/png" sizes="180x180" href="/img/apple-touch-icon.png" id="light3">
    <link rel="apple-touch-icon" type="image/png" sizes="120x120" href="/img/apple-touch-icon-120x120.png" id="light4">
    <link rel="apple-touch-icon" type="image/png" sizes="76x76" href="/img/apple-touch-icon-76x76.png" id="light5">
    <link rel="apple-touch-icon" type="image/png" sizes="60x60" href="/img/apple-touch-icon-60x60.png" id="light6">
    <!-- dark mode favicons -->
    <link rel="icon" type="image/png" sizes="16x16" href="/img/favicon-16x16-dark.png" id="dark1">
    <link rel="icon" type="image/png" sizes="32x32" href="/img/favicon-32x32-dark.png" id="dark2">
    <link rel="apple-touch-icon" type="image/png" sizes="180x180" href="/img/apple-touch-icon-dark.png" id="dark3">
    <link rel="apple-touch-icon" type="image/png" sizes="120x120" href="/img/apple-touch-icon-120x120-dark.png" id="dark4">
    <link rel="apple-touch-icon" type="image/png" sizes="76x76" href="/img/apple-touch-icon-76x76-dark.png" id="dark5">
    <link rel="apple-touch-icon" type="image/png" sizes="60x60" href="/img/apple-touch-icon-60x60-dark.png" id="dark6">

    <script>
      // Switch to the dark-mode favicons if prefers-color-scheme: dark
      function onUpdate() {
        light1 = document.querySelector('link#light1');
        light2 = document.querySelector('link#light2');
        light3 = document.querySelector('link#light3');
        light4 = document.querySelector('link#light4');
        light5 = document.querySelector('link#light5');
        light6 = document.querySelector('link#light6');

        dark1 = document.querySelector('link#dark1');
        dark2 = document.querySelector('link#dark2');
        dark3 = document.querySelector('link#dark3');
        dark4 = document.querySelector('link#dark4');
        dark5 = document.querySelector('link#dark5');
        dark6 = document.querySelector('link#dark6');

        if (matcher.matches) {
          light1.remove();
          light2.remove();
          light3.remove();
          light4.remove();
          light5.remove();
          light6.remove();
          document.head.append(dark1);
          document.head.append(dark2);
          document.head.append(dark3);
          document.head.append(dark4);
          document.head.append(dark5);
          document.head.append(dark6);
        } else {
          dark1.remove();
          dark2.remove();
          dark3.remove();
          dark4.remove();
          dark5.remove();
          dark6.remove();
          document.head.append(light1);
          document.head.append(light2);
          document.head.append(light3);
          document.head.append(light4);
          document.head.append(light5);
          document.head.append(light6);
        }
      }
      matcher = window.matchMedia('(prefers-color-scheme: dark)');
      matcher.addListener(onUpdate);
      onUpdate();
    </script>

    <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic,900">

    <link href="/css/main.css" rel="stylesheet">
    <link href="/css/syntax.css" rel="stylesheet">
    <script src="/javascript/main.js"></script>
    
    <!-- Matomo -->
<script>
  var _paq = window._paq = window._paq || [];
  /* tracker methods like "setCustomDimension" should be called before "trackPageView" */
  /* We explicitly disable cookie tracking to avoid privacy issues */
  _paq.push(['disableCookies']);
  _paq.push(['trackPageView']);
  _paq.push(['enableLinkTracking']);
  (function() {
    var u="https://analytics.apache.org/";
    _paq.push(['setTrackerUrl', u+'matomo.php']);
    _paq.push(['setSiteId', '20']);
    var d=document, g=d.createElement('script'), s=d.getElementsByTagName('script')[0];
    g.async=true; g.src=u+'matomo.js'; s.parentNode.insertBefore(g,s);
  })();
</script>
<!-- End Matomo Code -->

    
  </head>


<body class="wrap">
  <header>
    <nav class="navbar navbar-expand-md navbar-dark bg-dark">
  
  <a class="navbar-brand no-padding" href="/"><img src="/img/arrow-inverse-300px.png" height="40px"/></a>
  
   <button class="navbar-toggler ml-auto" type="button" data-toggle="collapse" data-target="#arrow-navbar" aria-controls="arrow-navbar" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>

    <!-- Collect the nav links, forms, and other content for toggling -->
    <div class="collapse navbar-collapse justify-content-end" id="arrow-navbar">
      <ul class="nav navbar-nav">
        <li class="nav-item"><a class="nav-link" href="/overview/" role="button" aria-haspopup="true" aria-expanded="false">Overview</a></li>
        <li class="nav-item"><a class="nav-link" href="/faq/" role="button" aria-haspopup="true" aria-expanded="false">FAQ</a></li>
        <li class="nav-item"><a class="nav-link" href="/blog" role="button" aria-haspopup="true" aria-expanded="false">Blog</a></li>
        <li class="nav-item dropdown">
          <a class="nav-link dropdown-toggle" href="#"
             id="navbarDropdownGetArrow" role="button" data-toggle="dropdown"
             aria-haspopup="true" aria-expanded="false">
             Get Arrow
          </a>
          <div class="dropdown-menu" aria-labelledby="navbarDropdownGetArrow">
            <a class="dropdown-item" href="/install/">Install</a>
            <a class="dropdown-item" href="/release/">Releases</a>
            <a class="dropdown-item" href="https://github.com/apache/arrow">Source Code</a>
          </div>
        </li>
        <li class="nav-item dropdown">
          <a class="nav-link dropdown-toggle" href="#"
             id="navbarDropdownDocumentation" role="button" data-toggle="dropdown"
             aria-haspopup="true" aria-expanded="false">
             Documentation
          </a>
          <div class="dropdown-menu" aria-labelledby="navbarDropdownDocumentation">
            <a class="dropdown-item" href="/docs">Project Docs</a>
            <a class="dropdown-item" href="/docs/format/Columnar.html">Format</a>
            <hr/>
            <a class="dropdown-item" href="/docs/c_glib">C GLib</a>
            <a class="dropdown-item" href="/docs/cpp">C++</a>
            <a class="dropdown-item" href="https://github.com/apache/arrow/blob/master/csharp/README.md">C#</a>
            <a class="dropdown-item" href="https://godoc.org/github.com/apache/arrow/go/arrow">Go</a>
            <a class="dropdown-item" href="/docs/java">Java</a>
            <a class="dropdown-item" href="/docs/js">JavaScript</a>
            <a class="dropdown-item" href="https://arrow.juliadata.org/stable/">Julia</a>
            <a class="dropdown-item" href="https://github.com/apache/arrow/blob/master/matlab/README.md">MATLAB</a>
            <a class="dropdown-item" href="/docs/python">Python</a>
            <a class="dropdown-item" href="/docs/r">R</a>
            <a class="dropdown-item" href="https://github.com/apache/arrow/blob/master/ruby/README.md">Ruby</a>
            <a class="dropdown-item" href="https://docs.rs/crate/arrow/">Rust</a>
          </div>
        </li>
        <li class="nav-item dropdown">
          <a class="nav-link dropdown-toggle" href="#"
             id="navbarDropdownSubprojects" role="button" data-toggle="dropdown"
             aria-haspopup="true" aria-expanded="false">
             Subprojects
          </a>
          <div class="dropdown-menu" aria-labelledby="navbarDropdownSubprojects">
            <a class="dropdown-item" href="/docs/format/Flight.html">Arrow Flight</a>
            <a class="dropdown-item" href="/docs/format/FlightSql.html">Arrow Flight SQL</a>
            <a class="dropdown-item" href="/datafusion">DataFusion</a>
          </div>
        </li>
        <li class="nav-item dropdown">
          <a class="nav-link dropdown-toggle" href="#"
             id="navbarDropdownCommunity" role="button" data-toggle="dropdown"
             aria-haspopup="true" aria-expanded="false">
             Community
          </a>
          <div class="dropdown-menu" aria-labelledby="navbarDropdownCommunity">
            <a class="dropdown-item" href="/community/">Communication</a>
            <a class="dropdown-item" href="/docs/developers/contributing.html">Contributing</a>
            <a class="dropdown-item" href="https://github.com/apache/arrow/issues">Issue Tracker</a>
            <a class="dropdown-item" href="/committers/">Governance</a>
            <a class="dropdown-item" href="/use_cases/">Use Cases</a>
            <a class="dropdown-item" href="/powered_by/">Powered By</a>
            <a class="dropdown-item" href="/visual_identity/">Visual Identity</a>
            <a class="dropdown-item" href="/security/">Security</a>
            <a class="dropdown-item" href="https://www.apache.org/foundation/policies/conduct.html">Code of Conduct</a>
          </div>
        </li>
        <li class="nav-item dropdown">
          <a class="nav-link dropdown-toggle" href="#"
             id="navbarDropdownASF" role="button" data-toggle="dropdown"
             aria-haspopup="true" aria-expanded="false">
             ASF Links
          </a>
          <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdownASF">
            <a class="dropdown-item" href="https://www.apache.org/">ASF Website</a>
            <a class="dropdown-item" href="https://www.apache.org/licenses/">License</a>
            <a class="dropdown-item" href="https://www.apache.org/foundation/sponsorship.html">Donate</a>
            <a class="dropdown-item" href="https://www.apache.org/foundation/thanks.html">Thanks</a>
            <a class="dropdown-item" href="https://www.apache.org/security/">Security</a>
          </div>
        </li>
      </ul>
    </div><!-- /.navbar-collapse -->
  </nav>

  </header>

  <div class="container p-4 pt-5">
    <div class="col-md-8 mx-auto">
      <main role="main" class="pb-5">
        
<h1>
  Apache Arrow 2.0.0 Rust Highlights
</h1>
<hr class="mt-4 mb-3">



<p class="mb-4 pb-1">
  <span class="badge badge-secondary">Published</span>
  <span class="published mr-3">
    27 Oct 2020
  </span>
  <br />
  <span class="badge badge-secondary">By</span>
  
    <a class="mr-3" href="https://arrow.apache.org">The Apache Arrow PMC (pmc) </a>
  

  
</p>


        <!--

-->

<p>Apache Arrow 2.0.0 is a significant release for the Apache Arrow project in general (<a href="https://arrow.apache.org/blog/2020/10/22/2.0.0-release/">release notes</a>), and the Rust subproject
in particular, with almost 200 issues resolved by 15 contributors. In this blog post, we will go through the main changes 
affecting core Arrow, Parquet support, and DataFusion query engine. The full list of resolved issues can be found 
<a href="https://issues.apache.org/jira/browse/ARROW-10295?jql=project%20%3D%20ARROW%20AND%20status%20not%20in%20%20(Open%2C%20%22In%20Progress%22%2C%20Reopened)%20and%20fixVersion%20%3D%202.0.0%20AND%20text%20~%20rust%20ORDER%20BY%20created%20DESC">here</a>.</p>

<p>While the Java and C/C++ (used by Python and R) Arrow implementations likely remain the most feature-rich, with the 
2.0.0 release, the Rust implementation is closing the feature gap quickly. Here are some of the highlights for this 
release.</p>

<h1 id="core-arrow-crate">Core Arrow Crate</h1>

<h2 id="iterator-trait">Iterator Trait</h2>

<ul>
  <li>Primitive arrays (e.g., array of integers) can now be converted to and initialized from an iterator. This exposes a 
very popular API - iterators - to arrow arrays. Work for other types will continue throughout 3.0.0.</li>
</ul>

<h2 id="improved-variable-sized-arrays">Improved Variable-sized Arrays</h2>

<ul>
  <li>Variable sized arrays (e.g., array of strings) have been internally refactored to more easily support their larger (64-bit 
size offset) version. This allowed us to generalize some of the kernels to both (32 and 64) versions and 
perform type checks when building them.</li>
</ul>

<h2 id="kernels">Kernels</h2>

<p>There have been numerous improvements in the Arrow compute kernels, including:</p>

<ul>
  <li>New kernels have been added for string operations, including substring, min, max, concat, and length.</li>
  <li>Aggregate sum is now implemented for SIMD with a 5x improvement over the non-SIMD operation</li>
  <li>Many kernels have been improved to support dictionary-encoded arrays</li>
  <li>Some kernels were optimized for arrays without nulls, making them significantly faster in that case.</li>
  <li>Many kernels were optimized in the number of memory copies that are needed to apply them and also on their 
implementation.</li>
</ul>

<h2 id="other-improvements">Other Improvements</h2>

<p>The Array trait now has <code class="language-plaintext highlighter-rouge">get_buffer_memory_size</code> and <code class="language-plaintext highlighter-rouge">get_array_memory_size</code> methods for determining the amount of 
memory allocated for the array.</p>

<h1 id="parquet">Parquet</h1>

<p>A significant effort is underway to create a Parquet writer for Arrow data. This work has not been released as part of 
2.0.0, and is planned for the 3.0.0 release. The development of this writer is being carried out on the 
<a href="https://github.com/apache/arrow/tree/rust-parquet-arrow-writer">rust-parquet-arrow-writer</a> branch, and the branch is regularly synchronized with the main branch.
As part of the writer, the necessary improvements and features are being added to the reader.</p>

<p>The main focus areas are:</p>
<ul>
  <li>Supporting nested Arrow types, such as <code class="language-plaintext highlighter-rouge">List&lt;Struct&lt;[Dictionary, String]&gt;&gt;</code></li>
  <li>Ensuring correct round-trip between the reader and writer by encoding Arrow schemas in the Parquet metadata</li>
  <li>Improve null value writing for Parquet</li>
</ul>

<p>A new <code class="language-plaintext highlighter-rouge">parquet_derive</code> crate has been created, which allows users to derive Parquet records for simple structs. Refer to 
the <a href="https://github.com/apache/arrow-rs/tree/master/parquet_derive">parquet_derive crate</a> for usage examples.</p>

<h1 id="datafusion">DataFusion</h1>

<p>DataFusion is an in-memory query engine with DataFrame and SQL APIs, built on top of base Arrow support.</p>

<h2 id="dataframe-api">DataFrame API</h2>

<p>DataFusion now has a richer <a href="https://docs.rs/datafusion/2.0.0/datafusion/dataframe/trait.DataFrame.html">DataFrame API</a> with improved documentation showing example usage, 
supporting the following operations:</p>

<ul>
  <li>select_columns</li>
  <li>select</li>
  <li>filter</li>
  <li>aggregate</li>
  <li>limit</li>
  <li>sort</li>
  <li>collect</li>
  <li>explain</li>
</ul>

<h2 id="performance--scalability">Performance &amp; Scalability</h2>

<p>DataFusion query execution now uses <code class="language-plaintext highlighter-rouge">async</code>/<code class="language-plaintext highlighter-rouge">await</code> with the tokio threaded runtime rather than launching dedicated 
threads, making queries scale much better across available cores.</p>

<p>The hash aggregate physical operator has been largely re-written, resulting in significant performance improvements.</p>

<h2 id="expressions-and-compute">Expressions and Compute</h2>

<h3 id="improved-scalar-functions">Improved Scalar Functions</h3>

<p>DataFusion has many new functions, both in the SQL and the DataFrame API:</p>
<ul>
  <li>Length of an string</li>
  <li>COUNT(DISTINCT column)</li>
  <li>to_timestamp</li>
  <li>IsNull and IsNotNull</li>
  <li>Min/Max for strings (lexicographic order)</li>
  <li>Array of columns</li>
  <li>Concatenation of strings</li>
  <li>Aliases of aggregate expressions</li>
</ul>

<p>Many existing expressions were also significantly optimized (2-3x speedups) by avoiding memory copies and leveraging 
Arrow format’s invariants.</p>

<p>Unary mathematical functions (such as sqrt) now support both 32 and 64-bit floats and return the corresponding type, 
thereby allowing faster operations when higher precision is not needed.</p>

<h3 id="improved-user-defined-functions-udfs">Improved User-defined Functions (UDFs)</h3>
<p>The API to use and register UDFs has been significantly improved, allowing users to register UDFs and call them both 
via SQL and the DataFrame API. UDFs now also have the same generality as DataFusion’s functions, including variadic 
and dynamically typed arguments.</p>

<h3 id="user-defined-aggregate-functions-udafs">User-defined Aggregate Functions (UDAFs)</h3>
<p>DataFusion now supports user-defined aggregate functions that can be used to perform operations than span multiple 
rows, batches, and partitions. UDAFs have the same generality as DataFusion’s functions and support both row updates 
and batch updates. You can check out <a href="https://github.com/apache/arrow-datafusion/blob/master/datafusion-examples/examples/simple_udaf.rs">this example</a> to learn how to declare and use a UDAF.</p>

<h3 id="user-defined-constants">User-defined Constants</h3>
<p>DataFusion now supports registering constants (e.g. “@version”) that live for the duration of the execution context 
and can be accessed from SQL.</p>

<h2 id="query-planning--optimization">Query Planning &amp; Optimization</h2>

<h3 id="user-defined-logical-plans">User-defined logical plans</h3>

<p>The Logical Plan enum is now extensible through an Extension variant which accepts a UserDefinedLogicalPlan trait using 
dynamic dispatch. Consequently, DataFusion now supports user-defined logical nodes, thereby allowing complex nodes to 
be planned and executed. You can check this example to learn how to declare a new node.</p>

<h3 id="predicate-push-down">Predicate push-down</h3>

<p>DataFusion now has a Predicate push-down optimizer rule that pushes filter operations as close as possible to scans, 
thereby speeding up the physical execution of suboptimal queries created via the DataFrame API.</p>

<h3 id="sql">SQL</h3>

<p>DataFusion now uses a more recent release of the sqlparser crate, which has much more comprehensive support for SQL 
syntax and also supports multiple dialects (Postgres, MS SQL, and MySQL).</p>

<p>It is now possible to see the query plan for a SQL statement using EXPLAIN syntax.</p>

<h1 id="benchmarks">Benchmarks</h1>

<p>The benchmark crate now contains a new benchmark based on TPC-H that can execute TPC-H query 1 against CSV, Parquet, 
and memory data sources. This is useful for running benchmarks against larger data sets.</p>

<h1 id="integration-testing--ipc">Integration Testing / IPC</h1>

<p>Arrow IPC is the format for serialization and interprocess communication. It is described in <a href="https://arrow.apache.org/">arrow.apache.org</a> and is 
the format used for file and stream I/O between applications wishing to interchange Arrow data.</p>

<p>The Arrow project released IPC version 5 of the Arrow IPC format in version 1.0.0. Before that, a message padding 
change was made in version 0.15.0 to change the default padding to 8 bytes, while remaining in IPC version 4. Arrow 
release 0.14.1 and earlier were the last releases to use the legacy 4 byte alignment.
As part of 2.0.0, the Rust implementation was updated to comply with the changes up to release 0.15.0 of Arrow. 
Work on supporting IPC version 5 is underway, and is expected to be completed in time for 3.0.0.</p>

<p>As part of the conformance work, Rust is being added to the Arrow integration suite, which tests that supported 
language implementations <a href="https://issues.apache.org/jira/browse/ARROW-3690">(ARROW-3690)</a>:</p>

<ul>
  <li>Comply with the Arrow IPC format</li>
  <li>Can read and write each other’s generated data</li>
  <li>IPC version 4 is being verified through the above work.</li>
</ul>

<h1 id="roadmap-for-300-and-beyond">Roadmap for 3.0.0 and Beyond</h1>

<p>Here are some of the initiatives that contributors are currently working on for future releases:</p>

<ul>
  <li>Support stable Rust</li>
  <li>Improved DictionaryArray support and performance</li>
  <li>Implement inner equijoins</li>
  <li>Support for various platforms like ARMv8</li>
  <li>Supporting the C Data Interface from Rust to better support interoperability with other Arrow implementations</li>
</ul>

<h1 id="how-to-get-involved">How to Get Involved</h1>

<p>If you are interested in contributing to the Rust subproject in Apache Arrow, you can find a list of open issues 
suitable for beginners <a href="https://issues.apache.org/jira/issues/?jql=project%20%3D%20ARROW%20AND%20resolution%20%3D%20Unresolved%20AND%20component%20in%20(Rust%2C%20%22Rust%20-%20DataFusion%22)%20AND%20labels%20%3D%20beginner">here</a> and the full list <a href="https://issues.apache.org/jira/issues/?jql=project%20%3D%20ARROW%20AND%20resolution%20%3D%20Unresolved%20AND%20component%20in%20(Rust%2C%20%22Rust%20-%20DataFusion%22)%20ORDER%20BY%20updated%20DESC%2C%20created%20DESC%2C%20priority%20DESC">here</a>.</p>

<p>Other ways to get involved include trying out Arrow on some of your data and filing bug reports, and helping to 
improve the documentation.</p>


      </main>
    </div>

    <hr/>
<footer class="footer">
  <div class="row">
    <div class="col-md-9">
      <p>Apache Arrow, Arrow, Apache, the Apache feather logo, and the Apache Arrow project logo are either registered trademarks or trademarks of The Apache Software Foundation in the United States and other countries.</p>
      <p>&copy; 2016-2022 The Apache Software Foundation</p>
    </div>
    <div class="col-md-3">
      <a class="d-sm-none d-md-inline pr-2" href="https://www.apache.org/events/current-event.html">
        <img src="https://www.apache.org/events/current-event-234x60.png"/>
      </a>
    </div>
  </div>
</footer>

  </div>
</body>
</html>
