<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- The above meta tags *must* come first in the head; any other head content must come *after* these tags -->
    
    <title>Apache Arrow 1.0.0 Release | Apache Arrow</title>
    

    <!-- Begin Jekyll SEO tag v2.7.1 -->
<meta name="generator" content="Jekyll v4.2.0" />
<meta property="og:title" content="Apache Arrow 1.0.0 Release" />
<meta name="author" content="pmc" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="The Apache Arrow team is pleased to announce the 1.0.0 release. This covers over 3 months of development work and includes 810 resolved issues from 100 distinct contributors. See the Install Page to learn how to get the libraries for your platform. Despite a “1.0.0” version, this is the 18th major release of Apache Arrow and marks a transition to binary stability of the columnar format (which was already informally backward-compatible going back to December 2017) and a transition to Semantic Versioning for the Arrow software libraries. The release notes below are not exhaustive and only expose selected highlights of the release. Many other bugfixes and improvements have been made: we refer you to the complete changelog. 1.0.0 Columnar Format and Stability Guarantees The 1.0.0 release indicates that the Arrow columnar format is declared stable, with forward and backward compatibility guarantees. The Arrow columnar format received several recent changes and additions, leading to the 1.0.0 format version: The metadata version was bumped to a new version V5, indicating an incompatible change in the buffer layout of Union types. All other types keep the same layout as in V4. V5 also includes format additions to assist with forward compatibility (detecting unsupported changes sent by future library versions). Libraries remain backward compatible with data generated by all libraries back to 0.8.0 (December 2017) and the Java and C++ libraries are capable of generating V4-compatible messages (for sending data to applications using 0.8.0 to 0.17.1). Dictionary indices are now allowed to be unsigned integers rather than only signed integers. Using UInt64 is still discouraged because of poor Java support. A “Feature” enum has been added to announce the use of specific optional features in an IPC stream, such as buffer compression. This new field is not used by any implementation yet. Optional buffer compression using LZ4 or ZStandard was added to the IPC format. Decimal types now have an optional “bitWidth” field, defaulting to 128. This will allow for future support of other decimal widths such as 32- and 64-bit. The validity bitmap buffer has been removed from Union types. The nullity of a slot in a Union array is determined exclusively by the constituent arrays forming the union. Integration testing has been expanded to test for extension types and nested dictionaries. See the implementation matrix for details. Community Since the last release, we have added two new committers: Liya Fan Ji Liu Thank you for all your contributions! Arrow Flight RPC notes Flight now offers DoExchange, a fully bidirectional data endpoint, in addition to DoGet and DoPut, in C++, Java, and Python. Middlewares in all languages now expose binary-valued headers. Additionally, servers and clients can set Arrow IPC read/write options in all languages, making compatibility easier with earlier versions of Arrow Flight. In C++ and Python, Flight now exposes more options from gRPC, including the address of the client (on the server) and the ability to set low-level gRPC client options. Flight also supports mutual TLS authentication and the ability for a client to control the size of a data message on the wire. C++ notes Support for static linking with Arrow has been vastly improved, including the introduction of a libarrow_bundled_dependencies.a library bundling all external dependencies that are built from source by Arrow’s build system rather than installed by an external package manager. This makes it significantly easier to create dependency-free applications with all libraries statically-linked. Following the Arrow format changes, Union arrays cannot have a top-level bitmap anymore. A number of improvements were made to reduce the overall generated binary size in the Arrow library. A convenience API GetBuildInfo allows querying the characteristics of the Arrow library. We encourage you to suggest any desired addition to the returned information. We added an optional dependency to the utf8proc library, used in several compute functions (see below). Instead of sharing the same concrete classes, sparse and dense unions now have separated classes (SparseUnionType and DenseUnionType, as well as SparseUnionArray, DenseUnionArray, SparseUnionScalar, DenseUnionScalar). Arrow can now be built for iOS using the right set of CMake options, though we don’t officially support it. See this writeup for details. Compute functions The compute kernel layer was extensively reworked. It now offers a generic function lookup, dispatch and execution mechanism. Furthermore, new internal scaffoldings make it vastly easier to write new function kernels, with many common details like type checking and function dispatch based on type combinations handled by the framework rather than implemented manually by the function developer. Around 30 new array compute functions have been added. For example, Unicode-compliant predicates and transforms, such as lowercase and uppercase transforms, are now available. The available compute functions are listed exhaustively in the Sphinx-generated documentation. Datasets Datasets can now be read from CSV files. Datasets can be expanded to their component fragments, enabling fine grained interoperability with other consumers of data files. Where applicable, metadata is available as a property of the fragment, including partition information and (for the parquet format) per-column statistics. Datasets of parquet files can now be assembled from a single _metadata file, such as those created by systems like Dask and Spark. _metadata contains the metadata of all fragments, allowing construction of a statistics- aware dataset with a single IO call. Feather The Feather format is now available in version 2, which is simply the Arrow IPC file format with another name. IPC By default, we now write IPC streams with metadata V5. However, metadata V4 can be requested by setting the appropriate member in IpcWriteOptions. V4 as well as V5 metadata IPC streams can be read properly, with one exception: a V4 metadata stream containing Union arrays with top-level null values will refuse reading. As noted above, there are no changes between V4 and V5 that break backwards compatibility. For forward compatibility scenarios (where you need to generate data to be read by an older Arrow library), you can set the V4 compatibility mode. Support for dictionary replacement and dictionary delta was implemented. Parquet Writing files with the LZ4 codec is disabled because it produces files incompatible with the widely-used Hadoop Parquet implementation. Support will be reenabled once we align the LZ4 implementation with the special buffer encoding expected by Hadoop. Java notes The Java package introduces a number of low level changes in this release. Most notable are the work in support of allocating large arrow buffers and removing Netty from the public API. Users will have to update their dependencies to use one of the two supported allocators Netty: arrow-memory-netty or Unsafe (internal java api for direct memory) arrow-memory-unsafe. The Java Vector implementation has improved its interoperability having verified LargeVarChar, LargeBinary, LargeList, Union, Extension types and duplicate field names in Structs are binary compatible with C++ and the specification. Python notes The size of wheel packages is significantly reduced, up to 75%. One side effect is that these wheels do not enable Gandiva anymore (which requires the LLVM runtime to be statically-linked). We are interested in providing Gandiva as an add-on package as a separate Python wheel in the future. The Scalar class hierarchy was reworked to more closely follow its C++ counterpart. TLS CA certificates are looked up more reliably when using the S3 filesystem, especially with manylinux wheels. The encoding of CSV files can now be specified explicitly, defaulting to UTF8. Custom timestamp parsers can now be used for CSV files. Filesystems can now be implemented in pure Python. As a result, fsspec-based filesystems can now be used in datasets. parquet.read_table is now backed by the dataset API by default, enabling filters on any column and more flexible partitioning. R notes The R package added support for converting to and from many additional Arrow types. Tables showing how R types are mapped to Arrow types and vice versa have been added to the introductory vignette, and nearly all types are handled. In addition, R attributes like custom classes and metadata are now preserved when converting a data.frame to an Arrow Table and are restored when loading them back into R. For more on what’s in the 1.0.0 R package, see the R changelog. Ruby and C GLib notes The Ruby and C GLib packages added support for the new compute function framework, in which users can find a compute function dynamically and call it. Users don’t need to wait for a C GLib binding for new compute functions: if the C++ package provides a new compute function, users can use it without additional code in the Ruby and C GLib packages. The Ruby and C GLib packages added support for Apache Arrow Dataset. The Ruby package provides a new gem for Apache Arrow Dataset, red-arrow-dataset. The C GLib package provides a new module for Apache Arrow Dataset, arrow-dataset-glib. They just have a few features for now but we will add more in future releases. The Ruby and C GLib packages added support for reading only the specified row group in an Apache Parquet file. Ruby The Ruby package added support for column level compression in writing Apache Parquet files. The Ruby package changed the Arrow::DictionaryArray#[] behavior. It now returns the dictionary value instead of the dictionary index. This is a backwards-incompatible change. Rust notes A new integration test crate has been added, allowing the Rust implementation to participate in integration testing. A new benchmark crate has been added for benchmarking performance against popular data sets. The initial examples run SQL queries against the NYC Taxi data set using DataFusion. This is useful for comparing performance against other Arrow implementations. Rust toolchain has been upgraded to 1.44 nightly. Arrow Core Support for binary, string, and list arrays with i64 offsets to support large lists. A new sort kernel has been added. There have been various improvements to dictionary array support. CSV reader enhancements include a new CsvReadOptions struct and support for schema inference from multiple CSV files. There are significant (10x - 40x) performance improvements to SIMD comparison kernels. DataFusion There are numerous UX improvements to LogicalPlan and LogicalPlanBuilder, including support for named columns. General improvements to code base, such as removing many uses of Arc and using slices instead of &amp;Vec as function arguments. ParquetScanExec performance improvement (almost 2x). ExecutionContext can now be shared between threads. Rust closures can now be used as Scalar UDFs. Sort support has been added to SQL and LogicalPlan." />
<meta property="og:description" content="The Apache Arrow team is pleased to announce the 1.0.0 release. This covers over 3 months of development work and includes 810 resolved issues from 100 distinct contributors. See the Install Page to learn how to get the libraries for your platform. Despite a “1.0.0” version, this is the 18th major release of Apache Arrow and marks a transition to binary stability of the columnar format (which was already informally backward-compatible going back to December 2017) and a transition to Semantic Versioning for the Arrow software libraries. The release notes below are not exhaustive and only expose selected highlights of the release. Many other bugfixes and improvements have been made: we refer you to the complete changelog. 1.0.0 Columnar Format and Stability Guarantees The 1.0.0 release indicates that the Arrow columnar format is declared stable, with forward and backward compatibility guarantees. The Arrow columnar format received several recent changes and additions, leading to the 1.0.0 format version: The metadata version was bumped to a new version V5, indicating an incompatible change in the buffer layout of Union types. All other types keep the same layout as in V4. V5 also includes format additions to assist with forward compatibility (detecting unsupported changes sent by future library versions). Libraries remain backward compatible with data generated by all libraries back to 0.8.0 (December 2017) and the Java and C++ libraries are capable of generating V4-compatible messages (for sending data to applications using 0.8.0 to 0.17.1). Dictionary indices are now allowed to be unsigned integers rather than only signed integers. Using UInt64 is still discouraged because of poor Java support. A “Feature” enum has been added to announce the use of specific optional features in an IPC stream, such as buffer compression. This new field is not used by any implementation yet. Optional buffer compression using LZ4 or ZStandard was added to the IPC format. Decimal types now have an optional “bitWidth” field, defaulting to 128. This will allow for future support of other decimal widths such as 32- and 64-bit. The validity bitmap buffer has been removed from Union types. The nullity of a slot in a Union array is determined exclusively by the constituent arrays forming the union. Integration testing has been expanded to test for extension types and nested dictionaries. See the implementation matrix for details. Community Since the last release, we have added two new committers: Liya Fan Ji Liu Thank you for all your contributions! Arrow Flight RPC notes Flight now offers DoExchange, a fully bidirectional data endpoint, in addition to DoGet and DoPut, in C++, Java, and Python. Middlewares in all languages now expose binary-valued headers. Additionally, servers and clients can set Arrow IPC read/write options in all languages, making compatibility easier with earlier versions of Arrow Flight. In C++ and Python, Flight now exposes more options from gRPC, including the address of the client (on the server) and the ability to set low-level gRPC client options. Flight also supports mutual TLS authentication and the ability for a client to control the size of a data message on the wire. C++ notes Support for static linking with Arrow has been vastly improved, including the introduction of a libarrow_bundled_dependencies.a library bundling all external dependencies that are built from source by Arrow’s build system rather than installed by an external package manager. This makes it significantly easier to create dependency-free applications with all libraries statically-linked. Following the Arrow format changes, Union arrays cannot have a top-level bitmap anymore. A number of improvements were made to reduce the overall generated binary size in the Arrow library. A convenience API GetBuildInfo allows querying the characteristics of the Arrow library. We encourage you to suggest any desired addition to the returned information. We added an optional dependency to the utf8proc library, used in several compute functions (see below). Instead of sharing the same concrete classes, sparse and dense unions now have separated classes (SparseUnionType and DenseUnionType, as well as SparseUnionArray, DenseUnionArray, SparseUnionScalar, DenseUnionScalar). Arrow can now be built for iOS using the right set of CMake options, though we don’t officially support it. See this writeup for details. Compute functions The compute kernel layer was extensively reworked. It now offers a generic function lookup, dispatch and execution mechanism. Furthermore, new internal scaffoldings make it vastly easier to write new function kernels, with many common details like type checking and function dispatch based on type combinations handled by the framework rather than implemented manually by the function developer. Around 30 new array compute functions have been added. For example, Unicode-compliant predicates and transforms, such as lowercase and uppercase transforms, are now available. The available compute functions are listed exhaustively in the Sphinx-generated documentation. Datasets Datasets can now be read from CSV files. Datasets can be expanded to their component fragments, enabling fine grained interoperability with other consumers of data files. Where applicable, metadata is available as a property of the fragment, including partition information and (for the parquet format) per-column statistics. Datasets of parquet files can now be assembled from a single _metadata file, such as those created by systems like Dask and Spark. _metadata contains the metadata of all fragments, allowing construction of a statistics- aware dataset with a single IO call. Feather The Feather format is now available in version 2, which is simply the Arrow IPC file format with another name. IPC By default, we now write IPC streams with metadata V5. However, metadata V4 can be requested by setting the appropriate member in IpcWriteOptions. V4 as well as V5 metadata IPC streams can be read properly, with one exception: a V4 metadata stream containing Union arrays with top-level null values will refuse reading. As noted above, there are no changes between V4 and V5 that break backwards compatibility. For forward compatibility scenarios (where you need to generate data to be read by an older Arrow library), you can set the V4 compatibility mode. Support for dictionary replacement and dictionary delta was implemented. Parquet Writing files with the LZ4 codec is disabled because it produces files incompatible with the widely-used Hadoop Parquet implementation. Support will be reenabled once we align the LZ4 implementation with the special buffer encoding expected by Hadoop. Java notes The Java package introduces a number of low level changes in this release. Most notable are the work in support of allocating large arrow buffers and removing Netty from the public API. Users will have to update their dependencies to use one of the two supported allocators Netty: arrow-memory-netty or Unsafe (internal java api for direct memory) arrow-memory-unsafe. The Java Vector implementation has improved its interoperability having verified LargeVarChar, LargeBinary, LargeList, Union, Extension types and duplicate field names in Structs are binary compatible with C++ and the specification. Python notes The size of wheel packages is significantly reduced, up to 75%. One side effect is that these wheels do not enable Gandiva anymore (which requires the LLVM runtime to be statically-linked). We are interested in providing Gandiva as an add-on package as a separate Python wheel in the future. The Scalar class hierarchy was reworked to more closely follow its C++ counterpart. TLS CA certificates are looked up more reliably when using the S3 filesystem, especially with manylinux wheels. The encoding of CSV files can now be specified explicitly, defaulting to UTF8. Custom timestamp parsers can now be used for CSV files. Filesystems can now be implemented in pure Python. As a result, fsspec-based filesystems can now be used in datasets. parquet.read_table is now backed by the dataset API by default, enabling filters on any column and more flexible partitioning. R notes The R package added support for converting to and from many additional Arrow types. Tables showing how R types are mapped to Arrow types and vice versa have been added to the introductory vignette, and nearly all types are handled. In addition, R attributes like custom classes and metadata are now preserved when converting a data.frame to an Arrow Table and are restored when loading them back into R. For more on what’s in the 1.0.0 R package, see the R changelog. Ruby and C GLib notes The Ruby and C GLib packages added support for the new compute function framework, in which users can find a compute function dynamically and call it. Users don’t need to wait for a C GLib binding for new compute functions: if the C++ package provides a new compute function, users can use it without additional code in the Ruby and C GLib packages. The Ruby and C GLib packages added support for Apache Arrow Dataset. The Ruby package provides a new gem for Apache Arrow Dataset, red-arrow-dataset. The C GLib package provides a new module for Apache Arrow Dataset, arrow-dataset-glib. They just have a few features for now but we will add more in future releases. The Ruby and C GLib packages added support for reading only the specified row group in an Apache Parquet file. Ruby The Ruby package added support for column level compression in writing Apache Parquet files. The Ruby package changed the Arrow::DictionaryArray#[] behavior. It now returns the dictionary value instead of the dictionary index. This is a backwards-incompatible change. Rust notes A new integration test crate has been added, allowing the Rust implementation to participate in integration testing. A new benchmark crate has been added for benchmarking performance against popular data sets. The initial examples run SQL queries against the NYC Taxi data set using DataFusion. This is useful for comparing performance against other Arrow implementations. Rust toolchain has been upgraded to 1.44 nightly. Arrow Core Support for binary, string, and list arrays with i64 offsets to support large lists. A new sort kernel has been added. There have been various improvements to dictionary array support. CSV reader enhancements include a new CsvReadOptions struct and support for schema inference from multiple CSV files. There are significant (10x - 40x) performance improvements to SIMD comparison kernels. DataFusion There are numerous UX improvements to LogicalPlan and LogicalPlanBuilder, including support for named columns. General improvements to code base, such as removing many uses of Arc and using slices instead of &amp;Vec as function arguments. ParquetScanExec performance improvement (almost 2x). ExecutionContext can now be shared between threads. Rust closures can now be used as Scalar UDFs. Sort support has been added to SQL and LogicalPlan." />
<link rel="canonical" href="https://arrow.apache.org/blog/2020/07/24/1.0.0-release/" />
<meta property="og:url" content="https://arrow.apache.org/blog/2020/07/24/1.0.0-release/" />
<meta property="og:site_name" content="Apache Arrow" />
<meta property="og:image" content="https://arrow.apache.org/img/arrow.png" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-07-24T02:00:00-04:00" />
<meta name="twitter:card" content="summary_large_image" />
<meta property="twitter:image" content="https://arrow.apache.org/img/arrow.png" />
<meta property="twitter:title" content="Apache Arrow 1.0.0 Release" />
<meta name="twitter:site" content="@ApacheArrow" />
<meta name="twitter:creator" content="@pmc" />
<script type="application/ld+json">
{"url":"https://arrow.apache.org/blog/2020/07/24/1.0.0-release/","headline":"Apache Arrow 1.0.0 Release","dateModified":"2020-07-24T02:00:00-04:00","datePublished":"2020-07-24T02:00:00-04:00","description":"The Apache Arrow team is pleased to announce the 1.0.0 release. This covers over 3 months of development work and includes 810 resolved issues from 100 distinct contributors. See the Install Page to learn how to get the libraries for your platform. Despite a “1.0.0” version, this is the 18th major release of Apache Arrow and marks a transition to binary stability of the columnar format (which was already informally backward-compatible going back to December 2017) and a transition to Semantic Versioning for the Arrow software libraries. The release notes below are not exhaustive and only expose selected highlights of the release. Many other bugfixes and improvements have been made: we refer you to the complete changelog. 1.0.0 Columnar Format and Stability Guarantees The 1.0.0 release indicates that the Arrow columnar format is declared stable, with forward and backward compatibility guarantees. The Arrow columnar format received several recent changes and additions, leading to the 1.0.0 format version: The metadata version was bumped to a new version V5, indicating an incompatible change in the buffer layout of Union types. All other types keep the same layout as in V4. V5 also includes format additions to assist with forward compatibility (detecting unsupported changes sent by future library versions). Libraries remain backward compatible with data generated by all libraries back to 0.8.0 (December 2017) and the Java and C++ libraries are capable of generating V4-compatible messages (for sending data to applications using 0.8.0 to 0.17.1). Dictionary indices are now allowed to be unsigned integers rather than only signed integers. Using UInt64 is still discouraged because of poor Java support. A “Feature” enum has been added to announce the use of specific optional features in an IPC stream, such as buffer compression. This new field is not used by any implementation yet. Optional buffer compression using LZ4 or ZStandard was added to the IPC format. Decimal types now have an optional “bitWidth” field, defaulting to 128. This will allow for future support of other decimal widths such as 32- and 64-bit. The validity bitmap buffer has been removed from Union types. The nullity of a slot in a Union array is determined exclusively by the constituent arrays forming the union. Integration testing has been expanded to test for extension types and nested dictionaries. See the implementation matrix for details. Community Since the last release, we have added two new committers: Liya Fan Ji Liu Thank you for all your contributions! Arrow Flight RPC notes Flight now offers DoExchange, a fully bidirectional data endpoint, in addition to DoGet and DoPut, in C++, Java, and Python. Middlewares in all languages now expose binary-valued headers. Additionally, servers and clients can set Arrow IPC read/write options in all languages, making compatibility easier with earlier versions of Arrow Flight. In C++ and Python, Flight now exposes more options from gRPC, including the address of the client (on the server) and the ability to set low-level gRPC client options. Flight also supports mutual TLS authentication and the ability for a client to control the size of a data message on the wire. C++ notes Support for static linking with Arrow has been vastly improved, including the introduction of a libarrow_bundled_dependencies.a library bundling all external dependencies that are built from source by Arrow’s build system rather than installed by an external package manager. This makes it significantly easier to create dependency-free applications with all libraries statically-linked. Following the Arrow format changes, Union arrays cannot have a top-level bitmap anymore. A number of improvements were made to reduce the overall generated binary size in the Arrow library. A convenience API GetBuildInfo allows querying the characteristics of the Arrow library. We encourage you to suggest any desired addition to the returned information. We added an optional dependency to the utf8proc library, used in several compute functions (see below). Instead of sharing the same concrete classes, sparse and dense unions now have separated classes (SparseUnionType and DenseUnionType, as well as SparseUnionArray, DenseUnionArray, SparseUnionScalar, DenseUnionScalar). Arrow can now be built for iOS using the right set of CMake options, though we don’t officially support it. See this writeup for details. Compute functions The compute kernel layer was extensively reworked. It now offers a generic function lookup, dispatch and execution mechanism. Furthermore, new internal scaffoldings make it vastly easier to write new function kernels, with many common details like type checking and function dispatch based on type combinations handled by the framework rather than implemented manually by the function developer. Around 30 new array compute functions have been added. For example, Unicode-compliant predicates and transforms, such as lowercase and uppercase transforms, are now available. The available compute functions are listed exhaustively in the Sphinx-generated documentation. Datasets Datasets can now be read from CSV files. Datasets can be expanded to their component fragments, enabling fine grained interoperability with other consumers of data files. Where applicable, metadata is available as a property of the fragment, including partition information and (for the parquet format) per-column statistics. Datasets of parquet files can now be assembled from a single _metadata file, such as those created by systems like Dask and Spark. _metadata contains the metadata of all fragments, allowing construction of a statistics- aware dataset with a single IO call. Feather The Feather format is now available in version 2, which is simply the Arrow IPC file format with another name. IPC By default, we now write IPC streams with metadata V5. However, metadata V4 can be requested by setting the appropriate member in IpcWriteOptions. V4 as well as V5 metadata IPC streams can be read properly, with one exception: a V4 metadata stream containing Union arrays with top-level null values will refuse reading. As noted above, there are no changes between V4 and V5 that break backwards compatibility. For forward compatibility scenarios (where you need to generate data to be read by an older Arrow library), you can set the V4 compatibility mode. Support for dictionary replacement and dictionary delta was implemented. Parquet Writing files with the LZ4 codec is disabled because it produces files incompatible with the widely-used Hadoop Parquet implementation. Support will be reenabled once we align the LZ4 implementation with the special buffer encoding expected by Hadoop. Java notes The Java package introduces a number of low level changes in this release. Most notable are the work in support of allocating large arrow buffers and removing Netty from the public API. Users will have to update their dependencies to use one of the two supported allocators Netty: arrow-memory-netty or Unsafe (internal java api for direct memory) arrow-memory-unsafe. The Java Vector implementation has improved its interoperability having verified LargeVarChar, LargeBinary, LargeList, Union, Extension types and duplicate field names in Structs are binary compatible with C++ and the specification. Python notes The size of wheel packages is significantly reduced, up to 75%. One side effect is that these wheels do not enable Gandiva anymore (which requires the LLVM runtime to be statically-linked). We are interested in providing Gandiva as an add-on package as a separate Python wheel in the future. The Scalar class hierarchy was reworked to more closely follow its C++ counterpart. TLS CA certificates are looked up more reliably when using the S3 filesystem, especially with manylinux wheels. The encoding of CSV files can now be specified explicitly, defaulting to UTF8. Custom timestamp parsers can now be used for CSV files. Filesystems can now be implemented in pure Python. As a result, fsspec-based filesystems can now be used in datasets. parquet.read_table is now backed by the dataset API by default, enabling filters on any column and more flexible partitioning. R notes The R package added support for converting to and from many additional Arrow types. Tables showing how R types are mapped to Arrow types and vice versa have been added to the introductory vignette, and nearly all types are handled. In addition, R attributes like custom classes and metadata are now preserved when converting a data.frame to an Arrow Table and are restored when loading them back into R. For more on what’s in the 1.0.0 R package, see the R changelog. Ruby and C GLib notes The Ruby and C GLib packages added support for the new compute function framework, in which users can find a compute function dynamically and call it. Users don’t need to wait for a C GLib binding for new compute functions: if the C++ package provides a new compute function, users can use it without additional code in the Ruby and C GLib packages. The Ruby and C GLib packages added support for Apache Arrow Dataset. The Ruby package provides a new gem for Apache Arrow Dataset, red-arrow-dataset. The C GLib package provides a new module for Apache Arrow Dataset, arrow-dataset-glib. They just have a few features for now but we will add more in future releases. The Ruby and C GLib packages added support for reading only the specified row group in an Apache Parquet file. Ruby The Ruby package added support for column level compression in writing Apache Parquet files. The Ruby package changed the Arrow::DictionaryArray#[] behavior. It now returns the dictionary value instead of the dictionary index. This is a backwards-incompatible change. Rust notes A new integration test crate has been added, allowing the Rust implementation to participate in integration testing. A new benchmark crate has been added for benchmarking performance against popular data sets. The initial examples run SQL queries against the NYC Taxi data set using DataFusion. This is useful for comparing performance against other Arrow implementations. Rust toolchain has been upgraded to 1.44 nightly. Arrow Core Support for binary, string, and list arrays with i64 offsets to support large lists. A new sort kernel has been added. There have been various improvements to dictionary array support. CSV reader enhancements include a new CsvReadOptions struct and support for schema inference from multiple CSV files. There are significant (10x - 40x) performance improvements to SIMD comparison kernels. DataFusion There are numerous UX improvements to LogicalPlan and LogicalPlanBuilder, including support for named columns. General improvements to code base, such as removing many uses of Arc and using slices instead of &amp;Vec as function arguments. ParquetScanExec performance improvement (almost 2x). ExecutionContext can now be shared between threads. Rust closures can now be used as Scalar UDFs. Sort support has been added to SQL and LogicalPlan.","publisher":{"@type":"Organization","logo":{"@type":"ImageObject","url":"https://arrow.apache.org/img/logo.png"},"name":"pmc"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://arrow.apache.org/blog/2020/07/24/1.0.0-release/"},"image":"https://arrow.apache.org/img/arrow.png","author":{"@type":"Person","name":"pmc"},"@type":"BlogPosting","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->


    <!-- favicons -->
    <link rel="icon" type="image/png" sizes="16x16" href="/img/favicon-16x16.png" id="light1">
    <link rel="icon" type="image/png" sizes="32x32" href="/img/favicon-32x32.png" id="light2">
    <link rel="apple-touch-icon" type="image/png" sizes="180x180" href="/img/apple-touch-icon.png" id="light3">
    <link rel="apple-touch-icon" type="image/png" sizes="120x120" href="/img/apple-touch-icon-120x120.png" id="light4">
    <link rel="apple-touch-icon" type="image/png" sizes="76x76" href="/img/apple-touch-icon-76x76.png" id="light5">
    <link rel="apple-touch-icon" type="image/png" sizes="60x60" href="/img/apple-touch-icon-60x60.png" id="light6">
    <!-- dark mode favicons -->
    <link rel="icon" type="image/png" sizes="16x16" href="/img/favicon-16x16-dark.png" id="dark1">
    <link rel="icon" type="image/png" sizes="32x32" href="/img/favicon-32x32-dark.png" id="dark2">
    <link rel="apple-touch-icon" type="image/png" sizes="180x180" href="/img/apple-touch-icon-dark.png" id="dark3">
    <link rel="apple-touch-icon" type="image/png" sizes="120x120" href="/img/apple-touch-icon-120x120-dark.png" id="dark4">
    <link rel="apple-touch-icon" type="image/png" sizes="76x76" href="/img/apple-touch-icon-76x76-dark.png" id="dark5">
    <link rel="apple-touch-icon" type="image/png" sizes="60x60" href="/img/apple-touch-icon-60x60-dark.png" id="dark6">

    <script>
      // Switch to the dark-mode favicons if prefers-color-scheme: dark
      function onUpdate() {
        light1 = document.querySelector('link#light1');
        light2 = document.querySelector('link#light2');
        light3 = document.querySelector('link#light3');
        light4 = document.querySelector('link#light4');
        light5 = document.querySelector('link#light5');
        light6 = document.querySelector('link#light6');

        dark1 = document.querySelector('link#dark1');
        dark2 = document.querySelector('link#dark2');
        dark3 = document.querySelector('link#dark3');
        dark4 = document.querySelector('link#dark4');
        dark5 = document.querySelector('link#dark5');
        dark6 = document.querySelector('link#dark6');

        if (matcher.matches) {
          light1.remove();
          light2.remove();
          light3.remove();
          light4.remove();
          light5.remove();
          light6.remove();
          document.head.append(dark1);
          document.head.append(dark2);
          document.head.append(dark3);
          document.head.append(dark4);
          document.head.append(dark5);
          document.head.append(dark6);
        } else {
          dark1.remove();
          dark2.remove();
          dark3.remove();
          dark4.remove();
          dark5.remove();
          dark6.remove();
          document.head.append(light1);
          document.head.append(light2);
          document.head.append(light3);
          document.head.append(light4);
          document.head.append(light5);
          document.head.append(light6);
        }
      }
      matcher = window.matchMedia('(prefers-color-scheme: dark)');
      matcher.addListener(onUpdate);
      onUpdate();
    </script>

    <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic,900">

    <link href="/css/main.css" rel="stylesheet">
    <link href="/css/syntax.css" rel="stylesheet">
    <script src="/javascript/main.js"></script>
    
    <!-- Global Site Tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-107500873-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments)};
  gtag('js', new Date());

  gtag('config', 'UA-107500873-1');
</script>

    
  </head>


<body class="wrap">
  <header>
    <nav class="navbar navbar-expand-md navbar-dark bg-dark">
  
  <a class="navbar-brand no-padding" href="/"><img src="/img/arrow-inverse-300px.png" height="40px"/></a>
  
   <button class="navbar-toggler ml-auto" type="button" data-toggle="collapse" data-target="#arrow-navbar" aria-controls="arrow-navbar" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>

    <!-- Collect the nav links, forms, and other content for toggling -->
    <div class="collapse navbar-collapse justify-content-end" id="arrow-navbar">
      <ul class="nav navbar-nav">
        <li class="nav-item"><a class="nav-link" href="/overview/" role="button" aria-haspopup="true" aria-expanded="false">Overview</a></li>
        <li class="nav-item"><a class="nav-link" href="/faq/" role="button" aria-haspopup="true" aria-expanded="false">FAQ</a></li>
        <li class="nav-item"><a class="nav-link" href="/blog" role="button" aria-haspopup="true" aria-expanded="false">Blog</a></li>
        <li class="nav-item dropdown">
          <a class="nav-link dropdown-toggle" href="#"
             id="navbarDropdownGetArrow" role="button" data-toggle="dropdown"
             aria-haspopup="true" aria-expanded="false">
             Get Arrow
          </a>
          <div class="dropdown-menu" aria-labelledby="navbarDropdownGetArrow">
            <a class="dropdown-item" href="/install/">Install</a>
            <a class="dropdown-item" href="/release/">Releases</a>
            <a class="dropdown-item" href="https://github.com/apache/arrow">Source Code</a>
          </div>
        </li>
        <li class="nav-item dropdown">
          <a class="nav-link dropdown-toggle" href="#"
             id="navbarDropdownDocumentation" role="button" data-toggle="dropdown"
             aria-haspopup="true" aria-expanded="false">
             Documentation
          </a>
          <div class="dropdown-menu" aria-labelledby="navbarDropdownDocumentation">
            <a class="dropdown-item" href="/docs">Project Docs</a>
            <a class="dropdown-item" href="/docs/format/Columnar.html">Format</a>
            <hr/>
            <a class="dropdown-item" href="/docs/c_glib">C GLib</a>
            <a class="dropdown-item" href="/docs/cpp">C++</a>
            <a class="dropdown-item" href="https://github.com/apache/arrow/blob/master/csharp/README.md">C#</a>
            <a class="dropdown-item" href="https://godoc.org/github.com/apache/arrow/go/arrow">Go</a>
            <a class="dropdown-item" href="/docs/java">Java</a>
            <a class="dropdown-item" href="/docs/js">JavaScript</a>
            <a class="dropdown-item" href="https://arrow.juliadata.org/stable/">Julia</a>
            <a class="dropdown-item" href="https://github.com/apache/arrow/blob/master/matlab/README.md">MATLAB</a>
            <a class="dropdown-item" href="/docs/python">Python</a>
            <a class="dropdown-item" href="/docs/r">R</a>
            <a class="dropdown-item" href="https://github.com/apache/arrow/blob/master/ruby/README.md">Ruby</a>
            <a class="dropdown-item" href="https://docs.rs/crate/arrow/">Rust</a>
          </div>
        </li>
        <li class="nav-item dropdown">
          <a class="nav-link dropdown-toggle" href="#"
             id="navbarDropdownCommunity" role="button" data-toggle="dropdown"
             aria-haspopup="true" aria-expanded="false">
             Community
          </a>
          <div class="dropdown-menu" aria-labelledby="navbarDropdownCommunity">
            <a class="dropdown-item" href="/community/">Communication</a>
            <a class="dropdown-item" href="/docs/developers/contributing.html">Contributing</a>
            <a class="dropdown-item" href="https://issues.apache.org/jira/browse/ARROW">Issue Tracker</a>
            <a class="dropdown-item" href="/committers/">Governance</a>
            <a class="dropdown-item" href="/use_cases/">Use Cases</a>
            <a class="dropdown-item" href="/powered_by/">Powered By</a>
            <a class="dropdown-item" href="/security/">Security</a>
            <a class="dropdown-item" href="https://www.apache.org/foundation/policies/conduct.html">Code of Conduct</a>
          </div>
        </li>
        <li class="nav-item dropdown">
          <a class="nav-link dropdown-toggle" href="#"
             id="navbarDropdownASF" role="button" data-toggle="dropdown"
             aria-haspopup="true" aria-expanded="false">
             ASF Links
          </a>
          <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdownASF">
            <a class="dropdown-item" href="http://www.apache.org/">ASF Website</a>
            <a class="dropdown-item" href="http://www.apache.org/licenses/">License</a>
            <a class="dropdown-item" href="http://www.apache.org/foundation/sponsorship.html">Donate</a>
            <a class="dropdown-item" href="http://www.apache.org/foundation/thanks.html">Thanks</a>
            <a class="dropdown-item" href="http://www.apache.org/security/">Security</a>
          </div>
        </li>
      </ul>
    </div><!-- /.navbar-collapse -->
  </nav>

  </header>

  <div class="container p-4 pt-5">
    <div class="col-md-8 mx-auto">
      <main role="main" class="pb-5">
        
<h1>
  Apache Arrow 1.0.0 Release
</h1>
<hr class="mt-4 mb-3">



<p class="mb-4 pb-1">
  <span class="badge badge-secondary">Published</span>
  <span class="published mr-3">
    24 Jul 2020
  </span>
  <br />
  <span class="badge badge-secondary">By</span>
  
    <a class="mr-3" href="https://arrow.apache.org">The Apache Arrow PMC (pmc) </a>
  

  
</p>


        <!--

-->

<p>The Apache Arrow team is pleased to announce the 1.0.0 release. This covers
over 3 months of development work and includes <a href="https://issues.apache.org/jira/issues/?jql=project%20%3D%20ARROW%20AND%20status%20%3D%20Resolved%20AND%20fixVersion%20%3D%201.0.0"><strong>810 resolved issues</strong></a> from
<a href="/release/1.0.0.html#contributors"><strong>100 distinct contributors</strong></a>. See the Install Page to learn how to get the
libraries for your platform.</p>

<p>Despite a “1.0.0” version, this is the 18th major release of Apache Arrow and
marks a transition to binary stability of the columnar format (which was
already informally backward-compatible going back to December 2017) and a
transition to Semantic Versioning for the Arrow software libraries.</p>

<p>The release notes below are not exhaustive and only expose selected highlights
of the release. Many other bugfixes and improvements have been made: we refer
you to the <a href="/release/1.0.0.html">complete changelog</a>.</p>

<h2 id="100-columnar-format-and-stability-guarantees">1.0.0 Columnar Format and Stability Guarantees</h2>

<p>The 1.0.0 release indicates that the Arrow columnar format is declared stable,
with <a href="/docs/format/Versioning.html">forward and backward compatibility guarantees</a>.</p>

<p>The Arrow columnar format received several recent changes and additions,
leading to the 1.0.0 format version:</p>

<ul>
  <li>
    <p>The metadata version was bumped to a new version V5, indicating an
incompatible change in the buffer layout of Union types. All
other types keep the same layout as in V4. V5 also includes format additions
to assist with forward compatibility (detecting unsupported changes sent by
future library versions). Libraries remain backward compatible with data
generated by all libraries back to 0.8.0 (December 2017) and the Java and C++
libraries are capable of generating V4-compatible messages (for sending data
to applications using 0.8.0 to 0.17.1).</p>
  </li>
  <li>
    <p>Dictionary indices are now allowed to be unsigned integers rather than only
signed integers. Using UInt64 is still discouraged because of
poor Java support.</p>
  </li>
  <li>
    <p>A “Feature” enum has been added to announce the use of specific optional
features in an IPC stream, such as buffer compression.  This
new field is not used by any implementation yet.</p>
  </li>
  <li>
    <p>Optional buffer compression using LZ4 or ZStandard was added to the IPC
format.</p>
  </li>
  <li>
    <p>Decimal types now have an optional “bitWidth” field, defaulting to 128.<br />
This will allow for future support of other decimal widths
such as 32- and 64-bit.</p>
  </li>
  <li>
    <p>The validity bitmap buffer has been removed from Union types. The nullity of
a slot in a Union array is determined exclusively by the constituent arrays
forming the union.</p>
  </li>
</ul>

<p>Integration testing has been expanded to test for extension types and
nested dictionaries. See the <a href="/docs/status.html">implementation matrix</a> for details.</p>

<h2 id="community">Community</h2>

<p>Since the last release, we have added two new committers:</p>

<ul>
  <li>Liya Fan</li>
  <li>Ji Liu</li>
</ul>

<p>Thank you for all your contributions!</p>

<h2 id="arrow-flight-rpc-notes">Arrow Flight RPC notes</h2>

<p>Flight now offers DoExchange, a fully bidirectional data endpoint, in addition
to DoGet and DoPut, in C++, Java, and Python. Middlewares in all languages now
expose binary-valued headers. Additionally, servers and clients can set Arrow
IPC read/write options in all languages, making compatibility easier with earlier
versions of Arrow Flight.</p>

<p>In C++ and Python, Flight now exposes more options from gRPC, including the
address of the client (on the server) and the ability to set low-level gRPC
client options. Flight also supports mutual TLS authentication and the ability
for a client to control the size of a data message on the wire.</p>

<h2 id="c-notes">C++ notes</h2>

<ul>
  <li>Support for static linking with Arrow has been vastly improved, including the
introduction of a <code class="language-plaintext highlighter-rouge">libarrow_bundled_dependencies.a</code> library bundling all
external dependencies that are built from source by Arrow’s build system
rather than installed by an external package manager. This makes
it significantly easier to create dependency-free applications with all
libraries statically-linked.</li>
  <li>Following the Arrow format changes, Union arrays cannot have a top-level
bitmap anymore.</li>
  <li>A number of improvements were made to reduce the overall generated binary
size in the Arrow library.</li>
  <li>A convenience API <code class="language-plaintext highlighter-rouge">GetBuildInfo</code> allows querying the characteristics of the
Arrow library.  We encourage you to suggest any desired addition to the
returned information.</li>
  <li>We added an optional dependency to the <code class="language-plaintext highlighter-rouge">utf8proc</code> library, used in several
compute functions (see below).</li>
  <li>Instead of sharing the same concrete classes, sparse and dense unions now
have separated classes (<code class="language-plaintext highlighter-rouge">SparseUnionType</code> and <code class="language-plaintext highlighter-rouge">DenseUnionType</code>, as well as
<code class="language-plaintext highlighter-rouge">SparseUnionArray</code>, <code class="language-plaintext highlighter-rouge">DenseUnionArray</code>, <code class="language-plaintext highlighter-rouge">SparseUnionScalar</code>,
<code class="language-plaintext highlighter-rouge">DenseUnionScalar</code>).</li>
  <li>Arrow can now be built for iOS using the right set of CMake options, though
we don’t officially support it.  See <a href="https://github.com/UnfoldedInc/deck.gl-native-dependencies/blob/master/docs/iOS-BUILD.md#arrow-v0170">this writeup</a> for details.</li>
</ul>

<h3 id="compute-functions">Compute functions</h3>

<p>The compute kernel layer was extensively reworked.  It now offers
a generic function lookup, dispatch and execution mechanism.  Furthermore, new
internal scaffoldings make it vastly easier to write new function kernels, with
many common details like type checking and function dispatch based on type
combinations handled by the framework rather than implemented manually by the
function developer.</p>

<p>Around 30 new array compute functions have been added. For example,
Unicode-compliant predicates and transforms, such as lowercase and uppercase
transforms, are now available.</p>

<p>The available compute functions are listed exhaustively in the Sphinx-generated
<a href="/docs/cpp/compute.html">documentation</a>.</p>

<h3 id="datasets">Datasets</h3>

<p>Datasets can now be read from CSV files.</p>

<p>Datasets can be expanded to their component fragments, enabling fine grained
interoperability with other consumers of data files. Where applicable, metadata
is available as a property of the fragment, including partition information and
(for the parquet format) per-column statistics.</p>

<p>Datasets of parquet files can now be assembled from a single <code class="language-plaintext highlighter-rouge">_metadata</code> file,
such as those created by systems like Dask and Spark. <code class="language-plaintext highlighter-rouge">_metadata</code>
contains the metadata of all fragments, allowing construction of a statistics-
aware dataset with a single IO call.</p>

<h3 id="feather">Feather</h3>

<p>The Feather format is now available in version 2, which is simply the Arrow
IPC file format with another name.</p>

<h3 id="ipc">IPC</h3>

<p>By default, we now write IPC streams with metadata V5.  However, metadata V4
can be requested by setting the appropriate member in <code class="language-plaintext highlighter-rouge">IpcWriteOptions</code>. V4 as
well as V5 metadata IPC streams can be read properly, with one exception: a V4
metadata stream containing Union arrays with top-level null values will refuse
reading.</p>

<p>As noted above, there are no changes between V4 and V5 that break
backwards compatibility. For forward compatibility scenarios (where you need to
generate data to be read by an older Arrow library), you can set the V4
compatibility mode.</p>

<p>Support for dictionary replacement and dictionary delta was implemented.</p>

<h3 id="parquet">Parquet</h3>

<p>Writing files with the LZ4 codec is disabled because it produces files
incompatible with the widely-used Hadoop Parquet implementation.  Support will
be reenabled once we align the LZ4 implementation with the special buffer
encoding expected by Hadoop.</p>

<h2 id="java-notes">Java notes</h2>

<p>The Java package introduces a number of low level changes in this release.
Most notable are the work in support of allocating large arrow buffers and
removing Netty from the public API. Users will have to update their
dependencies to use one of the two supported allocators Netty:
<code class="language-plaintext highlighter-rouge">arrow-memory-netty</code> or Unsafe (internal java api for direct memory)
<code class="language-plaintext highlighter-rouge">arrow-memory-unsafe</code>.</p>

<p>The Java Vector implementation has improved its interoperability having
verified <code class="language-plaintext highlighter-rouge">LargeVarChar</code>, <code class="language-plaintext highlighter-rouge">LargeBinary</code>, <code class="language-plaintext highlighter-rouge">LargeList</code>, <code class="language-plaintext highlighter-rouge">Union</code>, Extension types
and duplicate field names in <code class="language-plaintext highlighter-rouge">Structs</code> are binary compatible with C++ and the
specification.</p>

<h2 id="python-notes">Python notes</h2>

<p>The size of wheel packages is significantly reduced, up to 75%.  One side
effect is that these wheels do not enable Gandiva anymore (which requires the
LLVM runtime to be statically-linked). We are interested in providing Gandiva
as an add-on package as a separate Python wheel in the future.</p>

<p>The Scalar class hierarchy was reworked to more closely follow its C++
counterpart.</p>

<p>TLS CA certificates are looked up more reliably when using the S3 filesystem,
especially with manylinux wheels.</p>

<p>The encoding of CSV files can now be specified explicitly, defaulting to UTF8.
Custom timestamp parsers can now be used for CSV files.</p>

<p>Filesystems can now be implemented in pure Python.  As a result,
<a href="https://filesystem-spec.readthedocs.io">fsspec</a>-based filesystems can now
be used in datasets.</p>

<p><code class="language-plaintext highlighter-rouge">parquet.read_table</code> is now backed by the dataset API by default, enabling
filters on any column and more flexible partitioning.</p>

<h2 id="r-notes">R notes</h2>

<p>The R package added support for converting to and from many additional Arrow
types. Tables showing how R types are mapped to Arrow types and vice versa have
been added to the <a href="/docs/r/articles/arrow.html">introductory vignette</a>, and nearly all types are handled.
In addition, R <code class="language-plaintext highlighter-rouge">attributes</code> like custom classes and metadata are now preserved
when converting a <code class="language-plaintext highlighter-rouge">data.frame</code> to an Arrow Table and are restored when loading
them back into R.</p>

<p>For more on what’s in the 1.0.0 R package, see the <a href="/docs/r/news/">R changelog</a>.</p>

<h2 id="ruby-and-c-glib-notes">Ruby and C GLib notes</h2>

<p>The Ruby and C GLib packages added support for the new compute function
framework, in which users can find a
compute function dynamically and call it. Users don’t need to wait for a C
GLib binding for new compute functions: if the C++ package provides a
new compute function, users can use it
without additional code in the Ruby and C GLib packages.</p>

<p>The Ruby and C GLib packages added support for Apache Arrow
Dataset. The Ruby package provides a new gem for Apache Arrow Dataset,
<code class="language-plaintext highlighter-rouge">red-arrow-dataset</code>. The C GLib package provides a new module for
Apache Arrow Dataset, <code class="language-plaintext highlighter-rouge">arrow-dataset-glib</code>. They just have a few
features for now but we will add more in future releases.</p>

<p>The Ruby and C GLib packages added support for reading only the
specified row group in an Apache Parquet file.</p>

<h3 id="ruby">Ruby</h3>

<p>The Ruby package added support for column level compression in writing
Apache Parquet files.</p>

<p>The Ruby package changed the <code class="language-plaintext highlighter-rouge">Arrow::DictionaryArray#[]</code> behavior. It now
returns the dictionary value instead of the dictionary index. This is a
backwards-incompatible change.</p>

<h2 id="rust-notes">Rust notes</h2>

<ul>
  <li>A new integration test crate has been added, allowing the Rust
implementation to participate in integration testing.</li>
  <li>A new benchmark crate has been added for benchmarking performance
against popular data sets. The initial examples run SQL queries against
the NYC Taxi data set using DataFusion. This is useful for comparing
performance against other Arrow implementations.</li>
  <li>Rust toolchain has been upgraded to 1.44 nightly.</li>
</ul>

<h3 id="arrow-core">Arrow Core</h3>

<ul>
  <li>Support for binary, string, and list arrays with i64 offsets to support
large lists.</li>
  <li>A new sort kernel has been added.</li>
  <li>There have been various improvements to dictionary array support.</li>
  <li>CSV reader enhancements include a new CsvReadOptions struct and support
for schema inference from multiple CSV files.</li>
  <li>There are significant (10x - 40x) performance improvements to SIMD
comparison kernels.</li>
</ul>

<h3 id="datafusion">DataFusion</h3>

<ul>
  <li>There are numerous UX improvements to LogicalPlan and LogicalPlanBuilder,
including support for named columns.</li>
  <li>General improvements to code base, such as removing many uses of <code class="language-plaintext highlighter-rouge">Arc</code>
and using slices instead of <code class="language-plaintext highlighter-rouge">&amp;Vec</code> as function arguments.</li>
  <li>ParquetScanExec performance improvement (almost 2x).</li>
  <li>ExecutionContext can now be shared between threads.</li>
  <li>Rust closures can now be used as Scalar UDFs.</li>
  <li>Sort support has been added to SQL and LogicalPlan.</li>
</ul>


      </main>
    </div>

    <hr/>
<footer class="footer">
  <div class="row">
    <div class="col-md-9">
      <p>Apache Arrow, Arrow, Apache, the Apache feather logo, and the Apache Arrow project logo are either registered trademarks or trademarks of The Apache Software Foundation in the United States and other countries.</p>
      <p>&copy; 2016-2021 The Apache Software Foundation</p>
    </div>
    <div class="col-md-3">
      <a class="d-sm-none d-md-inline pr-2" href="https://www.apache.org/events/current-event.html">
        <img src="https://www.apache.org/events/current-event-234x60.png"/>
      </a>
    </div>
  </div>
</footer>

  </div>
</body>
</html>
