<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- The above meta tags *must* come first in the head; any other head content must come *after* these tags -->
    
    <title>Fast Streaming Inserts in DuckDB with ADBC | Apache Arrow</title>
    

    <!-- Begin Jekyll SEO tag v2.8.0 -->
<meta name="generator" content="Jekyll v4.4.1" />
<meta property="og:title" content="Fast Streaming Inserts in DuckDB with ADBC" />
<meta name="author" content="loicalleyne" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="ADBC enables high throughput insertion into DuckDB" />
<meta property="og:description" content="ADBC enables high throughput insertion into DuckDB" />
<link rel="canonical" href="https://arrow.apache.org/blog/2025/03/10/fast-streaming-inserts-in-duckdb-with-adbc/" />
<meta property="og:url" content="https://arrow.apache.org/blog/2025/03/10/fast-streaming-inserts-in-duckdb-with-adbc/" />
<meta property="og:site_name" content="Apache Arrow" />
<meta property="og:image" content="https://arrow.apache.org/img/adbc-duckdb/adbc-duckdb.png" />
<meta property="og:image:height" content="560" />
<meta property="og:image:width" content="1200" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2025-03-10T00:00:00-04:00" />
<meta name="twitter:card" content="summary_large_image" />
<meta property="twitter:image" content="https://arrow.apache.org/img/adbc-duckdb/adbc-duckdb.png" />
<meta property="twitter:title" content="Fast Streaming Inserts in DuckDB with ADBC" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"loicalleyne"},"dateModified":"2025-03-10T00:00:00-04:00","datePublished":"2025-03-10T00:00:00-04:00","description":"ADBC enables high throughput insertion into DuckDB","headline":"Fast Streaming Inserts in DuckDB with ADBC","image":{"height":560,"width":1200,"url":"https://arrow.apache.org/img/adbc-duckdb/adbc-duckdb.png","@type":"imageObject"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://arrow.apache.org/blog/2025/03/10/fast-streaming-inserts-in-duckdb-with-adbc/"},"publisher":{"@type":"Organization","logo":{"@type":"ImageObject","url":"https://arrow.apache.org/img/logo.png"},"name":"loicalleyne"},"url":"https://arrow.apache.org/blog/2025/03/10/fast-streaming-inserts-in-duckdb-with-adbc/"}</script>
<!-- End Jekyll SEO tag -->


    <!-- favicons -->
    <link rel="icon" type="image/png" sizes="16x16" href="/img/favicon-16x16.png" id="light1">
    <link rel="icon" type="image/png" sizes="32x32" href="/img/favicon-32x32.png" id="light2">
    <link rel="apple-touch-icon" type="image/png" sizes="180x180" href="/img/apple-touch-icon.png" id="light3">
    <link rel="apple-touch-icon" type="image/png" sizes="120x120" href="/img/apple-touch-icon-120x120.png" id="light4">
    <link rel="apple-touch-icon" type="image/png" sizes="76x76" href="/img/apple-touch-icon-76x76.png" id="light5">
    <link rel="apple-touch-icon" type="image/png" sizes="60x60" href="/img/apple-touch-icon-60x60.png" id="light6">
    <!-- dark mode favicons -->
    <link rel="icon" type="image/png" sizes="16x16" href="/img/favicon-16x16-dark.png" id="dark1">
    <link rel="icon" type="image/png" sizes="32x32" href="/img/favicon-32x32-dark.png" id="dark2">
    <link rel="apple-touch-icon" type="image/png" sizes="180x180" href="/img/apple-touch-icon-dark.png" id="dark3">
    <link rel="apple-touch-icon" type="image/png" sizes="120x120" href="/img/apple-touch-icon-120x120-dark.png" id="dark4">
    <link rel="apple-touch-icon" type="image/png" sizes="76x76" href="/img/apple-touch-icon-76x76-dark.png" id="dark5">
    <link rel="apple-touch-icon" type="image/png" sizes="60x60" href="/img/apple-touch-icon-60x60-dark.png" id="dark6">

    <script>
      // Switch to the dark-mode favicons if prefers-color-scheme: dark
      function onUpdate() {
        light1 = document.querySelector('link#light1');
        light2 = document.querySelector('link#light2');
        light3 = document.querySelector('link#light3');
        light4 = document.querySelector('link#light4');
        light5 = document.querySelector('link#light5');
        light6 = document.querySelector('link#light6');

        dark1 = document.querySelector('link#dark1');
        dark2 = document.querySelector('link#dark2');
        dark3 = document.querySelector('link#dark3');
        dark4 = document.querySelector('link#dark4');
        dark5 = document.querySelector('link#dark5');
        dark6 = document.querySelector('link#dark6');

        if (matcher.matches) {
          light1.remove();
          light2.remove();
          light3.remove();
          light4.remove();
          light5.remove();
          light6.remove();
          document.head.append(dark1);
          document.head.append(dark2);
          document.head.append(dark3);
          document.head.append(dark4);
          document.head.append(dark5);
          document.head.append(dark6);
        } else {
          dark1.remove();
          dark2.remove();
          dark3.remove();
          dark4.remove();
          dark5.remove();
          dark6.remove();
          document.head.append(light1);
          document.head.append(light2);
          document.head.append(light3);
          document.head.append(light4);
          document.head.append(light5);
          document.head.append(light6);
        }
      }
      matcher = window.matchMedia('(prefers-color-scheme: dark)');
      matcher.addListener(onUpdate);
      onUpdate();
    </script>

    <link href="/css/main.css" rel="stylesheet">
    <link href="/css/syntax.css" rel="stylesheet">
    <script src="/javascript/main.js"></script>
    
    <!-- Matomo -->
<script>
  var _paq = window._paq = window._paq || [];
  /* tracker methods like "setCustomDimension" should be called before "trackPageView" */
  /* We explicitly disable cookie tracking to avoid privacy issues */
  _paq.push(['disableCookies']);
  _paq.push(['trackPageView']);
  _paq.push(['enableLinkTracking']);
  (function() {
    var u="https://analytics.apache.org/";
    _paq.push(['setTrackerUrl', u+'matomo.php']);
    _paq.push(['setSiteId', '20']);
    var d=document, g=d.createElement('script'), s=d.getElementsByTagName('script')[0];
    g.async=true; g.src=u+'matomo.js'; s.parentNode.insertBefore(g,s);
  })();
</script>
<!-- End Matomo Code -->

    
    <link type="application/atom+xml" rel="alternate" href="https://arrow.apache.org/feed.xml" title="Apache Arrow" />
  </head>


<body class="wrap">
  <header>
    <nav class="navbar navbar-expand-md navbar-dark bg-dark">
  
  <a class="navbar-brand no-padding" href="/"><img src="/img/arrow-inverse-300px.png" height="40px"/></a>
  
   <button class="navbar-toggler ml-auto" type="button" data-toggle="collapse" data-target="#arrow-navbar" aria-controls="arrow-navbar" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>

    <!-- Collect the nav links, forms, and other content for toggling -->
    <div class="collapse navbar-collapse justify-content-end" id="arrow-navbar">
      <ul class="nav navbar-nav">
        <li class="nav-item"><a class="nav-link" href="/overview/" role="button" aria-haspopup="true" aria-expanded="false">Overview</a></li>
        <li class="nav-item"><a class="nav-link" href="/faq/" role="button" aria-haspopup="true" aria-expanded="false">FAQ</a></li>
        <li class="nav-item"><a class="nav-link" href="/blog" role="button" aria-haspopup="true" aria-expanded="false">Blog</a></li>
        <li class="nav-item dropdown">
          <a class="nav-link dropdown-toggle" href="#"
             id="navbarDropdownGetArrow" role="button" data-toggle="dropdown"
             aria-haspopup="true" aria-expanded="false">
             Get Arrow
          </a>
          <div class="dropdown-menu" aria-labelledby="navbarDropdownGetArrow">
            <a class="dropdown-item" href="/install/">Install</a>
            <a class="dropdown-item" href="/release/">Releases</a>
          </div>
        </li>
        <li class="nav-item dropdown">
          <a class="nav-link dropdown-toggle" href="#"
             id="navbarDropdownDocumentation" role="button" data-toggle="dropdown"
             aria-haspopup="true" aria-expanded="false">
             Docs
          </a>
          <div class="dropdown-menu" aria-labelledby="navbarDropdownDocumentation">
            <a class="dropdown-item" href="/docs">Project Docs</a>
            <a class="dropdown-item" href="/docs/format/Columnar.html">Format</a>
            <hr/>
            <a class="dropdown-item" href="/docs/c_glib">C GLib</a>
            <a class="dropdown-item" href="/docs/cpp">C++</a>
            <a class="dropdown-item" href="https://github.com/apache/arrow/blob/main/csharp/README.md">C#</a>
            <a class="dropdown-item" href="https://godoc.org/github.com/apache/arrow/go/arrow">Go</a>
            <a class="dropdown-item" href="/docs/java">Java</a>
            <a class="dropdown-item" href="/docs/js">JavaScript</a>
            <a class="dropdown-item" href="/julia/">Julia</a>
            <a class="dropdown-item" href="https://github.com/apache/arrow/blob/main/matlab/README.md">MATLAB</a>
            <a class="dropdown-item" href="/docs/python">Python</a>
            <a class="dropdown-item" href="/docs/r">R</a>
            <a class="dropdown-item" href="https://github.com/apache/arrow/blob/main/ruby/README.md">Ruby</a>
            <a class="dropdown-item" href="https://docs.rs/arrow/latest">Rust</a>
            <a class="dropdown-item" href="/swift">Swift</a>
          </div>
        </li>
        <li class="nav-item dropdown">
          <a class="nav-link dropdown-toggle" href="#"
             id="navbarDropdownSource" role="button" data-toggle="dropdown"
             aria-haspopup="true" aria-expanded="false">
             Source
          </a>
          <div class="dropdown-menu" aria-labelledby="navbarDropdownSource">
            <a class="dropdown-item" href="https://github.com/apache/arrow">Main Repo</a>
            <hr/>
            <a class="dropdown-item" href="https://github.com/apache/arrow/tree/main/c_glib">C GLib</a>
            <a class="dropdown-item" href="https://github.com/apache/arrow/tree/main/cpp">C++</a>
            <a class="dropdown-item" href="https://github.com/apache/arrow/tree/main/csharp">C#</a>
            <a class="dropdown-item" href="https://github.com/apache/arrow-go">Go</a>
            <a class="dropdown-item" href="https://github.com/apache/arrow-java">Java</a>
            <a class="dropdown-item" href="https://github.com/apache/arrow-js">JavaScript</a>
            <a class="dropdown-item" href="https://github.com/apache/arrow-julia">Julia</a>
            <a class="dropdown-item" href="https://github.com/apache/arrow/tree/main/matlab">MATLAB</a>
            <a class="dropdown-item" href="https://github.com/apache/arrow/tree/main/python">Python</a>
            <a class="dropdown-item" href="https://github.com/apache/arrow/tree/main/r">R</a>
            <a class="dropdown-item" href="https://github.com/apache/arrow/tree/main/ruby">Ruby</a>
            <a class="dropdown-item" href="https://github.com/apache/arrow-rs">Rust</a>
            <a class="dropdown-item" href="https://github.com/apache/arrow-swift">Swift</a>
          </div>
        </li>
        <li class="nav-item dropdown">
          <a class="nav-link dropdown-toggle" href="#"
             id="navbarDropdownSubprojects" role="button" data-toggle="dropdown"
             aria-haspopup="true" aria-expanded="false">
             Subprojects
          </a>
          <div class="dropdown-menu" aria-labelledby="navbarDropdownSubprojects">
            <a class="dropdown-item" href="/adbc">ADBC</a>
            <a class="dropdown-item" href="/docs/format/Flight.html">Arrow Flight</a>
            <a class="dropdown-item" href="/docs/format/FlightSql.html">Arrow Flight SQL</a>
            <a class="dropdown-item" href="https://datafusion.apache.org">DataFusion</a>
            <a class="dropdown-item" href="/nanoarrow">nanoarrow</a>
          </div>
        </li>
        <li class="nav-item dropdown">
          <a class="nav-link dropdown-toggle" href="#"
             id="navbarDropdownCommunity" role="button" data-toggle="dropdown"
             aria-haspopup="true" aria-expanded="false">
             Community
          </a>
          <div class="dropdown-menu" aria-labelledby="navbarDropdownCommunity">
            <a class="dropdown-item" href="/community/">Communication</a>
            <a class="dropdown-item" href="/docs/developers/index.html">Contributing</a>
            <a class="dropdown-item" href="https://github.com/apache/arrow/issues">Issue Tracker</a>
            <a class="dropdown-item" href="/committers/">Governance</a>
            <a class="dropdown-item" href="/use_cases/">Use Cases</a>
            <a class="dropdown-item" href="/powered_by/">Powered By</a>
            <a class="dropdown-item" href="/visual_identity/">Visual Identity</a>
            <a class="dropdown-item" href="/security/">Security</a>
            <a class="dropdown-item" href="https://www.apache.org/foundation/policies/conduct.html">Code of Conduct</a>
          </div>
        </li>
        <li class="nav-item dropdown">
          <a class="nav-link dropdown-toggle" href="#"
             id="navbarDropdownASF" role="button" data-toggle="dropdown"
             aria-haspopup="true" aria-expanded="false">
             ASF Links
          </a>
          <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdownASF">
            <a class="dropdown-item" href="https://www.apache.org/">ASF Website</a>
            <a class="dropdown-item" href="https://www.apache.org/licenses/">License</a>
            <a class="dropdown-item" href="https://www.apache.org/foundation/sponsorship.html">Donate</a>
            <a class="dropdown-item" href="https://www.apache.org/foundation/thanks.html">Thanks</a>
            <a class="dropdown-item" href="https://www.apache.org/security/">Security</a>
          </div>
        </li>
      </ul>
    </div><!-- /.navbar-collapse -->
  </nav>

  </header>

  <div class="container p-4 pt-5">
    <div class="col-md-8 mx-auto">
      <main role="main" class="pb-5">
        
<h1>
  Fast Streaming Inserts in DuckDB with ADBC
</h1>
<hr class="mt-4 mb-3">



<p class="mb-4 pb-1">
  <span class="badge badge-secondary">Published</span>
  <span class="published mr-3">
    10 Mar 2025
  </span>
  <br />
  <span class="badge badge-secondary">By</span>
  
    <a class="mr-3" href="https://github.com/loicalleyne">Loïc Alleyne (loicalleyne) </a>
  

  
</p>


        <!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>DuckDB is rapidly becoming an essential part of data practitioners' toolbox, finding use cases in data engineering, machine learning, and local analytics. In many cases DuckDB has been used to query and process data that has already been saved to storage (file-based or external database) by another process. Arrow Database Connectivity APIs enable high-throughput data processing using DuckDB as the engine.</p>
<h1>How it started</h1>
<p>The company I work for is the leading digital out-of-home marketing platform, including a programmatic ad tech stack. For several years, my technical operations team was making use of logs emitted by the real-time programmatic auction system in the <a href="http://avro.apache.org/">Apache Avro</a> format. Over time we've built an entire operations and analytics back end using this data. Avro files are row-based which is less than ideal for analytics at scale, in fact it's downright painful. So much so that I developed and contributed an Avro reader feature to the <a href="https://github.com/apache/arrow-go">Apache Arrow  Go</a> library to be able to convert Avro files to parquet. This data pipeline is now humming along transforming hundreds of GB/day from Avro to Parquet.</p>
<p>Since &quot;any problem in computer science can be solved with another layer of indirection&quot;, the original system has grown layers (like an onion) and started to emit other logs, this time in <a href="https://parquet.apache.org/">Apache Parquet</a> format...</p>
<!-- raw HTML omitted -->
<ul>
<li>the new onion-layer (ahem...system component) sends Protobuf encoded messages to Kafka topics</li>
<li>a Kafka Connect cluster with the S3 sink connector consumes topics and saves the parquet files to object storage</li>
</ul>
<p>Due to the firehose of data, the cluster size over time grew to &gt; 25 nodes and was producing thousands of small Parquet files (13 MB or smaller) an hour. This led to ever-increasing query latency, in some cases breaking our tools due to query timeouts (aka <a href="https://www.dremio.com/blog/compaction-in-apache-iceberg-fine-tuning-your-iceberg-tables-data-files/#h-the-small-files-problem">the small files problem</a>). Not to mention that running aggregations on the raw data in our data warehouse wasn't fast or cheap.</p>
<h1>DuckDB to the rescue... I think</h1>
<p>I'd used DuckDB to process and analyse Parquet data so I knew it could do that very quickly. Then I came across this post on LinkedIn (<a href="https://www.linkedin.com/posts/shubham-dhal-349626ba_real-time-analytics-with-kafka-and-duckdb-activity-7258424841538555904-xfU6">Real-Time Analytics using Kafka and DuckDB</a>), where someone has built a system for near-realtime analytics in Go using DuckDB.</p>
<p>The slides listed DuckDB's limitations:<br />
<!-- raw HTML omitted -->
The poster's solution batches data at the application layer managing to scale up ingestion 100x to ~20k inserts/second, noting that they thought that using the DuckDB Appender API could possibly increase this 10x. So, potentially ~200k inserts/second. Yayyyyy...</p>
<!-- raw HTML omitted -->
<p>Then I noticed the data schema in the slides was flat and had only 4 fields (vs. <a href="https://github.com/InteractiveAdvertisingBureau/openrtb2.x/blob/main/2.6.md#31---object-model-">OpenRTB</a> schema with deeply nested Lists and Structs); and then looked at our monitoring dashboards whereupon I realized that at peak our system was emitting &gt;250k events/second. [cue sad trombone]</p>
<p>Undeterred (and not particularly enamored with the idea of setting up/running/maintaining a Spark cluster), I suspected that Apache Arrow's columnar memory representation might still make DuckDB viable since it has an Arrow API; getting Parquet files would be as easy as running <code>COPY...TO (format parquet)</code>.</p>
<p>Using a pattern found in a Github issue, I wrote a POC using <a href="http://github.com/marcboeker/go-duckdb">github.com/marcboeker/go-duckdb</a> to connect to a DB, retrieve an Arrow, create an Arrow Reader, register a view on the reader, then run an INSERT statement from the view.</p>
<p>This felt a bit like a rabbit pulling itself out of a hat, but no matter, it managed between ~74k and ~110k rows/sec on my laptop.</p>
<p>To make sure this was really the right solution, I also tried out DuckDB's Appender API (at time of writing the official recommendation for fast inserts) and managed... ~63k rows/sec on my laptop. OK, but... meh.</p>
<h1>A new hope</h1>
<p>In a discussion on the Gopher Slack, Matthew Topol aka <a href="https://github.com/zeroshade">zeroshade</a> suggested using <a href="http://arrow.apache.org/adbc">ADBC</a> with its much simpler API. Who is Matt Topol you ask? Just the guy who <em>literally</em> wrote the book on Apache Arrow, that's who (<a href="https://www.packtpub.com/en-us/product/in-memory-analytics-with-apache-arrow-9781835461228"><em><strong>In-Memory Analytics with Apache Arrow: Accelerate data analytics for efficient processing of flat and hierarchical data structures 2nd Edition</strong></em></a>). It's an excellent resource and guide for working with Arrow.<br />
BTW, should you prefer an acronym to remember the name of the book, it's <em><strong>IMAAA:ADAFEPOFAHDS2E</strong></em>.<br />
<!-- raw HTML omitted --><br />
But I digress. Matt is also a member of the Apache Arrow PMC, a major contributor to the Go implementation of Apache Iceberg and generally a nice, helpful guy.</p>
<h1>ADBC</h1>
<p>ADBC is:</p>
<ul>
<li>
<p>A set of <a href="https://arrow.apache.org/adbc/current/format/specification.html">abstract APIs</a> in different languages (C/C++, Go, and Java, with more on the way) for working with databases and Arrow data.</p>
<p>For example, result sets of queries in ADBC are all returned as streams of Arrow data, not row-by-row.</p>
</li>
<li>
<p>A set of implementations of that API in different languages (C/C++, C#/.NET, Go, Java, Python, and Ruby) that target different databases (e.g. PostgreSQL, SQLite, DuckDB, any database supporting Flight SQL).</p>
</li>
</ul>
<p>Going back to the drawing board, I created <a href="https://github.com/loicalleyne/quacfka">Quacfka</a>, a Go library built using ADBC and split out my system into 3 worker pools, connected by channels:</p>
<ul>
<li>Kafka clients consuming topic messages and writing the bytes to a message channel</li>
<li>Processing routines using the <a href="https://github.com/loicalleyne/bufarrow">Bufarrow</a> library to deserialize Protobuf data and append it to Arrow arrays, writing Arrow Records to a record channel</li>
<li>DuckDB inserters binding the Arrow Records to ADBC statements and executing insertions</li>
</ul>
<p>I first ran these in series to determine how fast each could run:</p>
<!-- raw HTML omitted -->
<p>With this architecture decided, I then started running the workers concurrently, instrumenting the system, profiling my code to identify performance issues and tweaking the settings to maximize throughput. It seemed to me that there was enough performance headroom to allow for in-flight aggregations.</p>
<p>One issue: Despite DuckDB's excellent <a href="https://duckdb.org/2022/10/28/lightweight-compression.html">lightweight compression</a>, inserts from this source were making the file size increase at a rate of <em><strong>~8GB/minute</strong></em>. Putting inserts on hold to export the Parquet files and release the storage would reduce the overall throughput to an unacceptable level. I decided to implement a rotation of database files based on a file size threshold.</p>
<p>DuckDB being able to query Hive partitioned parquet on disk or in object storage, the analytics part could be decoupled from the data ingestion pipeline by running a separate querying server pointing at wherever the parquet files would end up.</p>
<p>Iterating, I created several APIs to try to make in-flight aggregations efficient enough to keep the overall throughput above my 250k rows/second target.</p>
<p>The first two either ran into issues of data locality or weren't optimized enough:</p>
<ul>
<li><strong>CustomArrows</strong> : functions to run on each Arrow Record to create a new Record to insert along with the original</li>
<li><strong>DuckRunner</strong> : run a series of queries on the database file before rotation</li>
</ul>
<p>Reasoning that if unnesting deeply nested data in Arrow Record arrays was causing data locality issues:</p>
<ul>
<li><strong>Normalizer</strong>: a Bufarrow API used in the in the deserialization function to normalize the message data and append it to another Arrow Record, inserted into a separate table</li>
</ul>
<p>This approach allowed throughput to go back to levels almost as high as without Normalizer - flat data is much faster to process and insert.</p>
<h1>Oh, we're halfway there...livin' on a prayer</h1>
<p>Next, I tried opening concurrent connections to multiple databases. <strong>BAM!</strong> <em><strong>Segfault</strong></em>. DuckDB concurrency model isn't <a href="https://duckdb.org/docs/stable/connect/concurrency.html#handling-concurrency">designed</a> that way. From within a process only a single database (in-memory or file) can be opened, then other database files can be <a href="https://duckdb.org/docs/stable/sql/statements/attach.html">attached</a> to the central db's catalog.</p>
<p>Having already decided to rotate DB files, I decided to make a separate program (<a href="https://github.com/loicalleyne/quacfka-runner">Runner</a>) to process the database files as they were rotated, running aggregations on normalized data and table dumps to parquet. This meant setting up an RPC connection between the two and figuring out a backpressure mechanism to avoid <code>disk full</code> events.</p>
<p>However having the two running simultaneously was causing memory pressure issues, not to mention massively slowing down the throughput. Upgrading the VM to one with more vCPUs and memory only helped a little, there was clearly some resource contention going on.</p>
<p>Since Go 1.5, the default <code>GOMAXPROCS</code> value is the number of CPU cores available. What if this was reduced to &quot;sandbox&quot; the ingestion process, along with setting the DuckDB thread count in the Runner? This actually worked so well, it increased the overall throughput. <a href="https://github.com/loicalleyne/quacfka-runner">Runner</a> runs the <code>COPY...TO...parquet</code> queries, walks the parquet output folder, uploads files to object storage and deletes the uploaded files. Balancing the DuckDB file rotation size threshold in <a href="https://github.com/loicalleyne/quacfka-service">Quafka-Service</a> allows Runner to keep up and avoid a backlog of DB files on disk.</p>
<h1>Results</h1>
<!-- raw HTML omitted -->
<p>Ingesting the raw data (14 fields with one deeply nested LIST.STRUCT.LIST field) + normalized data:</p>
<!-- raw HTML omitted -->
<p>How many rows/second could we get if we only inserted the flat, normalized data? (Note: original records are still processed, just not inserted):</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>Once deployed, the number of parquet files fell from ~3000 small files per hour to &lt; 20 files per hour. Goodbye small files!</p>
<!-- raw HTML omitted -->
<h1>Challenges/Learnings</h1>
<ul>
<li>DuckDB insertions are the bottleneck; network speed, Protobuf deserialization, <strong>building Arrow Records are not</strong>.</li>
<li>For fastest insertion into DuckDB, Arrow Record Batches should contain at least 122880 rows (to align with DuckDB storage row group size).</li>
<li>DuckDB won't let you open more than one database at once within the same process (results in a segfault). DuckDB is designed to run only once in a process, with a central database's catalog having the ability to add connections to other databases.
<ul>
<li>Workarounds:
<ul>
<li>Use separate processes for writing and reading multiple database files.</li>
<li>Open a single DuckDB database and use <a href="https://duckdb.org/docs/stable/sql/statements/attach.html">ATTACH</a> to attach other DB files.</li>
</ul>
</li>
</ul>
</li>
<li>Flat data is much, much faster to insert than nested data.</li>
</ul>
<!-- raw HTML omitted -->
<p>ADBC provides DuckDB with a truly high-throughput data ingestion API, unlocking a slew of use cases for using DuckDB with streaming data, making this an ever more useful tool for data practitioners.</p>

      </main>
    </div>

    <hr/>
<footer class="footer">
  <div class="row">
    <div class="col-md-9">
      <p>Apache Arrow, Arrow, Apache, the Apache feather logo, and the Apache Arrow project logo are either registered trademarks or trademarks of The Apache Software Foundation in the United States and other countries.</p>
      <p>&copy; 2016-2025 The Apache Software Foundation</p>
    </div>
    <div class="col-md-3">
      <a class="d-sm-none d-md-inline pr-2" href="https://www.apache.org/events/current-event.html">
        <img src="https://www.apache.org/events/current-event-234x60.png"/>
      </a>
    </div>
  </div>
</footer>

  </div>
</body>
</html>
