<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="UTF-8">
    <title>Apache Arrow Homepage</title>
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="generator" content="Jekyll v3.8.4">
    <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->
    <link rel="icon" type="image/x-icon" href="/favicon.ico">

    <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic,900">

    <link href="/css/main.css" rel="stylesheet">
    <link href="/css/syntax.css" rel="stylesheet">
    <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.3/umd/popper.min.js" integrity="sha384-ZMP7rVo3mIykV+2+9J3UJ46jBk0WLaUAdn689aCwoqbBJiSnjAK/l8WvCWPIPm49" crossorigin="anonymous"></script>
    
    <!-- Global Site Tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-107500873-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments)};
  gtag('js', new Date());

  gtag('config', 'UA-107500873-1');
</script>

    
  </head>


<body class="wrap">
  <header>
    <nav class="navbar navbar-expand-md navbar-dark bg-dark">
  <a class="navbar-brand" href="/"><img src="/img/arrow-inverse-300px.png" height="60px"/></a>
  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#arrow-navbar" aria-controls="arrow-navbar" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>

    <!-- Collect the nav links, forms, and other content for toggling -->
    <div class="collapse navbar-collapse" id="arrow-navbar">
      <ul class="nav navbar-nav">
        <li class="nav-item dropdown">
          <a class="nav-link dropdown-toggle" href="#"
             id="navbarDropdownProjectLinks" role="button" data-toggle="dropdown"
             aria-haspopup="true" aria-expanded="false">
             Project Links
          </a>
          <div class="dropdown-menu" aria-labelledby="navbarDropdownProjectLinks">
            <a class="dropdown-item" href="/install/">Installation</a>
            <a class="dropdown-item" href="/release/">Releases</a>
            <a class="dropdown-item" href="/faq/">FAQ</a>
            <a class="dropdown-item" href="/blog/">Blog</a>
            <a class="dropdown-item" href="https://github.com/apache/arrow">Source Code</a>
            <a class="dropdown-item" href="https://issues.apache.org/jira/browse/ARROW">Issue Tracker</a>
          </div>
        </li>
        <li class="nav-item dropdown">
          <a class="nav-link dropdown-toggle" href="#"
             id="navbarDropdownCommunity" role="button" data-toggle="dropdown"
             aria-haspopup="true" aria-expanded="false">
             Community
          </a>
          <div class="dropdown-menu" aria-labelledby="navbarDropdownCommunity">
            <a class="dropdown-item" href="http://mail-archives.apache.org/mod_mbox/arrow-user/">User Mailing List</a>
            <a class="dropdown-item" href="http://mail-archives.apache.org/mod_mbox/arrow-dev/">Dev Mailing List</a>
            <a class="dropdown-item" href="https://cwiki.apache.org/confluence/display/ARROW">Developer Wiki</a>
            <a class="dropdown-item" href="/committers/">Committers</a>
            <a class="dropdown-item" href="/powered_by/">Powered By</a>
          </div>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="/docs/format/README.html"
             role="button" aria-haspopup="true" aria-expanded="false">
             Specification
          </a>
        </li>
        <li class="nav-item dropdown">
          <a class="nav-link dropdown-toggle" href="#"
             id="navbarDropdownDocumentation" role="button" data-toggle="dropdown"
             aria-haspopup="true" aria-expanded="false">
             Documentation
          </a>
          <div class="dropdown-menu" aria-labelledby="navbarDropdownDocumentation">
            <a class="dropdown-item" href="/docs">Project Docs</a>
            <a class="dropdown-item" href="/docs/python">Python</a>
            <a class="dropdown-item" href="/docs/cpp">C++</a>
            <a class="dropdown-item" href="/docs/java">Java</a>
            <a class="dropdown-item" href="/docs/c_glib">C GLib</a>
            <a class="dropdown-item" href="/docs/js">JavaScript</a>
            <a class="dropdown-item" href="/docs/r">R</a>
          </div>
        </li>
        <!-- <li><a href="/blog">Blog</a></li> -->
        <li class="nav-item dropdown">
          <a class="nav-link dropdown-toggle" href="#"
             id="navbarDropdownASF" role="button" data-toggle="dropdown"
             aria-haspopup="true" aria-expanded="false">
             ASF Links
          </a>
          <div class="dropdown-menu" aria-labelledby="navbarDropdownASF">
            <a class="dropdown-item" href="http://www.apache.org/">ASF Website</a>
            <a class="dropdown-item" href="http://www.apache.org/licenses/">License</a>
            <a class="dropdown-item" href="http://www.apache.org/foundation/sponsorship.html">Donate</a>
            <a class="dropdown-item" href="http://www.apache.org/foundation/thanks.html">Thanks</a>
            <a class="dropdown-item" href="http://www.apache.org/security/">Security</a>
          </div>
        </li>
      </ul>
      <div class="flex-row justify-content-end ml-md-auto">
        <a class="d-sm-none d-md-inline pr-2" href="https://www.apache.org/events/current-event.html">
          <img src="https://www.apache.org/events/current-event-234x60.png"/>
        </a>
        <a href="http://www.apache.org/">
          <img src="/img/asf_logo.svg" width="120px"/>
        </a>
      </div>
      </div><!-- /.navbar-collapse -->
    </div>
  </nav>

  </header>

  <div class="container p-lg-4">
    <main role="main">
      

<h2>Project News and Blog</h2>
<hr>


  
    
  <div class="blog-post" style="margin-bottom: 4rem">
    
<h1>
  Faster C++ Apache Parquet performance on dictionary-encoded string data coming in Apache Arrow 0.15
  <a href="/blog/2019/09/05/faster-strings-cpp-parquet/" class="permalink" title="Permalink">∞</a>
</h1>



<p>
  <span class="badge badge-secondary">Published</span>
  <span class="published">
    05 Sep 2019
  </span>
  <br />
  <span class="badge badge-secondary">By</span>
  
    Wes McKinney
  
</p>

    <!--

-->

<p>We have been implementing a series of optimizations in the Apache Parquet C++
internals to improve read and write efficiency (both performance and memory
use) for Arrow columnar binary and string data, with new “native” support for
Arrow’s dictionary types. This should have a big impact on users of the C++,
MATLAB, Python, R, and Ruby interfaces to Parquet files.</p>

<p>This post reviews work that was done and shows benchmarks comparing Arrow
0.12.1 with the current development version (to be released soon as Arrow
0.15.0).</p>

<h1 id="summary-of-work">Summary of work</h1>

<p>One of the largest and most complex optimizations involves encoding and
decoding Parquet files’ internal dictionary-encoded data streams to and from
Arrow’s in-memory dictionary-encoded <code class="highlighter-rouge">DictionaryArray</code>
representation. Dictionary encoding is a compression strategy in Parquet, and
there is no formal “dictionary” or “categorical” type. I will go into more
detail about this below.</p>

<p>Some of the particular JIRA issues related to this work include:</p>

<ul>
  <li>Vectorize comparators for computing statistics (<a href="https://issues.apache.org/jira/browse/PARQUET-1523">PARQUET-1523</a>)</li>
  <li>Read binary directly data directly into dictionary builder
(<a href="https://issues.apache.org/jira/browse/ARROW-3769">ARROW-3769</a>)</li>
  <li>Writing Parquet’s dictionary indices directly into dictionary builder
(<a href="https://issues.apache.org/jira/browse/ARROW-3772">ARROW-3772</a>)</li>
  <li>Write dense (non-dictionary) Arrow arrays directly into Parquet data encoders
(<a href="https://issues.apache.org/jira/browse/ARROW-6152">ARROW-6152</a>)</li>
  <li>Direct writing of <code class="highlighter-rouge">arrow::DictionaryArray</code> to Parquet column writers (<a href="https://issues.apache.org/jira/browse/ARROW-3246">ARROW-3246</a>)</li>
  <li>Supporting changing dictionaries (<a href="https://issues.apache.org/jira/browse/ARROW-3144">ARROW-3144</a>)</li>
  <li>Internal IO optimizations and improved raw <code class="highlighter-rouge">BYTE_ARRAY</code> encoding performance
(<a href="https://issues.apache.org/jira/browse/ARROW-4398">ARROW-4398</a>)</li>
</ul>

<p>One of the challenges of developing the Parquet C++ library is that we maintain
low-level read and write APIs that do not involve the Arrow columnar data
structures. So we have had to take care to implement Arrow-related
optimizations without impacting non-Arrow Parquet users, which includes
database systems like Clickhouse and Vertica.</p>

<h1 id="background-how-parquet-files-do-dictionary-encoding">Background: how Parquet files do dictionary encoding</h1>

<p>Many direct and indirect users of Apache Arrow use dictionary encoding to
improve performance and memory use on binary or string data types that include
many repeated values. MATLAB or pandas users will know this as the Categorical
type (see <a href="https://www.mathworks.com/help/matlab/categorical-arrays.html">MATLAB docs</a> or <a href="https://pandas.pydata.org/pandas-docs/stable/user_guide/categorical.html">pandas docs</a>) while in R such encoding is
known as <a href="https://stat.ethz.ch/R-manual/R-devel/library/base/html/factor.html"><code class="highlighter-rouge">factor</code></a>. In the Arrow C++ library and various bindings we have
the <code class="highlighter-rouge">DictionaryArray</code> object for representing such data in memory.</p>

<p>For example, an array such as</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>['apple', 'orange', 'apple', NULL, 'orange', 'orange']
</code></pre></div></div>

<p>has dictionary-encoded form</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>dictionary: ['apple', 'orange']
indices: [0, 1, 0, NULL, 1, 1]
</code></pre></div></div>

<p>The <a href="https://github.com/apache/parquet-format/blob/master/Encodings.md">Parquet format uses dictionary encoding</a> to compress data, and it is
used for all Parquet data types, not just binary or string data. Parquet
further uses bit-packing and run-length encoding (RLE) to compress the
dictionary indices, so if you had data like</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>['apple', 'apple', 'apple', 'apple', 'apple', 'apple', 'orange']
</code></pre></div></div>

<p>the indices would be encoded like</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[rle-run=(6, 0),
 bit-packed-run=[1]]
</code></pre></div></div>

<p>The full details of the rle-bitpacking encoding are found in the <a href="https://github.com/apache/parquet-format/blob/master/Encodings.md">Parquet
specification</a>.</p>

<p>When writing a Parquet file, most implementations will use dictionary encoding
to compress a column until the dictionary itself reaches a certain size
threshold, usually around 1 megabyte. At this point, the column writer will
“fall back” to <code class="highlighter-rouge">PLAIN</code> encoding where values are written end-to-end in “data
pages” and then usually compressed with Snappy or Gzip. See the following rough
diagram:</p>

<div align="center">
<img src="/img/20190903-parquet-dictionary-column-chunk.png" alt="Internal ColumnChunk structure" width="80%" class="img-responsive" />
</div>

<h1 id="faster-reading-and-writing-of-dictionary-encoded-data">Faster reading and writing of dictionary-encoded data</h1>

<p>When reading a Parquet file, the dictionary-encoded portions are usually
materialized to their non-dictionary-encoded form, causing binary or string
values to be duplicated in memory. So an obvious (but not trivial) optimization
is to skip this “dense” materialization. There are several issues to deal with:</p>

<ul>
  <li>A Parquet file often contains multiple ColumnChunks for each semantic column,
and the dictionary values may be different in each ColumnChunk</li>
  <li>We must gracefully handle the “fall back” portion which is not
dictionary-encoded</li>
</ul>

<p>We pursued several avenues to help with this:</p>

<ul>
  <li>Allowing each <code class="highlighter-rouge">DictionaryArray</code> to have a different dictionary (before, the
dictionary was part of the <code class="highlighter-rouge">DictionaryType</code>, which caused problems)</li>
  <li>We enabled the Parquet dictionary indices to be directly written into an
Arrow <code class="highlighter-rouge">DictionaryBuilder</code> without rehashing the data</li>
  <li>When decoding a ColumnChunk, we first append the dictionary values and
indices into an Arrow <code class="highlighter-rouge">DictionaryBuilder</code>, and when we encounter the “fall
back” portion we use a hash table to convert those values to
dictionary-encoded form</li>
  <li>We override the “fall back” logic when writing a ColumnChunk from an
<code class="highlighter-rouge">DictionaryArray</code> so that reading such data back is more efficient</li>
</ul>

<p>All of these things together have produced some excellent performance results
that we will detail below.</p>

<p>The other class of optimizations we implemented was removing an abstraction
layer between the low-level Parquet column data encoder and decoder classes and
the Arrow columnar data structures. This involves:</p>

<ul>
  <li>Adding <code class="highlighter-rouge">ColumnWriter::WriteArrow</code> and <code class="highlighter-rouge">Encoder::Put</code> methods that accept
<code class="highlighter-rouge">arrow::Array</code> objects directly</li>
  <li>Adding <code class="highlighter-rouge">ByteArrayDecoder::DecodeArrow</code> method to decode binary data directly
into an <code class="highlighter-rouge">arrow::BinaryBuilder</code>.</li>
</ul>

<p>While the performance improvements from this work are less dramatic than for
dictionary-encoded data, they are still meaningful in real-world applications.</p>

<h1 id="performance-benchmarks">Performance Benchmarks</h1>

<p>We ran some benchmarks comparing Arrow 0.12.1 with the current master
branch. We construct two kinds of Arrow tables with 10 columns each:</p>

<ul>
  <li>“Low cardinality” and “high cardinality” variants. The “low cardinality” case
has 1,000 unique string values of 32-bytes each. The “high cardinality” has
100,000 unique values</li>
  <li>“Dense” (non-dictionary) and “Dictionary” variants</li>
</ul>

<p><a href="https://gist.github.com/wesm/b4554e2d6028243a30eeed2c644a9066">See the full benchmark script.</a></p>

<p>We show both single-threaded and multithreaded read performance. The test
machine is an Intel i9-9960X using gcc 8.3.0 (on Ubuntu 18.04) with 16 physical
cores and 32 virtual cores. All time measurements are reported in seconds, but
we are most interested in showing the relative performance.</p>

<p>First, the writing benchmarks:</p>

<div align="center">
<img src="/img/20190903_parquet_write_perf.png" alt="Parquet write benchmarks" width="80%" class="img-responsive" />
</div>

<p>Writing <code class="highlighter-rouge">DictionaryArray</code> is dramatically faster due to the optimizations
described above. We have achieved a small improvement in writing dense
(non-dictionary) binary arrays.</p>

<p>Then, the reading benchmarks:</p>

<div align="center">
<img src="/img/20190903_parquet_read_perf.png" alt="Parquet read benchmarks" width="80%" class="img-responsive" />
</div>

<p>Here, similarly reading <code class="highlighter-rouge">DictionaryArray</code> directly is many times faster.</p>

<p>These benchmarks show that parallel reads of dense binary data may be slightly
slower though single-threaded reads are now faster. We may want to do some
profiling and see what we can do to bring read performance back in
line. Optimizing the dense read path has not been too much of a priority
relative to the dictionary read path in this work.</p>

<h1 id="memory-use-improvements">Memory Use Improvements</h1>

<p>In addition to faster performance, reading columns as dictionary-encoded can
yield significantly less memory use.</p>

<p>In the <code class="highlighter-rouge">dict-random</code> case above, we found that the master branch uses 405 MB of
RAM at peak while loading a 152 MB dataset. In v0.12.1, loading the same
Parquet file without the accelerated dictionary support uses 1.94 GB of peak
memory while the resulting non-dictionary table occupies 1.01 GB.</p>

<p>Note that we had a memory overuse bug in versions 0.14.0 and 0.14.1 fixed in
ARROW-6060, so if you are hitting this bug you will want to upgrade to 0.15.0
as soon as it comes out.</p>

<h1 id="conclusion">Conclusion</h1>

<p>There are still many Parquet-related optimizations that we may pursue in the
future, but the ones here can be very helpful to people working with
string-heavy datasets, both in performance and memory use. If you’d like to
discuss this development work, we’d be glad to hear from you on our developer
mailing list dev@arrow.apache.org.</p>


  </div>

  

  
    
  <div class="blog-post" style="margin-bottom: 4rem">
    
<h1>
  Apache Arrow R Package On CRAN
  <a href="/blog/2019/08/08/r-package-on-cran/" class="permalink" title="Permalink">∞</a>
</h1>



<p>
  <span class="badge badge-secondary">Published</span>
  <span class="published">
    08 Aug 2019
  </span>
  <br />
  <span class="badge badge-secondary">By</span>
  
    <a href="http://github.com/nealrichardson">Neal Richardson (npr) </a>
  
</p>

    <!--

-->

<p>We are very excited to announce that the <code class="highlighter-rouge">arrow</code> R package is now available on
<a href="https://cran.r-project.org/">CRAN</a>.</p>

<p><a href="https://arrow.apache.org/">Apache Arrow</a> is a cross-language development
platform for in-memory data that specifies a standardized columnar memory
format for flat and hierarchical data, organized for efficient analytic
operations on modern hardware. The <code class="highlighter-rouge">arrow</code> package provides an R interface to
the Arrow C++ library, including support for working with Parquet and Feather
files, as well as lower-level access to Arrow memory and messages.</p>

<p>You can install the package from CRAN with</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">install.packages</span><span class="p">(</span><span class="s2">"arrow"</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<p>On macOS and Windows, installing a binary package from CRAN will generally
handle Arrow’s C++ dependencies for you. However, the macOS CRAN binaries are
unfortunately incomplete for this version, so to install 0.14.1, you’ll first
need to use Homebrew to get the Arrow C++ library (<code class="highlighter-rouge">brew install
apache-arrow</code>), and then from R you can <code class="highlighter-rouge">install.packages("arrow", type =
"source")</code>.</p>

<p>Windows binaries are not yet available on CRAN but should be published soon.</p>

<p>On Linux, you’ll need to first install the C++ library. See the <a href="https://arrow.apache.org/install/">Arrow project
installation page</a> to find pre-compiled
binary packages for some common Linux distributions, including Debian, Ubuntu,
and CentOS. You’ll need to install <code class="highlighter-rouge">libparquet-dev</code> on Debian and Ubuntu, or
<code class="highlighter-rouge">parquet-devel</code> on CentOS. This will also automatically install the Arrow C++
library as a dependency. Other Linux distributions must install the C++ library
from source.</p>

<p>If you install the <code class="highlighter-rouge">arrow</code> R package from source and the C++ library is not
found, the R package functions will notify you that Arrow is not
available. Call</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">arrow</span><span class="o">::</span><span class="n">install_arrow</span><span class="p">()</span><span class="w">
</span></code></pre></div></div>

<p>for version- and platform-specific guidance on installing the Arrow C++
library.</p>

<h2 id="parquet-files">Parquet files</h2>

<p>This package introduces basic read and write support for the <a href="https://parquet.apache.org/">Apache
Parquet</a> columnar data file format. Prior to its
availability, options for accessing Parquet data in R were limited; the most
common recommendation was to use Apache Spark. The <code class="highlighter-rouge">arrow</code> package greatly
simplifies this access and lets you go from a Parquet file to a <code class="highlighter-rouge">data.frame</code>
and back easily, without having to set up a database.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">library</span><span class="p">(</span><span class="n">arrow</span><span class="p">)</span><span class="w">
</span><span class="n">df</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">read_parquet</span><span class="p">(</span><span class="s2">"path/to/file.parquet"</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<p>This function, along with the other readers in the package, takes an optional
<code class="highlighter-rouge">col_select</code> argument, inspired by the
<a href="https://vroom.r-lib.org/reference/vroom.html"><code class="highlighter-rouge">vroom</code></a> package. This argument
lets you use the <a href="https://tidyselect.r-lib.org/reference/select_helpers.html">“tidyselect” helper
functions</a>, as you
can do in <code class="highlighter-rouge">dplyr::select()</code>, to specify that you only want to keep certain
columns. By narrowing your selection at read time, you can load a <code class="highlighter-rouge">data.frame</code>
with less memory overhead.</p>

<p>For example, suppose you had written the <code class="highlighter-rouge">iris</code> dataset to Parquet. You could
read a <code class="highlighter-rouge">data.frame</code> with only the columns <code class="highlighter-rouge">c("Sepal.Length", "Sepal.Width")</code> by
doing</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">read_parquet</span><span class="p">(</span><span class="s2">"iris.parquet"</span><span class="p">,</span><span class="w"> </span><span class="n">col_select</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">starts_with</span><span class="p">(</span><span class="s2">"Sepal"</span><span class="p">))</span><span class="w">
</span></code></pre></div></div>

<p>Just as you can read, you can write Parquet files:</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">write_parquet</span><span class="p">(</span><span class="n">df</span><span class="p">,</span><span class="w"> </span><span class="s2">"path/to/different_file.parquet"</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<p>Note that this read and write support for Parquet files in R is in its early
stages of development. The Python Arrow library
(<a href="https://arrow.apache.org/docs/python/">pyarrow</a>) still has much richer
support for Parquet files, including working with multi-file datasets. We
intend to reach feature equivalency between the R and Python packages in the
future.</p>

<h2 id="feather-files">Feather files</h2>

<p>This package also includes a faster and more robust implementation of the
Feather file format, providing <code class="highlighter-rouge">read_feather()</code> and
<code class="highlighter-rouge">write_feather()</code>. <a href="https://github.com/wesm/feather">Feather</a> was one of the
initial applications of Apache Arrow for Python and R, providing an efficient,
common file format language-agnostic data frame storage, along with
implementations in R and Python.</p>

<p>As Arrow progressed, development of Feather moved to the
<a href="https://github.com/apache/arrow"><code class="highlighter-rouge">apache/arrow</code></a> project, and for the last two
years, the Python implementation of Feather has just been a wrapper around
<code class="highlighter-rouge">pyarrow</code>. This meant that as Arrow progressed and bugs were fixed, the Python
version of Feather got the improvements but sadly R did not.</p>

<p>With the <code class="highlighter-rouge">arrow</code> package, the R implementation of Feather catches up and now
depends on the same underlying C++ library as the Python version does. This
should result in more reliable and consistent behavior across the two
languages, as well as <a href="https://wesmckinney.com/blog/feather-arrow-future/">improved
performance</a>.</p>

<p>We encourage all R users of <code class="highlighter-rouge">feather</code> to switch to using
<code class="highlighter-rouge">arrow::read_feather()</code> and <code class="highlighter-rouge">arrow::write_feather()</code>.</p>

<p>Note that both Feather and Parquet are columnar data formats that allow sharing
data frames across R, Pandas, and other tools. When should you use Feather and
when should you use Parquet? Parquet balances space-efficiency with
deserialization costs, making it an ideal choice for remote storage systems
like HDFS or Amazon S3. Feather is designed for fast local reads, particularly
with solid-state drives, and is not intended for use with remote storage
systems. Feather files can be memory-mapped and accessed as Arrow columnar data
in-memory without any deserialization while Parquet files always must be
decompressed and decoded. See the <a href="https://arrow.apache.org/faq/">Arrow project
FAQ</a> for more.</p>

<h2 id="other-capabilities">Other capabilities</h2>

<p>In addition to these readers and writers, the <code class="highlighter-rouge">arrow</code> package has wrappers for
other readers in the C++ library; see <code class="highlighter-rouge">?read_csv_arrow</code> and
<code class="highlighter-rouge">?read_json_arrow</code>. These readers are being developed to optimize for the
memory layout of the Arrow columnar format and are not intended as a direct
replacement for existing R CSV readers (<code class="highlighter-rouge">base::read.csv</code>, <code class="highlighter-rouge">readr::read_csv</code>,
<code class="highlighter-rouge">data.table::fread</code>) that return an R <code class="highlighter-rouge">data.frame</code>.</p>

<p>It also provides many lower-level bindings to the C++ library, which enable you
to access and manipulate Arrow objects. You can use these to build connectors
to other applications and services that use Arrow. One example is Spark: the
<a href="https://spark.rstudio.com/"><code class="highlighter-rouge">sparklyr</code></a> package has support for using Arrow to
move data to and from Spark, yielding <a href="http://arrow.apache.org/blog/2019/01/25/r-spark-improvements/">significant performance
gains</a>.</p>

<h2 id="acknowledgements">Acknowledgements</h2>

<p>In addition to the work on wiring the R package up to the Arrow Parquet C++
library, a lot of effort went into building and packaging Arrow for R users,
ensuring its ease of installation across platforms. We’d like to thank the
support of Jeroen Ooms, Javier Luraschi, JJ Allaire, Davis Vaughan, the CRAN
team, and many others in the Apache Arrow community for helping us get to this
point.</p>

  </div>

  

  
    
  <div class="blog-post" style="margin-bottom: 4rem">
    
<h1>
  Apache Arrow 0.14.0 Release
  <a href="/blog/2019/07/02/0.14.0-release/" class="permalink" title="Permalink">∞</a>
</h1>



<p>
  <span class="badge badge-secondary">Published</span>
  <span class="published">
    02 Jul 2019
  </span>
  <br />
  <span class="badge badge-secondary">By</span>
  
    <a href="https://arrow.apache.org">The Apache Arrow PMC (pmc) </a>
  
</p>

    <!--

-->

<p>The Apache Arrow team is pleased to announce the 0.14.0 release. This
covers 3 months of development work and includes <a href="https://issues.apache.org/jira/issues/?jql=project%20%3D%20ARROW%20AND%20status%20%3D%20Resolved%20AND%20fixVersion%20%3D%200.13.0"><strong>602 resolved
issues</strong></a> from <a href="https://arrow.apache.org/release/0.14.0.html#contributors"><strong>75 distinct contributors</strong></a>.  See the Install
Page to learn how to get the libraries for your platform. The
<a href="https://arrow.apache.org/release/0.14.0.html">complete changelog</a> is also available.</p>

<p>This post will give some brief highlights in the project since the
0.13.0 release from April.</p>

<h2 id="new-committers">New committers</h2>

<p>Since the 0.13.0 release, the following have been added:</p>

<ul>
  <li><a href="https://github.com/nevi-me">Neville Dipale</a> was added as a committer</li>
  <li><a href="https://github.com/fsaintjacques">François Saint-Jacques</a> was added as a committer</li>
  <li><a href="https://github.com/praveenbingo">Praveen Kumar</a> was added as a committer</li>
</ul>

<p>Thank you for all your contributions!</p>

<h2 id="upcoming-100-format-stability-release">Upcoming 1.0.0 Format Stability Release</h2>

<p>We are planning for our next major release to move from 0.14.0 to
1.0.0. The major version number will indicate stability of the Arrow
columnar format and binary protocol. While the format has already been
stable since December 2017, we believe it is a good idea to make this
stability official and to indicate that it is safe to persist
serialized Arrow data in applications. This means that applications
will be able to safely upgrade to new Arrow versions without having to
worry about backwards incompatibilities. We will write in a future
blog post about the stability guarantees we intend to provide to help
application developers plan accordingly.</p>

<h2 id="packaging">Packaging</h2>

<p>We added support for the following platforms:</p>

<ul>
  <li>Debian GNU/Linux buster</li>
  <li>Ubuntu 19.04</li>
</ul>

<p>We dropped support for Ubuntu 14.04.</p>

<h2 id="development-infrastructure-and-tooling">Development Infrastructure and Tooling</h2>

<p>As the project has grown larger and more diverse, we are increasingly
outgrowing what we can test in public continuous integration services
like Travis CI and Appveyor. In addition, we share these resources
with the entire Apache Software Foundation, and given the high volume
of pull requests into Apache Arrow, maintainers are frequently waiting
many hours for the green light to merge patches.</p>

<p>The complexity of our testing is driven by the number of different
components and programming languages as well as increasingly long
compilation and test execution times as individual libraries grow
larger. The 50 minute time limit of public CI services is simply too
limited to comprehensively test the project. Additionally, the CI host
machines are constrained in their features and memory limits,
preventing us from testing features that are only relevant on large
amounts of data (10GB or more) or functionality that requires a
CUDA-enabled GPU.</p>

<p>Organizations that contribute to Apache Arrow are working on physical
build infrastructure and tools to improve build times and build
scalability. One such new tool is <code class="highlighter-rouge">ursabot</code>, a GitHub-enabled bot
that can be used to trigger builds either on physical build or in the
cloud. It can also be used to trigger benchmark timing comparisons. If
you are contributing to the project, you may see Ursabot being
employed to trigger tests in pull requests.</p>

<p>To help assist with migrating away from Travis CI, we are also working
to make as many of our builds reproducible with Docker and not reliant
on Travis CI-specific configuration details. This will also help
contributors reproduce build failures locally without having to wait
for Travis CI.</p>

<h2 id="columnar-format-notes">Columnar Format Notes</h2>

<ul>
  <li>User-defined “extension” types have been formalized in the Arrow
format, enabling library users to embed custom data types in the
Arrow columnar format. Initial support is available in C++, Java,
and Python.</li>
  <li>A new Duration logical type was added to represent absolute lengths
of time.</li>
</ul>

<h2 id="arrow-flight-notes">Arrow Flight notes</h2>

<p>Flight now supports many of the features of a complete RPC
framework.</p>

<ul>
  <li>Authentication APIs are now supported across all languages (ARROW-5137)</li>
  <li>Encrypted communication using OpenSSL is supported (ARROW-5643,
ARROW-5529)</li>
  <li>Clients can specify timeouts on remote calls (ARROW-5136)</li>
  <li>On the protocol level, endpoints are now identified with URIs, to
support an open-ended number of potential transports (including TLS
and Unix sockets, and perhaps even non-gRPC-based transports in the
future) (ARROW-4651)</li>
  <li>Application-defined metadata can be sent alongside data (ARROW-4626,
ARROW-4627).</li>
</ul>

<p>Windows is now a supported platform for Flight in C++ and Python
(ARROW-3294), and Python wheels are shipped for all languages
(ARROW-3150, ARROW-5656). C++, Python, and Java have been brought to
parity, now that actions can return streaming results in Java
(ARROW-5254).</p>

<h2 id="c-notes">C++ notes</h2>

<p>188 resolved issues related to the C++ implementation, so we summarize
some of the work here.</p>

<h3 id="general-platform-improvements">General platform improvements</h3>

<ul>
  <li>A FileSystem abstraction (ARROW-767) has been added, which paves the
way for a future Arrow Datasets library allowing to access sharded
data on arbitrary storage systems, including remote or cloud
storage. A first draft of the Datasets API was committed in
ARROW-5512. Right now, this comes with no implementation, but we
expect to slowly build it up in the coming weeks or months. Early
feedback is welcome on this API.</li>
  <li>The dictionary API has been reworked in ARROW-3144. The dictionary
values used to be tied to the DictionaryType instance, which ended
up too inflexible. Since dictionary-encoding is more often an
optimization than a semantic property of the data, we decided to
move the dictionary values to the ArrayData structure, making it
natural for dictionary-encoded arrays to share the same DataType
instance, regardless of the encoding details.</li>
  <li>The FixedSizeList and Map types have been implemented, including in
integration tests. The Map type is akin to a List of Struct(key,
value) entries, but making it explicit that the underlying data has
key-value mapping semantics. Also, map entries are always non-null.</li>
  <li>A <code class="highlighter-rouge">Result&lt;T&gt;</code> class has been introduced in ARROW-4800. The aim is to
allow to return an error as w ell as a function’s logical result
without resorting to pointer-out arguments.</li>
  <li>The Parquet C++ library has been refactored to use common Arrow IO
classes for improved C++ platform interoperability.</li>
</ul>

<h3 id="line-delimited-json-reader">Line-delimited JSON reader</h3>

<p>A multithreaded line-delimited JSON reader (powered internally by
RapidJSON) is now available for use (also in Python and R via
bindings) . This will likely be expanded to support more kinds of JSON
storage in the future.</p>

<h3 id="new-computational-kernels">New computational kernels</h3>

<p>A number of new computational kernels have been developed</p>

<ul>
  <li>Compare filter for logical comparisons yielding boolean arrays</li>
  <li>Filter kernel for selecting elements of an input array according to
a boolean selection array.</li>
  <li>Take kernel, which selects elements by integer index, has been
expanded to support nested types</li>
</ul>

<h2 id="c-notes-1">C# Notes</h2>

<p>The native C# implementation has continued to mature since 0.13. This
release includes a number of performance, memory use, and usability
improvements.</p>

<h2 id="go-notes">Go notes</h2>

<p>Go’s support for the Arrow columnar format continues to expand. Go now
supports reading and writing the Arrow columnar binary protocol, and
it has also been <strong>added to the cross language integration
tests</strong>. There are now four languages (C++, Go, Java, and JavaScript)
included in our integration tests to verify cross-language
interoperability.</p>

<h2 id="java-notes">Java notes</h2>

<ul>
  <li>Support for referencing arbitrary memory using <code class="highlighter-rouge">ArrowBuf</code> has been
implemented, paving the way for memory map support in Java</li>
  <li>A number of performance improvements around vector value access were
added (see ARROW-5264, ARROW-5290).</li>
  <li>The Map type has been implemented in Java and integration tested
with C++</li>
  <li>Several microbenchmarks have been added and improved.  Including a
significant speed-up of zeroing out buffers.</li>
  <li>A new algorithms package has been started to contain reference
implementations of common algorithms.  The initial contribution is
for Array/Vector sorting.</li>
</ul>

<h2 id="javascript-notes">JavaScript Notes</h2>

<p>A new incremental <a href="https://github.com/apache/arrow/tree/master/js/src/builder">array builder API</a> is available.</p>

<h2 id="matlab-notes">MATLAB Notes</h2>

<p>Version 0.14.0 features improved Feather file support in the MEX bindings.</p>

<h2 id="python-notes">Python notes</h2>

<ul>
  <li>We fixed a problem with the Python wheels causing the Python wheels
to be much larger in 0.13.0 than they were in 0.12.0. Since the
introduction of LLVM into our build toolchain, the wheels are going
to still be significantly bigger. We are interested in approaches to
enable pyarrow to be installed in pieces with pip or conda rather
than monolithically.</li>
  <li>It is now possible to define ExtensionTypes with a Python
implementation (ARROW-840). Those ExtensionTypes can survive a
roundtrip through C++ and serialization.</li>
  <li>The Flight improvements highlighted above (see C++ notes) are all
available from Python. Furthermore, Flight is now bundled in our
binary wheels and conda packages for Linux, Windows and macOS
(ARROW-3150, ARROW-5656).</li>
  <li>We will build “manylinux2010” binary wheels for Linux systems, in
addition to “manylinux1” wheels (ARROW-2461). Manylinux2010 is a
newer standard for more recent systems, with less limiting toolchain
constraints. Installing manylinux2010 wheels requires an up-to-date
version of pip.</li>
  <li>Various bug fixes for CSV reading in Python and C++ including the
ability to parse Decimal(x, y) columns.</li>
</ul>

<h3 id="parquet-improvements">Parquet improvements</h3>

<ul>
  <li>Column statistics for logical types like unicode strings, unsigned
integers, and timestamps are casted to compatible Python types (see
ARROW-4139)</li>
  <li>It’s now possible to configure “data page” sizes when writing a file
from Python</li>
</ul>

<h2 id="ruby-and-c-glib-notes">Ruby and C GLib notes</h2>

<p>The GLib and Ruby bindings have been tracking features in the C++
project. This release includes bindings for Gandiva, JSON reader, and
other C++ features.</p>

<h2 id="rust-notes">Rust notes</h2>

<p>There is ongoing work in Rust happening on Parquet file support,
computational kernels, and the DataFusion query engine. See the full
changelog for details.</p>

<h2 id="r-notes">R notes</h2>

<p>We have been working on build and packaging for R so that community
members can hopefully release the project to CRAN in the near
future. Feature development for R has continued to follow the upstream
C++ project.</p>

<h2 id="community-discussions-ongoing">Community Discussions Ongoing</h2>

<p>There are a number of active discussions ongoing on the developer
dev@arrow.apache.org mailing list. We look forward to hearing from the
community there:</p>

<ul>
  <li><a href="https://lists.apache.org/thread.html/44a7a3d256ab5dbd62da6fe45b56951b435697426bf4adedb6520907@%3Cdev.arrow.apache.org%3E">Timing and scope of 1.0.0 release</a></li>
  <li><a href="https://lists.apache.org/thread.html/96b2e22606e8a7b0ad7dc4aae16f232724d1059b34636676ed971d40@%3Cdev.arrow.apache.org%3E">Solutions to increase continuous integration capacity</a></li>
  <li><a href="https://lists.apache.org/thread.html/5715a4d402c835d22d929a8069c5c0cf232077a660ee98639d544af8@%3Cdev.arrow.apache.org%3E">A proposal for versioning and forward/backward compatibility
guarantees for the 1.0.0 release</a> was shared, not much discussion has
occurred yet.</li>
  <li><a href="https://lists.apache.org/thread.html/8440be572c49b7b2ffb76b63e6d935ada9efd9c1c2021369b6d27786@%3Cdev.arrow.apache.org%3E">Addressing possible unaligned access and undefined behavior concerns</a>
in the Arrow binary protocol</li>
  <li><a href="https://lists.apache.org/thread.html/31b00086c2991104bd71fb1a2173f32b4a2f569d8e7b5b41e836f3a3@%3Cdev.arrow.apache.org%3E">Supporting smaller than 128-bit encoding of fixed width decimals</a></li>
  <li><a href="https://lists.apache.org/thread.html/97d78112ab583eecb155a7d78342c1063df65d64ec3ccfa0b18737c3@%3Cdev.arrow.apache.org%3E">Forking the Avro C++ implementation</a> so as to adapt it to Arrow’s
needs</li>
  <li><a href="https://lists.apache.org/thread.html/a99124e57c14c3c9ef9d98f3c80cfe1dd25496bf3ff7046778add937@%3Cdev.arrow.apache.org%3E">Sparse representation and compression in Arrow</a></li>
  <li><a href="https://lists.apache.org/thread.html/82a7c026ad18dbe9fdbcffa3560979aff6fd86dd56a49f40d9cfb46e@%3Cdev.arrow.apache.org%3E">Flight extensions: middleware API and generalized Put operations</a></li>
</ul>


  </div>

  

  
    
  <div class="blog-post" style="margin-bottom: 4rem">
    
<h1>
  Apache Arrow 0.13.0 Release
  <a href="/blog/2019/04/02/0.13.0-release/" class="permalink" title="Permalink">∞</a>
</h1>



<p>
  <span class="badge badge-secondary">Published</span>
  <span class="published">
    02 Apr 2019
  </span>
  <br />
  <span class="badge badge-secondary">By</span>
  
    <a href="https://wesmckinney.com">Wes McKinney (wesm) </a>
  
</p>

    <!--

-->

<p>The Apache Arrow team is pleased to announce the 0.13.0 release. This covers
more than 2 months of development work and includes <a href="https://issues.apache.org/jira/issues/?jql=project%20%3D%20ARROW%20AND%20status%20%3D%20Resolved%20AND%20fixVersion%20%3D%200.13.0"><strong>550 resolved
issues</strong></a> from <a href="https://arrow.apache.org/release/0.13.0.html#contributors"><strong>81 distinct contributors</strong></a>.</p>

<p>See the <a href="https://arrow.apache.org/install">Install Page</a> to learn how to get the libraries for your
platform. The <a href="https://arrow.apache.org/release/0.13.0.html">complete changelog</a> is also available.</p>

<p>While it’s a large release, this post will give some brief highlights in the
project since the 0.12.0 release from January.</p>

<h2 id="new-committers-and-pmc-member">New committers and PMC member</h2>

<p>The Arrow team is growing! Since the 0.12.0 release we have increased the size
of our committer and PMC rosters.</p>

<ul>
  <li><a href="https://github.com/andygrove">Andy Grove</a> was promoted to PMC member</li>
  <li><a href="https://github.com/paddyhoran">Paddy Horan</a> was added as a committer</li>
  <li><a href="https://github.com/emkornfield">Micah Kornfield</a> was added as a committer</li>
  <li><a href="https://github.com/pravindra">Ravindra Pindikura</a> was added as a committer</li>
  <li><a href="https://github.com/sunchao">Chao Sun</a> was added as a committer</li>
</ul>

<p>Thank you for all your contributions!</p>

<h2 id="rust-datafusion-query-engine-donation">Rust DataFusion Query Engine donation</h2>

<p>Since the last release, we received a donation of <a href="http://incubator.apache.org/ip-clearance/arrow-rust-datafusion.html">DataFusion</a>, a
Rust-native query engine for the Arrow columnar format, whose development had
been led prior by Andy Grove. <a href="http://arrow.apache.org/blog/2019/02/04/datafusion-donation/">Read more about DataFusion</a> in our February
blog post.</p>

<p>This is an exciting development for the Rust community, and we look forward to
developing more analytical query processing within the Apache Arrow project.</p>

<h2 id="arrow-flight-grpc-progress">Arrow Flight gRPC progress</h2>

<p>Over the last couple months, we have made significant progress on Arrow Flight,
an Arrow-native data messaging framework. We have integration tests to check
C++ and Java compatibility, and we have added Python bindings for the C++
library. We will write a future blog post to go into more detail about how
Flight works.</p>

<h2 id="c-notes">C++ notes</h2>

<p>There were 231 issues relating to C++ in this release, far too much to
summarize in a blog post. Some notable items include:</p>

<ul>
  <li>An experimental <code class="highlighter-rouge">ExtensionType</code> was developed for creating user-defined data
types that can be embedded in the Arrow binary protocol. This is not yet
finalized, but <a href="https://github.com/apache/arrow/blob/master/cpp/src/arrow/extension_type.h">feedback would be welcome</a>.</li>
  <li>We have undertaken a significant reworking of our CMake build system for C++
to make the third party dependencies more configurable. Among other things,
this eases work on packaging for Linux distributions. Read more about this in
the <a href="https://github.com/apache/arrow/blob/master/docs/source/developers/cpp.rst#build-dependency-management">C++ developer documentation</a>.</li>
  <li>Laying more groundwork for an Arrow-native in-memory query engine</li>
  <li>We began building a reader for line-delimited JSON files</li>
  <li>Gandiva can now be compiled on Windows with Visual Studio</li>
</ul>

<h2 id="c-notes-1">C# Notes</h2>

<p>C# .NET development has picked up since the initial code donation last
fall. 11 issues were resolved this release cycle.</p>

<p>The Arrow C# package is <a href="https://www.nuget.org/packages/Apache.Arrow/0.13.0">now available via NuGet</a>.</p>

<h2 id="go-notes">Go notes</h2>

<p>8 Go-related issues were resolved. A notable feature is the addition of a CSV
file writer.</p>

<h2 id="java-notes">Java notes</h2>

<p>26 Java issues were resolved. Outside of Flight-related work, some notable
items include:</p>

<ul>
  <li>Migration to Java 8 date and time APIs from Joda</li>
  <li>Array type support in JDBC adapter</li>
</ul>

<h2 id="javascript-notes">Javascript Notes</h2>

<p>The recent <a href="https://www.npmjs.com/package/apache-arrow/v/0.4.1">JavaScript 0.4.1 release</a> is the last JavaScript-only release
of Apache Arrow. Starting with 0.13 the Javascript implementation is now
included in mainline Arrow releases! The version number of the released
JavaScript packages will now be in sync with the mainline version number.</p>

<h2 id="python-notes">Python notes</h2>

<p>86 Python-related issues were resolved. Some highlights include:</p>

<ul>
  <li>The Gandiva LLVM expression compiler is now available in the Python wheels
through the <code class="highlighter-rouge">pyarrow.gandiva</code> module.</li>
  <li>Flight RPC bindings</li>
  <li>Improved pandas serialization performance with RangeIndex</li>
  <li>pyarrow can be used without pandas installed</li>
</ul>

<p>Note that Apache Arrow will continue to support Python 2.7 until January 2020.</p>

<h2 id="ruby-and-c-glib-notes">Ruby and C GLib notes</h2>

<p>36 C/GLib- and Ruby-related issues were resolved. The work continues to follow
the upstream work in the C++ project.</p>

<ul>
  <li><code class="highlighter-rouge">Arrow::RecordBatch#raw_records</code> was added. It can convert a record batch to
a Ruby’s array in 10x-200x faster than the same conversion by a pure-Ruby
implementation.</li>
</ul>

<h2 id="rust-notes">Rust notes</h2>

<p>69 Rust-related issues were resolved. Many of these relate to ongoing work in
the DataFusion query engine. Some notable items include:</p>

<ul>
  <li>Date/time support</li>
  <li>SIMD for arithmetic operations</li>
  <li>Writing CSV and reading line-delimited JSON</li>
  <li>Parquet data source support for DataFusion</li>
  <li>Prototype DataFrame-style API for DataFusion</li>
  <li>Continued evolution of Parquet file reader</li>
</ul>

<h2 id="r-development-progress">R development progress</h2>

<p>The Arrow R developers have expanded the scope of the R language bindings and
additionally worked on packaging support to be able to submit the package to
CRAN in the near future. 23 issues were resolved for this release.</p>

<p><a href="http://arrow.apache.org/blog/2019/01/25/r-spark-improvements/">We wrote in January about ongoing work</a> to accelerate R work on Apache Spark
using Arrow.</p>

<h2 id="community-discussions-ongoing">Community Discussions Ongoing</h2>

<p>There are a number of active discussions ongoing on the developer
<code class="highlighter-rouge">dev@arrow.apache.org</code> mailing list. We look forward to hearing from the
community there:</p>

<ul>
  <li><strong>Benchmarking</strong>: we are working to create tools for tracking all of our
benchmark results on a commit-by-commit basis in a centralized database
schema so that we can monitor for performance regressions over time. We hope
to develop a publicly viewable benchmark result dashboard.</li>
  <li><strong>C++ Datasets</strong>: development of a unified API for reading and writing
datasets stored in various common formats like Parquet, JSON, and CSV.</li>
  <li><strong>C++ Query Engine</strong>: architecture of a parallel Arrow-native query engine
for C++</li>
  <li><strong>Arrow Flight Evolution</strong>: adding features to support different real-world
data messaging use cases</li>
  <li><strong>Arrow Columnar Format evolution</strong>: we are discussing a new “duration” or
“time interval” type and some other additions to the Arrow columnar format.</li>
</ul>


  </div>

  

  
    
  <div class="blog-post" style="margin-bottom: 4rem">
    
<h1>
  Reducing Python String Memory Use in Apache Arrow 0.12
  <a href="/blog/2019/02/05/python-string-memory-0.12/" class="permalink" title="Permalink">∞</a>
</h1>



<p>
  <span class="badge badge-secondary">Published</span>
  <span class="published">
    05 Feb 2019
  </span>
  <br />
  <span class="badge badge-secondary">By</span>
  
    <a href="https://wesmckinney.com">Wes McKinney (wesm) </a>
  
</p>

    <!--

-->

<p>Python users who upgrade to recently released <code class="highlighter-rouge">pyarrow</code> 0.12 may find that
their applications use significantly less memory when converting Arrow string
data to pandas format. This includes using <code class="highlighter-rouge">pyarrow.parquet.read_table</code> and
<code class="highlighter-rouge">pandas.read_parquet</code>. This article details some of what is going on under the
hood, and why Python applications dealing with large amounts of strings are
prone to memory use problems.</p>

<h2 id="why-python-strings-can-use-a-lot-of-memory">Why Python strings can use a lot of memory</h2>

<p>Let’s start with some possibly surprising facts. I’m going to create an empty
<code class="highlighter-rouge">bytes</code> object and an empty <code class="highlighter-rouge">str</code> (unicode) object in Python 3.7:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>In [1]: val = b''

In [2]: unicode_val = u''
</code></pre></div></div>

<p>The <code class="highlighter-rouge">sys.getsizeof</code> function accurately reports the number of bytes used by
built-in Python objects. You might be surprised to find that:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>In [4]: import sys
In [5]: sys.getsizeof(val)
Out[5]: 33

In [6]: sys.getsizeof(unicode_val)
Out[6]: 49
</code></pre></div></div>

<p>Since strings in Python are nul-terminated, we can infer that a bytes object
has 32 bytes of overhead while unicode has 48 bytes. One must also account for
<code class="highlighter-rouge">PyObject*</code> pointer references to the objects, so the actual overhead is 40 and
56 bytes, respectively. With large strings and text, this overhead may not
matter much, but when you have a lot of small strings, such as those arising
from reading a CSV or Apache Parquet file, they can take up an unexpected
amount of memory. pandas represents strings in NumPy arrays of <code class="highlighter-rouge">PyObject*</code>
pointers, so the total memory used by a unique unicode string is</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>8 (PyObject*) + 48 (Python C struct) + string_length + 1
</code></pre></div></div>

<p>Suppose that we read a CSV file with</p>

<ul>
  <li>1 column</li>
  <li>1 million rows</li>
  <li>Each value in the column is a string with 10 characters</li>
</ul>

<p>On disk this file would take approximately 10MB. Read into memory, however, it
could take up over 60MB, as a 10 character string object takes up 67 bytes in a
<code class="highlighter-rouge">pandas.Series</code>.</p>

<h2 id="how-apache-arrow-represents-strings">How Apache Arrow represents strings</h2>

<p>While a Python unicode string can have 57 bytes of overhead, a string in the
Arrow columnar format has only 4 (32 bits) or 4.125 (33 bits) bytes of
overhead. 32-bit integer offsets encodes the position and size of a string
value in a contiguous chunk of memory:</p>

<div align="center">
<img src="/img/20190205-arrow-string.png" alt="Apache Arrow string memory layout" width="80%" class="img-responsive" />
</div>

<p>When you call <code class="highlighter-rouge">table.to_pandas()</code> or <code class="highlighter-rouge">array.to_pandas()</code> with <code class="highlighter-rouge">pyarrow</code>, we
have to convert this compact string representation back to pandas’s
Python-based strings. This can use a huge amount of memory when we have a large
number of small strings. It is a quite common occurrence when working with web
analytics data, which compresses to a compact size when stored in the Parquet
columnar file format.</p>

<p>Note that the Arrow string memory format has other benefits beyond memory
use. It is also much more efficient for analytics due to the guarantee of data
locality; all strings are next to each other in memory. In the case of pandas
and Python strings, the string data can be located anywhere in the process
heap. Arrow PMC member Uwe Korn did some work to <a href="https://www.slideshare.net/xhochy/extending-pandas-using-apache-arrow-and-numba">extend pandas with Arrow
string arrays</a> for improved performance and memory use.</p>

<h2 id="reducing-pandas-memory-use-when-converting-from-arrow">Reducing pandas memory use when converting from Arrow</h2>

<p>For many years, the <code class="highlighter-rouge">pandas.read_csv</code> function has relied on a trick to limit
the amount of string memory allocated. Because pandas uses arrays of
<code class="highlighter-rouge">PyObject*</code> pointers to refer to objects in the Python heap, we can avoid
creating multiple strings with the same value, instead reusing existing objects
and incrementing their reference counts.</p>

<p>Schematically, we have the following:</p>

<div align="center">
<img src="/img/20190205-numpy-string.png" alt="pandas string memory optimization" width="80%" class="img-responsive" />
</div>

<p>In <code class="highlighter-rouge">pyarrow</code> 0.12, we have implemented this when calling <code class="highlighter-rouge">to_pandas</code>. It
requires using a hash table to deduplicate the Arrow string data as it’s being
converted to pandas. Hashing data is not free, but counterintuitively it can be
faster in addition to being vastly more memory efficient in the common case in
analytics where we have table columns with many instances of the same string
values.</p>

<h2 id="memory-and-performance-benchmarks">Memory and Performance Benchmarks</h2>

<p>We can use the <a href="https://pypi.org/project/memory-profiler/"><code class="highlighter-rouge">memory_profiler</code></a> Python package to easily get process
memory usage within a running Python application.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">memory_profiler</span>
<span class="k">def</span> <span class="nf">mem</span><span class="p">():</span>
    <span class="k">return</span> <span class="n">memory_profiler</span><span class="o">.</span><span class="n">memory_usage</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
</code></pre></div></div>

<p>In a new application I have:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>In [7]: mem()
Out[7]: 86.21875
</code></pre></div></div>

<p>I will generate approximate 1 gigabyte of string data represented as Python
strings with length 10. The <code class="highlighter-rouge">pandas.util.testing</code> module has a handy <code class="highlighter-rouge">rands</code>
function for generating random strings. Here is the data generation function:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">pandas.util.testing</span> <span class="kn">import</span> <span class="n">rands</span>
<span class="k">def</span> <span class="nf">generate_strings</span><span class="p">(</span><span class="n">length</span><span class="p">,</span> <span class="n">nunique</span><span class="p">,</span> <span class="n">string_length</span><span class="o">=</span><span class="mi">10</span><span class="p">):</span>
    <span class="n">unique_values</span> <span class="o">=</span> <span class="p">[</span><span class="n">rands</span><span class="p">(</span><span class="n">string_length</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nunique</span><span class="p">)]</span>
    <span class="n">values</span> <span class="o">=</span> <span class="n">unique_values</span> <span class="o">*</span> <span class="p">(</span><span class="n">length</span> <span class="o">//</span> <span class="n">nunique</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">values</span>
</code></pre></div></div>

<p>This generates a certain number of unique strings, then duplicates then to
yield the desired number of total strings. So I’m going to create 100 million
strings with only 10000 unique values:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>In [8]: values = generate_strings(100000000, 10000)

In [9]: mem()
Out[9]: 852.140625
</code></pre></div></div>

<p>100 million <code class="highlighter-rouge">PyObject*</code> values is only 745 MB, so this increase of a little
over 770 MB is consistent with what we know so far. Now I’m going to convert
this to Arrow format:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>In [11]: arr = pa.array(values)

In [12]: mem()
Out[12]: 2276.9609375
</code></pre></div></div>

<p>Since <code class="highlighter-rouge">pyarrow</code> exactly accounts for all of its memory allocations, we also
check that</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>In [13]: pa.total_allocated_bytes()
Out[13]: 1416777280
</code></pre></div></div>

<p>Since each string takes about 14 bytes (10 bytes plus 4 bytes of overhead),
this is what we expect.</p>

<p>Now, converting <code class="highlighter-rouge">arr</code> back to pandas is where things get tricky. The <em>minimum</em>
amount of memory that pandas can use is a little under 800 MB as above as we
need 100 million <code class="highlighter-rouge">PyObject*</code> values, which are 8 bytes each.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>In [14]: arr_as_pandas = arr.to_pandas()

In [15]: mem()
Out[15]: 3041.78125
</code></pre></div></div>

<p>Doing the math, we used 765 MB which seems right. We can disable the string
deduplication logic by passing <code class="highlighter-rouge">deduplicate_objects=False</code> to <code class="highlighter-rouge">to_pandas</code>:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>In [16]: arr_as_pandas_no_dedup = arr.to_pandas(deduplicate_objects=False)

In [17]: mem()
Out[17]: 10006.95703125
</code></pre></div></div>

<p>Without object deduplication, we use 6965 megabytes, or an average of 73 bytes
per value. This is a little bit higher than the theoretical size of 67 bytes
computed above.</p>

<p>One of the more surprising results is that the new behavior is about twice as fast:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>In [18]: %time arr_as_pandas_time = arr.to_pandas()
CPU times: user 2.94 s, sys: 213 ms, total: 3.15 s
Wall time: 3.14 s

In [19]: %time arr_as_pandas_no_dedup_time = arr.to_pandas(deduplicate_objects=False)
CPU times: user 4.19 s, sys: 2.04 s, total: 6.23 s
Wall time: 6.21 s
</code></pre></div></div>

<p>The reason for this is that creating so many Python objects is more expensive
than hashing the 10 byte values and looking them up in a hash table.</p>

<p>Note that when you convert Arrow data with mostly unique values back to pandas,
the memory use benefits here won’t have as much of an impact.</p>

<h2 id="takeaways">Takeaways</h2>

<p>In Apache Arrow, our goal is to develop computational tools to operate natively
on the cache- and SIMD-friendly efficient Arrow columnar format. In the
meantime, though, we recognize that users have legacy applications using the
native memory layout of pandas or other analytics tools. We will do our best to
provide fast and memory-efficient interoperability with pandas and other
popular libraries.</p>


  </div>

  

  
    
  <div class="blog-post" style="margin-bottom: 4rem">
    
<h1>
  DataFusion: A Rust-native Query Engine for Apache Arrow
  <a href="/blog/2019/02/04/datafusion-donation/" class="permalink" title="Permalink">∞</a>
</h1>



<p>
  <span class="badge badge-secondary">Published</span>
  <span class="published">
    04 Feb 2019
  </span>
  <br />
  <span class="badge badge-secondary">By</span>
  
    <a href="http://github.com/andygrove">Andy Grove (agrove) </a>
  
</p>

    <!--

-->

<p>We are excited to announce that <a href="https://github.com/apache/arrow/tree/master/rust/datafusion">DataFusion</a> has been donated to the Apache Arrow project. DataFusion is an in-memory query engine for the Rust implementation of Apache Arrow.</p>

<p>Although DataFusion was started two years ago, it was recently re-implemented to be Arrow-native and currently has limited capabilities but does support SQL queries against iterators of RecordBatch and has support for CSV files. There are plans to <a href="https://issues.apache.org/jira/browse/ARROW-4466">add support for Parquet files</a>.</p>

<p>SQL support is limited to projection (<code class="highlighter-rouge">SELECT</code>), selection (<code class="highlighter-rouge">WHERE</code>), and simple aggregates (<code class="highlighter-rouge">MIN</code>, <code class="highlighter-rouge">MAX</code>, <code class="highlighter-rouge">SUM</code>) with an optional <code class="highlighter-rouge">GROUP BY</code> clause.</p>

<p>Supported expressions are identifiers, literals, simple math operations (<code class="highlighter-rouge">+</code>, <code class="highlighter-rouge">-</code>, <code class="highlighter-rouge">*</code>, <code class="highlighter-rouge">/</code>), binary expressions (<code class="highlighter-rouge">AND</code>, <code class="highlighter-rouge">OR</code>), equality and comparison operators (<code class="highlighter-rouge">=</code>, <code class="highlighter-rouge">!=</code>, <code class="highlighter-rouge">&lt;</code>, <code class="highlighter-rouge">&lt;=</code>, <code class="highlighter-rouge">&gt;=</code>, <code class="highlighter-rouge">&gt;</code>), and <code class="highlighter-rouge">CAST(expr AS type)</code>.</p>

<h2 id="example">Example</h2>

<p>The following example demonstrates running a simple aggregate SQL query against a CSV file.</p>

<div class="language-rust highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">// create execution context</span>
<span class="k">let</span> <span class="k">mut</span> <span class="n">ctx</span> <span class="o">=</span> <span class="nn">ExecutionContext</span><span class="p">::</span><span class="nf">new</span><span class="p">();</span>

<span class="c">// define schema for data source (csv file)</span>
<span class="k">let</span> <span class="n">schema</span> <span class="o">=</span> <span class="nn">Arc</span><span class="p">::</span><span class="nf">new</span><span class="p">(</span><span class="nn">Schema</span><span class="p">::</span><span class="nf">new</span><span class="p">(</span><span class="nd">vec!</span><span class="p">[</span>
    <span class="nn">Field</span><span class="p">::</span><span class="nf">new</span><span class="p">(</span><span class="s">"c1"</span><span class="p">,</span> <span class="nn">DataType</span><span class="p">::</span><span class="n">Utf8</span><span class="p">,</span> <span class="kc">false</span><span class="p">),</span>
    <span class="nn">Field</span><span class="p">::</span><span class="nf">new</span><span class="p">(</span><span class="s">"c2"</span><span class="p">,</span> <span class="nn">DataType</span><span class="p">::</span><span class="n">UInt32</span><span class="p">,</span> <span class="kc">false</span><span class="p">),</span>
    <span class="nn">Field</span><span class="p">::</span><span class="nf">new</span><span class="p">(</span><span class="s">"c3"</span><span class="p">,</span> <span class="nn">DataType</span><span class="p">::</span><span class="n">Int8</span><span class="p">,</span> <span class="kc">false</span><span class="p">),</span>
    <span class="nn">Field</span><span class="p">::</span><span class="nf">new</span><span class="p">(</span><span class="s">"c4"</span><span class="p">,</span> <span class="nn">DataType</span><span class="p">::</span><span class="n">Int16</span><span class="p">,</span> <span class="kc">false</span><span class="p">),</span>
    <span class="nn">Field</span><span class="p">::</span><span class="nf">new</span><span class="p">(</span><span class="s">"c5"</span><span class="p">,</span> <span class="nn">DataType</span><span class="p">::</span><span class="n">Int32</span><span class="p">,</span> <span class="kc">false</span><span class="p">),</span>
    <span class="nn">Field</span><span class="p">::</span><span class="nf">new</span><span class="p">(</span><span class="s">"c6"</span><span class="p">,</span> <span class="nn">DataType</span><span class="p">::</span><span class="n">Int64</span><span class="p">,</span> <span class="kc">false</span><span class="p">),</span>
    <span class="nn">Field</span><span class="p">::</span><span class="nf">new</span><span class="p">(</span><span class="s">"c7"</span><span class="p">,</span> <span class="nn">DataType</span><span class="p">::</span><span class="n">UInt8</span><span class="p">,</span> <span class="kc">false</span><span class="p">),</span>
    <span class="nn">Field</span><span class="p">::</span><span class="nf">new</span><span class="p">(</span><span class="s">"c8"</span><span class="p">,</span> <span class="nn">DataType</span><span class="p">::</span><span class="n">UInt16</span><span class="p">,</span> <span class="kc">false</span><span class="p">),</span>
    <span class="nn">Field</span><span class="p">::</span><span class="nf">new</span><span class="p">(</span><span class="s">"c9"</span><span class="p">,</span> <span class="nn">DataType</span><span class="p">::</span><span class="n">UInt32</span><span class="p">,</span> <span class="kc">false</span><span class="p">),</span>
    <span class="nn">Field</span><span class="p">::</span><span class="nf">new</span><span class="p">(</span><span class="s">"c10"</span><span class="p">,</span> <span class="nn">DataType</span><span class="p">::</span><span class="n">UInt64</span><span class="p">,</span> <span class="kc">false</span><span class="p">),</span>
    <span class="nn">Field</span><span class="p">::</span><span class="nf">new</span><span class="p">(</span><span class="s">"c11"</span><span class="p">,</span> <span class="nn">DataType</span><span class="p">::</span><span class="n">Float32</span><span class="p">,</span> <span class="kc">false</span><span class="p">),</span>
    <span class="nn">Field</span><span class="p">::</span><span class="nf">new</span><span class="p">(</span><span class="s">"c12"</span><span class="p">,</span> <span class="nn">DataType</span><span class="p">::</span><span class="n">Float64</span><span class="p">,</span> <span class="kc">false</span><span class="p">),</span>
    <span class="nn">Field</span><span class="p">::</span><span class="nf">new</span><span class="p">(</span><span class="s">"c13"</span><span class="p">,</span> <span class="nn">DataType</span><span class="p">::</span><span class="n">Utf8</span><span class="p">,</span> <span class="kc">false</span><span class="p">),</span>
<span class="p">]));</span>

<span class="c">// register csv file with the execution context</span>
<span class="k">let</span> <span class="n">csv_datasource</span> <span class="o">=</span>
    <span class="nn">CsvDataSource</span><span class="p">::</span><span class="nf">new</span><span class="p">(</span><span class="s">"test/data/aggregate_test_100.csv"</span><span class="p">,</span> <span class="n">schema</span><span class="nf">.clone</span><span class="p">(),</span> <span class="mi">1024</span><span class="p">);</span>
<span class="n">ctx</span><span class="nf">.register_datasource</span><span class="p">(</span><span class="s">"aggregate_test_100"</span><span class="p">,</span> <span class="nn">Rc</span><span class="p">::</span><span class="nf">new</span><span class="p">(</span><span class="nn">RefCell</span><span class="p">::</span><span class="nf">new</span><span class="p">(</span><span class="n">csv_datasource</span><span class="p">)));</span>

<span class="k">let</span> <span class="n">sql</span> <span class="o">=</span> <span class="s">"SELECT c1, MIN(c12), MAX(c12) FROM aggregate_test_100 WHERE c11 &gt; 0.1 AND c11 &lt; 0.9 GROUP BY c1"</span><span class="p">;</span>

<span class="c">// execute the query</span>
<span class="k">let</span> <span class="n">relation</span> <span class="o">=</span> <span class="n">ctx</span><span class="nf">.sql</span><span class="p">(</span><span class="o">&amp;</span><span class="n">sql</span><span class="p">)</span><span class="nf">.unwrap</span><span class="p">();</span>
<span class="k">let</span> <span class="k">mut</span> <span class="n">results</span> <span class="o">=</span> <span class="n">relation</span><span class="nf">.borrow_mut</span><span class="p">();</span>

<span class="c">// iterate over the results</span>
<span class="k">while</span> <span class="k">let</span> <span class="nf">Some</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span> <span class="o">=</span> <span class="n">results</span><span class="nf">.next</span><span class="p">()</span><span class="nf">.unwrap</span><span class="p">()</span> <span class="p">{</span>
    <span class="nd">println!</span><span class="p">(</span>
        <span class="s">"RecordBatch has {} rows and {} columns"</span><span class="p">,</span>
        <span class="n">batch</span><span class="nf">.num_rows</span><span class="p">(),</span>
        <span class="n">batch</span><span class="nf">.num_columns</span><span class="p">()</span>
    <span class="p">);</span>

    <span class="k">let</span> <span class="n">c1</span> <span class="o">=</span> <span class="n">batch</span>
        <span class="nf">.column</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="nf">.as_any</span><span class="p">()</span>
        <span class="py">.downcast_ref</span><span class="p">::</span><span class="o">&lt;</span><span class="n">BinaryArray</span><span class="o">&gt;</span><span class="p">()</span>
        <span class="nf">.unwrap</span><span class="p">();</span>

    <span class="k">let</span> <span class="n">min</span> <span class="o">=</span> <span class="n">batch</span>
        <span class="nf">.column</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="nf">.as_any</span><span class="p">()</span>
        <span class="py">.downcast_ref</span><span class="p">::</span><span class="o">&lt;</span><span class="n">Float64Array</span><span class="o">&gt;</span><span class="p">()</span>
        <span class="nf">.unwrap</span><span class="p">();</span>

    <span class="k">let</span> <span class="n">max</span> <span class="o">=</span> <span class="n">batch</span>
        <span class="nf">.column</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
        <span class="nf">.as_any</span><span class="p">()</span>
        <span class="py">.downcast_ref</span><span class="p">::</span><span class="o">&lt;</span><span class="n">Float64Array</span><span class="o">&gt;</span><span class="p">()</span>
        <span class="nf">.unwrap</span><span class="p">();</span>

    <span class="k">for</span> <span class="n">i</span> <span class="n">in</span> <span class="mi">0</span><span class="o">..</span><span class="n">batch</span><span class="nf">.num_rows</span><span class="p">()</span> <span class="p">{</span>
        <span class="k">let</span> <span class="n">c1_value</span><span class="p">:</span> <span class="nb">String</span> <span class="o">=</span> <span class="nn">String</span><span class="p">::</span><span class="nf">from_utf8</span><span class="p">(</span><span class="n">c1</span><span class="nf">.value</span><span class="p">(</span><span class="n">i</span><span class="p">)</span><span class="nf">.to_vec</span><span class="p">())</span><span class="nf">.unwrap</span><span class="p">();</span>
        <span class="nd">println!</span><span class="p">(</span><span class="s">"{}, Min: {}, Max: {}"</span><span class="p">,</span> <span class="n">c1_value</span><span class="p">,</span> <span class="n">min</span><span class="nf">.value</span><span class="p">(</span><span class="n">i</span><span class="p">),</span> <span class="n">max</span><span class="nf">.value</span><span class="p">(</span><span class="n">i</span><span class="p">),);</span>
    <span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div>

<h2 id="roadmap">Roadmap</h2>

<p>The roadmap for DataFusion will depend on interest from the Rust community, but here are some of the short term items that are planned:</p>

<ul>
  <li>Extending test coverage of the existing functionality</li>
  <li>Adding support for Parquet data sources</li>
  <li>Implementing more SQL features such as <code class="highlighter-rouge">JOIN</code>, <code class="highlighter-rouge">ORDER BY</code> and <code class="highlighter-rouge">LIMIT</code></li>
  <li>Implement a DataFrame API as an alternative to SQL</li>
  <li>Adding support for partitioning and parallel query execution using Rust’s async and await functionality</li>
  <li>Creating a Docker image to make it easy to use DataFusion as a standalone query tool for interactive and batch queries</li>
</ul>

<h2 id="contributors-welcome">Contributors Welcome!</h2>

<p>If you are excited about being able to use Rust for data science and would like to contribute to this work then there are many ways to get involved. The simplest way to get started is to try out DataFusion against your own data sources and file bug reports for any issues that you find. You could also check out the current <a href="https://cwiki.apache.org/confluence/display/ARROW/Rust+JIRA+Dashboard">list of issues</a> and have a go at fixing one. You can also join the <a href="http://mail-archives.apache.org/mod_mbox/arrow-user/">user mailing list</a> to ask questions.</p>


  </div>

  

  
    
  <div class="blog-post" style="margin-bottom: 4rem">
    
<h1>
  Speeding up R and Apache Spark using Apache Arrow
  <a href="/blog/2019/01/25/r-spark-improvements/" class="permalink" title="Permalink">∞</a>
</h1>



<p>
  <span class="badge badge-secondary">Published</span>
  <span class="published">
    25 Jan 2019
  </span>
  <br />
  <span class="badge badge-secondary">By</span>
  
    Javier Luraschi
  
</p>

    <!--

-->

<p><em><a href="https://github.com/javierluraschi">Javier Luraschi</a> is a software engineer at <a href="https://rstudio.com">RStudio</a></em></p>

<p>Support for Apache Arrow in Apache Spark with R is currently under active
development in the <a href="https://github.com/rstudio/sparklyr">sparklyr</a> and <a href="https://spark.apache.org/docs/latest/sparkr.html">SparkR</a> projects. This post explores early, yet
promising, performance improvements achieved when using R with <a href="https://spark.apache.org">Apache Spark</a>,
Arrow and <code class="highlighter-rouge">sparklyr</code>.</p>

<h1 id="setup">Setup</h1>

<p>Since this work is under active development, install <code class="highlighter-rouge">sparklyr</code> and
<code class="highlighter-rouge">arrow</code> from GitHub as follows:</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">devtools</span><span class="o">::</span><span class="n">install_github</span><span class="p">(</span><span class="s2">"apache/arrow"</span><span class="p">,</span><span class="w"> </span><span class="n">subdir</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"r"</span><span class="p">,</span><span class="w"> </span><span class="n">ref</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"apache-arrow-0.12.0"</span><span class="p">)</span><span class="w">
</span><span class="n">devtools</span><span class="o">::</span><span class="n">install_github</span><span class="p">(</span><span class="s2">"rstudio/sparklyr"</span><span class="p">,</span><span class="w"> </span><span class="n">ref</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"apache-arrow-0.12.0"</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<p>In this benchmark, we will use <a href="https://dplyr.tidyverse.org">dplyr</a>, but similar improvements can
be  expected from using <a href="https://cran.r-project.org/package=DBI">DBI</a>, or <a href="https://spark.rstudio.com/reference/#section-spark-dataframes">Spark DataFrames</a> in <code class="highlighter-rouge">sparklyr</code>.
The local Spark connection and dataframe with 10M numeric rows was
initialized as follows:</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">library</span><span class="p">(</span><span class="n">sparklyr</span><span class="p">)</span><span class="w">
</span><span class="n">library</span><span class="p">(</span><span class="n">dplyr</span><span class="p">)</span><span class="w">

</span><span class="n">sc</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">spark_connect</span><span class="p">(</span><span class="n">master</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"local"</span><span class="p">,</span><span class="w"> </span><span class="n">config</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">list</span><span class="p">(</span><span class="s2">"sparklyr.shell.driver-memory"</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"6g"</span><span class="p">))</span><span class="w">
</span><span class="n">data</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">data.frame</span><span class="p">(</span><span class="n">y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">runif</span><span class="p">(</span><span class="m">10</span><span class="o">^</span><span class="m">7</span><span class="p">,</span><span class="w"> </span><span class="m">0</span><span class="p">,</span><span class="w"> </span><span class="m">1</span><span class="p">))</span><span class="w">
</span></code></pre></div></div>

<h1 id="copying">Copying</h1>

<p>Currently, copying data to Spark using <code class="highlighter-rouge">sparklyr</code> is performed by persisting
data on-disk from R and reading it back from Spark. This was meant to be used
for small datasets since there are better tools to transfer data into
distributed storage systems. Nevertheless, many users have requested support to
transfer more data at fast speeds into Spark.</p>

<p>Using <code class="highlighter-rouge">arrow</code> with <code class="highlighter-rouge">sparklyr</code>, we can transfer data directly from R to
Spark without having to serialize this data in R or persist in disk.</p>

<p>The following example copies 10M rows from R into Spark using <code class="highlighter-rouge">sparklyr</code>
with and without <code class="highlighter-rouge">arrow</code>, there is close to a 16x improvement using <code class="highlighter-rouge">arrow</code>.</p>

<p>This benchmark uses the <a href="https://CRAN.R-project.org/package=microbenchmark">microbenchmark</a> R package, which runs code
multiple times, provides stats on total execution time and plots each
excecution time to understand the distribution over each iteration.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">microbenchmark</span><span class="o">::</span><span class="n">microbenchmark</span><span class="p">(</span><span class="w">
  </span><span class="n">setup</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">library</span><span class="p">(</span><span class="n">arrow</span><span class="p">),</span><span class="w">
  </span><span class="n">arrow_on</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="n">sparklyr_df</span><span class="w"> </span><span class="o">&lt;&lt;-</span><span class="w"> </span><span class="n">copy_to</span><span class="p">(</span><span class="n">sc</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="p">,</span><span class="w"> </span><span class="n">overwrite</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nb">T</span><span class="p">)</span><span class="w">
    </span><span class="n">count</span><span class="p">(</span><span class="n">sparklyr_df</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span><span class="n">collect</span><span class="p">()</span><span class="w">
  </span><span class="p">},</span><span class="w">
  </span><span class="n">arrow_off</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="s2">"arrow"</span><span class="w"> </span><span class="o">%in%</span><span class="w"> </span><span class="n">.packages</span><span class="p">())</span><span class="w"> </span><span class="n">detach</span><span class="p">(</span><span class="s2">"package:arrow"</span><span class="p">)</span><span class="w">
    </span><span class="n">sparklyr_df</span><span class="w"> </span><span class="o">&lt;&lt;-</span><span class="w"> </span><span class="n">copy_to</span><span class="p">(</span><span class="n">sc</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="p">,</span><span class="w"> </span><span class="n">overwrite</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nb">T</span><span class="p">)</span><span class="w">
    </span><span class="n">count</span><span class="p">(</span><span class="n">sparklyr_df</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span><span class="n">collect</span><span class="p">()</span><span class="w">
  </span><span class="p">},</span><span class="w">
  </span><span class="n">times</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">10</span><span class="w">
</span><span class="p">)</span><span class="w"> </span><span class="o">%T&gt;%</span><span class="w"> </span><span class="n">print</span><span class="p">()</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span><span class="n">ggplot2</span><span class="o">::</span><span class="n">autoplot</span><span class="p">()</span><span class="w">
</span></code></pre></div></div>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code> Unit: seconds
      expr       min        lq       mean    median         uq       max neval
  arrow_on  3.011515  4.250025   7.257739  7.273011   8.974331  14.23325    10
 arrow_off 50.051947 68.523081 119.946947 71.898908 138.743419 390.44028    10
</code></pre></div></div>

<div align="center">
<img src="/img/arrow-r-spark-copying.png" alt="Copying data with R into Spark with and without Arrow" width="60%" class="img-responsive" />
</div>

<h1 id="collecting">Collecting</h1>

<p>Similarly, <code class="highlighter-rouge">arrow</code> with <code class="highlighter-rouge">sparklyr</code> can now avoid deserializing data in R
while collecting data from Spark into R. These improvements are not as
significant as copying data since, <code class="highlighter-rouge">sparklyr</code> already collects data in
columnar format.</p>

<p>The following benchmark collects 10M rows from Spark into R and shows that
<code class="highlighter-rouge">arrow</code> can bring 3x improvements.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">microbenchmark</span><span class="o">::</span><span class="n">microbenchmark</span><span class="p">(</span><span class="w">
  </span><span class="n">setup</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">library</span><span class="p">(</span><span class="n">arrow</span><span class="p">),</span><span class="w">
  </span><span class="n">arrow_on</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="n">collect</span><span class="p">(</span><span class="n">sparklyr_df</span><span class="p">)</span><span class="w">
  </span><span class="p">},</span><span class="w">
  </span><span class="n">arrow_off</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="s2">"arrow"</span><span class="w"> </span><span class="o">%in%</span><span class="w"> </span><span class="n">.packages</span><span class="p">())</span><span class="w"> </span><span class="n">detach</span><span class="p">(</span><span class="s2">"package:arrow"</span><span class="p">)</span><span class="w">
    </span><span class="n">collect</span><span class="p">(</span><span class="n">sparklyr_df</span><span class="p">)</span><span class="w">
  </span><span class="p">},</span><span class="w">
  </span><span class="n">times</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">10</span><span class="w">
</span><span class="p">)</span><span class="w"> </span><span class="o">%T&gt;%</span><span class="w"> </span><span class="n">print</span><span class="p">()</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span><span class="n">ggplot2</span><span class="o">::</span><span class="n">autoplot</span><span class="p">()</span><span class="w">
</span></code></pre></div></div>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Unit: seconds
      expr      min        lq      mean    median        uq       max neval
  arrow_on 4.520593  5.609812  6.154509  5.928099  6.217447  9.432221    10
 arrow_off 7.882841 13.358113 16.670708 16.127704 21.051382 24.373331    10
</code></pre></div></div>

<div align="center">
<img src="/img/arrow-r-spark-collecting.png" alt="Collecting data with R from Spark with and without Arrow" width="60%" class="img-responsive" />
</div>

<h1 id="transforming">Transforming</h1>

<p>Today, custom transformations of data using R functions are performed in
<code class="highlighter-rouge">sparklyr</code> by moving data in row-format from Spark into an R process through a
socket connection, transferring data in row-format is inefficient since
multiple data types need to be deserialized over each row, then the data gets
converted to columnar format (R was originally designed to use columnar data),
once R finishes this computation, data is again converted to row-format,
serialized row-by-row and then sent back to Spark over the socket connection.</p>

<p>By adding support for <code class="highlighter-rouge">arrow</code> in <code class="highlighter-rouge">sparklyr</code>, it makes Spark perform the
row-format to column-format conversion in parallel in Spark. Data
is then transferred through the socket but no custom serialization takes place.
All the R process needs to do is copy this data from the socket into its heap,
transform it and copy it back to the socket connection.</p>

<p>The following example transforms 100K rows with and without <code class="highlighter-rouge">arrow</code> enabled,
<code class="highlighter-rouge">arrow</code> makes transformation with R functions close to 41x faster.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">microbenchmark</span><span class="o">::</span><span class="n">microbenchmark</span><span class="p">(</span><span class="w">
  </span><span class="n">setup</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">library</span><span class="p">(</span><span class="n">arrow</span><span class="p">),</span><span class="w">
  </span><span class="n">arrow_on</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="n">sample_n</span><span class="p">(</span><span class="n">sparklyr_df</span><span class="p">,</span><span class="w"> </span><span class="m">10</span><span class="o">^</span><span class="m">5</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span><span class="n">spark_apply</span><span class="p">(</span><span class="o">~</span><span class="w"> </span><span class="n">.x</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="m">2</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span><span class="n">count</span><span class="p">()</span><span class="w">
  </span><span class="p">},</span><span class="w">
  </span><span class="n">arrow_off</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="s2">"arrow"</span><span class="w"> </span><span class="o">%in%</span><span class="w"> </span><span class="n">.packages</span><span class="p">())</span><span class="w"> </span><span class="n">detach</span><span class="p">(</span><span class="s2">"package:arrow"</span><span class="p">)</span><span class="w">
    </span><span class="n">sample_n</span><span class="p">(</span><span class="n">sparklyr_df</span><span class="p">,</span><span class="w"> </span><span class="m">10</span><span class="o">^</span><span class="m">5</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span><span class="n">spark_apply</span><span class="p">(</span><span class="o">~</span><span class="w"> </span><span class="n">.x</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="m">2</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span><span class="n">count</span><span class="p">()</span><span class="w">
  </span><span class="p">},</span><span class="w">
  </span><span class="n">times</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">10</span><span class="w">
</span><span class="p">)</span><span class="w"> </span><span class="o">%T&gt;%</span><span class="w"> </span><span class="n">print</span><span class="p">()</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span><span class="n">ggplot2</span><span class="o">::</span><span class="n">autoplot</span><span class="p">()</span><span class="w">
</span></code></pre></div></div>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Unit: seconds
      expr        min         lq       mean     median         uq        max neval
  arrow_on   3.881293   4.038376   5.136604   4.772739   5.759082   7.873711    10
 arrow_off 178.605733 183.654887 213.296238 227.182018 233.601885 238.877341    10
</code></pre></div></div>

<div align="center">
<img src="/img/arrow-r-spark-transforming.png" alt="Transforming data with R in Spark with and without Arrow" width="60%" class="img-responsive" />
</div>

<p>Additional benchmarks and fine-tuning parameters can be found under <code class="highlighter-rouge">sparklyr</code>
<a href="https://github.com/rstudio/sparklyr/pull/1611">/rstudio/sparklyr/pull/1611</a> and <code class="highlighter-rouge">SparkR</code> <a href="https://github.com/apache/spark/pull/22954">/apache/spark/pull/22954</a>. Looking forward to bringing this feature
to the Spark, Arrow and R communities.</p>


  </div>

  

  
    
  <div class="blog-post" style="margin-bottom: 4rem">
    
<h1>
  Apache Arrow 0.12.0 Release
  <a href="/blog/2019/01/21/0.12.0-release/" class="permalink" title="Permalink">∞</a>
</h1>



<p>
  <span class="badge badge-secondary">Published</span>
  <span class="published">
    21 Jan 2019
  </span>
  <br />
  <span class="badge badge-secondary">By</span>
  
    <a href="https://wesmckinney.com">Wes McKinney (wesm) </a>
  
</p>

    <!--

-->

<p>The Apache Arrow team is pleased to announce the 0.12.0 release. This is the
largest release yet in the project, covering 3 months of development work and
includes <a href="https://issues.apache.org/jira/issues/?jql=project%20%3D%20ARROW%20AND%20status%20in%20(Resolved%2C%20Closed)%20AND%20fixVersion%20%3D%200.12.0"><strong>614 resolved issues</strong></a> from <a href="https://arrow.apache.org/release/0.12.0.html#contributors"><strong>77 distinct contributors</strong></a>.</p>

<p>See the <a href="https://arrow.apache.org/install">Install Page</a> to learn how to get the libraries for your
platform. The <a href="https://arrow.apache.org/release/0.12.0.html">complete changelog</a> is also available.</p>

<p>It’s a huge release, but we’ll give some brief highlights and new from the
project to help guide you to the parts of the project that may be of interest.</p>

<h2 id="new-committers-and-pmc-member">New committers and PMC member</h2>

<p>The Arrow team is growing! Since the 0.11.0 release we have added 3 new
committers:</p>

<ul>
  <li><a href="https://github.com/sbinet">Sebastien Binet</a>, who has mainly worked on the Go implementation</li>
  <li><a href="https://github.com/romainfrancois">Romain Francois</a>, who has mainly worked on the R implementation</li>
  <li><a href="https://github.com/shiro615">Yosuke Shiro</a>, who has mainly worked on the GLib (C) and Ruby
implementations</li>
</ul>

<p>We also pleased to announce that <a href="https://github.com/kszucs">Krisztián Szűcs</a> has been promoted
from committer to PMC (Project Management Committee) member.</p>

<p>Thank you for all your contributions!</p>

<h2 id="code-donations">Code donations</h2>

<p>Since the last release, we have received 3 code donations into the Apache
project.</p>

<ul>
  <li>A <a href="http://incubator.apache.org/ip-clearance/arrow-csharp-library.html">native C# .NET library</a> donated by Feyen Zylstra LLC.</li>
  <li>A <a href="http://incubator.apache.org/ip-clearance/arrow-parquet-ruby.html">Ruby library for Parquet files</a> which uses the existing GLib bindings to
the C++ Parquet library.</li>
  <li>A <a href="http://incubator.apache.org/ip-clearance/arrow-parquet-rust.html">native Rust Parquet library</a></li>
</ul>

<p>We are excited to continue to grow the Apache Arrow development community.</p>

<h2 id="combined-project-level-documentation">Combined project-level documentation</h2>

<p>Since the last release, we have merged the Python and C++ documentation to
create a combined project-wide documentation site:
https://arrow.apache.org/docs. There is now some prose documentation about many
parts of the C++ library. We intend to keep adding documentation for other
parts of Apache Arrow to this site.</p>

<h2 id="packages">Packages</h2>

<p>We start providing the official APT and Yum repositories for C++ and
GLib (C). See the <a href="https://arrow.apache.org/install/">install document</a> for details.</p>

<h2 id="c-notes">C++ notes</h2>

<p>Much of the C++ development work the last 3 months concerned internal code
refactoring and performance improvements. Some user-visible highlights of note:</p>

<ul>
  <li>Experimental support for <a href="https://github.com/apache/arrow/blob/master/cpp/src/arrow/sparse_tensor.h">in-memory sparse tensors (or ndarrays)</a>, with
support for zero-copy IPC</li>
  <li>Support for building on Alpine Linux</li>
  <li>Significantly improved hash table utilities, with improved hash table
performance in many parts of the library</li>
  <li>IO library improvements for both read and write buffering</li>
  <li>A fast <a href="https://github.com/apache/arrow/blob/master/cpp/src/arrow/util/trie.h">trie implementation</a> for string searching</li>
  <li>Many improvements to the parallel CSV reader in performance and features. See
the changelog</li>
</ul>

<p>Since the LLVM-based Gandiva expression compiler was donated to Apache Arrow
during the last release cycle, development there has been moving along. We
expect to have Windows support for Gandiva and to ship this in downstream
packages (like Python) in the 0.13 release time frame.</p>

<h2 id="go-notes">Go notes</h2>

<p>The Arrow Go development team has been expanding. The Go library has gained
support for many missing features from the columnar format as well as semantic
constructs like chunked arrays and tables that are used heavily in the C++
project.</p>

<h2 id="glib-and-ruby-notes">GLib and Ruby notes</h2>

<p>Development of the GLib-based C bindings and corresponding Ruby interfaces have
advanced in lock-step with the C++, Python, and R libraries. In this release,
there are many new features in C and Ruby:</p>

<ul>
  <li>Compressed file read/write support</li>
  <li>Support for using the C++ parallel CSV reader</li>
  <li>Feather file support</li>
  <li>Gandiva bindings</li>
  <li>Plasma bindings</li>
</ul>

<h2 id="python-notes">Python notes</h2>

<p>We fixed a ton of bugs and made many improvements throughout the Python
project. Some highlights from the Python side include:</p>

<ul>
  <li>Python 3.7 support: wheels and conda packages are now available for Python
3.7</li>
  <li>Substantially improved memory use when converting strings types to pandas
format, including when reading Parquet files. Parquet users should notice
significantly lower memory use in common use cases</li>
  <li>Support for reading and writing compressed files, can be used for CSV files,
IPC, or any other form of IO</li>
  <li>The new <code class="highlighter-rouge">pyarrow.input_stream</code> and <code class="highlighter-rouge">pyarrow.output_stream</code> functions support
read and write buffering. This is analogous to <code class="highlighter-rouge">BufferedIOBase</code> from the
Python standard library, but the internals are implemented natively in C++.</li>
  <li>Gandiva (LLVM expression compiler) bindings, though not yet available in
pip/conda yet. Look for this in 0.13.0.</li>
  <li>Many improvements to Arrow CUDA integration, including interoperability with
Numba</li>
</ul>

<h2 id="r-notes">R notes</h2>

<p>The R library made huge progress in 0.12, with work led by new committer Romain
Francois. The R project’s features are not far behind the Python library, and
we are hoping to be able to make the R library available to CRAN users for use
with Apache Spark or for reading and writing Parquet files over the next
quarter.</p>

<p>Users of the <code class="highlighter-rouge">feather</code> R library will see significant speed increases in many
cases when reading Feather files with the new Arrow R library.</p>

<h2 id="rust-notes">Rust notes</h2>

<p>Rust development had an active last 3 months; see the changelog for details.</p>

<p>A native Rust implementation was just donated to the project, and the community
intends to provide a similar level of functionality for reading and writing
Parquet files using the Arrow in-memory columnar format as an intermediary.</p>

<h2 id="upcoming-roadmap-outlook-for-2019">Upcoming Roadmap, Outlook for 2019</h2>

<p>Apache Arrow has become a large, diverse open source project. It is now being
used in dozens of downstream open source and commercial projects. Work will be
proceeding in many areas in 2019:</p>

<ul>
  <li>Development of in-memory query execution engines (e.g. in C++, Rust)</li>
  <li>Expanded support for reading and writing the Apache Parquet format, and other
common data formats like Apache Avro, CSV, JSON, and Apache ORC.</li>
  <li>New Flight RPC system for fast messaging of Arrow datasets</li>
  <li>Expanded support in existing programming languages</li>
  <li>New programming language bindings or native implementations</li>
</ul>

<p>It promises to be an exciting 2019. We look forward to having you involved in
the development community.</p>


  </div>

  

  
    
  <div class="blog-post" style="margin-bottom: 4rem">
    
<h1>
  Gandiva: A LLVM-based Analytical Expression Compiler for Apache Arrow
  <a href="/blog/2018/12/05/gandiva-donation/" class="permalink" title="Permalink">∞</a>
</h1>



<p>
  <span class="badge badge-secondary">Published</span>
  <span class="published">
    05 Dec 2018
  </span>
  <br />
  <span class="badge badge-secondary">By</span>
  
    <a href="http://github.com/jacques-n">Jacques Nadeau (jacques) </a>
  
</p>

    <!--

-->

<p>Today we’re happy to announce that the Gandiva Initiative for Apache Arrow, an
LLVM-based execution kernel, is now part of the Apache Arrow project. Gandiva
was kindly donated by <a href="https://www.dremio.com/">Dremio</a>, where it was
originally developed and open-sourced. Gandiva extends Arrow’s capabilities to
provide high performance analytical execution and is composed of two main
components:</p>

<ul>
  <li>
    <p>A runtime expression compiler leveraging LLVM</p>
  </li>
  <li>
    <p>A high performance execution environment</p>
  </li>
</ul>

<p>Gandiva works as follows: applications submit an expression tree to the
compiler, built in a language agnostic protobuf-based expression
representation. From there, Gandiva then compiles the expression tree to native
code for the current runtime environment and hardware. Once compiled, the
Gandiva execution kernel then consumes and produces Arrow columnar batches. The
generated code is highly optimized for parallel processing on modern CPUs. For
example, on AVX-128 processors Gandiva can process 8 pairs of 2 byte values in
a single vectorized operation, and on AVX-512 processors Gandiva can process 4x
as many values in a single operation. Gandiva is built from the ground up to
understand Arrow’s in-memory representation and optimize processing against it.</p>

<p>While Gandiva is just starting within the Arrow community, it already supports
hundreds of <a href="https://github.com/apache/arrow/blob/master/cpp/src/gandiva/function_registry.cc">expressions</a>, ranging from math functions to case
statements. Gandiva was built as a standalone C++ library built on top of the
core Apache Arrow codebase and was donated with C++ and Java APIs construction
and execution APIs for projection and filtering operations. The Arrow community
is already looking to expand Gandiva’s capabilities. This will include
incorporating more operations and supporting many new language bindings. As an
example, multiple community members are already actively building new language
bindings that allow use of Gandiva within Python and Ruby.</p>

<p>While young within the Arrow community, Gandiva is already shipped and used in
production by many Dremio customers as part of Dremio’s execution
engine. Experiments have demonstrated <a href="https://www.dremio.com/gandiva-performance-improvements-production-query/">70x performance improvement</a> on many
SQL queries. We expect to see similar performance gains for many other projects
that leverage Arrow.</p>

<p>The Arrow community is working to ship the first formal Apache Arrow release
that includes Gandiva, and we hope this will be available within the next
couple months. This should make it much easier for the broader analytics and
data science development communities to leverage runtime code generation for
high-performance data processing in a variety of contexts and projects.</p>

<p>We started the Arrow project a couple of years ago with the objective of
creating an industry-standard columnar in-memory data representation for
analytics. Within this short period of time, Apache Arrow has been adopted by
dozens of both open source and commercial software products. Some key examples
include technologies such as Apache Spark, Pandas, Nvidia RAPIDS, Dremio, and
InfluxDB. This success has driven Arrow to now be downloaded more than 1
million times per month. Over 200 developers have already contributed to Apache
Arrow. If you’re interested in contributing to Gandiva or any other part of the
Apache Arrow project, feel free to reach out on the mailing list and join us!</p>

<p>For additional technical details on Gandiva, you can check out some of the
following resources:</p>

<ul>
  <li>
    <p><a href="https://www.dremio.com/announcing-gandiva-initiative-for-apache-arrow/">https://www.dremio.com/announcing-gandiva-initiative-for-apache-arrow/</a></p>
  </li>
  <li>
    <p><a href="https://www.dremio.com/gandiva-performance-improvements-production-query/">https://www.dremio.com/gandiva-performance-improvements-production-query/</a></p>
  </li>
  <li>
    <p><a href="https://www.dremio.com/webinars/vectorized-query-processing-apache-arrow/">https://www.dremio.com/webinars/vectorized-query-processing-apache-arrow/</a></p>
  </li>
  <li>
    <p><a href="https://www.dremio.com/adding-a-user-define-function-to-gandiva/">https://www.dremio.com/adding-a-user-define-function-to-gandiva/</a></p>
  </li>
</ul>


  </div>

  

  
    
  <div class="blog-post" style="margin-bottom: 4rem">
    
<h1>
  Apache Arrow 0.11.0 Release
  <a href="/blog/2018/10/09/0.11.0-release/" class="permalink" title="Permalink">∞</a>
</h1>



<p>
  <span class="badge badge-secondary">Published</span>
  <span class="published">
    09 Oct 2018
  </span>
  <br />
  <span class="badge badge-secondary">By</span>
  
    <a href="https://wesmckinney.com">Wes McKinney (wesm) </a>
  
</p>

    <!--

-->

<p>The Apache Arrow team is pleased to announce the 0.11.0 release. It is the
product of 2 months of development and includes <a href="https://issues.apache.org/jira/issues/?jql=project%20%3D%20ARROW%20AND%20status%20in%20(Resolved%2C%20Closed)%20AND%20fixVersion%20%3D%200.11.0"><strong>287 resolved
issues</strong></a>.</p>

<p>See the <a href="https://arrow.apache.org/install">Install Page</a> to learn how to get the libraries for your
platform. The <a href="https://arrow.apache.org/release/0.11.0.html">complete changelog</a> is also available.</p>

<p>We discuss some highlights from the release and other project news in this
post.</p>

<h2 id="arrow-flight-rpc-and-messaging-framework">Arrow Flight RPC and Messaging Framework</h2>

<p>We are developing a new Arrow-native RPC framework, Arrow Flight, based on
<a href="http://grpc.io">gRPC</a> for high performance Arrow-based messaging. Through low-level
extensions to gRPC’s internal memory management, we are able to avoid expensive
parsing when receiving datasets over the wire, unlocking unprecedented levels
of performance in moving datasets from one machine to another. We will be
writing more about Flight on the Arrow blog in the future.</p>

<p>Prototype implementations are available in Java and C++, and we will be focused
in the coming months on hardening the Flight RPC framework for enterprise-grade
production use cases.</p>

<h2 id="parquet-and-arrow-c-communities-joining-forces">Parquet and Arrow C++ communities joining forces</h2>

<p>After discussion over the last year, the Apache Arrow and Apache Parquet C++
communities decide to merge the Parquet C++ codebase into the Arrow C++
codebase and work together in a “monorepo” structure. This should result in
better developer productivity in core Parquet work as well as in Arrow
integration.</p>

<p>Before this codebase merge, we had a circular dependency between the Arrow and
Parquet codebases, since the Parquet C++ library is used in the Arrow Python
library.</p>

<h2 id="gandiva-llvm-expression-compiler-donation">Gandiva LLVM Expression Compiler donation</h2>

<p><a href="http://dremio.com">Dremio Corporation</a> has donated the <a href="http://github.com/dremio/gandiva">Gandiva</a> LLVM expression compiler
to Apache Arrow. We will be working on cross-platform builds, packaging, and
language bindings (e.g. in Python) for Gandiva in the upcoming 0.12 release and
beyond. We will write more about Gandiva in the future.</p>

<h2 id="parquet-c-glib-bindings-donation">Parquet C GLib Bindings Donation</h2>

<p>PMC member <a href="https://github.com/kou">Kouhei Sutou</a> has donated GLib bindings for the Parquet C++
libraries, which are designed to work together with the existing Arrow GLib
bindings.</p>

<h2 id="c-csv-reader-project">C++ CSV Reader Project</h2>

<p>We have begun developing a general purpose multithreaded CSV file parser in
C++. The purpose of this library is to parse and convert comma-separated text
files into Arrow columnar record batches as efficiently as possible. The
prototype version features Python bindings, and any language that can use the
C++ libraries (including C, R, and Ruby).</p>

<h2 id="new-matlab-bindings">New MATLAB bindings</h2>

<p><a href="https://mathworks.com">The MathWorks</a> has contributed an initial MEX file binding to the Arrow
C++ libraries. Initially, it is possible to read Arrow-based Feather files in
MATLAB. We are looking forward to seeing more developments for MATLAB users.</p>

<h2 id="r-library-in-development">R Library in Development</h2>

<p>The community has begun implementing <a href="https://github.com/apache/arrow/tree/master/r">R language bindings and interoperability</a>
with the Arrow C++ libraries. This will include support for zero-copy shared
memory IPC and other tools needed to improve R integration with Apache Spark
and more.</p>

<h2 id="support-for-cuda-based-gpus-in-python">Support for CUDA-based GPUs in Python</h2>

<p>This release includes Python bindings to the Arrow CUDA integration C++
library. This work is targeting interoperability with <a href="https://github.com/numba/numba">Numba</a> and the <a href="http://gpuopenanalytics.com/">GPU
Open Analytics Initiative</a>.</p>

<h2 id="upcoming-roadmap">Upcoming Roadmap</h2>

<p>In the coming months, we will continue to make progress on many fronts, with
Gandiva packaging, expanded language support (especially in R), and improved
data access (e.g. CSV, Parquet files) in focus.</p>


  </div>

  

  
    
  <div class="blog-post" style="margin-bottom: 4rem">
    
<h1>
  Apache Arrow 0.10.0 Release
  <a href="/blog/2018/08/07/0.10.0-release/" class="permalink" title="Permalink">∞</a>
</h1>



<p>
  <span class="badge badge-secondary">Published</span>
  <span class="published">
    07 Aug 2018
  </span>
  <br />
  <span class="badge badge-secondary">By</span>
  
    <a href="https://wesmckinney.com">Wes McKinney (wesm) </a>
  
</p>

    <!--

-->

<p>The Apache Arrow team is pleased to announce the 0.10.0 release. It is the
product of over 4 months of development and includes <a href="https://issues.apache.org/jira/issues/?jql=project%20%3D%20ARROW%20AND%20status%20in%20(Resolved%2C%20Closed)%20AND%20fixVersion%20%3D%200.10.0"><strong>470 resolved
issues</strong></a>. It is the largest release so far in the project’s history. 90
individuals contributed to this release.</p>

<p>See the <a href="https://arrow.apache.org/install">Install Page</a> to learn how to get the libraries for your
platform. The <a href="https://arrow.apache.org/release/0.10.0.html">complete changelog</a> is also available.</p>

<p>We discuss some highlights from the release and other project news in this
post.</p>

<h2 id="offical-binary-packages-and-packaging-automation">Offical Binary Packages and Packaging Automation</h2>

<p>One of the largest projects in this release cycle was automating our build and
packaging tooling to be able to easily and reproducibly create a <a href="https://www.apache.org/dyn/closer.cgi/arrow/arrow-0.10.0/binaries">comprehensive
set of binary artifacts</a> which have been approved and released by the Arrow
PMC. We developed a tool called <strong>Crossbow</strong> which uses Appveyor and Travis CI
to build each of the different supported packages on all 3 platforms (Linux,
macOS, and Windows). As a result of our efforts, we should be able to make more
frequent Arrow releases. This work was led by Phillip Cloud, Kouhei Sutou, and
Krisztián Szűcs. Bravo!</p>

<h2 id="new-programming-languages-go-ruby-rust">New Programming Languages: Go, Ruby, Rust</h2>

<p>This release also adds 3 new programming languages to the project: Go, Ruby,
and Rust. Together with C, C++, Java, JavaScript, and Python, <strong>we now have
some level of support for 8 programming languages</strong>.</p>

<h2 id="upcoming-roadmap">Upcoming Roadmap</h2>

<p>In the coming months, we will be working to move Apache Arrow closer to a 1.0.0
release. We will continue to grow new features, improve performance and
stability, and expand support for currently supported and new programming
languages.</p>


  </div>

  

  
    
  <div class="blog-post" style="margin-bottom: 4rem">
    
<h1>
  Faster, scalable memory allocations in Apache Arrow with jemalloc
  <a href="/blog/2018/07/20/jemalloc/" class="permalink" title="Permalink">∞</a>
</h1>



<p>
  <span class="badge badge-secondary">Published</span>
  <span class="published">
    20 Jul 2018
  </span>
  <br />
  <span class="badge badge-secondary">By</span>
  
    <a href="http://github.com/xhochy">Uwe Korn (uwe) </a>
  
</p>

    <!--

-->

<p>With the release of the 0.9 version of Apache Arrow, we have switched our
default allocator for array buffers from the system allocator to jemalloc on
OSX and Linux. This applies to the C++/GLib/Python implementations of Arrow.
In most cases changing the default allocator is normally done to avoid problems
that occur with many small, frequent (de)allocations. In contrast, in Arrow we
normally deal with large in-memory datasets. While jemalloc provides good
strategies for <a href="https://zapier.com/engineering/celery-python-jemalloc/">avoiding RAM fragmentation for allocations that are lower than
a memory page (4kb)</a>, it also provides functionality that improves
performance on allocations that span several memory pages.</p>

<p>Outside of Apache Arrow, <a href="https://www.facebook.com/notes/facebook-engineering/scalable-memory-allocation-using-jemalloc/480222803919/">jemalloc powers the infrastructure of Facebook</a>
(this is also where most of its development happens). It is also used as the
<a href="https://github.com/rust-lang/rust/pull/6895">default allocator in Rust</a> as well as it helps <a href="http://download.redis.io/redis-stable/README.md">Redis reduce the memory
fragmentation on Linux</a> (“Allocator”).</p>

<p>One allocation specialty that we require in Arrow is that memory should be
64byte aligned. This is so that we can get the most performance out of SIMD
instruction sets like AVX. While the most modern SIMD instructions also work on
unaligned memory, their performance is much better on aligned memory. To get the
best performance for our analytical applications, we want all memory to be
allocated such that SIMD performance is maximized.</p>

<p>For aligned allocations, the POSIX APIs only provide the
<code class="highlighter-rouge">aligned_alloc(void** ptr, size_t alignment, size_t size)</code> function to
allocate aligned memory. There is also 
<code class="highlighter-rouge">posix_memalign(void **ptr, size_t alignment, size_t size)</code> to modify an
allocation to the preferred alignment. But neither of them cater for expansions
of the allocation. While the <code class="highlighter-rouge">realloc</code> function can often expand allocations
without moving them physically, it does not ensure that in the case the
allocation is moved that the alignment is kept.</p>

<p>In the case when Arrow was built without jemalloc being enabled, this resulted
in copying the data on each new expansion of an allocation. To reduce the number
of memory copies, we use jemalloc’s <code class="highlighter-rouge">*allocx()</code>-APIs to create, modify and free
aligned allocations. One of the typical tasks where this gives us a major
speedup is on the incremental construction of an Arrow table that consists of
several columns. We often don’t know the size of the table in advance and need
to expand our allocations as the data is loaded.</p>

<p>To incrementally build a vector using memory expansion of a factor of 2, we
would use the following C-code with the standard POSIX APIs:</p>

<div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">size_t</span> <span class="n">size</span> <span class="o">=</span> <span class="mi">128</span> <span class="o">*</span> <span class="mi">1024</span><span class="p">;</span>
<span class="kt">void</span><span class="o">*</span> <span class="n">ptr</span> <span class="o">=</span> <span class="n">aligned_alloc</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="n">size</span><span class="p">);</span>
<span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="mi">10</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
  <span class="kt">size_t</span> <span class="n">new_size</span> <span class="o">=</span> <span class="n">size</span> <span class="o">*</span> <span class="mi">2</span><span class="p">;</span>
  <span class="kt">void</span><span class="o">*</span> <span class="n">ptr2</span> <span class="o">=</span> <span class="n">aligned_alloc</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="n">new_size</span><span class="p">);</span>
  <span class="n">memcpy</span><span class="p">(</span><span class="n">ptr2</span><span class="p">,</span> <span class="n">ptr</span><span class="p">,</span> <span class="n">size</span><span class="p">);</span>
  <span class="n">free</span><span class="p">(</span><span class="n">ptr</span><span class="p">);</span>
  <span class="n">ptr</span> <span class="o">=</span> <span class="n">ptr2</span><span class="p">;</span>
  <span class="n">size</span> <span class="o">=</span> <span class="n">new_size</span><span class="p">;</span>
<span class="p">}</span>
<span class="n">free</span><span class="p">(</span><span class="n">ptr</span><span class="p">);</span>
</code></pre></div></div>

<p>With jemalloc’s special APIs, we are able to omit the explicit call to <code class="highlighter-rouge">memcpy</code>.
In the case where a memory expansion cannot be done in-place, it is still called
by the allocator but not needed on all occasions. This simplifies our user code
to:</p>

<div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">size_t</span> <span class="n">size</span> <span class="o">=</span> <span class="mi">128</span> <span class="o">*</span> <span class="mi">1024</span><span class="p">;</span>
<span class="kt">void</span><span class="o">*</span> <span class="n">ptr</span> <span class="o">=</span> <span class="n">mallocx</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">MALLOCX_ALIGN</span><span class="p">(</span><span class="mi">64</span><span class="p">));</span>
<span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="mi">10</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
  <span class="n">size</span> <span class="o">*=</span> <span class="mi">2</span><span class="p">;</span>
  <span class="n">ptr</span> <span class="o">=</span> <span class="n">rallocx</span><span class="p">(</span><span class="n">ptr</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">MALLOCX_ALIGN</span><span class="p">(</span><span class="mi">64</span><span class="p">));</span>
<span class="p">}</span>
<span class="n">dallocx</span><span class="p">(</span><span class="n">ptr</span><span class="p">,</span> <span class="n">MALLOCX_ALIGN</span><span class="p">(</span><span class="mi">64</span><span class="p">));</span>
</code></pre></div></div>

<p>To see the real world benefits of using jemalloc, we look at the benchmarks in
Arrow C++. There we have modeled a typical use case of incrementally building up
an array of primitive values. For the build-up of the array, we don’t know the
number of elements in the final array so we need to continuously expand the
memory region in which the data is stored. The code for this benchmark is part
of the <code class="highlighter-rouge">builder-benchmark</code> in the Arrow C++ sources as
<code class="highlighter-rouge">BuildPrimitiveArrayNoNulls</code>.</p>

<p>Runtimes without <code class="highlighter-rouge">jemalloc</code>:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>BM_BuildPrimitiveArrayNoNulls/repeats:3                 636726 us   804.114MB/s
BM_BuildPrimitiveArrayNoNulls/repeats:3                 621345 us   824.019MB/s
BM_BuildPrimitiveArrayNoNulls/repeats:3                 625008 us    819.19MB/s
BM_BuildPrimitiveArrayNoNulls/repeats:3_mean            627693 us   815.774MB/s
BM_BuildPrimitiveArrayNoNulls/repeats:3_median          625008 us    819.19MB/s
BM_BuildPrimitiveArrayNoNulls/repeats:3_stddev            8034 us   10.3829MB/s
</code></pre></div></div>

<p>Runtimes with <code class="highlighter-rouge">jemalloc</code>:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>BM_BuildPrimitiveArrayNoNulls/repeats:3                 630881 us   811.563MB/s
BM_BuildPrimitiveArrayNoNulls/repeats:3                 352891 us   1.41687GB/s
BM_BuildPrimitiveArrayNoNulls/repeats:3                 351039 us   1.42434GB/s
BM_BuildPrimitiveArrayNoNulls/repeats:3_mean            444937 us   1.21125GB/s
BM_BuildPrimitiveArrayNoNulls/repeats:3_median          352891 us   1.41687GB/s
BM_BuildPrimitiveArrayNoNulls/repeats:3_stddev          161035 us   371.335MB/s
</code></pre></div></div>

<p>The benchmark was run three times for each configuration to see the performance
differences. The first run in each configuration yielded the same performance but
in all subsequent runs, the version using jemalloc was about twice as fast. In
these cases, the memory region that was used for constructing the array could be
expanded in place without moving the data around. This was possible as there
were memory pages assigned to the process that were unused but not reclaimed by
the operating system. Without <code class="highlighter-rouge">jemalloc</code>, we cannot make use of them simply by
the fact that the default allocator has no API that provides aligned
reallocation.</p>


  </div>

  

  
    
  <div class="blog-post" style="margin-bottom: 4rem">
    
<h1>
  A Native Go Library for Apache Arrow
  <a href="/blog/2018/03/22/go-code-donation/" class="permalink" title="Permalink">∞</a>
</h1>



<p>
  <span class="badge badge-secondary">Published</span>
  <span class="published">
    22 Mar 2018
  </span>
  <br />
  <span class="badge badge-secondary">By</span>
  
    <a href="https://arrow.apache.org">The Apache Arrow PMC (pmc) </a>
  
</p>

    <!--

-->

<p>Since launching in early 2016, Apache Arrow has been growing fast. We have made
nine major releases through the efforts of over 120 distinct contributors. The
project’s scope has also expanded. We began by focusing on the development of
the standardized in-memory columnar data format, which now serves as a pillar
of the project. Since then, we have been growing into a more general
cross-language platform for in-memory data analysis through new additions to
the project like the <a href="http://arrow.apache.org/blog/2017/08/16/0.6.0-release/">Plasma shared memory object store</a>. A primary goal of
the project is to enable data system developers to process and move data fast.</p>

<p>So far, we officially have developed native Arrow implementations in C++, Java,
and JavaScript. We have created binding layers for the C++ libraries in C
(using the GLib libraries) and Python. We have also seen efforts to develop
interfaces to the Arrow C++ libraries in Go, Lua, Ruby, and Rust. While binding
layers serve many purposes, there can be benefits to native implementations,
and so we’ve been keen to see future work on native implementations in growing
systems languages like Go and Rust.</p>

<p>This past October, engineers <a href="https://github.com/stuartcarnie">Stuart Carnie</a>, <a href="https://github.com/nathanielc">Nathaniel Cook</a>, and
<a href="https://github.com/goller">Chris Goller</a>, employees of <a href="https://influxdata.com">InfluxData</a>, began developing a native [Go
language implementation of the <a href="https://github.com/influxdata/arrow">Apache Arrow</a> in-memory columnar format for
use in Go-based database systems like InfluxDB. We are excited to announce that
InfluxData has <a href="https://www.businesswire.com/news/home/20180322005393/en/InfluxData-Announces-Language-Implementation-Contribution-Apache-Arrow">donated this native Go implementation to the Apache Arrow
project</a>, where it will continue to be developed. This work features
low-level integration with the Go runtime and native support for SIMD
instruction sets. We are looking forward to working more closely with the Go
community on solving in-memory analytics and data interoperability problems.</p>

<div align="center">
<img src="/img/native_go_implementation.png" alt="Apache Arrow implementations and bindings" width="60%" class="img-responsive" />
</div>

<p>One of the mantras in <a href="https://www.apache.org">The Apache Software Foundation</a> is “Community over
Code”. By building an open and collaborative development community across many
programming language ecosystems, we will be able to development better and
longer-lived solutions to the systems problems faced by data developers.</p>

<p>We are excited for what the future holds for the Apache Arrow project. Adding
first-class support for a popular systems programming language like Go is an
important step along the way. We welcome others from the Go community to get
involved in the project. We also welcome others who wish to explore building
Arrow support for other programming languages not yet represented. Learn more
at <a href="https://arrow.apache.org">https://arrow.apache.org</a> and join the mailing list
<a href="https://lists.apache.org/list.html?dev@arrow.apache.org">dev@arrow.apache.org</a>.</p>


  </div>

  

  
    
  <div class="blog-post" style="margin-bottom: 4rem">
    
<h1>
  Apache Arrow 0.9.0 Release
  <a href="/blog/2018/03/22/0.9.0-release/" class="permalink" title="Permalink">∞</a>
</h1>



<p>
  <span class="badge badge-secondary">Published</span>
  <span class="published">
    22 Mar 2018
  </span>
  <br />
  <span class="badge badge-secondary">By</span>
  
    <a href="https://wesmckinney.com">Wes McKinney (wesm) </a>
  
</p>

    <!--

-->

<p>The Apache Arrow team is pleased to announce the 0.9.0 release. It is the
product of over 3 months of development and includes <a href="https://issues.apache.org/jira/issues/?jql=project%20%3D%20ARROW%20AND%20status%20in%20(Resolved%2C%20Closed)%20AND%20fixVersion%20%3D%200.9.0"><strong>260 resolved
JIRAs</strong></a>.</p>

<p>While we made some of backwards-incompatible columnar binary format changes in
last December’s 0.8.0 release, the 0.9.0 release is backwards-compatible with
0.8.0. We will be working toward a 1.0.0 release this year, which will mark
longer-term binary stability for the Arrow columnar format and metadata.</p>

<p>See the <a href="https://arrow.apache.org/install">Install Page</a> to learn how to get the libraries for your
platform. The <a href="https://arrow.apache.org/release/0.8.0.html">complete changelog</a> is also available.</p>

<p>We discuss some highlights from the release and other project news in this
post. This release has been overall focused more on bug fixes, compatibility,
and stability compared with previous releases which have pushed more on new and
expanded features.</p>

<h2 id="new-arrow-committers-and-pmc-members">New Arrow committers and PMC members</h2>

<p>Since the last release, we have added 2 new Arrow committers: <a href="https://github.com/theneuralbit">Brian
Hulette</a> and <a href="https://github.com/robertnishihara">Robert Nishihara</a>. Additionally, <a href="https://github.com/cpcloud">Phillip Cloud</a> and
<a href="https://github.com/pcmoritz">Philipp Moritz</a> have been promoted from committer to PMC
member. Congratulations and thank you for your contributions!</p>

<h2 id="plasma-object-store-improvements">Plasma Object Store Improvements</h2>

<p>The Plasma Object Store now supports managing interprocess shared memory on
CUDA-enabled GPUs. We are excited to see more GPU-related functionality develop
in Apache Arrow, as this has become a key computing environment for scalable
machine learning.</p>

<h2 id="python-improvements">Python Improvements</h2>

<p><a href="https://github.com/pitrou">Antoine Pitrou</a> has joined the Python development efforts and helped
significantly this release with interoperability with built-in CPython data
structures and NumPy structured data types.</p>

<ul>
  <li>New experimental support for reading Apache ORC files</li>
  <li><code class="highlighter-rouge">pyarrow.array</code> now accepts lists of tuples or Python dicts for creating
Arrow struct type arrays.</li>
  <li>NumPy structured dtypes (which are row/record-oriented) can be directly
converted to Arrow struct (column-oriented) arrays</li>
  <li>Python 3.6 <code class="highlighter-rouge">pathlib</code> objects for file paths are now accepted in many file
APIs, including for Parquet files</li>
  <li>Arrow integer arrays with nulls can now be converted to NumPy object arrays
with <code class="highlighter-rouge">None</code> values</li>
  <li>New <code class="highlighter-rouge">pyarrow.foreign_buffer</code> API for interacting with memory blocks located
at particular memory addresses</li>
</ul>

<h2 id="java-improvements">Java Improvements</h2>

<p>Java now fully supports the <code class="highlighter-rouge">FixedSizeBinary</code> data type.</p>

<h2 id="javascript-improvements">JavaScript Improvements</h2>

<p>The JavaScript library has been significantly refactored and expanded. We are
making separate Apache releases (most recently <code class="highlighter-rouge">JS-0.3.1</code>) for JavaScript,
which are being <a href="https://www.npmjs.com/package/apache-arrow">published to NPM</a>.</p>

<h2 id="upcoming-roadmap">Upcoming Roadmap</h2>

<p>In the coming months, we will be working to move Apache Arrow closer to a 1.0.0
release. We will also be discussing plans to develop native Arrow-based
computational libraries within the project.</p>


  </div>

  

  
    
  <div class="blog-post" style="margin-bottom: 4rem">
    
<h1>
  Apache Arrow 0.8.0 Release
  <a href="/blog/2017/12/18/0.8.0-release/" class="permalink" title="Permalink">∞</a>
</h1>



<p>
  <span class="badge badge-secondary">Published</span>
  <span class="published">
    18 Dec 2017
  </span>
  <br />
  <span class="badge badge-secondary">By</span>
  
    <a href="https://wesmckinney.com">Wes McKinney (wesm) </a>
  
</p>

    <!--

-->

<p>The Apache Arrow team is pleased to announce the 0.8.0 release. It is the
product of 10 weeks of development and includes <a href="https://issues.apache.org/jira/issues/?jql=project%20%3D%20ARROW%20AND%20status%20in%20(Resolved%2C%20Closed)%20AND%20fixVersion%20%3D%200.8.0"><strong>286 resolved JIRAs</strong></a> with
many new features and bug fixes to the various language implementations. This
is the largest release since 0.3.0 earlier this year.</p>

<p>As part of work towards a stabilizing the Arrow format and making a 1.0.0
release sometime in 2018, we made a series of backwards-incompatible changes to
the serialized Arrow metadata that requires Arrow readers and writers (0.7.1
and earlier) to upgrade in order to be compatible with 0.8.0 and higher. We
expect future backwards-incompatible changes to be rare going forward.</p>

<p>See the <a href="https://arrow.apache.org/install">Install Page</a> to learn how to get the libraries for your
platform. The <a href="https://arrow.apache.org/release/0.8.0.html">complete changelog</a> is also available.</p>

<p>We discuss some highlights from the release and other project news in this
post.</p>

<h2 id="projects-powered-by-apache-arrow">Projects “Powered By” Apache Arrow</h2>

<p>A growing ecosystem of projects are using Arrow to solve in-memory analytics
and data interchange problems. We have added a new <a href="http://arrow.apache.org/powered_by/">Powered By</a> page to the
Arrow website where we can acknowledge open source projects and companies which
are using Arrow. If you would like to add your project to the list as an Arrow
user, please let us know.</p>

<h2 id="new-arrow-committers">New Arrow committers</h2>

<p>Since the last release, we have added 5 new Apache committers:</p>

<ul>
  <li><a href="https://github.com/cpcloud">Phillip Cloud</a>, who has mainly contributed to C++ and Python</li>
  <li><a href="https://github.com/BryanCutler">Bryan Cutler</a>, who has mainly contributed to Java and Spark integration</li>
  <li><a href="https://github.com/icexelloss">Li Jin</a>, who has mainly contributed to Java and Spark integration</li>
  <li><a href="https://github.com/trxcllnt">Paul Taylor</a>, who has mainly contributed to JavaScript</li>
  <li><a href="https://github.com/siddharthteotia">Siddharth Teotia</a>, who has mainly contributed to Java</li>
</ul>

<p>Welcome to the Arrow team, and thank you for your contributions!</p>

<h2 id="improved-java-vector-api-performance-improvements">Improved Java vector API, performance improvements</h2>

<p>Siddharth Teotia led efforts to revamp the Java vector API to make things
simpler and faster. As part of this, we removed the dichotomy between nullable
and non-nullable vectors.</p>

<p>See <a href="https://arrow.apache.org/blog/2017/12/19/java-vector-improvements/">Sidd’s blog post</a> for more about these changes.</p>

<h2 id="decimal-support-in-c-python-consistency-with-java">Decimal support in C++, Python, consistency with Java</h2>

<p><a href="https://github.com/cpcloud">Phillip Cloud</a> led efforts this release to harden details about exact
decimal values in the Arrow specification and ensure a consistent
implementation across Java, C++, and Python.</p>

<p>Arrow now supports decimals represented internally as a 128-bit little-endian
integer, with a set precision and scale (as defined in many SQL-based
systems). As part of this work, we needed to change Java’s internal
representation from big- to little-endian.</p>

<p>We are now integration testing decimals between Java, C++, and Python, which
will facilitate Arrow adoption in Apache Spark and other systems that use both
Java and Python.</p>

<p>Decimal data can now be read and written by the <a href="https://github.com/apache/parquet-cpp">Apache Parquet C++
library</a>, including via pyarrow.</p>

<p>In the future, we may implement support for smaller-precision decimals
represented by 32- or 64-bit integers.</p>

<h2 id="c-improvements-expanded-kernels-library-and-more">C++ improvements: expanded kernels library and more</h2>

<p>In C++, we have continued developing the new <code class="highlighter-rouge">arrow::compute</code> submodule
consisting of native computation fuctions for Arrow data. New contributor
<a href="https://github.com/licht-t">Licht Takeuchi</a> helped expand the supported types for type casting in
<code class="highlighter-rouge">compute::Cast</code>. We have also implemented new kernels <code class="highlighter-rouge">Unique</code> and
<code class="highlighter-rouge">DictionaryEncode</code> for computing the distinct elements of an array and
dictionary encoding (conversion to categorical), respectively.</p>

<p>We expect the C++ computation “kernel” library to be a major expansion area for
the project over the next year and beyond. Here, we can also implement SIMD-
and GPU-accelerated versions of basic in-memory analytics functionality.</p>

<p>As minor breaking API change in C++, we have made the <code class="highlighter-rouge">RecordBatch</code> and <code class="highlighter-rouge">Table</code>
APIs “virtual” or abstract interfaces, to enable different implementations of a
record batch or table which conform to the standard interface. This will help
enable features like lazy IO or column loading.</p>

<p>There was significant work improving the C++ library generally and supporting
work happening in Python and C. See the change log for full details.</p>

<h2 id="glib-c-improvements-meson-build-gpu-support">GLib C improvements: Meson build, GPU support</h2>

<p>Developing of the GLib-based C bindings has generally tracked work happening in
the C++ library. These bindings are being used to develop <a href="https://github.com/red-data-tools">data science tools
for Ruby users</a> and elsewhere.</p>

<p>The C bindings now support the <a href="https://mesonbuild.com">Meson build system</a> in addition to
autotools, which enables them to be built on Windows.</p>

<p>The Arrow GPU extension library is now also supported in the C bindings.</p>

<h2 id="javascript-first-independent-release-on-npm">JavaScript: first independent release on NPM</h2>

<p><a href="https://github.com/TheNeuralBit">Brian Hulette</a> and <a href="https://github.com/trxcllnt">Paul Taylor</a> have been continuing to drive efforts
on the TypeScript-based JavaScript implementation.</p>

<p>Since the last release, we made a first JavaScript-only Apache release, version
0.2.0, which is <a href="http://npmjs.org/package/apache-arrow">now available on NPM</a>. We decided to make separate
JavaScript releases to enable the JS library to release more frequently than
the rest of the project.</p>

<h2 id="python-improvements">Python improvements</h2>

<p>In addition to some of the new features mentioned above, we have made a variety
of usability and performance improvements for integrations with pandas, NumPy,
Dask, and other Python projects which may make use of pyarrow, the Arrow Python
library.</p>

<p>Some of these improvements include:</p>

<ul>
  <li><a href="http://arrow.apache.org/docs/python/ipc.html">Component-based serialization</a> for more flexible and memory-efficient
transport of large or complex Python objects</li>
  <li>Substantially improved serialization performance for pandas objects when
using <code class="highlighter-rouge">pyarrow.serialize</code> and <code class="highlighter-rouge">pyarrow.deserialize</code>. This includes a special
<code class="highlighter-rouge">pyarrow.pandas_serialization_context</code> which further accelerates certain
internal details of pandas serialization * Support zero-copy reads for</li>
  <li><code class="highlighter-rouge">pandas.DataFrame</code> using <code class="highlighter-rouge">pyarrow.deserialize</code> for objects without Python
objects</li>
  <li>Multithreaded conversions from <code class="highlighter-rouge">pandas.DataFrame</code> to <code class="highlighter-rouge">pyarrow.Table</code> (we
already supported multithreaded conversions from Arrow back to pandas)</li>
  <li>More efficient conversion from 1-dimensional NumPy arrays to Arrow format</li>
  <li>New generic buffer compression and decompression APIs <code class="highlighter-rouge">pyarrow.compress</code> and
<code class="highlighter-rouge">pyarrow.decompress</code></li>
  <li>Enhanced Parquet cross-compatibility with <a href="https://github.com/dask/fastparquet">fastparquet</a> and improved Dask
support</li>
  <li>Python support for accessing Parquet row group column statistics</li>
</ul>

<h2 id="upcoming-roadmap">Upcoming Roadmap</h2>

<p>The 0.8.0 release includes some API and format changes, but upcoming releases
will focus on ompleting and stabilizing critical functionality to move the
project closer to a 1.0.0 release.</p>

<p>With the ecosystem of projects using Arrow expanding rapidly, we will be
working to improve and expand the libraries in support of downstream use cases.</p>

<p>We continue to look for more JavaScript, Julia, R, Rust, and other programming
language developers to join the project and expand the available
implementations and bindings to more languages.</p>


  </div>

  

  
    
  <div class="blog-post" style="margin-bottom: 4rem">
    
<h1>
  Improvements to Java Vector API in Apache Arrow 0.8.0
  <a href="/blog/2017/12/18/java-vector-improvements/" class="permalink" title="Permalink">∞</a>
</h1>



<p>
  <span class="badge badge-secondary">Published</span>
  <span class="published">
    18 Dec 2017
  </span>
  <br />
  <span class="badge badge-secondary">By</span>
  
    Siddharth Teotia
  
</p>

    <!--

-->

<p>This post gives insight into the major improvements in the Java implementation
of vectors. We undertook this work over the last 10 weeks since the last Arrow
release.</p>

<h2 id="design-goals">Design Goals</h2>

<ol>
  <li>Improved maintainability and extensibility</li>
  <li>Improved heap memory usage</li>
  <li>No performance overhead on hot code paths</li>
</ol>

<h2 id="background">Background</h2>

<h3 id="improved-maintainability-and-extensibility">Improved maintainability and extensibility</h3>

<p>We use templates in several places for compile time Java code generation for
different vector classes, readers, writers etc. Templates are helpful as the
developers don’t have to write a lot of duplicate code.</p>

<p>However, we realized that over a period of time some specific Java
templates became extremely complex with giant if-else blocks, poor code indentation
and documentation. All this impacted the ability to easily extend these templates
for adding new functionality or improving the existing infrastructure.</p>

<p>So we evaluated the usage of templates for compile time code generation and
decided not to use complex templates in some places by writing small amount of
duplicate code which is elegant, well documented and extensible.</p>

<h3 id="improved-heap-usage">Improved heap usage</h3>

<p>We did extensive memory analysis downstream in <a href="https://www.dremio.com/">Dremio</a> where Arrow is used
heavily for in-memory query execution on columnar data. The general conclusion
was that Arrow’s Java vector classes have non-negligible heap overhead and
volume of objects was too high. There were places in code where we were
creating objects unnecessarily and using structures that could be substituted
with better alternatives.</p>

<h3 id="no-performance-overhead-on-hot-code-paths">No performance overhead on hot code paths</h3>

<p>Java vectors used delegation and abstraction heavily throughout the object
hierarchy. The performance critical get/set methods of vectors went through a
chain of function calls back and forth between different objects before doing
meaningful work. We also evaluated the usage of branches in vector APIs and
reimplemented some of them by avoiding branches completely.</p>

<p>We took inspiration from how the Java memory code in <code class="highlighter-rouge">ArrowBuf</code> works. For all
the performance critical methods, <code class="highlighter-rouge">ArrowBuf</code> bypasses all the netty object
hierarchy, grabs the target virtual address and directly interacts with the
memory.</p>

<p>There were cases where branches could be avoided all together.</p>

<p>In case of nullable vectors, we were doing multiple checks to confirm if
the value at a given position in the vector is null or not.</p>

<h2 id="our-implementation-approach">Our implementation approach</h2>

<ul>
  <li>For scalars, the inheritance tree was simplified by writing different
abstract base classes for fixed and variable width scalars.</li>
  <li>The base classes contained all the common functionality across different
types.</li>
  <li>The individual subclasses implemented type specific APIs for fixed and
variable width scalar vectors.</li>
  <li>For the performance critical methods, all the work is done either in
the vector class or corresponding ArrowBuf. There is no delegation to any
internal object.</li>
  <li>The mutator and accessor based access to vector APIs is removed. These
objects led to unnecessary heap overhead and complicated the use of APIs.</li>
  <li>Both scalar and complex vectors directly interact with underlying buffers
that manage the offsets, data and validity. Earlier we were creating different
inner vectors for each vector and delegating all the functionality to inner
vectors. This introduced a lot of bugs in memory management, excessive heap
overhead and performance penalty due to chain of delegations.</li>
  <li>We reduced the number of vector classes by removing non-nullable vectors.
In the new implementation, all vectors in Java are nullable in nature.</li>
</ul>


  </div>

  

  
    
  <div class="blog-post" style="margin-bottom: 4rem">
    
<h1>
  Fast Python Serialization with Ray and Apache Arrow
  <a href="/blog/2017/10/15/fast-python-serialization-with-ray-and-arrow/" class="permalink" title="Permalink">∞</a>
</h1>



<p>
  <span class="badge badge-secondary">Published</span>
  <span class="published">
    15 Oct 2017
  </span>
  <br />
  <span class="badge badge-secondary">By</span>
  
    Philipp Moritz, Robert Nishihara
  
</p>

    <!--

-->

<p><em>This was originally posted on the <a href="https://ray-project.github.io/">Ray blog</a>. <a href="https://people.eecs.berkeley.edu/~pcmoritz/">Philipp Moritz</a> and <a href="http://www.robertnishihara.com">Robert Nishihara</a> are graduate students at UC Berkeley.</em></p>

<p>This post elaborates on the integration between <a href="http://ray.readthedocs.io/en/latest/index.html">Ray</a> and <a href="https://arrow.apache.org/">Apache Arrow</a>.
The main problem this addresses is <a href="https://en.wikipedia.org/wiki/Serialization">data serialization</a>.</p>

<p>From <a href="https://en.wikipedia.org/wiki/Serialization">Wikipedia</a>, <strong>serialization</strong> is</p>

<blockquote>
  <p>… the process of translating data structures or object state into a format
that can be stored … or transmitted … and reconstructed later (possibly
in a different computer environment).</p>
</blockquote>

<p>Why is any translation necessary? Well, when you create a Python object, it may
have pointers to other Python objects, and these objects are all allocated in
different regions of memory, and all of this has to make sense when unpacked by
another process on another machine.</p>

<p>Serialization and deserialization are <strong>bottlenecks in parallel and distributed
computing</strong>, especially in machine learning applications with large objects and
large quantities of data.</p>

<h2 id="design-goals">Design Goals</h2>

<p>As Ray is optimized for machine learning and AI applications, we have focused a
lot on serialization and data handling, with the following design goals:</p>

<ol>
  <li>It should be very efficient with <strong>large numerical data</strong> (this includes
NumPy arrays and Pandas DataFrames, as well as objects that recursively contain
Numpy arrays and Pandas DataFrames).</li>
  <li>It should be about as fast as Pickle for <strong>general Python types</strong>.</li>
  <li>It should be compatible with <strong>shared memory</strong>, allowing multiple processes
to use the same data without copying it.</li>
  <li><strong>Deserialization</strong> should be extremely fast (when possible, it should not
require reading the entire serialized object).</li>
  <li>It should be <strong>language independent</strong> (eventually we’d like to enable Python
workers to use objects created by workers in Java or other languages and vice
versa).</li>
</ol>

<h2 id="our-approach-and-alternatives">Our Approach and Alternatives</h2>

<p>The go-to serialization approach in Python is the <strong>pickle</strong> module. Pickle is
very general, especially if you use variants like <a href="https://github.com/cloudpipe/cloudpickle/">cloudpickle</a>. However, it
does not satisfy requirements 1, 3, 4, or 5. Alternatives like <strong>json</strong> satisfy
5, but not 1-4.</p>

<p><strong>Our Approach:</strong> To satisfy requirements 1-5, we chose to use the
<a href="https://arrow.apache.org/">Apache Arrow</a> format as our underlying data representation. In collaboration
with the Apache Arrow team, we built <a href="https://arrow.apache.org/docs/python/ipc.html#arbitrary-object-serialization">libraries</a> for mapping general Python
objects to and from the Arrow format. Some properties of this approach:</p>

<ul>
  <li>The data layout is language independent (requirement 5).</li>
  <li>Offsets into a serialized data blob can be computed in constant time without
reading the full object (requirements 1 and 4).</li>
  <li>Arrow supports <strong>zero-copy reads</strong>, so objects can naturally be stored in
shared memory and used by multiple processes (requirements 1 and 3).</li>
  <li>We can naturally fall back to pickle for anything we can’t handle well
(requirement 2).</li>
</ul>

<p><strong>Alternatives to Arrow:</strong> We could have built on top of
<a href="https://developers.google.com/protocol-buffers/"><strong>Protocol Buffers</strong></a>, but protocol buffers really isn’t designed for
numerical data, and that approach wouldn’t satisfy 1, 3, or 4. Building on top
of <a href="https://google.github.io/flatbuffers/"><strong>Flatbuffers</strong></a> actually could be made to work, but it would have
required implementing a lot of the facilities that Arrow already has and we
preferred a columnar data layout more optimized for big data.</p>

<h2 id="speedups">Speedups</h2>

<p>Here we show some performance improvements over Python’s pickle module. The
experiments were done using <code class="highlighter-rouge">pickle.HIGHEST_PROTOCOL</code>. Code for generating these
plots is included at the end of the post.</p>

<p><strong>With NumPy arrays:</strong> In machine learning and AI applications, data (e.g.,
images, neural network weights, text documents) are typically represented as
data structures containing NumPy arrays. When using NumPy arrays, the speedups
are impressive.</p>

<p>The fact that the Ray bars for deserialization are barely visible is not a
mistake. This is a consequence of the support for zero-copy reads (the savings
largely come from the lack of memory movement).</p>

<div align="center">
<img src="/assets/fast_python_serialization_with_ray_and_arrow/speedups0.png" width="365" height="255" />
<img src="/assets/fast_python_serialization_with_ray_and_arrow/speedups1.png" width="365" height="255" />
</div>

<p>Note that the biggest wins are with deserialization. The speedups here are
multiple orders of magnitude and get better as the NumPy arrays get larger
(thanks to design goals 1, 3, and 4). Making <strong>deserialization</strong> fast is
important for two reasons. First, an object may be serialized once and then
deserialized many times (e.g., an object that is broadcast to all workers).
Second, a common pattern is for many objects to be serialized in parallel and
then aggregated and deserialized one at a time on a single worker making
deserialization the bottleneck.</p>

<p><strong>Without NumPy arrays:</strong> When using regular Python objects, for which we
cannot take advantage of shared memory, the results are comparable to pickle.</p>

<div align="center">
<img src="/assets/fast_python_serialization_with_ray_and_arrow/speedups2.png" width="365" height="255" />
<img src="/assets/fast_python_serialization_with_ray_and_arrow/speedups3.png" width="365" height="255" />
</div>

<p>These are just a few examples of interesting Python objects. The most important
case is the case where NumPy arrays are nested within other objects. Note that
our serialization library works with very general Python types including custom
Python classes and deeply nested objects.</p>

<h2 id="the-api">The API</h2>

<p>The serialization library can be used directly through pyarrow as follows. More
documentation is available <a href="https://arrow.apache.org/docs/python/ipc.html#arbitrary-object-serialization">here</a>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span> <span class="o">=</span> <span class="p">[(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="s">'hello'</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">5.0</span><span class="p">,</span> <span class="mf">6.0</span><span class="p">])]</span>
<span class="n">serialized_x</span> <span class="o">=</span> <span class="n">pyarrow</span><span class="o">.</span><span class="n">serialize</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">to_buffer</span><span class="p">()</span>
<span class="n">deserialized_x</span> <span class="o">=</span> <span class="n">pyarrow</span><span class="o">.</span><span class="n">deserialize</span><span class="p">(</span><span class="n">serialized_x</span><span class="p">)</span>
</code></pre></div></div>

<p>It can be used directly through the Ray API as follows.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span> <span class="o">=</span> <span class="p">[(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="s">'hello'</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">5.0</span><span class="p">,</span> <span class="mf">6.0</span><span class="p">])]</span>
<span class="n">x_id</span> <span class="o">=</span> <span class="n">ray</span><span class="o">.</span><span class="n">put</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">deserialized_x</span> <span class="o">=</span> <span class="n">ray</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">x_id</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="data-representation">Data Representation</h2>

<p>We use Apache Arrow as the underlying language-independent data layout. Objects
are stored in two parts: a <strong>schema</strong> and a <strong>data blob</strong>. At a high level, the
data blob is roughly a flattened concatenation of all of the data values
recursively contained in the object, and the schema defines the types and
nesting structure of the data blob.</p>

<p><strong>Technical Details:</strong> Python sequences (e.g., dictionaries, lists, tuples,
sets) are encoded as Arrow <a href="http://arrow.apache.org/docs/memory_layout.html#dense-union-type">UnionArrays</a> of other types (e.g., bools, ints,
strings, bytes, floats, doubles, date64s, tensors (i.e., NumPy arrays), lists,
tuples, dicts and sets). Nested sequences are encoded using Arrow
<a href="http://arrow.apache.org/docs/memory_layout.html#list-type">ListArrays</a>. All tensors are collected and appended to the end of the
serialized object, and the UnionArray contains references to these tensors.</p>

<p>To give a concrete example, consider the following object.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">[(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="s">'hello'</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">5.0</span><span class="p">,</span> <span class="mf">6.0</span><span class="p">])]</span>
</code></pre></div></div>

<p>It would be represented in Arrow with the following structure.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>UnionArray(type_ids=[tuple, string, int, int, ndarray],
           tuples=ListArray(offsets=[0, 2],
                            UnionArray(type_ids=[int, int],
                                       ints=[1, 2])),
           strings=['hello'],
           ints=[3, 4],
           ndarrays=[&lt;offset of numpy array&gt;])
</code></pre></div></div>

<p>Arrow uses Flatbuffers to encode serialized schemas. <strong>Using only the schema, we
can compute the offsets of each value in the data blob without scanning through
the data blob</strong> (unlike Pickle, this is what enables fast deserialization). This
means that we can avoid copying or otherwise converting large arrays and other
values during deserialization. Tensors are appended at the end of the UnionArray
and can be efficiently shared and accessed using shared memory.</p>

<p>Note that the actual object would be laid out in memory as shown below.</p>

<div align="center">
<img src="/assets/fast_python_serialization_with_ray_and_arrow/python_object.png" width="600" />
</div>
<div><i>The layout of a Python object in the heap. Each box is allocated in a
different memory region, and arrows between boxes represent pointers.</i></div>
<p><br /></p>

<p>The Arrow serialized representation would be as follows.</p>

<div align="center">
<img src="/assets/fast_python_serialization_with_ray_and_arrow/arrow_object.png" width="400" />
</div>
<div><i>The memory layout of the Arrow-serialized object.</i></div>
<p><br /></p>

<h2 id="getting-involved">Getting Involved</h2>

<p>We welcome contributions, especially in the following areas.</p>

<ul>
  <li>Use the C++ and Java implementations of Arrow to implement versions of this
for C++ and Java.</li>
  <li>Implement support for more Python types and better test coverage.</li>
</ul>

<h2 id="reproducing-the-figures-above">Reproducing the Figures Above</h2>

<p>For reference, the figures can be reproduced with the following code.
Benchmarking <code class="highlighter-rouge">ray.put</code> and <code class="highlighter-rouge">ray.get</code> instead of <code class="highlighter-rouge">pyarrow.serialize</code> and
<code class="highlighter-rouge">pyarrow.deserialize</code> gives similar figures. The plots were generated at this
<a href="https://github.com/apache/arrow/tree/894f7400977693b4e0e8f4b9845fd89481f6bf29">commit</a>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">pickle</span>
<span class="kn">import</span> <span class="nn">pyarrow</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">timeit</span>


<span class="k">def</span> <span class="nf">benchmark_object</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="n">number</span><span class="o">=</span><span class="mi">10</span><span class="p">):</span>
    <span class="c1"># Time serialization and deserialization for pickle.
</span>    <span class="n">pickle_serialize</span> <span class="o">=</span> <span class="n">timeit</span><span class="o">.</span><span class="n">timeit</span><span class="p">(</span>
        <span class="k">lambda</span><span class="p">:</span> <span class="n">pickle</span><span class="o">.</span><span class="n">dumps</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="n">protocol</span><span class="o">=</span><span class="n">pickle</span><span class="o">.</span><span class="n">HIGHEST_PROTOCOL</span><span class="p">),</span>
        <span class="n">number</span><span class="o">=</span><span class="n">number</span><span class="p">)</span>
    <span class="n">serialized_obj</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">dumps</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="n">pickle</span><span class="o">.</span><span class="n">HIGHEST_PROTOCOL</span><span class="p">)</span>
    <span class="n">pickle_deserialize</span> <span class="o">=</span> <span class="n">timeit</span><span class="o">.</span><span class="n">timeit</span><span class="p">(</span><span class="k">lambda</span><span class="p">:</span> <span class="n">pickle</span><span class="o">.</span><span class="n">loads</span><span class="p">(</span><span class="n">serialized_obj</span><span class="p">),</span>
                                       <span class="n">number</span><span class="o">=</span><span class="n">number</span><span class="p">)</span>

    <span class="c1"># Time serialization and deserialization for Ray.
</span>    <span class="n">ray_serialize</span> <span class="o">=</span> <span class="n">timeit</span><span class="o">.</span><span class="n">timeit</span><span class="p">(</span>
        <span class="k">lambda</span><span class="p">:</span> <span class="n">pyarrow</span><span class="o">.</span><span class="n">serialize</span><span class="p">(</span><span class="n">obj</span><span class="p">)</span><span class="o">.</span><span class="n">to_buffer</span><span class="p">(),</span> <span class="n">number</span><span class="o">=</span><span class="n">number</span><span class="p">)</span>
    <span class="n">serialized_obj</span> <span class="o">=</span> <span class="n">pyarrow</span><span class="o">.</span><span class="n">serialize</span><span class="p">(</span><span class="n">obj</span><span class="p">)</span><span class="o">.</span><span class="n">to_buffer</span><span class="p">()</span>
    <span class="n">ray_deserialize</span> <span class="o">=</span> <span class="n">timeit</span><span class="o">.</span><span class="n">timeit</span><span class="p">(</span>
        <span class="k">lambda</span><span class="p">:</span> <span class="n">pyarrow</span><span class="o">.</span><span class="n">deserialize</span><span class="p">(</span><span class="n">serialized_obj</span><span class="p">),</span> <span class="n">number</span><span class="o">=</span><span class="n">number</span><span class="p">)</span>

    <span class="k">return</span> <span class="p">[[</span><span class="n">pickle_serialize</span><span class="p">,</span> <span class="n">pickle_deserialize</span><span class="p">],</span>
            <span class="p">[</span><span class="n">ray_serialize</span><span class="p">,</span> <span class="n">ray_deserialize</span><span class="p">]]</span>


<span class="k">def</span> <span class="nf">plot</span><span class="p">(</span><span class="n">pickle_times</span><span class="p">,</span> <span class="n">ray_times</span><span class="p">,</span> <span class="n">title</span><span class="p">,</span> <span class="n">i</span><span class="p">):</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
    <span class="n">fig</span><span class="o">.</span><span class="n">set_size_inches</span><span class="p">(</span><span class="mf">3.8</span><span class="p">,</span> <span class="mf">2.7</span><span class="p">)</span>

    <span class="n">bar_width</span> <span class="o">=</span> <span class="mf">0.35</span>
    <span class="n">index</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">opacity</span> <span class="o">=</span> <span class="mf">0.6</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">index</span><span class="p">,</span> <span class="n">pickle_times</span><span class="p">,</span> <span class="n">bar_width</span><span class="p">,</span>
            <span class="n">alpha</span><span class="o">=</span><span class="n">opacity</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'r'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'Pickle'</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">index</span> <span class="o">+</span> <span class="n">bar_width</span><span class="p">,</span> <span class="n">ray_times</span><span class="p">,</span> <span class="n">bar_width</span><span class="p">,</span>
            <span class="n">alpha</span><span class="o">=</span><span class="n">opacity</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'c'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'Ray'</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="n">title</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s">'bold'</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'Time (seconds)'</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">labels</span> <span class="o">=</span> <span class="p">[</span><span class="s">'serialization'</span><span class="p">,</span> <span class="s">'deserialization'</span><span class="p">]</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">index</span> <span class="o">+</span> <span class="n">bar_width</span> <span class="o">/</span> <span class="mi">2</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">bbox_to_anchor</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s">'plot-'</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="o">+</span> <span class="s">'.png'</span><span class="p">,</span> <span class="nb">format</span><span class="o">=</span><span class="s">'png'</span><span class="p">)</span>


<span class="n">test_objects</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">50000</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">)],</span>
    <span class="p">{</span><span class="s">'weight-'</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">):</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">50000</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">)},</span>
    <span class="p">{</span><span class="n">i</span><span class="p">:</span> <span class="nb">set</span><span class="p">([</span><span class="s">'string1'</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">),</span> <span class="s">'string2'</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)])</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100000</span><span class="p">)},</span>
    <span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">200000</span><span class="p">)]</span>
<span class="p">]</span>

<span class="n">titles</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s">'List of large numpy arrays'</span><span class="p">,</span>
    <span class="s">'Dictionary of large numpy arrays'</span><span class="p">,</span>
    <span class="s">'Large dictionary of small sets'</span><span class="p">,</span>
    <span class="s">'Large list of strings'</span>
<span class="p">]</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">test_objects</span><span class="p">)):</span>
    <span class="n">plot</span><span class="p">(</span><span class="o">*</span><span class="n">benchmark_object</span><span class="p">(</span><span class="n">test_objects</span><span class="p">[</span><span class="n">i</span><span class="p">]),</span> <span class="n">titles</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">i</span><span class="p">)</span>
</code></pre></div></div>


  </div>

  

  
    
  <div class="blog-post" style="margin-bottom: 4rem">
    
<h1>
  Apache Arrow 0.7.0 Release
  <a href="/blog/2017/09/19/0.7.0-release/" class="permalink" title="Permalink">∞</a>
</h1>



<p>
  <span class="badge badge-secondary">Published</span>
  <span class="published">
    19 Sep 2017
  </span>
  <br />
  <span class="badge badge-secondary">By</span>
  
    <a href="https://wesmckinney.com">Wes McKinney (wesm) </a>
  
</p>

    <!--

-->

<p>The Apache Arrow team is pleased to announce the 0.7.0 release. It includes
<a href="https://issues.apache.org/jira/issues/?jql=project%20%3D%20ARROW%20AND%20status%20in%20(Resolved%2C%20Closed)%20AND%20fixVersion%20%3D%200.7.0"><strong>133 resolved JIRAs</strong></a> many new features and bug fixes to the various
language implementations. The Arrow memory format remains stable since the
0.3.x release.</p>

<p>See the <a href="http://arrow.apache.org/install">Install Page</a> to learn how to get the libraries for your
platform. The <a href="http://arrow.apache.org/release/0.7.0.html">complete changelog</a> is also available.</p>

<p>We include some highlights from the release in this post.</p>

<h2 id="new-pmc-member-kouhei-sutou">New PMC Member: Kouhei Sutou</h2>

<p>Since the last release we have added <a href="https://github.com/kou">Kou</a> to the Arrow Project Management
Committee. He is also a PMC for Apache Subversion, and a major contributor to
many other open source projects.</p>

<p>As an active member of the Ruby community in Japan, Kou has been developing the
GLib-based C bindings for Arrow with associated Ruby wrappers, to enable Ruby
users to benefit from the work that’s happening in Apache Arrow.</p>

<p>We are excited to be collaborating with the Ruby community on shared
infrastructure for in-memory analytics and data science.</p>

<h2 id="expanded-javascript-typescript-implementation">Expanded JavaScript (TypeScript) Implementation</h2>

<p><a href="https://github.com/trxcllnt">Paul Taylor</a> from the <a href="https://github.com/netflix/falcor">Falcor</a> and <a href="http://reactivex.io">ReactiveX</a> projects has worked to
expand the JavaScript implementation (which is written in TypeScript), using
the latest in modern JavaScript build and packaging technology. We are looking
forward to building out the JS implementation and bringing it up to full
functionality with the C++ and Java implementations.</p>

<p>We are looking for more JavaScript developers to join the project and work
together to make Arrow for JS work well with many kinds of front end use cases,
like real time data visualization.</p>

<h2 id="type-casting-for-c-and-python">Type casting for C++ and Python</h2>

<p>As part of longer-term efforts to build an Arrow-native in-memory analytics
library, we implemented a variety of type conversion functions. These functions
are essential in ETL tasks when conforming one table schema to another. These
are similar to the <code class="highlighter-rouge">astype</code> function in NumPy.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">In</span> <span class="p">[</span><span class="mi">17</span><span class="p">]:</span> <span class="kn">import</span> <span class="nn">pyarrow</span> <span class="k">as</span> <span class="n">pa</span>

<span class="n">In</span> <span class="p">[</span><span class="mi">18</span><span class="p">]:</span> <span class="n">arr</span> <span class="o">=</span> <span class="n">pa</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="bp">True</span><span class="p">,</span> <span class="bp">False</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">True</span><span class="p">])</span>

<span class="n">In</span> <span class="p">[</span><span class="mi">19</span><span class="p">]:</span> <span class="n">arr</span>
<span class="n">Out</span><span class="p">[</span><span class="mi">19</span><span class="p">]:</span>
<span class="o">&lt;</span><span class="n">pyarrow</span><span class="o">.</span><span class="n">lib</span><span class="o">.</span><span class="n">BooleanArray</span> <span class="nb">object</span> <span class="n">at</span> <span class="mh">0x7ff6fb069b88</span><span class="o">&gt;</span>
<span class="p">[</span>
  <span class="bp">True</span><span class="p">,</span>
  <span class="bp">False</span><span class="p">,</span>
  <span class="n">NA</span><span class="p">,</span>
  <span class="bp">True</span>
<span class="p">]</span>

<span class="n">In</span> <span class="p">[</span><span class="mi">20</span><span class="p">]:</span> <span class="n">arr</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">pa</span><span class="o">.</span><span class="n">int32</span><span class="p">())</span>
<span class="n">Out</span><span class="p">[</span><span class="mi">20</span><span class="p">]:</span>
<span class="o">&lt;</span><span class="n">pyarrow</span><span class="o">.</span><span class="n">lib</span><span class="o">.</span><span class="n">Int32Array</span> <span class="nb">object</span> <span class="n">at</span> <span class="mh">0x7ff6fb0383b8</span><span class="o">&gt;</span>
<span class="p">[</span>
  <span class="mi">1</span><span class="p">,</span>
  <span class="mi">0</span><span class="p">,</span>
  <span class="n">NA</span><span class="p">,</span>
  <span class="mi">1</span>
<span class="p">]</span>
</code></pre></div></div>

<p>Over time these will expand to support as many input-and-output type
combinations with optimized conversions.</p>

<h2 id="new-arrow-gpu-cuda-extension-library-for-c">New Arrow GPU (CUDA) Extension Library for C++</h2>

<p>To help with GPU-related projects using Arrow, like the <a href="http://gpuopenanalytics.com/">GPU Open Analytics
Initiative</a>, we have started a C++ add-on library to simplify Arrow memory
management on CUDA-enabled graphics cards. We would like to expand this to
include a library of reusable CUDA kernel functions for GPU analytics on Arrow
columnar memory.</p>

<p>For example, we could write a record batch from CPU memory to GPU device memory
like so (some error checking omitted):</p>

<div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="cp">#include &lt;arrow/api.h&gt;
#include &lt;arrow/gpu/cuda_api.h&gt;
</span>
<span class="k">using</span> <span class="k">namespace</span> <span class="n">arrow</span><span class="p">;</span>

<span class="n">gpu</span><span class="o">::</span><span class="n">CudaDeviceManager</span><span class="o">*</span> <span class="n">manager</span><span class="p">;</span>
<span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">gpu</span><span class="o">::</span><span class="n">CudaContext</span><span class="o">&gt;</span> <span class="n">context</span><span class="p">;</span>

<span class="n">gpu</span><span class="o">::</span><span class="n">CudaDeviceManager</span><span class="o">::</span><span class="n">GetInstance</span><span class="p">(</span><span class="o">&amp;</span><span class="n">manager</span><span class="p">)</span>
<span class="n">manager_</span><span class="o">-&gt;</span><span class="n">GetContext</span><span class="p">(</span><span class="n">kGpuNumber</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">context</span><span class="p">);</span>

<span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">RecordBatch</span><span class="o">&gt;</span> <span class="n">batch</span> <span class="o">=</span> <span class="n">GetCpuData</span><span class="p">();</span>

<span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">gpu</span><span class="o">::</span><span class="n">CudaBuffer</span><span class="o">&gt;</span> <span class="n">device_serialized</span><span class="p">;</span>
<span class="n">gpu</span><span class="o">::</span><span class="n">SerializeRecordBatch</span><span class="p">(</span><span class="o">*</span><span class="n">batch</span><span class="p">,</span> <span class="n">context_</span><span class="p">.</span><span class="n">get</span><span class="p">(),</span> <span class="o">&amp;</span><span class="n">device_serialized</span><span class="p">));</span>
</code></pre></div></div>

<p>We can then “read” the GPU record batch, but the returned <code class="highlighter-rouge">arrow::RecordBatch</code>
internally will contain GPU device pointers that you can use for CUDA kernel
calls:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>std::shared_ptr&lt;RecordBatch&gt; device_batch;
gpu::ReadRecordBatch(batch-&gt;schema(), device_serialized,
                     default_memory_pool(), &amp;device_batch));

// Now run some CUDA kernels on device_batch
</code></pre></div></div>

<h2 id="decimal-integration-tests">Decimal Integration Tests</h2>

<p><a href="http://github.com/cpcloud">Phillip Cloud</a> has been working on decimal support in C++ to enable Parquet
read/write support in C++ and Python, and also end-to-end testing against the
Arrow Java libraries.</p>

<p>In the upcoming releases, we hope to complete the remaining data types that
need end-to-end testing between Java and C++:</p>

<ul>
  <li>Fixed size lists (variable-size lists already implemented)</li>
  <li>Fixes size binary</li>
  <li>Unions</li>
  <li>Maps</li>
  <li>Time intervals</li>
</ul>

<h2 id="other-notable-python-changes">Other Notable Python Changes</h2>

<p>Some highlights of Python development outside of bug fixes and general API
improvements include:</p>

<ul>
  <li>Simplified <code class="highlighter-rouge">put</code> and <code class="highlighter-rouge">get</code> arbitrary Python objects in Plasma objects</li>
  <li><a href="http://arrow.apache.org/docs/python/ipc.html">High-speed, memory efficient object serialization</a>. This is important
enough that we will likely write a dedicated blog post about it.</li>
  <li>New <code class="highlighter-rouge">flavor='spark'</code> option to <code class="highlighter-rouge">pyarrow.parquet.write_table</code> to enable easy
writing of Parquet files maximized for Spark compatibility</li>
  <li><code class="highlighter-rouge">parquet.write_to_dataset</code> function with support for partitioned writes</li>
  <li>Improved support for Dask filesystems</li>
  <li>Improved Python usability for IPC: read and write schemas and record batches
more easily. See the <a href="http://arrow.apache.org/docs/python/api.html">API docs</a> for more about these.</li>
</ul>

<h2 id="the-road-ahead">The Road Ahead</h2>

<p>Upcoming Arrow releases will continue to expand the project to cover more use
cases. In addition to completing end-to-end testing for all the major data
types, some of us will be shifting attention to building Arrow-native in-memory
analytics libraries.</p>

<p>We are looking for more JavaScript, R, and other programming language
developers to join the project and expand the available implementations and
bindings to more languages.</p>


  </div>

  

  
    
  <div class="blog-post" style="margin-bottom: 4rem">
    
<h1>
  Apache Arrow 0.6.0 Release
  <a href="/blog/2017/08/16/0.6.0-release/" class="permalink" title="Permalink">∞</a>
</h1>



<p>
  <span class="badge badge-secondary">Published</span>
  <span class="published">
    16 Aug 2017
  </span>
  <br />
  <span class="badge badge-secondary">By</span>
  
    <a href="https://wesmckinney.com">Wes McKinney (wesm) </a>
  
</p>

    <!--

-->

<p>The Apache Arrow team is pleased to announce the 0.6.0 release. It includes
<a href="https://issues.apache.org/jira/issues/?jql=project%20%3D%20ARROW%20AND%20status%20in%20(Resolved%2C%20Closed)%20AND%20fixVersion%20%3D%200.6.0"><strong>90 resolved JIRAs</strong></a> with the new Plasma shared memory object store, and
improvements and bug fixes to the various language implementations. The Arrow
memory format remains stable since the 0.3.x release.</p>

<p>See the <a href="http://arrow.apache.org/install">Install Page</a> to learn how to get the libraries for your
platform. The <a href="http://arrow.apache.org/release/0.6.0.html">complete changelog</a> is also available.</p>

<h2 id="plasma-shared-memory-object-store">Plasma Shared Memory Object Store</h2>

<p>This release includes the <a href="http://arrow.apache.org/blog/2017/08/08/plasma-in-memory-object-store/">Plasma Store</a>, which you can read more about in
the linked blog post. This system was originally developed as part of the <a href="https://ray-project.github.io/ray/">Ray
Project</a> at the <a href="https://rise.cs.berkeley.edu/">UC Berkeley RISELab</a>. We recognized that Plasma would be
highly valuable to the Arrow community as a tool for shared memory management
and zero-copy deserialization. Additionally, we believe we will be able to
develop a stronger software stack through sharing of IO and buffer management
code.</p>

<p>The Plasma store is a server application which runs as a separate process. A
reference C++ client, with Python bindings, is made available in this
release. Clients can be developed in Java or other languages in the future to
enable simple sharing of complex datasets through shared memory.</p>

<h2 id="arrow-format-addition-map-type">Arrow Format Addition: Map type</h2>

<p>We added a Map logical type to represent ordered and unordered maps
in-memory. This corresponds to the <code class="highlighter-rouge">MAP</code> logical type annotation in the Parquet
format (where maps are represented as repeated structs).</p>

<p>Map is represented as a list of structs. It is the first example of a logical
type whose physical representation is a nested type. We have not yet created
implementations of Map containers in any of the implementations, but this can
be done in a future release.</p>

<p>As an example, the Python data:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>data = [{'a': 1, 'bb': 2, 'cc': 3}, {'dddd': 4}]
</code></pre></div></div>

<p>Could be represented in an Arrow <code class="highlighter-rouge">Map&lt;String, Int32&gt;</code> as:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Map&lt;String, Int32&gt; = List&lt;Struct&lt;keys: String, values: Int32&gt;&gt;
  is_valid: [true, true]
  offsets: [0, 3, 4]
  values: Struct&lt;keys: String, values: Int32&gt;
    children:
      - keys: String
          is_valid: [true, true, true, true]
          offsets: [0, 1, 3, 5, 9]
          data: abbccdddd
      - values: Int32
          is_valid: [true, true, true, true]
          data: [1, 2, 3, 4]
</code></pre></div></div>
<h2 id="python-changes">Python Changes</h2>

<p>Some highlights of Python development outside of bug fixes and general API
improvements include:</p>

<ul>
  <li>New <code class="highlighter-rouge">strings_to_categorical=True</code> option when calling <code class="highlighter-rouge">Table.to_pandas</code> will
yield pandas <code class="highlighter-rouge">Categorical</code> types from Arrow binary and string columns</li>
  <li>Expanded Hadoop Filesystem (HDFS) functionality to improve compatibility with
Dask and other HDFS-aware Python libraries.</li>
  <li>s3fs and other Dask-oriented filesystems can now be used with
<code class="highlighter-rouge">pyarrow.parquet.ParquetDataset</code></li>
  <li>More graceful handling of pandas’s nanosecond timestamps when writing to
Parquet format. You can now pass <code class="highlighter-rouge">coerce_timestamps='ms'</code> to cast to
milliseconds, or <code class="highlighter-rouge">'us'</code> for microseconds.</li>
</ul>

<h2 id="toward-arrow-100-and-beyond">Toward Arrow 1.0.0 and Beyond</h2>

<p>We are still discussing the roadmap to 1.0.0 release on the <a href="http://mail-archives.apache.org/mod_mbox/arrow-dev/">developer mailing
list</a>. The focus of the 1.0.0 release will likely be memory format stability
and hardening integration tests across the remaining data types implemented in
Java and C++. Please join the discussion there.</p>


  </div>

  

  
    
  <div class="blog-post" style="margin-bottom: 4rem">
    
<h1>
  Plasma In-Memory Object Store
  <a href="/blog/2017/08/08/plasma-in-memory-object-store/" class="permalink" title="Permalink">∞</a>
</h1>



<p>
  <span class="badge badge-secondary">Published</span>
  <span class="published">
    08 Aug 2017
  </span>
  <br />
  <span class="badge badge-secondary">By</span>
  
    Philipp Moritz and Robert Nishihara
  
</p>

    <!--

-->

<p><em><a href="https://people.eecs.berkeley.edu/~pcmoritz/">Philipp Moritz</a> and <a href="http://www.robertnishihara.com">Robert Nishihara</a> are graduate students at UC
 Berkeley.</em></p>

<h2 id="plasma-a-high-performance-shared-memory-object-store">Plasma: A High-Performance Shared-Memory Object Store</h2>

<h3 id="motivating-plasma">Motivating Plasma</h3>

<p>This blog post presents Plasma, an in-memory object store that is being
developed as part of Apache Arrow. <strong>Plasma holds immutable objects in shared
memory so that they can be accessed efficiently by many clients across process
boundaries.</strong> In light of the trend toward larger and larger multicore machines,
Plasma enables critical performance optimizations in the big data regime.</p>

<p>Plasma was initially developed as part of <a href="https://github.com/ray-project/ray">Ray</a>, and has recently been moved
to Apache Arrow in the hopes that it will be broadly useful.</p>

<p>One of the goals of Apache Arrow is to serve as a common data layer enabling
zero-copy data exchange between multiple frameworks. A key component of this
vision is the use of off-heap memory management (via Plasma) for storing and
sharing Arrow-serialized objects between applications.</p>

<p><strong>Expensive serialization and deserialization as well as data copying are a
common performance bottleneck in distributed computing.</strong> For example, a
Python-based execution framework that wishes to distribute computation across
multiple Python “worker” processes and then aggregate the results in a single
“driver” process may choose to serialize data using the built-in <code class="highlighter-rouge">pickle</code>
library. Assuming one Python process per core, each worker process would have to
copy and deserialize the data, resulting in excessive memory usage. The driver
process would then have to deserialize results from each of the workers,
resulting in a bottleneck.</p>

<p>Using Plasma plus Arrow, the data being operated on would be placed in the
Plasma store once, and all of the workers would read the data without copying or
deserializing it (the workers would map the relevant region of memory into their
own address spaces). The workers would then put the results of their computation
back into the Plasma store, which the driver could then read and aggregate
without copying or deserializing the data.</p>

<h3 id="the-plasma-api">The Plasma API:</h3>

<p>Below we illustrate a subset of the API. The C++ API is documented more fully
<a href="https://github.com/apache/arrow/blob/master/cpp/apidoc/tutorials/plasma.md">here</a>, and the Python API is documented <a href="https://github.com/apache/arrow/blob/master/python/doc/source/plasma.rst">here</a>.</p>

<p><strong>Object IDs:</strong> Each object is associated with a string of bytes.</p>

<p><strong>Creating an object:</strong> Objects are stored in Plasma in two stages. First, the
object store <em>creates</em> the object by allocating a buffer for it. At this point,
the client can write to the buffer and construct the object within the allocated
buffer. When the client is done, the client <em>seals</em> the buffer making the object
immutable and making it available to other Plasma clients.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Create an object.
</span><span class="n">object_id</span> <span class="o">=</span> <span class="n">pyarrow</span><span class="o">.</span><span class="n">plasma</span><span class="o">.</span><span class="n">ObjectID</span><span class="p">(</span><span class="mi">20</span> <span class="o">*</span> <span class="n">b</span><span class="s">'a'</span><span class="p">)</span>
<span class="n">object_size</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="nb">buffer</span> <span class="o">=</span> <span class="nb">memoryview</span><span class="p">(</span><span class="n">client</span><span class="o">.</span><span class="n">create</span><span class="p">(</span><span class="n">object_id</span><span class="p">,</span> <span class="n">object_size</span><span class="p">))</span>

<span class="c1"># Write to the buffer.
</span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1000</span><span class="p">):</span>
    <span class="nb">buffer</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>

<span class="c1"># Seal the object making it immutable and available to other clients.
</span><span class="n">client</span><span class="o">.</span><span class="n">seal</span><span class="p">(</span><span class="n">object_id</span><span class="p">)</span>
</code></pre></div></div>

<p><strong>Getting an object:</strong> After an object has been sealed, any client who knows the
object ID can get the object.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Get the object from the store. This blocks until the object has been sealed.
</span><span class="n">object_id</span> <span class="o">=</span> <span class="n">pyarrow</span><span class="o">.</span><span class="n">plasma</span><span class="o">.</span><span class="n">ObjectID</span><span class="p">(</span><span class="mi">20</span> <span class="o">*</span> <span class="n">b</span><span class="s">'a'</span><span class="p">)</span>
<span class="p">[</span><span class="n">buff</span><span class="p">]</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">get</span><span class="p">([</span><span class="n">object_id</span><span class="p">])</span>
<span class="nb">buffer</span> <span class="o">=</span> <span class="nb">memoryview</span><span class="p">(</span><span class="n">buff</span><span class="p">)</span>
</code></pre></div></div>

<p>If the object has not been sealed yet, then the call to <code class="highlighter-rouge">client.get</code> will block
until the object has been sealed.</p>

<h3 id="a-sorting-application">A sorting application</h3>

<p>To illustrate the benefits of Plasma, we demonstrate an <strong>11x speedup</strong> (on a
machine with 20 physical cores) for sorting a large pandas DataFrame (one
billion entries). The baseline is the built-in pandas sort function, which sorts
the DataFrame in 477 seconds. To leverage multiple cores, we implement the
following standard distributed sorting scheme.</p>

<ul>
  <li>We assume that the data is partitioned across K pandas DataFrames and that
each one already lives in the Plasma store.</li>
  <li>We subsample the data, sort the subsampled data, and use the result to define
L non-overlapping buckets.</li>
  <li>For each of the K data partitions and each of the L buckets, we find the
subset of the data partition that falls in the bucket, and we sort that
subset.</li>
  <li>For each of the L buckets, we gather all of the K sorted subsets that fall in
that bucket.</li>
  <li>For each of the L buckets, we merge the corresponding K sorted subsets.</li>
  <li>We turn each bucket into a pandas DataFrame and place it in the Plasma store.</li>
</ul>

<p>Using this scheme, we can sort the DataFrame (the data starts and ends in the
Plasma store), in 44 seconds, giving an 11x speedup over the baseline.</p>

<h3 id="design">Design</h3>

<p>The Plasma store runs as a separate process. It is written in C++ and is
designed as a single-threaded event loop based on the <a href="https://redis.io/">Redis</a> event loop library.
The plasma client library can be linked into applications. Clients communicate
with the Plasma store via messages serialized using <a href="https://google.github.io/flatbuffers/">Google Flatbuffers</a>.</p>

<h3 id="call-for-contributions">Call for contributions</h3>

<p>Plasma is a work in progress, and the API is currently unstable. Today Plasma is
primarily used in <a href="https://github.com/ray-project/ray">Ray</a> as an in-memory cache for Arrow serialized objects.
We are looking for a broader set of use cases to help refine Plasma’s API. In
addition, we are looking for contributions in a variety of areas including
improving performance and building other language bindings. Please let us know
if you are interested in getting involved with the project.</p>


  </div>

  

  
    
  <div class="blog-post" style="margin-bottom: 4rem">
    
<h1>
  Speeding up PySpark with Apache Arrow
  <a href="/blog/2017/07/26/spark-arrow/" class="permalink" title="Permalink">∞</a>
</h1>



<p>
  <span class="badge badge-secondary">Published</span>
  <span class="published">
    26 Jul 2017
  </span>
  <br />
  <span class="badge badge-secondary">By</span>
  
    BryanCutler
  
</p>

    <!--

-->

<p><em><a href="https://github.com/BryanCutler">Bryan Cutler</a> is a software engineer at IBM’s Spark Technology Center <a href="http://www.spark.tc/">STC</a></em></p>

<p>Beginning with <a href="https://spark.apache.org/">Apache Spark</a> version 2.3, <a href="https://arrow.apache.org/">Apache Arrow</a> will be a supported
dependency and begin to offer increased performance with columnar data transfer.
If you are a Spark user that prefers to work in Python and Pandas, this is a cause
to be excited over! The initial work is limited to collecting a Spark DataFrame
with <code class="highlighter-rouge">toPandas()</code>, which I will discuss below, however there are many additional
improvements that are currently <a href="https://issues.apache.org/jira/issues/?filter=12335725&amp;jql=project%20%3D%20SPARK%20AND%20status%20in%20(Open%2C%20%22In%20Progress%22%2C%20Reopened)%20AND%20text%20~%20%22arrow%22%20ORDER%20BY%20createdDate%20DESC">underway</a>.</p>

<h1 id="optimizing-spark-conversion-to-pandas">Optimizing Spark Conversion to Pandas</h1>

<p>The previous way of converting a Spark DataFrame to Pandas with <code class="highlighter-rouge">DataFrame.toPandas()</code>
in PySpark was painfully inefficient. Basically, it worked by first collecting all
rows to the Spark driver. Next, each row would get serialized into Python’s pickle
format and sent to a Python worker process. This child process unpickles each row into
a huge list of tuples. Finally, a Pandas DataFrame is created from the list using
<code class="highlighter-rouge">pandas.DataFrame.from_records()</code>.</p>

<p>This all might seem like standard procedure, but suffers from 2 glaring issues: 1)
even using CPickle, Python serialization is a slow process and 2) creating
a <code class="highlighter-rouge">pandas.DataFrame</code> using <code class="highlighter-rouge">from_records</code> must slowly iterate over the list of pure
Python data and convert each value to Pandas format. See <a href="https://gist.github.com/wesm/0cb5531b1c2e346a0007">here</a> for a detailed
analysis.</p>

<p>Here is where Arrow really shines to help optimize these steps: 1) Once the data is
in Arrow memory format, there is no need to serialize/pickle anymore as Arrow data can
be sent directly to the Python process, 2) When the Arrow data is received in Python,
then pyarrow can utilize zero-copy methods to create a <code class="highlighter-rouge">pandas.DataFrame</code> from entire
chunks of data at once instead of processing individual scalar values. Additionally,
the conversion to Arrow data can be done on the JVM and pushed back for the Spark
executors to perform in parallel, drastically reducing the load on the driver.</p>

<p>As of the merging of <a href="https://issues.apache.org/jira/browse/SPARK-13534">SPARK-13534</a>, the use of Arrow when calling <code class="highlighter-rouge">toPandas()</code>
needs to be enabled by setting the SQLConf “spark.sql.execution.arrow.enabled” to
“true”.  Let’s look at a simple usage example.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 2.3.0-SNAPSHOT
      /_/

Using Python version 2.7.13 (default, Dec 20 2016 23:09:15)
SparkSession available as 'spark'.

In [1]: from pyspark.sql.functions import rand
   ...: df = spark.range(1 &lt;&lt; 22).toDF("id").withColumn("x", rand())
   ...: df.printSchema()
   ...: 
root
 |-- id: long (nullable = false)
 |-- x: double (nullable = false)


In [2]: %time pdf = df.toPandas()
CPU times: user 17.4 s, sys: 792 ms, total: 18.1 s
Wall time: 20.7 s

In [3]: spark.conf.set("spark.sql.execution.arrow.enabled", "true")

In [4]: %time pdf = df.toPandas()
CPU times: user 40 ms, sys: 32 ms, total: 72 ms                                 
Wall time: 737 ms

In [5]: pdf.describe()
Out[5]: 
                 id             x
count  4.194304e+06  4.194304e+06
mean   2.097152e+06  4.998996e-01
std    1.210791e+06  2.887247e-01
min    0.000000e+00  8.291929e-07
25%    1.048576e+06  2.498116e-01
50%    2.097152e+06  4.999210e-01
75%    3.145727e+06  7.498380e-01
max    4.194303e+06  9.999996e-01
</code></pre></div></div>

<p>This example was run locally on my laptop using Spark defaults so the times
shown should not be taken precisely. Even though, it is clear there is a huge
performance boost and using Arrow took something that was excruciatingly slow
and speeds it up to be barely noticeable.</p>

<h1 id="notes-on-usage">Notes on Usage</h1>

<p>Here are some things to keep in mind before making use of this new feature. At
the time of writing this, pyarrow will not be installed automatically with
pyspark and needs to be manually installed, see installation <a href="https://github.com/apache/arrow/blob/master/site/install.md">instructions</a>.
It is planned to add pyarrow as a pyspark dependency so that 
<code class="highlighter-rouge">&gt; pip install pyspark</code> will also install pyarrow.</p>

<p>Currently, the controlling SQLConf is disabled by default. This can be enabled
programmatically as in the example above or by adding the line
“spark.sql.execution.arrow.enabled=true” to <code class="highlighter-rouge">SPARK_HOME/conf/spark-defaults.conf</code>.</p>

<p>Also, not all Spark data types are currently supported and limited to primitive
types. Expanded type support is in the works and expected to also be in the Spark
2.3 release.</p>

<h1 id="future-improvements">Future Improvements</h1>

<p>As mentioned, this was just a first step in using Arrow to make life easier for
Spark Python users. A few exciting initiatives in the works are to allow for
vectorized UDF evaluation (<a href="https://issues.apache.org/jira/browse/SPARK-21190">SPARK-21190</a>, <a href="https://issues.apache.org/jira/browse/SPARK-21404">SPARK-21404</a>), and the ability
to apply a function on grouped data using a Pandas DataFrame (<a href="https://issues.apache.org/jira/browse/SPARK-20396">SPARK-20396</a>).
Just as Arrow helped in converting a Spark to Pandas, it can also work in the
other direction when creating a Spark DataFrame from an existing Pandas
DataFrame (<a href="https://issues.apache.org/jira/browse/SPARK-20791">SPARK-20791</a>). Stay tuned for more!</p>

<h1 id="collaborators">Collaborators</h1>

<p>Reaching this first milestone was a group effort from both the Apache Arrow and
Spark communities. Thanks to the hard work of <a href="https://github.com/wesm">Wes McKinney</a>, <a href="https://github.com/icexelloss">Li Jin</a>,
<a href="https://github.com/holdenk">Holden Karau</a>, Reynold Xin, Wenchen Fan, Shane Knapp and many others that
helped push this effort forwards.</p>


  </div>

  

  
    
  <div class="blog-post" style="margin-bottom: 4rem">
    
<h1>
  Apache Arrow 0.5.0 Release
  <a href="/blog/2017/07/25/0.5.0-release/" class="permalink" title="Permalink">∞</a>
</h1>



<p>
  <span class="badge badge-secondary">Published</span>
  <span class="published">
    25 Jul 2017
  </span>
  <br />
  <span class="badge badge-secondary">By</span>
  
    <a href="https://wesmckinney.com">Wes McKinney (wesm) </a>
  
</p>

    <!--

-->

<p>The Apache Arrow team is pleased to announce the 0.5.0 release. It includes
<a href="https://issues.apache.org/jira/issues/?jql=project%20%3D%20ARROW%20AND%20status%20in%20(Resolved%2C%20Closed)%20AND%20fixVersion%20%3D%200.5.0"><strong>130 resolved JIRAs</strong></a> with some new features, expanded integration
testing between implementations, and bug fixes. The Arrow memory format remains
stable since the 0.3.x and 0.4.x releases.</p>

<p>See the <a href="http://arrow.apache.org/install">Install Page</a> to learn how to get the libraries for your
platform. The <a href="http://arrow.apache.org/release/0.5.0.html">complete changelog</a> is also available.</p>

<h2 id="expanded-integration-testing">Expanded Integration Testing</h2>

<p>In this release, we added compatibility tests for dictionary-encoded data
between Java and C++. This enables the distinct values (the <em>dictionary</em>) in a
vector to be transmitted as part of an Arrow schema while the record batches
contain integers which correspond to the dictionary.</p>

<p>So we might have:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>data (string): ['foo', 'bar', 'foo', 'bar']
</code></pre></div></div>

<p>In dictionary-encoded form, this could be represented as:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>indices (int8): [0, 1, 0, 1]
dictionary (string): ['foo', 'bar']
</code></pre></div></div>

<p>In upcoming releases, we plan to complete integration testing for the remaining
data types (including some more complicated types like unions and decimals) on
the road to a 1.0.0 release in the future.</p>

<h2 id="c-activity">C++ Activity</h2>

<p>We completed a number of significant pieces of work in the C++ part of Apache
Arrow.</p>

<h3 id="using-jemalloc-as-default-memory-allocator">Using jemalloc as default memory allocator</h3>

<p>We decided to use <a href="https://github.com/jemalloc/jemalloc">jemalloc</a> as the default memory allocator unless it is
explicitly disabled. This memory allocator has significant performance
advantages in Arrow workloads over the default <code class="highlighter-rouge">malloc</code> implementation. We will
publish a blog post going into more detail about this and why you might care.</p>

<h3 id="sharing-more-c-code-with-apache-parquet">Sharing more C++ code with Apache Parquet</h3>

<p>We imported the compression library interfaces and dictionary encoding
algorithms from the <a href="http://github.com/apache/parquet-cpp">Apache Parquet C++ library</a>. The Parquet library now
depends on this code in Arrow, and we will be able to use it more easily for
data compression in Arrow use cases.</p>

<p>As part of incorporating Parquet’s dictionary encoding utilities, we have
developed an <code class="highlighter-rouge">arrow::DictionaryBuilder</code> class to enable building
dictionary-encoded arrays iteratively. This can help save memory and yield
better performance when interacting with databases, Parquet files, or other
sources which may have columns having many duplicates.</p>

<h3 id="support-for-lz4-and-zstd-compressors">Support for LZ4 and ZSTD compressors</h3>

<p>We added LZ4 and ZSTD compression library support. In ARROW-300 and other
planned work, we intend to add some compression features for data sent via RPC.</p>

<h2 id="python-activity">Python Activity</h2>

<p>We fixed many bugs which were affecting Parquet and Feather users and fixed
several other rough edges with normal Arrow use. We also added some additional
Arrow type conversions: structs, lists embedded in pandas objects, and Arrow
time types (which deserialize to the <code class="highlighter-rouge">datetime.time</code> type).</p>

<p>In upcoming releases we plan to continue to improve <a href="http://github.com/dask/dask">Dask</a> support and
performance for distributed processing of Apache Parquet files with pyarrow.</p>

<h2 id="the-road-ahead">The Road Ahead</h2>

<p>We have much work ahead of us to build out Arrow integrations in other data
systems to improve their processing performance and interoperability with other
systems.</p>

<p>We are discussing the roadmap to a future 1.0.0 release on the <a href="http://mail-archives.apache.org/mod_mbox/arrow-dev/">developer
mailing list</a>. Please join the discussion there.</p>


  </div>

  

  
    
  <div class="blog-post" style="margin-bottom: 4rem">
    
<h1>
  Connecting Relational Databases to the Apache Arrow World with turbodbc
  <a href="/blog/2017/06/16/turbodbc-arrow/" class="permalink" title="Permalink">∞</a>
</h1>



<p>
  <span class="badge badge-secondary">Published</span>
  <span class="published">
    16 Jun 2017
  </span>
  <br />
  <span class="badge badge-secondary">By</span>
  
    <a href="http://github.com/MathMagique">Michael König (MathMagique) </a>
  
</p>

    <!--

-->

<p><em><a href="https://github.com/mathmagique">Michael König</a> is the lead developer of the <a href="https://github.com/blue-yonder/turbodbc">turbodbc project</a></em></p>

<p>The <a href="https://arrow.apache.org/">Apache Arrow</a> project set out to become the universal data layer for
column-oriented data processing systems without incurring serialization costs
or compromising on performance on a more general level. While relational
databases still lag behind in Apache Arrow adoption, the Python database module
<a href="https://github.com/blue-yonder/turbodbc">turbodbc</a> brings Apache Arrow support to these databases using a much
older, more specialized data exchange layer: <a href="https://en.wikipedia.org/wiki/Open_Database_Connectivity">ODBC</a>.</p>

<p>ODBC is a database interface that offers developers the option to transfer data
either in row-wise or column-wise fashion. Previous Python ODBC modules typically
use the row-wise approach, and often trade repeated database roundtrips for simplified
buffer handling. This makes them less suited for data-intensive applications,
particularly when interfacing with modern columnar analytical databases.</p>

<p>In contrast, turbodbc was designed to leverage columnar data processing from day
one. Naturally, this implies using the columnar portion of the ODBC API. Equally
important, however, is to find new ways of providing columnar data to Python users
that exceed the capabilities of the row-wise API mandated by Python’s <a href="https://www.python.org/dev/peps/pep-0249/">PEP 249</a>.
Turbodbc has adopted Apache Arrow for this very task with the recently released
version 2.0.0:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="kn">from</span> <span class="nn">turbodbc</span> <span class="kn">import</span> <span class="n">connect</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">connection</span> <span class="o">=</span> <span class="n">connect</span><span class="p">(</span><span class="n">dsn</span><span class="o">=</span><span class="s">"My columnar database"</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">cursor</span> <span class="o">=</span> <span class="n">connection</span><span class="o">.</span><span class="n">cursor</span><span class="p">()</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">cursor</span><span class="o">.</span><span class="n">execute</span><span class="p">(</span><span class="s">"SELECT some_integers, some_strings FROM my_table"</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">cursor</span><span class="o">.</span><span class="n">fetchallarrow</span><span class="p">()</span>
<span class="n">pyarrow</span><span class="o">.</span><span class="n">Table</span>
<span class="n">some_integers</span><span class="p">:</span> <span class="n">int64</span>
<span class="n">some_strings</span><span class="p">:</span> <span class="n">string</span>
</code></pre></div></div>

<p>With this new addition, the data flow for a result set of a typical SELECT query
is like this:</p>
<ul>
  <li>The database prepares the result set and exposes it to the ODBC driver using
either row-wise or column-wise storage.</li>
  <li>Turbodbc has the ODBC driver write chunks of the result set into columnar buffers.</li>
  <li>These buffers are exposed to turbodbc’s Apache Arrow frontend. This frontend
will create an Arrow table and fill in the buffered values.</li>
  <li>The previous steps are repeated until the entire result set is retrieved.</li>
</ul>

<p><img src="/img/turbodbc_arrow.png" alt="Data flow from relational databases to Python with turbodbc and the Apache Arrow frontend" class="img-responsive" width="75%" /></p>

<p>In practice, it is possible to achieve the following ideal situation: A 64-bit integer
column is stored as one contiguous block of memory in a columnar database. A huge chunk
of 64-bit integers is transferred over the network and the ODBC driver directly writes
it to a turbodbc buffer of 64-bit integers. The Arrow frontend accumulates these values
by copying the entire 64-bit buffer into a free portion of an Arrow table’s 64-bit
integer column.</p>

<p>Moving data from the database to an Arrow table and, thus, providing it to the Python
user can be as simple as copying memory blocks around, megabytes equivalent to hundred
thousands of rows at a time. The absence of serialization and conversion logic renders
the process extremely efficient.</p>

<p>Once the data is stored in an Arrow table, Python users can continue to do some
actual work. They can convert it into a <a href="https://arrow.apache.org/docs/python/pandas.html">Pandas DataFrame</a> for data analysis
(using a quick <code class="highlighter-rouge">table.to_pandas()</code>), pass it on to other data processing
systems such as <a href="http://spark.apache.org/">Apache Spark</a> or <a href="http://impala.apache.org/">Apache Impala (incubating)</a>, or store
it in the <a href="http://parquet.apache.org/">Apache Parquet</a> file format. This way, non-Python systems are
efficiently connected with relational databases.</p>

<p>In the future, turbodbc’s Arrow support will be extended to use more
sophisticated features such as <a href="https://arrow.apache.org/docs/memory_layout.html#dictionary-encoding">dictionary-encoded</a> string fields. We also
plan to pick smaller than 64-bit <a href="https://arrow.apache.org/docs/metadata.html#integers">data types</a> where possible. Last but not
least, Arrow support will be extended to cover the reverse direction of data
flow, so that Python users can quickly insert Arrow tables into relational
databases.</p>

<p>If you would like to learn more about turbodbc, check out the <a href="https://github.com/blue-yonder/turbodbc">GitHub project</a> and the
<a href="http://turbodbc.readthedocs.io/">project documentation</a>. If you want to learn more about how turbodbc implements the
nitty-gritty details, check out parts <a href="https://tech.blue-yonder.com/making-of-turbodbc-part-1-wrestling-with-the-side-effects-of-a-c-api/">one</a> and <a href="https://tech.blue-yonder.com/making-of-turbodbc-part-2-c-to-python/">two</a> of the
<a href="https://tech.blue-yonder.com/making-of-turbodbc-part-1-wrestling-with-the-side-effects-of-a-c-api/">“Making of turbodbc”</a> series at <a href="https://tech.blue-yonder.com/">Blue Yonder’s technology blog</a>.</p>


  </div>

  

  
    
  <div class="blog-post" style="margin-bottom: 4rem">
    
<h1>
  Apache Arrow 0.4.1 Release
  <a href="/blog/2017/06/14/0.4.1-release/" class="permalink" title="Permalink">∞</a>
</h1>



<p>
  <span class="badge badge-secondary">Published</span>
  <span class="published">
    14 Jun 2017
  </span>
  <br />
  <span class="badge badge-secondary">By</span>
  
    <a href="https://wesmckinney.com">Wes McKinney (wesm) </a>
  
</p>

    <!--

-->

<p>The Apache Arrow team is pleased to announce the 0.4.1 release of the
project. This is a bug fix release that addresses a regression with Decimal
types in the Java implementation introduced in 0.4.0 (see
<a href="https://issues.apache.org/jira/browse/ARROW-1091">ARROW-1091</a>). There were a total of <a href="https://issues.apache.org/jira/issues/?jql=project%20%3D%20ARROW%20AND%20status%20in%20(Resolved%2C%20Closed)%20AND%20fixVersion%20%3D%200.4.1">31 resolved JIRAs</a>.</p>

<p>See the <a href="http://arrow.apache.org/install">Install Page</a> to learn how to get the libraries for your platform.</p>

<h3 id="python-wheel-installers-for-windows">Python Wheel Installers for Windows</h3>

<p>Max Risuhin contributed fixes to enable binary wheel installers to be generated
for Python 3.5 and 3.6. Thus, 0.4.1 is the first Arrow release for which
PyArrow including bundled <a href="http://parquet.apache.org">Apache Parquet</a> support that can be installed
with either conda or pip across the 3 major platforms: Linux, macOS, and
Windows. Use one of:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip install pyarrow
conda install pyarrow -c conda-forge
</code></pre></div></div>

<h3 id="turbodbc-200-with-apache-arrow-support">Turbodbc 2.0.0 with Apache Arrow Support</h3>

<p><a href="http://turbodbc.readthedocs.io/">Turbodbc</a>, a fast C++ ODBC interface with Python bindings, released
version 2.0.0 including reading SQL result sets as Arrow record batches. The
team used the PyArrow C++ API introduced in version 0.4.0 to construct
<code class="highlighter-rouge">pyarrow.Table</code> objects inside the <code class="highlighter-rouge">turbodbc</code> library. Learn more in their
<a href="http://turbodbc.readthedocs.io/en/latest/pages/advanced_usage.html#apache-arrow-support">documentation</a> and install with one of:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip install turbodbc
conda install turbodbc -c conda-forge
</code></pre></div></div>


  </div>

  

  
    
  <div class="blog-post" style="margin-bottom: 4rem">
    
<h1>
  Apache Arrow 0.4.0 Release
  <a href="/blog/2017/05/23/0.4.0-release/" class="permalink" title="Permalink">∞</a>
</h1>



<p>
  <span class="badge badge-secondary">Published</span>
  <span class="published">
    23 May 2017
  </span>
  <br />
  <span class="badge badge-secondary">By</span>
  
    <a href="https://wesmckinney.com">Wes McKinney (wesm) </a>
  
</p>

    <!--

-->

<p>The Apache Arrow team is pleased to announce the 0.4.0 release of the
project. While only 17 days since the release, it includes <a href="https://issues.apache.org/jira/issues/?jql=project%20%3D%20ARROW%20AND%20status%20in%20(Resolved%2C%20Closed)%20AND%20fixVersion%20%3D%200.4.0"><strong>77 resolved
JIRAs</strong></a> with some important new features and bug fixes.</p>

<p>See the <a href="http://arrow.apache.org/install">Install Page</a> to learn how to get the libraries for your platform.</p>

<h3 id="expanded-javascript-implementation">Expanded JavaScript Implementation</h3>

<p>The TypeScript Arrow implementation has undergone some work since 0.3.0 and can
now read a substantial portion of the Arrow streaming binary format. As this
implementation develops, we will eventually want to include JS in the
integration test suite along with Java and C++ to ensure wire
cross-compatibility.</p>

<h3 id="python-support-for-apache-parquet-on-windows">Python Support for Apache Parquet on Windows</h3>

<p>With the <a href="https://github.com/apache/parquet-cpp/releases/tag/apache-parquet-cpp-1.1.0">1.1.0 C++ release</a> of <a href="http://parquet.apache.org">Apache Parquet</a>, we have enabled the
<code class="highlighter-rouge">pyarrow.parquet</code> extension on Windows for Python 3.5 and 3.6. This should
appear in conda-forge packages and PyPI in the near future. Developers can
follow the <a href="http://arrow.apache.org/docs/python/development.html">source build instructions</a>.</p>

<h3 id="generalizing-arrow-streams">Generalizing Arrow Streams</h3>

<p>In the 0.2.0 release, we defined the first version of the Arrow streaming
binary format for low-cost messaging with columnar data. These streams presume
that the message components are written as a continuous byte stream over a
socket or file.</p>

<p>We would like to be able to support other other transport protocols, like
<a href="http://grpc.io/">gRPC</a>, for the message components of Arrow streams. To that end, in C++ we
defined an abstract stream reader interface, for which the current contiguous
streaming format is one implementation:</p>

<figure class="highlight"><pre><code class="language-cpp" data-lang="cpp"><span class="k">class</span> <span class="nc">RecordBatchReader</span> <span class="p">{</span>
 <span class="nl">public:</span>
  <span class="k">virtual</span> <span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">Schema</span><span class="o">&gt;</span> <span class="n">schema</span><span class="p">()</span> <span class="k">const</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
  <span class="k">virtual</span> <span class="n">Status</span> <span class="n">GetNextRecordBatch</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">RecordBatch</span><span class="o">&gt;*</span> <span class="n">batch</span><span class="p">)</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">};</span></code></pre></figure>

<p>It would also be good to define abstract stream reader and writer interfaces in
the Java implementation.</p>

<p>In an upcoming blog post, we will explain in more depth how Arrow streams work,
but you can learn more about them by reading the <a href="http://arrow.apache.org/docs/ipc.html">IPC specification</a>.</p>

<h3 id="c-and-cython-api-for-python-extensions">C++ and Cython API for Python Extensions</h3>

<p>As other Python libraries with C or C++ extensions use Apache Arrow, they will
need to be able to return Python objects wrapping the underlying C++
objects. In this release, we have implemented a prototype C++ API which enables
Python wrapper objects to be constructed from C++ extension code:</p>

<figure class="highlight"><pre><code class="language-cpp" data-lang="cpp"><span class="cp">#include "arrow/python/pyarrow.h"
</span>
<span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">arrow</span><span class="o">::</span><span class="n">py</span><span class="o">::</span><span class="n">import_pyarrow</span><span class="p">())</span> <span class="p">{</span>
  <span class="c1">// Error</span>
<span class="p">}</span>

<span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">arrow</span><span class="o">::</span><span class="n">RecordBatch</span><span class="o">&gt;</span> <span class="n">cpp_batch</span> <span class="o">=</span> <span class="n">GetData</span><span class="p">(...);</span>
<span class="n">PyObject</span><span class="o">*</span> <span class="n">py_batch</span> <span class="o">=</span> <span class="n">arrow</span><span class="o">::</span><span class="n">py</span><span class="o">::</span><span class="n">wrap_batch</span><span class="p">(</span><span class="n">cpp_batch</span><span class="p">);</span></code></pre></figure>

<p>This API is intended to be usable from Cython code as well:</p>

<figure class="highlight"><pre><code class="language-cython" data-lang="cython"><span class="kn">cimport</span> <span class="nn">pyarrow</span>
<span class="n">pyarrow</span><span class="o">.</span><span class="nf">import_pyarrow</span><span class="p">()</span></code></pre></figure>

<h3 id="python-wheel-installers-on-macos">Python Wheel Installers on macOS</h3>

<p>With this release, <code class="highlighter-rouge">pip install pyarrow</code> works on macOS (OS X) as well as
Linux. We are working on providing binary wheel installers for Windows as well.</p>


  </div>

  

  
    
  <div class="blog-post" style="margin-bottom: 4rem">
    
<h1>
  Apache Arrow 0.3.0 Release
  <a href="/blog/2017/05/08/0.3-release/" class="permalink" title="Permalink">∞</a>
</h1>



<p>
  <span class="badge badge-secondary">Published</span>
  <span class="published">
    08 May 2017
  </span>
  <br />
  <span class="badge badge-secondary">By</span>
  
    <a href="https://wesmckinney.com">Wes McKinney (wesm) </a>
  
</p>

    <!--

-->

<p>Translations: <a href="/blog/2017/05/08/0.3-release-japanese/">日本語</a></p>

<p>The Apache Arrow team is pleased to announce the 0.3.0 release of the
project. It is the product of an intense 10 weeks of development since the
0.2.0 release from this past February. It includes <a href="https://issues.apache.org/jira/issues/?jql=project%20%3D%20ARROW%20AND%20status%20in%20(Resolved%2C%20Closed)%20AND%20fixVersion%20%3D%200.3.0"><strong>306 resolved JIRAs</strong></a>
from <a href="https://github.com/apache/arrow/graphs/contributors"><strong>23 contributors</strong></a>.</p>

<p>While we have added many new features to the different Arrow implementations,
one of the major development focuses in 2017 has been hardening the in-memory
format, type metadata, and messaging protocol to provide a <strong>stable,
production-ready foundation</strong> for big data applications. We are excited to be
collaborating with the <a href="http://spark.apache.org">Apache Spark</a> and <a href="http://www.geomesa.org/">GeoMesa</a> communities on
utilizing Arrow for high performance IO and in-memory data processing.</p>

<p>See the <a href="http://arrow.apache.org/install">Install Page</a> to learn how to get the libraries for your platform.</p>

<p>We will be publishing more information about the Apache Arrow roadmap as we
forge ahead with using Arrow to accelerate big data systems.</p>

<p>We are looking for more contributors from within our existing communities and
from other communities (such as Go, R, or Julia) to get involved in Arrow
development.</p>

<h3 id="file-and-streaming-format-hardening">File and Streaming Format Hardening</h3>

<p>The 0.2.0 release brought with it the first iterations of the <strong>random access</strong>
and <strong>streaming</strong> Arrow wire formats. See the <a href="http://arrow.apache.org/docs/ipc.html">IPC specification</a> for
implementation details and <a href="http://wesmckinney.com/blog/arrow-streaming-columnar/">example blog post</a> with some use cases. These
provide low-overhead, zero-copy access to Arrow record batch payloads.</p>

<p>In 0.3.0 we have solidified a number of small details with the binary format
and improved our integration and unit testing particularly in the Java, C++,
and Python libraries. Using the <a href="http://github.com/google/flatbuffers">Google Flatbuffers</a> project has helped with
adding new features to our metadata without breaking forward compatibility.</p>

<p>We are not yet ready to make a firm commitment to strong forward compatibility
(in case we find something needs to change) in the binary format, but we will
make efforts between major releases to not make unnecessary
breakages. Contributions to the website and component user and API
documentation would also be most welcome.</p>

<h3 id="dictionary-encoding-support">Dictionary Encoding Support</h3>

<p><a href="https://github.com/elahrvivaz">Emilio Lahr-Vivaz</a> from the <a href="http://www.geomesa.org/">GeoMesa</a> project contributed Java support
for dictionary-encoded Arrow vectors. We followed up with C++ and Python
support (and <code class="highlighter-rouge">pandas.Categorical</code> integration). We have not yet implemented
full integration tests for dictionaries (for sending this data between C++ and
Java), but hope to achieve this in the 0.4.0 Arrow release.</p>

<p>This common data representation technique for categorical data allows multiple
record batches to share a common “dictionary”, with the values in the batches
being represented as integers referencing the dictionary. This data is called
“categorical” or “factor” in statistical languages, while in file formats like
Apache Parquet it is strictly used for data compression.</p>

<h3 id="expanded-date-time-and-fixed-size-types">Expanded Date, Time, and Fixed Size Types</h3>

<p>A notable omission from the 0.2.0 release was complete and integration-tested
support for the gamut of date and time types that occur in the wild. These are
needed for <a href="http://parquet.apache.org">Apache Parquet</a> and Apache Spark integration.</p>

<ul>
  <li><strong>Date</strong>: 32-bit (days unit) and 64-bit (milliseconds unit)</li>
  <li><strong>Time</strong>: 64-bit integer with unit (second, millisecond, microsecond, nanosecond)</li>
  <li><strong>Timestamp</strong>: 64-bit integer with unit, with or without timezone</li>
  <li><strong>Fixed Size Binary</strong>: Primitive values occupying certain number of bytes</li>
  <li><strong>Fixed Size List</strong>: List values with constant size (no separate offsets vector)</li>
</ul>

<p>We have additionally added experimental support for exact decimals in C++ using
<a href="https://github.com/boostorg/multiprecision">Boost.Multiprecision</a>, though we have not yet hardened the Decimal memory
format between the Java and C++ implementations.</p>

<h3 id="c-and-python-support-on-windows">C++ and Python Support on Windows</h3>

<p>We have made many general improvements to development and packaging for general
C++ and Python development. 0.3.0 is the first release to bring full C++ and
Python support for Windows on Visual Studio (MSVC) 2015 and 2017. In addition
to adding Appveyor continuous integration for MSVC, we have also written guides
for building from source on Windows: <a href="https://github.com/apache/arrow/blob/master/cpp/apidoc/Windows.md">C++</a> and <a href="https://github.com/apache/arrow/blob/master/python/doc/source/development.rst">Python</a>.</p>

<p>For the first time, you can install the Arrow Python library on Windows from
<a href="https://conda-forge.github.io">conda-forge</a>:</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>conda <span class="nb">install </span>pyarrow <span class="nt">-c</span> conda-forge
</code></pre></div></div>

<h3 id="c-glib-bindings-with-support-for-ruby-lua-and-more">C (GLib) Bindings, with support for Ruby, Lua, and more</h3>

<p><a href="http://github.com/kou">Kouhei Sutou</a> is a new Apache Arrow contributor and has contributed GLib C
bindings (to the C++ libraries) for Linux. Using a C middleware framework
called <a href="https://wiki.gnome.org/Projects/GObjectIntrospection">GObject Introspection</a>, it is possible to use these bindings
seamlessly in Ruby, Lua, Go, and <a href="https://wiki.gnome.org/Projects/GObjectIntrospection/Users">other programming languages</a>. We will
probably need to publish some follow up blogs explaining how these bindings
work and how to use them.</p>

<h3 id="apache-spark-integration-for-pyspark">Apache Spark Integration for PySpark</h3>

<p>We have been collaborating with the Apache Spark community on <a href="https://issues.apache.org/jira/browse/SPARK-13534">SPARK-13534</a>
to add support for using Arrow to accelerate <code class="highlighter-rouge">DataFrame.toPandas</code> in
PySpark. We have observed over <a href="https://github.com/apache/spark/pull/15821#issuecomment-282175163"><strong>40x speedup</strong></a> from the more efficient
data serialization.</p>

<p>Using Arrow in PySpark opens the door to many other performance optimizations,
particularly around UDF evaluation (e.g. <code class="highlighter-rouge">map</code> and <code class="highlighter-rouge">filter</code> operations with
Python lambda functions).</p>

<h3 id="new-python-feature-memory-views-feather-apache-parquet-support">New Python Feature: Memory Views, Feather, Apache Parquet support</h3>

<p>Arrow’s Python library <code class="highlighter-rouge">pyarrow</code> is a Cython binding for the <code class="highlighter-rouge">libarrow</code> and
<code class="highlighter-rouge">libarrow_python</code> C++ libraries, which handle inteoperability with NumPy,
<a href="http://pandas.pydata.org">pandas</a>, and the Python standard library.</p>

<p>At the heart of Arrow’s C++ libraries is the <code class="highlighter-rouge">arrow::Buffer</code> object, which is a
managed memory view supporting zero-copy reads and slices. <a href="https://github.com/JeffKnupp">Jeff Knupp</a>
contributed integration between Arrow buffers and the Python buffer protocol
and memoryviews, so now code like this is possible:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">In</span> <span class="p">[</span><span class="mi">6</span><span class="p">]:</span> <span class="kn">import</span> <span class="nn">pyarrow</span> <span class="k">as</span> <span class="n">pa</span>

<span class="n">In</span> <span class="p">[</span><span class="mi">7</span><span class="p">]:</span> <span class="n">buf</span> <span class="o">=</span> <span class="n">pa</span><span class="o">.</span><span class="n">frombuffer</span><span class="p">(</span><span class="n">b</span><span class="s">'foobarbaz'</span><span class="p">)</span>

<span class="n">In</span> <span class="p">[</span><span class="mi">8</span><span class="p">]:</span> <span class="n">buf</span>
<span class="n">Out</span><span class="p">[</span><span class="mi">8</span><span class="p">]:</span> <span class="o">&lt;</span><span class="n">pyarrow</span><span class="o">.</span><span class="n">_io</span><span class="o">.</span><span class="n">Buffer</span> <span class="n">at</span> <span class="mh">0x7f6c0a84b538</span><span class="o">&gt;</span>

<span class="n">In</span> <span class="p">[</span><span class="mi">9</span><span class="p">]:</span> <span class="nb">memoryview</span><span class="p">(</span><span class="n">buf</span><span class="p">)</span>
<span class="n">Out</span><span class="p">[</span><span class="mi">9</span><span class="p">]:</span> <span class="o">&lt;</span><span class="n">memory</span> <span class="n">at</span> <span class="mh">0x7f6c0a8c5e88</span><span class="o">&gt;</span>

<span class="n">In</span> <span class="p">[</span><span class="mi">10</span><span class="p">]:</span> <span class="n">buf</span><span class="o">.</span><span class="n">to_pybytes</span><span class="p">()</span>
<span class="n">Out</span><span class="p">[</span><span class="mi">10</span><span class="p">]:</span> <span class="n">b</span><span class="s">'foobarbaz'</span>
</code></pre></div></div>

<p>We have significantly expanded <a href="http://parquet.apache.org"><strong>Apache Parquet</strong></a> support via the C++
Parquet implementation <a href="https://github.com/apache/parquet-cpp">parquet-cpp</a>. This includes support for partitioned
datasets on disk or in HDFS. We added initial Arrow-powered Parquet support <a href="https://github.com/dask/dask/commit/68f9e417924a985c1f2e2a587126833c70a2e9f4">in
the Dask project</a>, and look forward to more collaborations with the Dask
developers on distributed processing of pandas data.</p>

<p>With Arrow’s support for pandas maturing, we were able to merge in the
<a href="https://github.com/wesm/feather"><strong>Feather format</strong></a> implementation, which is essentially a special case of
the Arrow random access format. We’ll be continuing Feather development within
the Arrow codebase. For example, Feather can now read and write with Python
file objects using Arrow’s Python binding layer.</p>

<p>We also implemented more robust support for pandas-specific data types, like
<code class="highlighter-rouge">DatetimeTZ</code> and <code class="highlighter-rouge">Categorical</code>.</p>

<h3 id="support-for-tensors-and-beyond-in-c-library">Support for Tensors and beyond in C++ Library</h3>

<p>There has been increased interest in using Apache Arrow as a tool for zero-copy
shared memory management for machine learning applications. A flagship example
is the <a href="https://github.com/ray-project/ray">Ray project</a> from the UC Berkeley <a href="https://rise.cs.berkeley.edu/">RISELab</a>.</p>

<p>Machine learning deals in additional kinds of data structures beyond what the
Arrow columnar format supports, like multidimensional arrays aka “tensors”. As
such, we implemented the <a href="http://arrow.apache.org/docs/cpp/classarrow_1_1_tensor.html"><code class="highlighter-rouge">arrow::Tensor</code></a> C++ type which can utilize the
rest of Arrow’s zero-copy shared memory machinery (using <code class="highlighter-rouge">arrow::Buffer</code> for
managing memory lifetime). In C++ in particular, we will want to provide for
additional data structures utilizing common IO and memory management tools.</p>

<h3 id="start-of-javascript-typescript-implementation">Start of JavaScript (TypeScript) Implementation</h3>

<p><a href="https://github.com/TheNeuralBit">Brian Hulette</a> started developing an Arrow implementation in
<a href="https://github.com/apache/arrow/tree/master/js">TypeScript</a> for use in NodeJS and browser-side applications. We are
benefitting from Flatbuffers’ first class support for JavaScript.</p>

<h3 id="improved-website-and-developer-documentation">Improved Website and Developer Documentation</h3>

<p>Since 0.2.0 we have implemented a new website stack for publishing
documentation and blogs based on <a href="https://jekyllrb.com">Jekyll</a>. Kouhei Sutou developed a <a href="https://github.com/red-data-tools/jekyll-jupyter-notebook">Jekyll
Jupyter Notebook plugin</a> so that we can use Jupyter to author content for
the Arrow website.</p>

<p>On the website, we have now published API documentation for the C, C++, Java,
and Python subcomponents. Within these you will find easier-to-follow developer
instructions for getting started.</p>

<h3 id="contributors">Contributors</h3>

<p>Thanks to all who contributed patches to this release.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ git shortlog -sn apache-arrow-0.2.0..apache-arrow-0.3.0
    119 Wes McKinney
     55 Kouhei Sutou
     18 Uwe L. Korn
     17 Julien Le Dem
      9 Phillip Cloud
      6 Bryan Cutler
      5 Philipp Moritz
      5 Emilio Lahr-Vivaz
      4 Max Risuhin
      4 Johan Mabille
      4 Jeff Knupp
      3 Steven Phillips
      3 Miki Tebeka
      2 Leif Walsh
      2 Jeff Reback
      2 Brian Hulette
      1 Tsuyoshi Ozawa
      1 rvernica
      1 Nong Li
      1 Julien Lafaye
      1 Itai Incze
      1 Holden Karau
      1 Deepak Majeti
</code></pre></div></div>


  </div>

  

  


    </main>

    <hr/>
<footer class="footer">
  <p>Apache Arrow, Arrow, Apache, the Apache feather logo, and the Apache Arrow project logo are either registered trademarks or trademarks of The Apache Software Foundation in the United States and other countries.</p>
  <p>&copy; 2016-2019 The Apache Software Foundation</p>
  <script type="text/javascript" src="/assets/main-8d2a359fd27a888246eb638b36a4e8b68ac65b9f11c48b9fac601fa0c9a7d796.js" integrity="sha256-jSo1n9J6iIJG62OLNqTotorGW58RxIufrGAfoMmn15Y=" crossorigin="anonymous"></script>
</footer>

  </div>
</body>
</html>
