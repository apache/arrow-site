<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="UTF-8">
    <title>Apache Arrow Homepage</title>
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="generator" content="Jekyll v3.4.3">
    <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->
    <link rel="icon" type="image/x-icon" href="/favicon.ico">

    <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic,900">

    <link href="/css/main.css" rel="stylesheet">
    <link href="/css/syntax.css" rel="stylesheet">
    <script src="https://code.jquery.com/jquery-3.2.1.min.js"
            integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4="
            crossorigin="anonymous"></script>
    <script src="/assets/javascripts/bootstrap.min.js"></script>
    
    <!-- Global Site Tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-107500873-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments)};
  gtag('js', new Date());

  gtag('config', 'UA-107500873-1');
</script>

    
  </head>


<body class="wrap">
  <div class="container">
    <nav class="navbar navbar-default">
  <div class="container-fluid">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#arrow-navbar">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="/">Apache Arrow&#8482;&nbsp;&nbsp;&nbsp;</a>
    </div>

    <!-- Collect the nav links, forms, and other content for toggling -->
    <div class="collapse navbar-collapse" id="arrow-navbar">
      <ul class="nav navbar-nav">
        <li class="dropdown">
          <a href="#" class="dropdown-toggle" data-toggle="dropdown"
             role="button" aria-haspopup="true"
             aria-expanded="false">Project Links<span class="caret"></span>
          </a>
          <ul class="dropdown-menu">
            <li><a href="/install/">Install</a></li>
            <li><a href="/blog/">Blog</a></li>
            <li><a href="/release/">Releases</a></li>
            <li><a href="https://issues.apache.org/jira/browse/ARROW">Issue Tracker</a></li>
            <li><a href="https://github.com/apache/arrow">Source Code</a></li>
            <li><a href="http://mail-archives.apache.org/mod_mbox/arrow-dev/">Mailing List</a></li>
            <li><a href="https://apachearrowslackin.herokuapp.com">Slack Channel</a></li>
            <li><a href="/committers/">Committers</a></li>
          </ul>
        </li>
        <li class="dropdown">
          <a href="#" class="dropdown-toggle" data-toggle="dropdown"
             role="button" aria-haspopup="true"
             aria-expanded="false">Specification<span class="caret"></span>
          </a>
          <ul class="dropdown-menu">
            <li><a href="/docs/memory_layout.html">Memory Layout</a></li>
            <li><a href="/docs/metadata.html">Metadata</a></li>
            <li><a href="/docs/ipc.html">Messaging / IPC</a></li>
          </ul>
        </li>

        <li class="dropdown">
          <a href="#" class="dropdown-toggle" data-toggle="dropdown"
             role="button" aria-haspopup="true"
             aria-expanded="false">Documentation<span class="caret"></span>
          </a>
          <ul class="dropdown-menu">
            <li><a href="/docs/python">Python</a></li>
            <li><a href="/docs/cpp">C++ API</a></li>
            <li><a href="/docs/java">Java API</a></li>
            <li><a href="/docs/c_glib">C GLib API</a></li>
          </ul>
        </li>
        <!-- <li><a href="/blog">Blog</a></li> -->
        <li class="dropdown">
          <a href="#" class="dropdown-toggle" data-toggle="dropdown"
             role="button" aria-haspopup="true"
             aria-expanded="false">ASF Links<span class="caret"></span>
          </a>
          <ul class="dropdown-menu">
            <li><a href="http://www.apache.org/">ASF Website</a></li>
            <li><a href="http://www.apache.org/licenses/">License</a></li>
            <li><a href="http://www.apache.org/foundation/sponsorship.html">Donate</a></li>
            <li><a href="http://www.apache.org/foundation/thanks.html">Thanks</a></li>
            <li><a href="http://www.apache.org/security/">Security</a></li>
          </ul>
        </li>
      </ul>
      <a href="http://www.apache.org/">
        <img style="float:right;" src="/img/asf_logo.svg" width="120px"/>
      </a>
      </div><!-- /.navbar-collapse -->
    </div>
  </nav>


    

<h2>Project News and Blog</h2>
<hr>


  
    
  <div class="container">
    <h2>
      Fast Python Serialization with Ray and Apache Arrow
      <a href="/blog/2017/10/15/fast-python-serialization-with-ray-and-arrow/" class="permalink" title="Permalink">∞</a>
    </h2>

    

    <div class="panel">
      <div class="panel-body">
        <div>
          <span class="label label-default">Published</span>
          <span class="published">
            <i class="fa fa-calendar"></i>
            15 Oct 2017
          </span>
        </div>
        <div>
          <span class="label label-default">By</span>
          <a href=""><i class="fa fa-user"></i>  (Philipp Moritz, Robert Nishihara)</a>
        </div>
      </div>
    </div>
    <!--

-->

<p><em>This was originally posted on the <a href="https://ray-project.github.io/">Ray blog</a>. <a href="https://people.eecs.berkeley.edu/~pcmoritz/">Philipp Moritz</a> and <a href="http://www.robertnishihara.com">Robert Nishihara</a> are graduate students at UC Berkeley.</em></p>

<p>This post elaborates on the integration between <a href="http://ray.readthedocs.io/en/latest/index.html">Ray</a> and <a href="https://arrow.apache.org/">Apache Arrow</a>.
The main problem this addresses is <a href="https://en.wikipedia.org/wiki/Serialization">data serialization</a>.</p>

<p>From <a href="https://en.wikipedia.org/wiki/Serialization">Wikipedia</a>, <strong>serialization</strong> is</p>

<blockquote>
  <p>… the process of translating data structures or object state into a format
that can be stored … or transmitted … and reconstructed later (possibly
in a different computer environment).</p>
</blockquote>

<p>Why is any translation necessary? Well, when you create a Python object, it may
have pointers to other Python objects, and these objects are all allocated in
different regions of memory, and all of this has to make sense when unpacked by
another process on another machine.</p>

<p>Serialization and deserialization are <strong>bottlenecks in parallel and distributed
computing</strong>, especially in machine learning applications with large objects and
large quantities of data.</p>

<h2 id="design-goals">Design Goals</h2>

<p>As Ray is optimized for machine learning and AI applications, we have focused a
lot on serialization and data handling, with the following design goals:</p>

<ol>
  <li>It should be very efficient with <strong>large numerical data</strong> (this includes
NumPy arrays and Pandas DataFrames, as well as objects that recursively contain
Numpy arrays and Pandas DataFrames).</li>
  <li>It should be about as fast as Pickle for <strong>general Python types</strong>.</li>
  <li>It should be compatible with <strong>shared memory</strong>, allowing multiple processes
to use the same data without copying it.</li>
  <li><strong>Deserialization</strong> should be extremely fast (when possible, it should not
require reading the entire serialized object).</li>
  <li>It should be <strong>language independent</strong> (eventually we’d like to enable Python
workers to use objects created by workers in Java or other languages and vice
versa).</li>
</ol>

<h2 id="our-approach-and-alternatives">Our Approach and Alternatives</h2>

<p>The go-to serialization approach in Python is the <strong>pickle</strong> module. Pickle is
very general, especially if you use variants like <a href="https://github.com/cloudpipe/cloudpickle/">cloudpickle</a>. However, it
does not satisfy requirements 1, 3, 4, or 5. Alternatives like <strong>json</strong> satisfy
5, but not 1-4.</p>

<p><strong>Our Approach:</strong> To satisfy requirements 1-5, we chose to use the
<a href="https://arrow.apache.org/">Apache Arrow</a> format as our underlying data representation. In collaboration
with the Apache Arrow team, we built <a href="https://arrow.apache.org/docs/python/ipc.html#arbitrary-object-serialization">libraries</a> for mapping general Python
objects to and from the Arrow format. Some properties of this approach:</p>

<ul>
  <li>The data layout is language independent (requirement 5).</li>
  <li>Offsets into a serialized data blob can be computed in constant time without
reading the full object (requirements 1 and 4).</li>
  <li>Arrow supports <strong>zero-copy reads</strong>, so objects can naturally be stored in
shared memory and used by multiple processes (requirements 1 and 3).</li>
  <li>We can naturally fall back to pickle for anything we can’t handle well
(requirement 2).</li>
</ul>

<p><strong>Alternatives to Arrow:</strong> We could have built on top of
<a href="https://developers.google.com/protocol-buffers/"><strong>Protocol Buffers</strong></a>, but protocol buffers really isn’t designed for
numerical data, and that approach wouldn’t satisfy 1, 3, or 4. Building on top
of <a href="https://google.github.io/flatbuffers/"><strong>Flatbuffers</strong></a> actually could be made to work, but it would have
required implementing a lot of the facilities that Arrow already has and we
preferred a columnar data layout more optimized for big data.</p>

<h2 id="speedups">Speedups</h2>

<p>Here we show some performance improvements over Python’s pickle module. The
experiments were done using <code class="highlighter-rouge">pickle.HIGHEST_PROTOCOL</code>. Code for generating these
plots is included at the end of the post.</p>

<p><strong>With NumPy arrays:</strong> In machine learning and AI applications, data (e.g.,
images, neural network weights, text documents) are typically represented as
data structures containing NumPy arrays. When using NumPy arrays, the speedups
are impressive.</p>

<p>The fact that the Ray bars for deserialization are barely visible is not a
mistake. This is a consequence of the support for zero-copy reads (the savings
largely come from the lack of memory movement).</p>

<div align="center">
<img src="/assets/fast_python_serialization_with_ray_and_arrow/speedups0.png" width="365" height="255" />
<img src="/assets/fast_python_serialization_with_ray_and_arrow/speedups1.png" width="365" height="255" />
</div>

<p>Note that the biggest wins are with deserialization. The speedups here are
multiple orders of magnitude and get better as the NumPy arrays get larger
(thanks to design goals 1, 3, and 4). Making <strong>deserialization</strong> fast is
important for two reasons. First, an object may be serialized once and then
deserialized many times (e.g., an object that is broadcast to all workers).
Second, a common pattern is for many objects to be serialized in parallel and
then aggregated and deserialized one at a time on a single worker making
deserialization the bottleneck.</p>

<p><strong>Without NumPy arrays:</strong> When using regular Python objects, for which we
cannot take advantage of shared memory, the results are comparable to pickle.</p>

<div align="center">
<img src="/assets/fast_python_serialization_with_ray_and_arrow/speedups2.png" width="365" height="255" />
<img src="/assets/fast_python_serialization_with_ray_and_arrow/speedups3.png" width="365" height="255" />
</div>

<p>These are just a few examples of interesting Python objects. The most important
case is the case where NumPy arrays are nested within other objects. Note that
our serialization library works with very general Python types including custom
Python classes and deeply nested objects.</p>

<h2 id="the-api">The API</h2>

<p>The serialization library can be used directly through pyarrow as follows. More
documentation is available <a href="https://arrow.apache.org/docs/python/ipc.html#arbitrary-object-serialization">here</a>.</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">x</span> <span class="o">=</span> <span class="p">[(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="s">'hello'</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">5.0</span><span class="p">,</span> <span class="mf">6.0</span><span class="p">])]</span>
<span class="n">serialized_x</span> <span class="o">=</span> <span class="n">pyarrow</span><span class="o">.</span><span class="n">serialize</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">to_buffer</span><span class="p">()</span>
<span class="n">deserialized_x</span> <span class="o">=</span> <span class="n">pyarrow</span><span class="o">.</span><span class="n">deserialize</span><span class="p">(</span><span class="n">serialized_x</span><span class="p">)</span>
</code></pre>
</div>

<p>It can be used directly through the Ray API as follows.</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">x</span> <span class="o">=</span> <span class="p">[(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="s">'hello'</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">5.0</span><span class="p">,</span> <span class="mf">6.0</span><span class="p">])]</span>
<span class="n">x_id</span> <span class="o">=</span> <span class="n">ray</span><span class="o">.</span><span class="n">put</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">deserialized_x</span> <span class="o">=</span> <span class="n">ray</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">x_id</span><span class="p">)</span>
</code></pre>
</div>

<h2 id="data-representation">Data Representation</h2>

<p>We use Apache Arrow as the underlying language-independent data layout. Objects
are stored in two parts: a <strong>schema</strong> and a <strong>data blob</strong>. At a high level, the
data blob is roughly a flattened concatenation of all of the data values
recursively contained in the object, and the schema defines the types and
nesting structure of the data blob.</p>

<p><strong>Technical Details:</strong> Python sequences (e.g., dictionaries, lists, tuples,
sets) are encoded as Arrow <a href="http://arrow.apache.org/docs/memory_layout.html#dense-union-type">UnionArrays</a> of other types (e.g., bools, ints,
strings, bytes, floats, doubles, date64s, tensors (i.e., NumPy arrays), lists,
tuples, dicts and sets). Nested sequences are encoded using Arrow
<a href="http://arrow.apache.org/docs/memory_layout.html#list-type">ListArrays</a>. All tensors are collected and appended to the end of the
serialized object, and the UnionArray contains references to these tensors.</p>

<p>To give a concrete example, consider the following object.</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="p">[(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="s">'hello'</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">5.0</span><span class="p">,</span> <span class="mf">6.0</span><span class="p">])]</span>
</code></pre>
</div>

<p>It would be represented in Arrow with the following structure.</p>

<div class="highlighter-rouge"><pre class="highlight"><code>UnionArray(type_ids=[tuple, string, int, int, ndarray],
           tuples=ListArray(offsets=[0, 2],
                            UnionArray(type_ids=[int, int],
                                       ints=[1, 2])),
           strings=['hello'],
           ints=[3, 4],
           ndarrays=[&lt;offset of numpy array&gt;])
</code></pre>
</div>

<p>Arrow uses Flatbuffers to encode serialized schemas. <strong>Using only the schema, we
can compute the offsets of each value in the data blob without scanning through
the data blob</strong> (unlike Pickle, this is what enables fast deserialization). This
means that we can avoid copying or otherwise converting large arrays and other
values during deserialization. Tensors are appended at the end of the UnionArray
and can be efficiently shared and accessed using shared memory.</p>

<p>Note that the actual object would be laid out in memory as shown below.</p>

<div align="center">
<img src="/assets/fast_python_serialization_with_ray_and_arrow/python_object.png" width="600" />
</div>
<div><i>The layout of a Python object in the heap. Each box is allocated in a
different memory region, and arrows between boxes represent pointers.</i></div>
<p><br /></p>

<p>The Arrow serialized representation would be as follows.</p>

<div align="center">
<img src="/assets/fast_python_serialization_with_ray_and_arrow/arrow_object.png" width="400" />
</div>
<div><i>The memory layout of the Arrow-serialized object.</i></div>
<p><br /></p>

<h2 id="getting-involved">Getting Involved</h2>

<p>We welcome contributions, especially in the following areas.</p>

<ul>
  <li>Use the C++ and Java implementations of Arrow to implement versions of this
for C++ and Java.</li>
  <li>Implement support for more Python types and better test coverage.</li>
</ul>

<h2 id="reproducing-the-figures-above">Reproducing the Figures Above</h2>

<p>For reference, the figures can be reproduced with the following code.
Benchmarking <code class="highlighter-rouge">ray.put</code> and <code class="highlighter-rouge">ray.get</code> instead of <code class="highlighter-rouge">pyarrow.serialize</code> and
<code class="highlighter-rouge">pyarrow.deserialize</code> gives similar figures. The plots were generated at this
<a href="https://github.com/apache/arrow/tree/894f7400977693b4e0e8f4b9845fd89481f6bf29">commit</a>.</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">pickle</span>
<span class="kn">import</span> <span class="nn">pyarrow</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">timeit</span>


<span class="k">def</span> <span class="nf">benchmark_object</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="n">number</span><span class="o">=</span><span class="mi">10</span><span class="p">):</span>
    <span class="c"># Time serialization and deserialization for pickle.</span>
    <span class="n">pickle_serialize</span> <span class="o">=</span> <span class="n">timeit</span><span class="o">.</span><span class="n">timeit</span><span class="p">(</span>
        <span class="k">lambda</span><span class="p">:</span> <span class="n">pickle</span><span class="o">.</span><span class="n">dumps</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="n">protocol</span><span class="o">=</span><span class="n">pickle</span><span class="o">.</span><span class="n">HIGHEST_PROTOCOL</span><span class="p">),</span>
        <span class="n">number</span><span class="o">=</span><span class="n">number</span><span class="p">)</span>
    <span class="n">serialized_obj</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">dumps</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="n">pickle</span><span class="o">.</span><span class="n">HIGHEST_PROTOCOL</span><span class="p">)</span>
    <span class="n">pickle_deserialize</span> <span class="o">=</span> <span class="n">timeit</span><span class="o">.</span><span class="n">timeit</span><span class="p">(</span><span class="k">lambda</span><span class="p">:</span> <span class="n">pickle</span><span class="o">.</span><span class="n">loads</span><span class="p">(</span><span class="n">serialized_obj</span><span class="p">),</span>
                                       <span class="n">number</span><span class="o">=</span><span class="n">number</span><span class="p">)</span>

    <span class="c"># Time serialization and deserialization for Ray.</span>
    <span class="n">ray_serialize</span> <span class="o">=</span> <span class="n">timeit</span><span class="o">.</span><span class="n">timeit</span><span class="p">(</span>
        <span class="k">lambda</span><span class="p">:</span> <span class="n">pyarrow</span><span class="o">.</span><span class="n">serialize</span><span class="p">(</span><span class="n">obj</span><span class="p">)</span><span class="o">.</span><span class="n">to_buffer</span><span class="p">(),</span> <span class="n">number</span><span class="o">=</span><span class="n">number</span><span class="p">)</span>
    <span class="n">serialized_obj</span> <span class="o">=</span> <span class="n">pyarrow</span><span class="o">.</span><span class="n">serialize</span><span class="p">(</span><span class="n">obj</span><span class="p">)</span><span class="o">.</span><span class="n">to_buffer</span><span class="p">()</span>
    <span class="n">ray_deserialize</span> <span class="o">=</span> <span class="n">timeit</span><span class="o">.</span><span class="n">timeit</span><span class="p">(</span>
        <span class="k">lambda</span><span class="p">:</span> <span class="n">pyarrow</span><span class="o">.</span><span class="n">deserialize</span><span class="p">(</span><span class="n">serialized_obj</span><span class="p">),</span> <span class="n">number</span><span class="o">=</span><span class="n">number</span><span class="p">)</span>

    <span class="k">return</span> <span class="p">[[</span><span class="n">pickle_serialize</span><span class="p">,</span> <span class="n">pickle_deserialize</span><span class="p">],</span>
            <span class="p">[</span><span class="n">ray_serialize</span><span class="p">,</span> <span class="n">ray_deserialize</span><span class="p">]]</span>


<span class="k">def</span> <span class="nf">plot</span><span class="p">(</span><span class="n">pickle_times</span><span class="p">,</span> <span class="n">ray_times</span><span class="p">,</span> <span class="n">title</span><span class="p">,</span> <span class="n">i</span><span class="p">):</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
    <span class="n">fig</span><span class="o">.</span><span class="n">set_size_inches</span><span class="p">(</span><span class="mf">3.8</span><span class="p">,</span> <span class="mf">2.7</span><span class="p">)</span>

    <span class="n">bar_width</span> <span class="o">=</span> <span class="mf">0.35</span>
    <span class="n">index</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">opacity</span> <span class="o">=</span> <span class="mf">0.6</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">index</span><span class="p">,</span> <span class="n">pickle_times</span><span class="p">,</span> <span class="n">bar_width</span><span class="p">,</span>
            <span class="n">alpha</span><span class="o">=</span><span class="n">opacity</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'r'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'Pickle'</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">index</span> <span class="o">+</span> <span class="n">bar_width</span><span class="p">,</span> <span class="n">ray_times</span><span class="p">,</span> <span class="n">bar_width</span><span class="p">,</span>
            <span class="n">alpha</span><span class="o">=</span><span class="n">opacity</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'c'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'Ray'</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="n">title</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s">'bold'</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'Time (seconds)'</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">labels</span> <span class="o">=</span> <span class="p">[</span><span class="s">'serialization'</span><span class="p">,</span> <span class="s">'deserialization'</span><span class="p">]</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">index</span> <span class="o">+</span> <span class="n">bar_width</span> <span class="o">/</span> <span class="mi">2</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">bbox_to_anchor</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s">'plot-'</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="o">+</span> <span class="s">'.png'</span><span class="p">,</span> <span class="n">format</span><span class="o">=</span><span class="s">'png'</span><span class="p">)</span>


<span class="n">test_objects</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">50000</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">)],</span>
    <span class="p">{</span><span class="s">'weight-'</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">):</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">50000</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">)},</span>
    <span class="p">{</span><span class="n">i</span><span class="p">:</span> <span class="nb">set</span><span class="p">([</span><span class="s">'string1'</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">),</span> <span class="s">'string2'</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)])</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100000</span><span class="p">)},</span>
    <span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">200000</span><span class="p">)]</span>
<span class="p">]</span>

<span class="n">titles</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s">'List of large numpy arrays'</span><span class="p">,</span>
    <span class="s">'Dictionary of large numpy arrays'</span><span class="p">,</span>
    <span class="s">'Large dictionary of small sets'</span><span class="p">,</span>
    <span class="s">'Large list of strings'</span>
<span class="p">]</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">test_objects</span><span class="p">)):</span>
    <span class="n">plot</span><span class="p">(</span><span class="o">*</span><span class="n">benchmark_object</span><span class="p">(</span><span class="n">test_objects</span><span class="p">[</span><span class="n">i</span><span class="p">]),</span> <span class="n">titles</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">i</span><span class="p">)</span>
</code></pre>
</div>


  </div>

  

  
    
  <div class="container">
    <h2>
      Apache Arrow 0.7.0 Release
      <a href="/blog/2017/09/19/0.7.0-release/" class="permalink" title="Permalink">∞</a>
    </h2>

    

    <div class="panel">
      <div class="panel-body">
        <div>
          <span class="label label-default">Published</span>
          <span class="published">
            <i class="fa fa-calendar"></i>
            19 Sep 2017
          </span>
        </div>
        <div>
          <span class="label label-default">By</span>
          <a href="http://wesmckinney.com"><i class="fa fa-user"></i> Wes McKinney (wesm)</a>
        </div>
      </div>
    </div>
    <!--

-->

<p>The Apache Arrow team is pleased to announce the 0.7.0 release. It includes
<a href="https://issues.apache.org/jira/issues/?jql=project%20%3D%20ARROW%20AND%20status%20in%20(Resolved%2C%20Closed)%20AND%20fixVersion%20%3D%200.7.0"><strong>133 resolved JIRAs</strong></a> many new features and bug fixes to the various
language implementations. The Arrow memory format remains stable since the
0.3.x release.</p>

<p>See the <a href="http://arrow.apache.org/install">Install Page</a> to learn how to get the libraries for your
platform. The <a href="http://arrow.apache.org/release/0.7.0.html">complete changelog</a> is also available.</p>

<p>We include some highlights from the release in this post.</p>

<h2 id="new-pmc-member-kouhei-sutou">New PMC Member: Kouhei Sutou</h2>

<p>Since the last release we have added <a href="https://github.com/kou">Kou</a> to the Arrow Project Management
Committee. He is also a PMC for Apache Subversion, and a major contributor to
many other open source projects.</p>

<p>As an active member of the Ruby community in Japan, Kou has been developing the
GLib-based C bindings for Arrow with associated Ruby wrappers, to enable Ruby
users to benefit from the work that’s happening in Apache Arrow.</p>

<p>We are excited to be collaborating with the Ruby community on shared
infrastructure for in-memory analytics and data science.</p>

<h2 id="expanded-javascript-typescript-implementation">Expanded JavaScript (TypeScript) Implementation</h2>

<p><a href="https://github.com/trxcllnt">Paul Taylor</a> from the <a href="https://github.com/netflix/falcor">Falcor</a> and <a href="http://reactivex.io">ReactiveX</a> projects has worked to
expand the JavaScript implementation (which is written in TypeScript), using
the latest in modern JavaScript build and packaging technology. We are looking
forward to building out the JS implementation and bringing it up to full
functionality with the C++ and Java implementations.</p>

<p>We are looking for more JavaScript developers to join the project and work
together to make Arrow for JS work well with many kinds of front end use cases,
like real time data visualization.</p>

<h2 id="type-casting-for-c-and-python">Type casting for C++ and Python</h2>

<p>As part of longer-term efforts to build an Arrow-native in-memory analytics
library, we implemented a variety of type conversion functions. These functions
are essential in ETL tasks when conforming one table schema to another. These
are similar to the <code class="highlighter-rouge">astype</code> function in NumPy.</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">In</span> <span class="p">[</span><span class="mi">17</span><span class="p">]:</span> <span class="kn">import</span> <span class="nn">pyarrow</span> <span class="kn">as</span> <span class="nn">pa</span>

<span class="n">In</span> <span class="p">[</span><span class="mi">18</span><span class="p">]:</span> <span class="n">arr</span> <span class="o">=</span> <span class="n">pa</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="bp">True</span><span class="p">,</span> <span class="bp">False</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">True</span><span class="p">])</span>

<span class="n">In</span> <span class="p">[</span><span class="mi">19</span><span class="p">]:</span> <span class="n">arr</span>
<span class="n">Out</span><span class="p">[</span><span class="mi">19</span><span class="p">]:</span>
<span class="o">&lt;</span><span class="n">pyarrow</span><span class="o">.</span><span class="n">lib</span><span class="o">.</span><span class="n">BooleanArray</span> <span class="nb">object</span> <span class="n">at</span> <span class="mh">0x7ff6fb069b88</span><span class="o">&gt;</span>
<span class="p">[</span>
  <span class="bp">True</span><span class="p">,</span>
  <span class="bp">False</span><span class="p">,</span>
  <span class="n">NA</span><span class="p">,</span>
  <span class="bp">True</span>
<span class="p">]</span>

<span class="n">In</span> <span class="p">[</span><span class="mi">20</span><span class="p">]:</span> <span class="n">arr</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">pa</span><span class="o">.</span><span class="n">int32</span><span class="p">())</span>
<span class="n">Out</span><span class="p">[</span><span class="mi">20</span><span class="p">]:</span>
<span class="o">&lt;</span><span class="n">pyarrow</span><span class="o">.</span><span class="n">lib</span><span class="o">.</span><span class="n">Int32Array</span> <span class="nb">object</span> <span class="n">at</span> <span class="mh">0x7ff6fb0383b8</span><span class="o">&gt;</span>
<span class="p">[</span>
  <span class="mi">1</span><span class="p">,</span>
  <span class="mi">0</span><span class="p">,</span>
  <span class="n">NA</span><span class="p">,</span>
  <span class="mi">1</span>
<span class="p">]</span>
</code></pre>
</div>

<p>Over time these will expand to support as many input-and-output type
combinations with optimized conversions.</p>

<h2 id="new-arrow-gpu-cuda-extension-library-for-c">New Arrow GPU (CUDA) Extension Library for C++</h2>

<p>To help with GPU-related projects using Arrow, like the <a href="http://gpuopenanalytics.com/">GPU Open Analytics
Initiative</a>, we have started a C++ add-on library to simplify Arrow memory
management on CUDA-enabled graphics cards. We would like to expand this to
include a library of reusable CUDA kernel functions for GPU analytics on Arrow
columnar memory.</p>

<p>For example, we could write a record batch from CPU memory to GPU device memory
like so (some error checking omitted):</p>

<div class="language-c++ highlighter-rouge"><pre class="highlight"><code><span class="cp">#include &lt;arrow/api.h&gt;
#include &lt;arrow/gpu/cuda_api.h&gt;
</span>
<span class="k">using</span> <span class="k">namespace</span> <span class="n">arrow</span><span class="p">;</span>

<span class="n">gpu</span><span class="o">::</span><span class="n">CudaDeviceManager</span><span class="o">*</span> <span class="n">manager</span><span class="p">;</span>
<span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">gpu</span><span class="o">::</span><span class="n">CudaContext</span><span class="o">&gt;</span> <span class="n">context</span><span class="p">;</span>

<span class="n">gpu</span><span class="o">::</span><span class="n">CudaDeviceManager</span><span class="o">::</span><span class="n">GetInstance</span><span class="p">(</span><span class="o">&amp;</span><span class="n">manager</span><span class="p">)</span>
<span class="n">manager_</span><span class="o">-&gt;</span><span class="n">GetContext</span><span class="p">(</span><span class="n">kGpuNumber</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">context</span><span class="p">);</span>

<span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">RecordBatch</span><span class="o">&gt;</span> <span class="n">batch</span> <span class="o">=</span> <span class="n">GetCpuData</span><span class="p">();</span>

<span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">gpu</span><span class="o">::</span><span class="n">CudaBuffer</span><span class="o">&gt;</span> <span class="n">device_serialized</span><span class="p">;</span>
<span class="n">gpu</span><span class="o">::</span><span class="n">SerializeRecordBatch</span><span class="p">(</span><span class="o">*</span><span class="n">batch</span><span class="p">,</span> <span class="n">context_</span><span class="p">.</span><span class="n">get</span><span class="p">(),</span> <span class="o">&amp;</span><span class="n">device_serialized</span><span class="p">));</span>
</code></pre>
</div>

<p>We can then “read” the GPU record batch, but the returned <code class="highlighter-rouge">arrow::RecordBatch</code>
internally will contain GPU device pointers that you can use for CUDA kernel
calls:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>std::shared_ptr&lt;RecordBatch&gt; device_batch;
gpu::ReadRecordBatch(batch-&gt;schema(), device_serialized,
                     default_memory_pool(), &amp;device_batch));

// Now run some CUDA kernels on device_batch
</code></pre>
</div>

<h2 id="decimal-integration-tests">Decimal Integration Tests</h2>

<p><a href="http://github.com/cpcloud">Phillip Cloud</a> has been working on decimal support in C++ to enable Parquet
read/write support in C++ and Python, and also end-to-end testing against the
Arrow Java libraries.</p>

<p>In the upcoming releases, we hope to complete the remaining data types that
need end-to-end testing between Java and C++:</p>

<ul>
  <li>Fixed size lists (variable-size lists already implemented)</li>
  <li>Fixes size binary</li>
  <li>Unions</li>
  <li>Maps</li>
  <li>Time intervals</li>
</ul>

<h2 id="other-notable-python-changes">Other Notable Python Changes</h2>

<p>Some highlights of Python development outside of bug fixes and general API
improvements include:</p>

<ul>
  <li>Simplified <code class="highlighter-rouge">put</code> and <code class="highlighter-rouge">get</code> arbitrary Python objects in Plasma objects</li>
  <li><a href="http://arrow.apache.org/docs/python/ipc.html">High-speed, memory efficient object serialization</a>. This is important
enough that we will likely write a dedicated blog post about it.</li>
  <li>New <code class="highlighter-rouge">flavor='spark'</code> option to <code class="highlighter-rouge">pyarrow.parquet.write_table</code> to enable easy
writing of Parquet files maximized for Spark compatibility</li>
  <li><code class="highlighter-rouge">parquet.write_to_dataset</code> function with support for partitioned writes</li>
  <li>Improved support for Dask filesystems</li>
  <li>Improved Python usability for IPC: read and write schemas and record batches
more easily. See the <a href="http://arrow.apache.org/docs/python/api.html">API docs</a> for more about these.</li>
</ul>

<h2 id="the-road-ahead">The Road Ahead</h2>

<p>Upcoming Arrow releases will continue to expand the project to cover more use
cases. In addition to completing end-to-end testing for all the major data
types, some of us will be shifting attention to building Arrow-native in-memory
analytics libraries.</p>

<p>We are looking for more JavaScript, R, and other programming language
developers to join the project and expand the available implementations and
bindings to more languages.</p>


  </div>

  

  
    
  <div class="container">
    <h2>
      Apache Arrow 0.6.0 Release
      <a href="/blog/2017/08/16/0.6.0-release/" class="permalink" title="Permalink">∞</a>
    </h2>

    

    <div class="panel">
      <div class="panel-body">
        <div>
          <span class="label label-default">Published</span>
          <span class="published">
            <i class="fa fa-calendar"></i>
            16 Aug 2017
          </span>
        </div>
        <div>
          <span class="label label-default">By</span>
          <a href="http://wesmckinney.com"><i class="fa fa-user"></i> Wes McKinney (wesm)</a>
        </div>
      </div>
    </div>
    <!--

-->

<p>The Apache Arrow team is pleased to announce the 0.6.0 release. It includes
<a href="https://issues.apache.org/jira/issues/?jql=project%20%3D%20ARROW%20AND%20status%20in%20(Resolved%2C%20Closed)%20AND%20fixVersion%20%3D%200.6.0"><strong>90 resolved JIRAs</strong></a> with the new Plasma shared memory object store, and
improvements and bug fixes to the various language implementations. The Arrow
memory format remains stable since the 0.3.x release.</p>

<p>See the <a href="http://arrow.apache.org/install">Install Page</a> to learn how to get the libraries for your
platform. The <a href="http://arrow.apache.org/release/0.6.0.html">complete changelog</a> is also available.</p>

<h2 id="plasma-shared-memory-object-store">Plasma Shared Memory Object Store</h2>

<p>This release includes the <a href="http://arrow.apache.org/blog/2017/08/08/plasma-in-memory-object-store/">Plasma Store</a>, which you can read more about in
the linked blog post. This system was originally developed as part of the <a href="https://ray-project.github.io/ray/">Ray
Project</a> at the <a href="https://rise.cs.berkeley.edu/">UC Berkeley RISELab</a>. We recognized that Plasma would be
highly valuable to the Arrow community as a tool for shared memory management
and zero-copy deserialization. Additionally, we believe we will be able to
develop a stronger software stack through sharing of IO and buffer management
code.</p>

<p>The Plasma store is a server application which runs as a separate process. A
reference C++ client, with Python bindings, is made available in this
release. Clients can be developed in Java or other languages in the future to
enable simple sharing of complex datasets through shared memory.</p>

<h2 id="arrow-format-addition-map-type">Arrow Format Addition: Map type</h2>

<p>We added a Map logical type to represent ordered and unordered maps
in-memory. This corresponds to the <code class="highlighter-rouge">MAP</code> logical type annotation in the Parquet
format (where maps are represented as repeated structs).</p>

<p>Map is represented as a list of structs. It is the first example of a logical
type whose physical representation is a nested type. We have not yet created
implementations of Map containers in any of the implementations, but this can
be done in a future release.</p>

<p>As an example, the Python data:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>data = [{'a': 1, 'bb': 2, 'cc': 3}, {'dddd': 4}]
</code></pre>
</div>

<p>Could be represented in an Arrow <code class="highlighter-rouge">Map&lt;String, Int32&gt;</code> as:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>Map&lt;String, Int32&gt; = List&lt;Struct&lt;keys: String, values: Int32&gt;&gt;
  is_valid: [true, true]
  offsets: [0, 3, 4]
  values: Struct&lt;keys: String, values: Int32&gt;
    children:
      - keys: String
          is_valid: [true, true, true, true]
          offsets: [0, 1, 3, 5, 9]
          data: abbccdddd
      - values: Int32
          is_valid: [true, true, true, true]
          data: [1, 2, 3, 4]
</code></pre>
</div>
<h2 id="python-changes">Python Changes</h2>

<p>Some highlights of Python development outside of bug fixes and general API
improvements include:</p>

<ul>
  <li>New <code class="highlighter-rouge">strings_to_categorical=True</code> option when calling <code class="highlighter-rouge">Table.to_pandas</code> will
yield pandas <code class="highlighter-rouge">Categorical</code> types from Arrow binary and string columns</li>
  <li>Expanded Hadoop Filesystem (HDFS) functionality to improve compatibility with
Dask and other HDFS-aware Python libraries.</li>
  <li>s3fs and other Dask-oriented filesystems can now be used with
<code class="highlighter-rouge">pyarrow.parquet.ParquetDataset</code></li>
  <li>More graceful handling of pandas’s nanosecond timestamps when writing to
Parquet format. You can now pass <code class="highlighter-rouge">coerce_timestamps='ms'</code> to cast to
milliseconds, or <code class="highlighter-rouge">'us'</code> for microseconds.</li>
</ul>

<h2 id="toward-arrow-100-and-beyond">Toward Arrow 1.0.0 and Beyond</h2>

<p>We are still discussing the roadmap to 1.0.0 release on the <a href="http://mail-archives.apache.org/mod_mbox/arrow-dev/">developer mailing
list</a>. The focus of the 1.0.0 release will likely be memory format stability
and hardening integration tests across the remaining data types implemented in
Java and C++. Please join the discussion there.</p>


  </div>

  

  
    
  <div class="container">
    <h2>
      Plasma In-Memory Object Store
      <a href="/blog/2017/08/08/plasma-in-memory-object-store/" class="permalink" title="Permalink">∞</a>
    </h2>

    

    <div class="panel">
      <div class="panel-body">
        <div>
          <span class="label label-default">Published</span>
          <span class="published">
            <i class="fa fa-calendar"></i>
            08 Aug 2017
          </span>
        </div>
        <div>
          <span class="label label-default">By</span>
          <a href="http://wesmckinney.com"><i class="fa fa-user"></i> Wes McKinney (Philipp Moritz and Robert Nishihara)</a>
        </div>
      </div>
    </div>
    <!--

-->

<p><em><a href="https://people.eecs.berkeley.edu/~pcmoritz/">Philipp Moritz</a> and <a href="http://www.robertnishihara.com">Robert Nishihara</a> are graduate students at UC
 Berkeley.</em></p>

<h2 id="plasma-a-high-performance-shared-memory-object-store">Plasma: A High-Performance Shared-Memory Object Store</h2>

<h3 id="motivating-plasma">Motivating Plasma</h3>

<p>This blog post presents Plasma, an in-memory object store that is being
developed as part of Apache Arrow. <strong>Plasma holds immutable objects in shared
memory so that they can be accessed efficiently by many clients across process
boundaries.</strong> In light of the trend toward larger and larger multicore machines,
Plasma enables critical performance optimizations in the big data regime.</p>

<p>Plasma was initially developed as part of <a href="https://github.com/ray-project/ray">Ray</a>, and has recently been moved
to Apache Arrow in the hopes that it will be broadly useful.</p>

<p>One of the goals of Apache Arrow is to serve as a common data layer enabling
zero-copy data exchange between multiple frameworks. A key component of this
vision is the use of off-heap memory management (via Plasma) for storing and
sharing Arrow-serialized objects between applications.</p>

<p><strong>Expensive serialization and deserialization as well as data copying are a
common performance bottleneck in distributed computing.</strong> For example, a
Python-based execution framework that wishes to distribute computation across
multiple Python “worker” processes and then aggregate the results in a single
“driver” process may choose to serialize data using the built-in <code class="highlighter-rouge">pickle</code>
library. Assuming one Python process per core, each worker process would have to
copy and deserialize the data, resulting in excessive memory usage. The driver
process would then have to deserialize results from each of the workers,
resulting in a bottleneck.</p>

<p>Using Plasma plus Arrow, the data being operated on would be placed in the
Plasma store once, and all of the workers would read the data without copying or
deserializing it (the workers would map the relevant region of memory into their
own address spaces). The workers would then put the results of their computation
back into the Plasma store, which the driver could then read and aggregate
without copying or deserializing the data.</p>

<h3 id="the-plasma-api">The Plasma API:</h3>

<p>Below we illustrate a subset of the API. The C++ API is documented more fully
<a href="https://github.com/apache/arrow/blob/master/cpp/apidoc/tutorials/plasma.md">here</a>, and the Python API is documented <a href="https://github.com/apache/arrow/blob/master/python/doc/source/plasma.rst">here</a>.</p>

<p><strong>Object IDs:</strong> Each object is associated with a string of bytes.</p>

<p><strong>Creating an object:</strong> Objects are stored in Plasma in two stages. First, the
object store <em>creates</em> the object by allocating a buffer for it. At this point,
the client can write to the buffer and construct the object within the allocated
buffer. When the client is done, the client <em>seals</em> the buffer making the object
immutable and making it available to other Plasma clients.</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="c"># Create an object.</span>
<span class="n">object_id</span> <span class="o">=</span> <span class="n">pyarrow</span><span class="o">.</span><span class="n">plasma</span><span class="o">.</span><span class="n">ObjectID</span><span class="p">(</span><span class="mi">20</span> <span class="o">*</span> <span class="n">b</span><span class="s">'a'</span><span class="p">)</span>
<span class="n">object_size</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="nb">buffer</span> <span class="o">=</span> <span class="n">memoryview</span><span class="p">(</span><span class="n">client</span><span class="o">.</span><span class="n">create</span><span class="p">(</span><span class="n">object_id</span><span class="p">,</span> <span class="n">object_size</span><span class="p">))</span>

<span class="c"># Write to the buffer.</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1000</span><span class="p">):</span>
    <span class="nb">buffer</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>

<span class="c"># Seal the object making it immutable and available to other clients.</span>
<span class="n">client</span><span class="o">.</span><span class="n">seal</span><span class="p">(</span><span class="n">object_id</span><span class="p">)</span>
</code></pre>
</div>

<p><strong>Getting an object:</strong> After an object has been sealed, any client who knows the
object ID can get the object.</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="c"># Get the object from the store. This blocks until the object has been sealed.</span>
<span class="n">object_id</span> <span class="o">=</span> <span class="n">pyarrow</span><span class="o">.</span><span class="n">plasma</span><span class="o">.</span><span class="n">ObjectID</span><span class="p">(</span><span class="mi">20</span> <span class="o">*</span> <span class="n">b</span><span class="s">'a'</span><span class="p">)</span>
<span class="p">[</span><span class="n">buff</span><span class="p">]</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">get</span><span class="p">([</span><span class="n">object_id</span><span class="p">])</span>
<span class="nb">buffer</span> <span class="o">=</span> <span class="n">memoryview</span><span class="p">(</span><span class="n">buff</span><span class="p">)</span>
</code></pre>
</div>

<p>If the object has not been sealed yet, then the call to <code class="highlighter-rouge">client.get</code> will block
until the object has been sealed.</p>

<h3 id="a-sorting-application">A sorting application</h3>

<p>To illustrate the benefits of Plasma, we demonstrate an <strong>11x speedup</strong> (on a
machine with 20 physical cores) for sorting a large pandas DataFrame (one
billion entries). The baseline is the built-in pandas sort function, which sorts
the DataFrame in 477 seconds. To leverage multiple cores, we implement the
following standard distributed sorting scheme.</p>

<ul>
  <li>We assume that the data is partitioned across K pandas DataFrames and that
each one already lives in the Plasma store.</li>
  <li>We subsample the data, sort the subsampled data, and use the result to define
L non-overlapping buckets.</li>
  <li>For each of the K data partitions and each of the L buckets, we find the
subset of the data partition that falls in the bucket, and we sort that
subset.</li>
  <li>For each of the L buckets, we gather all of the K sorted subsets that fall in
that bucket.</li>
  <li>For each of the L buckets, we merge the corresponding K sorted subsets.</li>
  <li>We turn each bucket into a pandas DataFrame and place it in the Plasma store.</li>
</ul>

<p>Using this scheme, we can sort the DataFrame (the data starts and ends in the
Plasma store), in 44 seconds, giving an 11x speedup over the baseline.</p>

<h3 id="design">Design</h3>

<p>The Plasma store runs as a separate process. It is written in C++ and is
designed as a single-threaded event loop based on the <a href="https://redis.io/">Redis</a> event loop library.
The plasma client library can be linked into applications. Clients communicate
with the Plasma store via messages serialized using <a href="https://google.github.io/flatbuffers/">Google Flatbuffers</a>.</p>

<h3 id="call-for-contributions">Call for contributions</h3>

<p>Plasma is a work in progress, and the API is currently unstable. Today Plasma is
primarily used in <a href="https://github.com/ray-project/ray">Ray</a> as an in-memory cache for Arrow serialized objects.
We are looking for a broader set of use cases to help refine Plasma’s API. In
addition, we are looking for contributions in a variety of areas including
improving performance and building other language bindings. Please let us know
if you are interested in getting involved with the project.</p>


  </div>

  

  
    
  <div class="container">
    <h2>
      Speeding up PySpark with Apache Arrow
      <a href="/blog/2017/07/26/spark-arrow/" class="permalink" title="Permalink">∞</a>
    </h2>

    

    <div class="panel">
      <div class="panel-body">
        <div>
          <span class="label label-default">Published</span>
          <span class="published">
            <i class="fa fa-calendar"></i>
            26 Jul 2017
          </span>
        </div>
        <div>
          <span class="label label-default">By</span>
          <a href="http://wesmckinney.com"><i class="fa fa-user"></i> Wes McKinney (BryanCutler)</a>
        </div>
      </div>
    </div>
    <!--

-->

<p><em><a href="https://github.com/BryanCutler">Bryan Cutler</a> is a software engineer at IBM’s Spark Technology Center <a href="http://www.spark.tc/">STC</a></em></p>

<p>Beginning with <a href="https://spark.apache.org/">Apache Spark</a> version 2.3, <a href="https://arrow.apache.org/">Apache Arrow</a> will be a supported
dependency and begin to offer increased performance with columnar data transfer.
If you are a Spark user that prefers to work in Python and Pandas, this is a cause
to be excited over! The initial work is limited to collecting a Spark DataFrame
with <code class="highlighter-rouge">toPandas()</code>, which I will discuss below, however there are many additional
improvements that are currently <a href="https://issues.apache.org/jira/issues/?filter=12335725&amp;jql=project%20%3D%20SPARK%20AND%20status%20in%20(Open%2C%20%22In%20Progress%22%2C%20Reopened)%20AND%20text%20~%20%22arrow%22%20ORDER%20BY%20createdDate%20DESC">underway</a>.</p>

<h1 id="optimizing-spark-conversion-to-pandas">Optimizing Spark Conversion to Pandas</h1>

<p>The previous way of converting a Spark DataFrame to Pandas with <code class="highlighter-rouge">DataFrame.toPandas()</code>
in PySpark was painfully inefficient. Basically, it worked by first collecting all
rows to the Spark driver. Next, each row would get serialized into Python’s pickle
format and sent to a Python worker process. This child process unpickles each row into
a huge list of tuples. Finally, a Pandas DataFrame is created from the list using
<code class="highlighter-rouge">pandas.DataFrame.from_records()</code>.</p>

<p>This all might seem like standard procedure, but suffers from 2 glaring issues: 1)
even using CPickle, Python serialization is a slow process and 2) creating
a <code class="highlighter-rouge">pandas.DataFrame</code> using <code class="highlighter-rouge">from_records</code> must slowly iterate over the list of pure
Python data and convert each value to Pandas format. See <a href="https://gist.github.com/wesm/0cb5531b1c2e346a0007">here</a> for a detailed
analysis.</p>

<p>Here is where Arrow really shines to help optimize these steps: 1) Once the data is
in Arrow memory format, there is no need to serialize/pickle anymore as Arrow data can
be sent directly to the Python process, 2) When the Arrow data is received in Python,
then pyarrow can utilize zero-copy methods to create a <code class="highlighter-rouge">pandas.DataFrame</code> from entire
chunks of data at once instead of processing individual scalar values. Additionally,
the conversion to Arrow data can be done on the JVM and pushed back for the Spark
executors to perform in parallel, drastically reducing the load on the driver.</p>

<p>As of the merging of <a href="https://issues.apache.org/jira/browse/SPARK-13534">SPARK-13534</a>, the use of Arrow when calling <code class="highlighter-rouge">toPandas()</code>
needs to be enabled by setting the SQLConf “spark.sql.execution.arrow.enable” to
“true”.  Let’s look at a simple usage example.</p>

<div class="highlighter-rouge"><pre class="highlight"><code>Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 2.3.0-SNAPSHOT
      /_/

Using Python version 2.7.13 (default, Dec 20 2016 23:09:15)
SparkSession available as 'spark'.

In [1]: from pyspark.sql.functions import rand
   ...: df = spark.range(1 &lt;&lt; 22).toDF("id").withColumn("x", rand())
   ...: df.printSchema()
   ...: 
root
 |-- id: long (nullable = false)
 |-- x: double (nullable = false)


In [2]: %time pdf = df.toPandas()
CPU times: user 17.4 s, sys: 792 ms, total: 18.1 s
Wall time: 20.7 s

In [3]: spark.conf.set("spark.sql.execution.arrow.enable", "true")

In [4]: %time pdf = df.toPandas()
CPU times: user 40 ms, sys: 32 ms, total: 72 ms                                 
Wall time: 737 ms

In [5]: pdf.describe()
Out[5]: 
                 id             x
count  4.194304e+06  4.194304e+06
mean   2.097152e+06  4.998996e-01
std    1.210791e+06  2.887247e-01
min    0.000000e+00  8.291929e-07
25%    1.048576e+06  2.498116e-01
50%    2.097152e+06  4.999210e-01
75%    3.145727e+06  7.498380e-01
max    4.194303e+06  9.999996e-01
</code></pre>
</div>

<p>This example was run locally on my laptop using Spark defaults so the times
shown should not be taken precisely. Even though, it is clear there is a huge
performance boost and using Arrow took something that was excruciatingly slow
and speeds it up to be barely noticeable.</p>

<h1 id="notes-on-usage">Notes on Usage</h1>

<p>Here are some things to keep in mind before making use of this new feature. At
the time of writing this, pyarrow will not be installed automatically with
pyspark and needs to be manually installed, see installation <a href="https://github.com/apache/arrow/blob/master/site/install.md">instructions</a>.
It is planned to add pyarrow as a pyspark dependency so that 
<code class="highlighter-rouge">&gt; pip install pyspark</code> will also install pyarrow.</p>

<p>Currently, the controlling SQLConf is disabled by default. This can be enabled
programmatically as in the example above or by adding the line
“spark.sql.execution.arrow.enable=true” to <code class="highlighter-rouge">SPARK_HOME/conf/spark-defaults.conf</code>.</p>

<p>Also, not all Spark data types are currently supported and limited to primitive
types. Expanded type support is in the works and expected to also be in the Spark
2.3 release.</p>

<h1 id="future-improvements">Future Improvements</h1>

<p>As mentioned, this was just a first step in using Arrow to make life easier for
Spark Python users. A few exciting initiatives in the works are to allow for
vectorized UDF evaluation (<a href="https://issues.apache.org/jira/browse/SPARK-21190">SPARK-21190</a>, <a href="https://issues.apache.org/jira/browse/SPARK-21404">SPARK-21404</a>), and the ability
to apply a function on grouped data using a Pandas DataFrame (<a href="https://issues.apache.org/jira/browse/SPARK-20396">SPARK-20396</a>).
Just as Arrow helped in converting a Spark to Pandas, it can also work in the
other direction when creating a Spark DataFrame from an existing Pandas
DataFrame (<a href="https://issues.apache.org/jira/browse/SPARK-20791">SPARK-20791</a>). Stay tuned for more!</p>

<h1 id="collaborators">Collaborators</h1>

<p>Reaching this first milestone was a group effort from both the Apache Arrow and
Spark communities. Thanks to the hard work of <a href="https://github.com/wesm">Wes McKinney</a>, <a href="https://github.com/icexelloss">Li Jin</a>,
<a href="https://github.com/holdenk">Holden Karau</a>, Reynold Xin, Wenchen Fan, Shane Knapp and many others that
helped push this effort forwards.</p>


  </div>

  

  
    
  <div class="container">
    <h2>
      Apache Arrow 0.5.0 Release
      <a href="/blog/2017/07/25/0.5.0-release/" class="permalink" title="Permalink">∞</a>
    </h2>

    

    <div class="panel">
      <div class="panel-body">
        <div>
          <span class="label label-default">Published</span>
          <span class="published">
            <i class="fa fa-calendar"></i>
            25 Jul 2017
          </span>
        </div>
        <div>
          <span class="label label-default">By</span>
          <a href="http://wesmckinney.com"><i class="fa fa-user"></i> Wes McKinney (wesm)</a>
        </div>
      </div>
    </div>
    <!--

-->

<p>The Apache Arrow team is pleased to announce the 0.5.0 release. It includes
<a href="https://issues.apache.org/jira/issues/?jql=project%20%3D%20ARROW%20AND%20status%20in%20(Resolved%2C%20Closed)%20AND%20fixVersion%20%3D%200.5.0"><strong>130 resolved JIRAs</strong></a> with some new features, expanded integration
testing between implementations, and bug fixes. The Arrow memory format remains
stable since the 0.3.x and 0.4.x releases.</p>

<p>See the <a href="http://arrow.apache.org/install">Install Page</a> to learn how to get the libraries for your
platform. The <a href="http://arrow.apache.org/release/0.5.0.html">complete changelog</a> is also available.</p>

<h2 id="expanded-integration-testing">Expanded Integration Testing</h2>

<p>In this release, we added compatibility tests for dictionary-encoded data
between Java and C++. This enables the distinct values (the <em>dictionary</em>) in a
vector to be transmitted as part of an Arrow schema while the record batches
contain integers which correspond to the dictionary.</p>

<p>So we might have:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>data (string): ['foo', 'bar', 'foo', 'bar']
</code></pre>
</div>

<p>In dictionary-encoded form, this could be represented as:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>indices (int8): [0, 1, 0, 1]
dictionary (string): ['foo', 'bar']
</code></pre>
</div>

<p>In upcoming releases, we plan to complete integration testing for the remaining
data types (including some more complicated types like unions and decimals) on
the road to a 1.0.0 release in the future.</p>

<h2 id="c-activity">C++ Activity</h2>

<p>We completed a number of significant pieces of work in the C++ part of Apache
Arrow.</p>

<h3 id="using-jemalloc-as-default-memory-allocator">Using jemalloc as default memory allocator</h3>

<p>We decided to use <a href="https://github.com/jemalloc/jemalloc">jemalloc</a> as the default memory allocator unless it is
explicitly disabled. This memory allocator has significant performance
advantages in Arrow workloads over the default <code class="highlighter-rouge">malloc</code> implementation. We will
publish a blog post going into more detail about this and why you might care.</p>

<h3 id="sharing-more-c-code-with-apache-parquet">Sharing more C++ code with Apache Parquet</h3>

<p>We imported the compression library interfaces and dictionary encoding
algorithms from the <a href="http://github.com/apache/parquet-cpp">Apache Parquet C++ library</a>. The Parquet library now
depends on this code in Arrow, and we will be able to use it more easily for
data compression in Arrow use cases.</p>

<p>As part of incorporating Parquet’s dictionary encoding utilities, we have
developed an <code class="highlighter-rouge">arrow::DictionaryBuilder</code> class to enable building
dictionary-encoded arrays iteratively. This can help save memory and yield
better performance when interacting with databases, Parquet files, or other
sources which may have columns having many duplicates.</p>

<h3 id="support-for-lz4-and-zstd-compressors">Support for LZ4 and ZSTD compressors</h3>

<p>We added LZ4 and ZSTD compression library support. In ARROW-300 and other
planned work, we intend to add some compression features for data sent via RPC.</p>

<h2 id="python-activity">Python Activity</h2>

<p>We fixed many bugs which were affecting Parquet and Feather users and fixed
several other rough edges with normal Arrow use. We also added some additional
Arrow type conversions: structs, lists embedded in pandas objects, and Arrow
time types (which deserialize to the <code class="highlighter-rouge">datetime.time</code> type).</p>

<p>In upcoming releases we plan to continue to improve <a href="http://github.com/dask/dask">Dask</a> support and
performance for distributed processing of Apache Parquet files with pyarrow.</p>

<h2 id="the-road-ahead">The Road Ahead</h2>

<p>We have much work ahead of us to build out Arrow integrations in other data
systems to improve their processing performance and interoperability with other
systems.</p>

<p>We are discussing the roadmap to a future 1.0.0 release on the <a href="http://mail-archives.apache.org/mod_mbox/arrow-dev/">developer
mailing list</a>. Please join the discussion there.</p>


  </div>

  

  
    
  <div class="container">
    <h2>
      Connecting Relational Databases to the Apache Arrow World with turbodbc
      <a href="/blog/2017/06/16/turbodbc-arrow/" class="permalink" title="Permalink">∞</a>
    </h2>

    

    <div class="panel">
      <div class="panel-body">
        <div>
          <span class="label label-default">Published</span>
          <span class="published">
            <i class="fa fa-calendar"></i>
            16 Jun 2017
          </span>
        </div>
        <div>
          <span class="label label-default">By</span>
          <a href="http://github.com/MathMagique"><i class="fa fa-user"></i> Michael König (MathMagique)</a>
        </div>
      </div>
    </div>
    <!--

-->

<p><em><a href="https://github.com/mathmagique">Michael König</a> is the lead developer of the <a href="https://github.com/blue-yonder/turbodbc">turbodbc project</a></em></p>

<p>The <a href="https://arrow.apache.org/">Apache Arrow</a> project set out to become the universal data layer for
column-oriented data processing systems without incurring serialization costs
or compromising on performance on a more general level. While relational
databases still lag behind in Apache Arrow adoption, the Python database module
<a href="https://github.com/blue-yonder/turbodbc">turbodbc</a> brings Apache Arrow support to these databases using a much
older, more specialized data exchange layer: <a href="https://en.wikipedia.org/wiki/Open_Database_Connectivity">ODBC</a>.</p>

<p>ODBC is a database interface that offers developers the option to transfer data
either in row-wise or column-wise fashion. Previous Python ODBC modules typically
use the row-wise approach, and often trade repeated database roundtrips for simplified
buffer handling. This makes them less suited for data-intensive applications,
particularly when interfacing with modern columnar analytical databases.</p>

<p>In contrast, turbodbc was designed to leverage columnar data processing from day
one. Naturally, this implies using the columnar portion of the ODBC API. Equally
important, however, is to find new ways of providing columnar data to Python users
that exceed the capabilities of the row-wise API mandated by Python’s <a href="https://www.python.org/dev/peps/pep-0249/">PEP 249</a>.
Turbodbc has adopted Apache Arrow for this very task with the recently released
version 2.0.0:</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="kn">from</span> <span class="nn">turbodbc</span> <span class="kn">import</span> <span class="n">connect</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">connection</span> <span class="o">=</span> <span class="n">connect</span><span class="p">(</span><span class="n">dsn</span><span class="o">=</span><span class="s">"My columnar database"</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">cursor</span> <span class="o">=</span> <span class="n">connection</span><span class="o">.</span><span class="n">cursor</span><span class="p">()</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">cursor</span><span class="o">.</span><span class="n">execute</span><span class="p">(</span><span class="s">"SELECT some_integers, some_strings FROM my_table"</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">cursor</span><span class="o">.</span><span class="n">fetchallarrow</span><span class="p">()</span>
<span class="n">pyarrow</span><span class="o">.</span><span class="n">Table</span>
<span class="n">some_integers</span><span class="p">:</span> <span class="n">int64</span>
<span class="n">some_strings</span><span class="p">:</span> <span class="n">string</span>
</code></pre>
</div>

<p>With this new addition, the data flow for a result set of a typical SELECT query
is like this:</p>
<ul>
  <li>The database prepares the result set and exposes it to the ODBC driver using
either row-wise or column-wise storage.</li>
  <li>Turbodbc has the ODBC driver write chunks of the result set into columnar buffers.</li>
  <li>These buffers are exposed to turbodbc’s Apache Arrow frontend. This frontend
will create an Arrow table and fill in the buffered values.</li>
  <li>The previous steps are repeated until the entire result set is retrieved.</li>
</ul>

<p><img src="/img/turbodbc_arrow.png" alt="Data flow from relational databases to Python with turbodbc and the Apache Arrow frontend" class="img-responsive" width="75%" /></p>

<p>In practice, it is possible to achieve the following ideal situation: A 64-bit integer
column is stored as one contiguous block of memory in a columnar database. A huge chunk
of 64-bit integers is transferred over the network and the ODBC driver directly writes
it to a turbodbc buffer of 64-bit integers. The Arrow frontend accumulates these values
by copying the entire 64-bit buffer into a free portion of an Arrow table’s 64-bit
integer column.</p>

<p>Moving data from the database to an Arrow table and, thus, providing it to the Python
user can be as simple as copying memory blocks around, megabytes equivalent to hundred
thousands of rows at a time. The absence of serialization and conversion logic renders
the process extremely efficient.</p>

<p>Once the data is stored in an Arrow table, Python users can continue to do some
actual work. They can convert it into a <a href="https://arrow.apache.org/docs/python/pandas.html">Pandas DataFrame</a> for data analysis
(using a quick <code class="highlighter-rouge">table.to_pandas()</code>), pass it on to other data processing
systems such as <a href="http://spark.apache.org/">Apache Spark</a> or <a href="http://impala.apache.org/">Apache Impala (incubating)</a>, or store
it in the <a href="http://parquet.apache.org/">Apache Parquet</a> file format. This way, non-Python systems are
efficiently connected with relational databases.</p>

<p>In the future, turbodbc’s Arrow support will be extended to use more
sophisticated features such as <a href="https://arrow.apache.org/docs/memory_layout.html#dictionary-encoding">dictionary-encoded</a> string fields. We also
plan to pick smaller than 64-bit <a href="https://arrow.apache.org/docs/metadata.html#integers">data types</a> where possible. Last but not
least, Arrow support will be extended to cover the reverse direction of data
flow, so that Python users can quickly insert Arrow tables into relational
databases.</p>

<p>If you would like to learn more about turbodbc, check out the <a href="https://github.com/blue-yonder/turbodbc">GitHub project</a> and the
<a href="http://turbodbc.readthedocs.io/">project documentation</a>. If you want to learn more about how turbodbc implements the
nitty-gritty details, check out parts <a href="https://tech.blue-yonder.com/making-of-turbodbc-part-1-wrestling-with-the-side-effects-of-a-c-api/">one</a> and <a href="https://tech.blue-yonder.com/making-of-turbodbc-part-2-c-to-python/">two</a> of the
<a href="https://tech.blue-yonder.com/making-of-turbodbc-part-1-wrestling-with-the-side-effects-of-a-c-api/">“Making of turbodbc”</a> series at <a href="https://tech.blue-yonder.com/">Blue Yonder’s technology blog</a>.</p>


  </div>

  

  
    
  <div class="container">
    <h2>
      Apache Arrow 0.4.1 Release
      <a href="/blog/2017/06/14/0.4.1-release/" class="permalink" title="Permalink">∞</a>
    </h2>

    

    <div class="panel">
      <div class="panel-body">
        <div>
          <span class="label label-default">Published</span>
          <span class="published">
            <i class="fa fa-calendar"></i>
            14 Jun 2017
          </span>
        </div>
        <div>
          <span class="label label-default">By</span>
          <a href="http://wesmckinney.com"><i class="fa fa-user"></i> Wes McKinney (wesm)</a>
        </div>
      </div>
    </div>
    <!--

-->

<p>The Apache Arrow team is pleased to announce the 0.4.1 release of the
project. This is a bug fix release that addresses a regression with Decimal
types in the Java implementation introduced in 0.4.0 (see
<a href="https://issues.apache.org/jira/browse/ARROW-1091">ARROW-1091</a>). There were a total of <a href="https://issues.apache.org/jira/issues/?jql=project%20%3D%20ARROW%20AND%20status%20in%20(Resolved%2C%20Closed)%20AND%20fixVersion%20%3D%200.4.1">31 resolved JIRAs</a>.</p>

<p>See the <a href="http://arrow.apache.org/install">Install Page</a> to learn how to get the libraries for your platform.</p>

<h3 id="python-wheel-installers-for-windows">Python Wheel Installers for Windows</h3>

<p>Max Risuhin contributed fixes to enable binary wheel installers to be generated
for Python 3.5 and 3.6. Thus, 0.4.1 is the first Arrow release for which
PyArrow including bundled <a href="http://parquet.apache.org">Apache Parquet</a> support that can be installed
with either conda or pip across the 3 major platforms: Linux, macOS, and
Windows. Use one of:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>pip install pyarrow
conda install pyarrow -c conda-forge
</code></pre>
</div>

<h3 id="turbodbc-200-with-apache-arrow-support">Turbodbc 2.0.0 with Apache Arrow Support</h3>

<p><a href="http://turbodbc.readthedocs.io/">Turbodbc</a>, a fast C++ ODBC interface with Python bindings, released
version 2.0.0 including reading SQL result sets as Arrow record batches. The
team used the PyArrow C++ API introduced in version 0.4.0 to construct
<code class="highlighter-rouge">pyarrow.Table</code> objects inside the <code class="highlighter-rouge">turbodbc</code> library. Learn more in their
<a href="http://turbodbc.readthedocs.io/en/latest/pages/advanced_usage.html#apache-arrow-support">documentation</a> and install with one of:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>pip install turbodbc
conda install turbodbc -c conda-forge
</code></pre>
</div>


  </div>

  

  
    
  <div class="container">
    <h2>
      Apache Arrow 0.4.0 Release
      <a href="/blog/2017/05/23/0.4.0-release/" class="permalink" title="Permalink">∞</a>
    </h2>

    

    <div class="panel">
      <div class="panel-body">
        <div>
          <span class="label label-default">Published</span>
          <span class="published">
            <i class="fa fa-calendar"></i>
            23 May 2017
          </span>
        </div>
        <div>
          <span class="label label-default">By</span>
          <a href="http://wesmckinney.com"><i class="fa fa-user"></i> Wes McKinney (wesm)</a>
        </div>
      </div>
    </div>
    <!--

-->

<p>The Apache Arrow team is pleased to announce the 0.4.0 release of the
project. While only 17 days since the release, it includes <a href="https://issues.apache.org/jira/issues/?jql=project%20%3D%20ARROW%20AND%20status%20in%20(Resolved%2C%20Closed)%20AND%20fixVersion%20%3D%200.4.0"><strong>77 resolved
JIRAs</strong></a> with some important new features and bug fixes.</p>

<p>See the <a href="http://arrow.apache.org/install">Install Page</a> to learn how to get the libraries for your platform.</p>

<h3 id="expanded-javascript-implementation">Expanded JavaScript Implementation</h3>

<p>The TypeScript Arrow implementation has undergone some work since 0.3.0 and can
now read a substantial portion of the Arrow streaming binary format. As this
implementation develops, we will eventually want to include JS in the
integration test suite along with Java and C++ to ensure wire
cross-compatibility.</p>

<h3 id="python-support-for-apache-parquet-on-windows">Python Support for Apache Parquet on Windows</h3>

<p>With the <a href="https://github.com/apache/parquet-cpp/releases/tag/apache-parquet-cpp-1.1.0">1.1.0 C++ release</a> of <a href="http://parquet.apache.org">Apache Parquet</a>, we have enabled the
<code class="highlighter-rouge">pyarrow.parquet</code> extension on Windows for Python 3.5 and 3.6. This should
appear in conda-forge packages and PyPI in the near future. Developers can
follow the <a href="http://arrow.apache.org/docs/python/development.html">source build instructions</a>.</p>

<h3 id="generalizing-arrow-streams">Generalizing Arrow Streams</h3>

<p>In the 0.2.0 release, we defined the first version of the Arrow streaming
binary format for low-cost messaging with columnar data. These streams presume
that the message components are written as a continuous byte stream over a
socket or file.</p>

<p>We would like to be able to support other other transport protocols, like
<a href="http://grpc.io/">gRPC</a>, for the message components of Arrow streams. To that end, in C++ we
defined an abstract stream reader interface, for which the current contiguous
streaming format is one implementation:</p>

<figure class="highlight"><pre><code class="language-cpp" data-lang="cpp"><span class="k">class</span> <span class="nc">RecordBatchReader</span> <span class="p">{</span>
 <span class="k">public</span><span class="o">:</span>
  <span class="k">virtual</span> <span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">Schema</span><span class="o">&gt;</span> <span class="n">schema</span><span class="p">()</span> <span class="k">const</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
  <span class="k">virtual</span> <span class="n">Status</span> <span class="n">GetNextRecordBatch</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">RecordBatch</span><span class="o">&gt;*</span> <span class="n">batch</span><span class="p">)</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">};</span></code></pre></figure>

<p>It would also be good to define abstract stream reader and writer interfaces in
the Java implementation.</p>

<p>In an upcoming blog post, we will explain in more depth how Arrow streams work,
but you can learn more about them by reading the <a href="http://arrow.apache.org/docs/ipc.html">IPC specification</a>.</p>

<h3 id="c-and-cython-api-for-python-extensions">C++ and Cython API for Python Extensions</h3>

<p>As other Python libraries with C or C++ extensions use Apache Arrow, they will
need to be able to return Python objects wrapping the underlying C++
objects. In this release, we have implemented a prototype C++ API which enables
Python wrapper objects to be constructed from C++ extension code:</p>

<figure class="highlight"><pre><code class="language-cpp" data-lang="cpp"><span class="cp">#include "arrow/python/pyarrow.h"
</span>
<span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">arrow</span><span class="o">::</span><span class="n">py</span><span class="o">::</span><span class="n">import_pyarrow</span><span class="p">())</span> <span class="p">{</span>
  <span class="c1">// Error
</span><span class="p">}</span>

<span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">arrow</span><span class="o">::</span><span class="n">RecordBatch</span><span class="o">&gt;</span> <span class="n">cpp_batch</span> <span class="o">=</span> <span class="n">GetData</span><span class="p">(...);</span>
<span class="n">PyObject</span><span class="o">*</span> <span class="n">py_batch</span> <span class="o">=</span> <span class="n">arrow</span><span class="o">::</span><span class="n">py</span><span class="o">::</span><span class="n">wrap_batch</span><span class="p">(</span><span class="n">cpp_batch</span><span class="p">);</span></code></pre></figure>

<p>This API is intended to be usable from Cython code as well:</p>

<figure class="highlight"><pre><code class="language-cython" data-lang="cython">cimport pyarrow
pyarrow.import_pyarrow()</code></pre></figure>

<h3 id="python-wheel-installers-on-macos">Python Wheel Installers on macOS</h3>

<p>With this release, <code class="highlighter-rouge">pip install pyarrow</code> works on macOS (OS X) as well as
Linux. We are working on providing binary wheel installers for Windows as well.</p>


  </div>

  

  
    
  <div class="container">
    <h2>
      Apache Arrow 0.3.0 Release
      <a href="/blog/2017/05/08/0.3-release/" class="permalink" title="Permalink">∞</a>
    </h2>

    

    <div class="panel">
      <div class="panel-body">
        <div>
          <span class="label label-default">Published</span>
          <span class="published">
            <i class="fa fa-calendar"></i>
            08 May 2017
          </span>
        </div>
        <div>
          <span class="label label-default">By</span>
          <a href="http://wesmckinney.com"><i class="fa fa-user"></i> Wes McKinney (wesm)</a>
        </div>
      </div>
    </div>
    <!--

-->

<p>Translations: <a href="/blog/2017/05/08/0.3-release-japanese/">日本語</a></p>

<p>The Apache Arrow team is pleased to announce the 0.3.0 release of the
project. It is the product of an intense 10 weeks of development since the
0.2.0 release from this past February. It includes <a href="https://issues.apache.org/jira/issues/?jql=project%20%3D%20ARROW%20AND%20status%20in%20(Resolved%2C%20Closed)%20AND%20fixVersion%20%3D%200.3.0"><strong>306 resolved JIRAs</strong></a>
from <a href="https://github.com/apache/arrow/graphs/contributors"><strong>23 contributors</strong></a>.</p>

<p>While we have added many new features to the different Arrow implementations,
one of the major development focuses in 2017 has been hardening the in-memory
format, type metadata, and messaging protocol to provide a <strong>stable,
production-ready foundation</strong> for big data applications. We are excited to be
collaborating with the <a href="http://spark.apache.org">Apache Spark</a> and <a href="http://www.geomesa.org/">GeoMesa</a> communities on
utilizing Arrow for high performance IO and in-memory data processing.</p>

<p>See the <a href="http://arrow.apache.org/install">Install Page</a> to learn how to get the libraries for your platform.</p>

<p>We will be publishing more information about the Apache Arrow roadmap as we
forge ahead with using Arrow to accelerate big data systems.</p>

<p>We are looking for more contributors from within our existing communities and
from other communities (such as Go, R, or Julia) to get involved in Arrow
development.</p>

<h3 id="file-and-streaming-format-hardening">File and Streaming Format Hardening</h3>

<p>The 0.2.0 release brought with it the first iterations of the <strong>random access</strong>
and <strong>streaming</strong> Arrow wire formats. See the <a href="http://arrow.apache.org/docs/ipc.html">IPC specification</a> for
implementation details and <a href="http://wesmckinney.com/blog/arrow-streaming-columnar/">example blog post</a> with some use cases. These
provide low-overhead, zero-copy access to Arrow record batch payloads.</p>

<p>In 0.3.0 we have solidified a number of small details with the binary format
and improved our integration and unit testing particularly in the Java, C++,
and Python libraries. Using the <a href="http://github.com/google/flatbuffers">Google Flatbuffers</a> project has helped with
adding new features to our metadata without breaking forward compatibility.</p>

<p>We are not yet ready to make a firm commitment to strong forward compatibility
(in case we find something needs to change) in the binary format, but we will
make efforts between major releases to not make unnecessary
breakages. Contributions to the website and component user and API
documentation would also be most welcome.</p>

<h3 id="dictionary-encoding-support">Dictionary Encoding Support</h3>

<p><a href="https://github.com/elahrvivaz">Emilio Lahr-Vivaz</a> from the <a href="http://www.geomesa.org/">GeoMesa</a> project contributed Java support
for dictionary-encoded Arrow vectors. We followed up with C++ and Python
support (and <code class="highlighter-rouge">pandas.Categorical</code> integration). We have not yet implemented
full integration tests for dictionaries (for sending this data between C++ and
Java), but hope to achieve this in the 0.4.0 Arrow release.</p>

<p>This common data representation technique for categorical data allows multiple
record batches to share a common “dictionary”, with the values in the batches
being represented as integers referencing the dictionary. This data is called
“categorical” or “factor” in statistical languages, while in file formats like
Apache Parquet it is strictly used for data compression.</p>

<h3 id="expanded-date-time-and-fixed-size-types">Expanded Date, Time, and Fixed Size Types</h3>

<p>A notable omission from the 0.2.0 release was complete and integration-tested
support for the gamut of date and time types that occur in the wild. These are
needed for <a href="http://parquet.apache.org">Apache Parquet</a> and Apache Spark integration.</p>

<ul>
  <li><strong>Date</strong>: 32-bit (days unit) and 64-bit (milliseconds unit)</li>
  <li><strong>Time</strong>: 64-bit integer with unit (second, millisecond, microsecond, nanosecond)</li>
  <li><strong>Timestamp</strong>: 64-bit integer with unit, with or without timezone</li>
  <li><strong>Fixed Size Binary</strong>: Primitive values occupying certain number of bytes</li>
  <li><strong>Fixed Size List</strong>: List values with constant size (no separate offsets vector)</li>
</ul>

<p>We have additionally added experimental support for exact decimals in C++ using
<a href="https://github.com/boostorg/multiprecision">Boost.Multiprecision</a>, though we have not yet hardened the Decimal memory
format between the Java and C++ implementations.</p>

<h3 id="c-and-python-support-on-windows">C++ and Python Support on Windows</h3>

<p>We have made many general improvements to development and packaging for general
C++ and Python development. 0.3.0 is the first release to bring full C++ and
Python support for Windows on Visual Studio (MSVC) 2015 and 2017. In addition
to adding Appveyor continuous integration for MSVC, we have also written guides
for building from source on Windows: <a href="https://github.com/apache/arrow/blob/master/cpp/apidoc/Windows.md">C++</a> and <a href="https://github.com/apache/arrow/blob/master/python/doc/source/development.rst">Python</a>.</p>

<p>For the first time, you can install the Arrow Python library on Windows from
<a href="https://conda-forge.github.io">conda-forge</a>:</p>

<div class="language-shell highlighter-rouge"><pre class="highlight"><code>conda install pyarrow -c conda-forge
</code></pre>
</div>

<h3 id="c-glib-bindings-with-support-for-ruby-lua-and-more">C (GLib) Bindings, with support for Ruby, Lua, and more</h3>

<p><a href="http://github.com/kou">Kouhei Sutou</a> is a new Apache Arrow contributor and has contributed GLib C
bindings (to the C++ libraries) for Linux. Using a C middleware framework
called <a href="https://wiki.gnome.org/Projects/GObjectIntrospection">GObject Introspection</a>, it is possible to use these bindings
seamlessly in Ruby, Lua, Go, and <a href="https://wiki.gnome.org/Projects/GObjectIntrospection/Users">other programming languages</a>. We will
probably need to publish some follow up blogs explaining how these bindings
work and how to use them.</p>

<h3 id="apache-spark-integration-for-pyspark">Apache Spark Integration for PySpark</h3>

<p>We have been collaborating with the Apache Spark community on <a href="https://issues.apache.org/jira/browse/SPARK-13534">SPARK-13534</a>
to add support for using Arrow to accelerate <code class="highlighter-rouge">DataFrame.toPandas</code> in
PySpark. We have observed over <a href="https://github.com/apache/spark/pull/15821#issuecomment-282175163"><strong>40x speedup</strong></a> from the more efficient
data serialization.</p>

<p>Using Arrow in PySpark opens the door to many other performance optimizations,
particularly around UDF evaluation (e.g. <code class="highlighter-rouge">map</code> and <code class="highlighter-rouge">filter</code> operations with
Python lambda functions).</p>

<h3 id="new-python-feature-memory-views-feather-apache-parquet-support">New Python Feature: Memory Views, Feather, Apache Parquet support</h3>

<p>Arrow’s Python library <code class="highlighter-rouge">pyarrow</code> is a Cython binding for the <code class="highlighter-rouge">libarrow</code> and
<code class="highlighter-rouge">libarrow_python</code> C++ libraries, which handle inteoperability with NumPy,
<a href="http://pandas.pydata.org">pandas</a>, and the Python standard library.</p>

<p>At the heart of Arrow’s C++ libraries is the <code class="highlighter-rouge">arrow::Buffer</code> object, which is a
managed memory view supporting zero-copy reads and slices. <a href="https://github.com/JeffKnupp">Jeff Knupp</a>
contributed integration between Arrow buffers and the Python buffer protocol
and memoryviews, so now code like this is possible:</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">In</span> <span class="p">[</span><span class="mi">6</span><span class="p">]:</span> <span class="kn">import</span> <span class="nn">pyarrow</span> <span class="kn">as</span> <span class="nn">pa</span>

<span class="n">In</span> <span class="p">[</span><span class="mi">7</span><span class="p">]:</span> <span class="n">buf</span> <span class="o">=</span> <span class="n">pa</span><span class="o">.</span><span class="n">frombuffer</span><span class="p">(</span><span class="n">b</span><span class="s">'foobarbaz'</span><span class="p">)</span>

<span class="n">In</span> <span class="p">[</span><span class="mi">8</span><span class="p">]:</span> <span class="n">buf</span>
<span class="n">Out</span><span class="p">[</span><span class="mi">8</span><span class="p">]:</span> <span class="o">&lt;</span><span class="n">pyarrow</span><span class="o">.</span><span class="n">_io</span><span class="o">.</span><span class="n">Buffer</span> <span class="n">at</span> <span class="mh">0x7f6c0a84b538</span><span class="o">&gt;</span>

<span class="n">In</span> <span class="p">[</span><span class="mi">9</span><span class="p">]:</span> <span class="n">memoryview</span><span class="p">(</span><span class="n">buf</span><span class="p">)</span>
<span class="n">Out</span><span class="p">[</span><span class="mi">9</span><span class="p">]:</span> <span class="o">&lt;</span><span class="n">memory</span> <span class="n">at</span> <span class="mh">0x7f6c0a8c5e88</span><span class="o">&gt;</span>

<span class="n">In</span> <span class="p">[</span><span class="mi">10</span><span class="p">]:</span> <span class="n">buf</span><span class="o">.</span><span class="n">to_pybytes</span><span class="p">()</span>
<span class="n">Out</span><span class="p">[</span><span class="mi">10</span><span class="p">]:</span> <span class="n">b</span><span class="s">'foobarbaz'</span>
</code></pre>
</div>

<p>We have significantly expanded <a href="http://parquet.apache.org"><strong>Apache Parquet</strong></a> support via the C++
Parquet implementation <a href="https://github.com/apache/parquet-cpp">parquet-cpp</a>. This includes support for partitioned
datasets on disk or in HDFS. We added initial Arrow-powered Parquet support <a href="https://github.com/dask/dask/commit/68f9e417924a985c1f2e2a587126833c70a2e9f4">in
the Dask project</a>, and look forward to more collaborations with the Dask
developers on distributed processing of pandas data.</p>

<p>With Arrow’s support for pandas maturing, we were able to merge in the
<a href="https://github.com/wesm/feather"><strong>Feather format</strong></a> implementation, which is essentially a special case of
the Arrow random access format. We’ll be continuing Feather development within
the Arrow codebase. For example, Feather can now read and write with Python
file objects using Arrow’s Python binding layer.</p>

<p>We also implemented more robust support for pandas-specific data types, like
<code class="highlighter-rouge">DatetimeTZ</code> and <code class="highlighter-rouge">Categorical</code>.</p>

<h3 id="support-for-tensors-and-beyond-in-c-library">Support for Tensors and beyond in C++ Library</h3>

<p>There has been increased interest in using Apache Arrow as a tool for zero-copy
shared memory management for machine learning applications. A flagship example
is the <a href="https://github.com/ray-project/ray">Ray project</a> from the UC Berkeley <a href="https://rise.cs.berkeley.edu/">RISELab</a>.</p>

<p>Machine learning deals in additional kinds of data structures beyond what the
Arrow columnar format supports, like multidimensional arrays aka “tensors”. As
such, we implemented the <a href="http://arrow.apache.org/docs/cpp/classarrow_1_1_tensor.html"><code class="highlighter-rouge">arrow::Tensor</code></a> C++ type which can utilize the
rest of Arrow’s zero-copy shared memory machinery (using <code class="highlighter-rouge">arrow::Buffer</code> for
managing memory lifetime). In C++ in particular, we will want to provide for
additional data structures utilizing common IO and memory management tools.</p>

<h3 id="start-of-javascript-typescript-implementation">Start of JavaScript (TypeScript) Implementation</h3>

<p><a href="https://github.com/TheNeuralBit">Brian Hulette</a> started developing an Arrow implementation in
<a href="https://github.com/apache/arrow/tree/master/js">TypeScript</a> for use in NodeJS and browser-side applications. We are
benefitting from Flatbuffers’ first class support for JavaScript.</p>

<h3 id="improved-website-and-developer-documentation">Improved Website and Developer Documentation</h3>

<p>Since 0.2.0 we have implemented a new website stack for publishing
documentation and blogs based on <a href="https://jekyllrb.com">Jekyll</a>. Kouhei Sutou developed a <a href="https://github.com/red-data-tools/jekyll-jupyter-notebook">Jekyll
Jupyter Notebook plugin</a> so that we can use Jupyter to author content for
the Arrow website.</p>

<p>On the website, we have now published API documentation for the C, C++, Java,
and Python subcomponents. Within these you will find easier-to-follow developer
instructions for getting started.</p>

<h3 id="contributors">Contributors</h3>

<p>Thanks to all who contributed patches to this release.</p>

<div class="highlighter-rouge"><pre class="highlight"><code>$ git shortlog -sn apache-arrow-0.2.0..apache-arrow-0.3.0
    119 Wes McKinney
     55 Kouhei Sutou
     18 Uwe L. Korn
     17 Julien Le Dem
      9 Phillip Cloud
      6 Bryan Cutler
      5 Philipp Moritz
      5 Emilio Lahr-Vivaz
      4 Max Risuhin
      4 Johan Mabille
      4 Jeff Knupp
      3 Steven Phillips
      3 Miki Tebeka
      2 Leif Walsh
      2 Jeff Reback
      2 Brian Hulette
      1 Tsuyoshi Ozawa
      1 rvernica
      1 Nong Li
      1 Julien Lafaye
      1 Itai Incze
      1 Holden Karau
      1 Deepak Majeti
</code></pre>
</div>


  </div>

  

  



    <hr/>
<footer class="footer">
  <p>Apache Arrow, Arrow, Apache, the Apache feather logo, and the Apache Arrow project logo are either registered trademarks or trademarks of The Apache Software Foundation in the United States and other countries.</p>
  <p>&copy; 2017 Apache Software Foundation</p>
</footer>

  </div>
</body>
</html>
